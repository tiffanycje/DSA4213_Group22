1	29	Over the last few decades, causal models have found use in computational advertising (Bottou et al., 2013), biological systems (Meinshausen et al., 2016), sociology (Blalock, 1985), agriculture (Splawa-Neyman et al., 1990) and epidemiology (Joffe et al., 2012).
6	26	Our focus is on optimizing over a given set of interventions.
23	32	In the computational advertising example, the interventions correspond to changing the click through rate scoring algorithm (i.e P(click through rate|ads chosen, user query), whose input-output characteristics are well-studied.
40	16	We discuss in more detail in Sections 3.3, about how these guarantees can be exponentially better than the classical ones.
72	21	During a soft intervention k 2 [K] ([K] = {0, 1, ..., K 1}), the conditional distribution of V given its parents is set to Pk(V |pa(V )) and all other relationships in the causal graph are unchanged.
79	32	In this setting, we are interested in the following natural question: Which of the K soft interventions yield the highest expected value of the target (E[Y ]) and what is the misidentification error that can be achieved with a finite total budget T for samples ?
94	25	Therefore, we introduce two variants of an additional cost constraint that influences the choice of interventions.
97	20	We require that the total fraction of times the difficult arms are played does not exceed B i.e. P k2B ⌫k  B.
106	47	Then the probability of error e(T, B) (Audibert & Bubeck, 2010; Carpentier & Locatelli, 2016) is given by, e(T, B) = P ⇣ k̂(T, B) 6= k⇤ ⌘ (Simple Regret): Another important quantity that has been analyzed in the best arm identification setting is the simple regret (Lattimore et al., 2016).
112	18	Quantifying Information Leakage: Observe that, µk = Ek[Y ] = Ek0 h Y Pk(V |pa(V )) Pk0 (V |pa(V )) i .
119	19	Recall that Pi is the conditional distribution of node V given the state of its parents pa(V ).
125	17	This estimator adaptively weights samples depending on the relative Mij measures, and also uses clipping to control variance by introducing bias.
128	33	Let the total number of samples from all arms be denoted by ⌧ .
129	16	Further, let us index all the samples by s 2 {1, 2, .., ⌧}, and Tk ⇢ {1, 2, .., ⌧} be the indices of all the samples collected from arm k. Let Xj(s) denotes the sample collected for random variable X under intervention j, at time instant s. Finally, let Zk = P j2[K] ⌧j/Mkj .
130	23	We denote the estimate of µk by Ŷ ✏k (✏ is an indicator of the level of confidence desired).
136	80	Algorithm 1 starts by having all the K arms under consideration and then proceeds in phases, possibly rejecting one or more arms at the end of each phase.
137	21	At every phase, Estimator (1) with a phase specific choice of the ✏ parameter (i.e. controlling bias variance trade-off), is applied to all arms under consideration.
138	24	Using a phase specific threshold on these estimates, some arms are rejected at the end of each phase.
144	20	Let ⌧(`) be the total number of samples in phase `.
145	21	Let R be the set of arms remaining to compete with the optimal arm at the beginning of phase ` which is continuously updated.
150	29	This allocation depends on the average budget constraints and the relative log divergences between the arms (Definition 2).
157	18	Note that Line 6 uses only the samples acquired in that phase.
169	31	This is the first gap dependent characterization.
176	17	In Section D.1 (in the appendix), we demonstrate through simple examples that ⇤(B, R⇤( k)) can be significantly smaller than q R̃( k) (the corresponding term in e(T ) above) even when there are no average budget constraints.
221	30	To address interpretability, we segment the image into a number of superpixels/segments (using segmentation algorithms like SLIC (Achanta et al., 2010)) and infer which superpixels encourage the neural net to output a certain label (henceforth referred to as labelI; e.g ’drum’) in top-k (e.g. k = 10), and to what extent.
234	19	We are only allowed to sample using a different set of 200 arms that are dense distributions chosen uniformly at random from the 43-dimensional simplex.
240	17	This is an extreme special case of budget setting S2.
241	18	We see that our algorithm can generate meaningful interpretations for all the labels with relatively less number of runs of Inception.
242	38	Even sampling 10 times from each of the arms to be optimized over would require 30, 000 runs of Inception for a single image and label, while we use only 2500 runs by leveraging information leakage.
244	38	These bounds are a generalization of the classical best arm identification bounds (Audibert & Bubeck, 2010), when there is information leakage among the arms.
