{"id": "58600020", "bin": "0_100", "summary_sentences": ["This work deals with rotation equivariant convolutional filters.", "The idea is that when you rotate an image you should not need to relearn new filters to deal with this rotation.", "First we can look at how convolutions typically handle rotation and how we would expect a rotation invariant solution to perform below:  | | | | - | - | |  [url]"], "summary_text": "This work deals with rotation equivariant convolutional filters. The idea is that when you rotate an image you should not need to relearn new filters to deal with this rotation. First we can look at how convolutions typically handle rotation and how we would expect a rotation invariant solution to perform below:  | | | | - | - | |  [url]", "pdf_url": "http://arxiv.org/pdf/1612.09346v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/1612.09346.json"}
{"id": "70719243", "bin": "0_100", "summary_sentences": ["\"The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data.\""], "summary_text": "\"The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data.\"", "pdf_url": "http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/guyone03.json"}
{"id": "12642073", "bin": "1000_1100", "summary_sentences": ["The Network is Reliable – Bailis and Kingsbury 2014  This must be the easiest paper summary to write of the series so far.", "The network is reliable?", "Oh no it isn’t…  OK, here’s a little more detail :)  Network reliability matters because it prevents us from having reliable communication, and that in turn makes building distributed systems really hard.", "(Fallacy #1 in Peter Deutsch’s ‘ Eight fallacies of distributed computing ‘ is ‘The network is reliable’).", "In fact, if we look at this list for a moment we can see that the top three fallacies all correspond to interesting failure modes in networks as well as being something you have to take into account in the happy paths.", "Sudden large latency spikes for example can be very disruptive to fault detectors, and noisy network neighbours might consume lots of your bandwidth.", "The network is reliable.", "Latency is zero.", "Bandwidth is infinite.", "The network is secure.", "Topology doesn’t change.", "There is one administrator.", "Transport cost is zero.", "The network is homogeneous.", "No.", "5 is also something we’ve had to learn to accommodate to a whole new degree with cloud deployments.", "Topology changing all the time is the norm.", "If we can assume that ‘the network is reliable enough,’ then it might make sense as an engineering trade-off to become unavailable in the event of a partition.", "I’ve heard the argument made that ‘partitions only happen in the cloud,’ we never really see them in practice in our own data centers.", "Conscious trade-off or not, the work of one of the authors of this paper, Kyle Kingsbury, on the wonderful Jepsen Reports shows that many real-world systems struggle with partitions.", "The degree of reliability in deployment environments is critical in robust systems design and directly determines the kinds of operations that systems can reliably perform without waiting.", "Unfortunately, the degree to which networks are actually reliable in the real world is the subject of considerable and evolving debate.", "Some people have claimed that networks are reliable (or that partitions are rare enough in practice) and that we are too concerned with designing for theoretical failure modes.", "Conversely, others attest that partitions do occur in their deployments…  We have some pretty good statistics on disk, host, and rack failure rates, but not so much on network failures.", "Yet network failures can be much more disruptive than a disk failure for example.", "As a result, much of what we believe about the failure modes of real-world distributed systems is founded on guesswork and rumor.", "Sysadmins and developers will swap stories over beer, but detailed, public postmortems and comprehensive surveys of network availability are few and far between.", "In this article, we’d like to informally bring a few of these stories (which, in most cases, are unabashedly anecdotal) together…  There follows a long collection of stories of every kind of network failure you can imagine.", "It’s the cumulative effect that gets you as you read through – start off on page 2 thinking ‘yeah ok, isolated scenario(s)’, but the stories keep coming and coming and coming.", "By the time you get to page 12, you’ve probably come to the conclusion that Murphy must have been working as a network engineer at the time he formulated his famous law!", "Split-brains, partitions, device failures, link failures, high rates of packet loss, maintenance and admin issues, and more are all in here – resulting in comedies of errors in the systems built on top of them that would be fit for a christmas pantomine if the consequences weren’t so severe!", "This reads like a skit from a comedy show:  This 90-second network partition caused file servers using Pacemaker and DRBD (Distributed Replicated Block Device) for HA (high availability) failover to declare each other dead, and to issue STONITH (shoot the other node in the head) messages to one another.", "The network partition delayed delivery of those messages, causing some file-server pairs to believe they were both active.", "When the network recovered, both nodes shot each other at the same time.", "With both nodes dead, files belonging to the pair were unavailable.", "You really do need to read through the paper to get the overall impression, a summary cannot do it justice.", "As a consequence of all this:  Split-brain is not an academic concern: it happens to all kinds of systems—sometimes for days on end.", "Partitions deserve serious consideration… It’s important to consider this risk before a partition occurs, because it’s much easier to make decisions about partition behavior on a whiteboard than to redesign, reengineer, and upgrade a complex system in a production environment—especially when it’s throwing errors at your users.", "For some applications, failure is an option—but you should characterize and explicitly account for it as a part of your design.", "Finally, given the additional latency and coordination benefits of partition-aware designs, you might just find that accounting for these partitions delivers benefits in the average case as well.", "The authors also acknowledge that there might be reliable networks out there:  On the other hand, some networks really are reliable.", "Engineers at major financial firms have anecdotally reported that despite putting serious effort into designing systems that gracefully tolerate partitions, their networks rarely, if ever, exhibit partition behavior.", "Cautious engineering and aggressive network advances (along with lots of money) can prevent outages.", "Moreover, in this article, we have presented failure scenarios; we acknowledge it’s much harder to demonstrate that network failures have not occurred.", "But if I was thinking about future architectures and cloud deployments, I wouldn’t want to rely on it ;)."], "summary_text": "The Network is Reliable – Bailis and Kingsbury 2014  This must be the easiest paper summary to write of the series so far. The network is reliable? Oh no it isn’t…  OK, here’s a little more detail :)  Network reliability matters because it prevents us from having reliable communication, and that in turn makes building distributed systems really hard. (Fallacy #1 in Peter Deutsch’s ‘ Eight fallacies of distributed computing ‘ is ‘The network is reliable’). In fact, if we look at this list for a moment we can see that the top three fallacies all correspond to interesting failure modes in networks as well as being something you have to take into account in the happy paths. Sudden large latency spikes for example can be very disruptive to fault detectors, and noisy network neighbours might consume lots of your bandwidth. The network is reliable. Latency is zero. Bandwidth is infinite. The network is secure. Topology doesn’t change. There is one administrator. Transport cost is zero. The network is homogeneous. No. 5 is also something we’ve had to learn to accommodate to a whole new degree with cloud deployments. Topology changing all the time is the norm. If we can assume that ‘the network is reliable enough,’ then it might make sense as an engineering trade-off to become unavailable in the event of a partition. I’ve heard the argument made that ‘partitions only happen in the cloud,’ we never really see them in practice in our own data centers. Conscious trade-off or not, the work of one of the authors of this paper, Kyle Kingsbury, on the wonderful Jepsen Reports shows that many real-world systems struggle with partitions. The degree of reliability in deployment environments is critical in robust systems design and directly determines the kinds of operations that systems can reliably perform without waiting. Unfortunately, the degree to which networks are actually reliable in the real world is the subject of considerable and evolving debate. Some people have claimed that networks are reliable (or that partitions are rare enough in practice) and that we are too concerned with designing for theoretical failure modes. Conversely, others attest that partitions do occur in their deployments…  We have some pretty good statistics on disk, host, and rack failure rates, but not so much on network failures. Yet network failures can be much more disruptive than a disk failure for example. As a result, much of what we believe about the failure modes of real-world distributed systems is founded on guesswork and rumor. Sysadmins and developers will swap stories over beer, but detailed, public postmortems and comprehensive surveys of network availability are few and far between. In this article, we’d like to informally bring a few of these stories (which, in most cases, are unabashedly anecdotal) together…  There follows a long collection of stories of every kind of network failure you can imagine. It’s the cumulative effect that gets you as you read through – start off on page 2 thinking ‘yeah ok, isolated scenario(s)’, but the stories keep coming and coming and coming. By the time you get to page 12, you’ve probably come to the conclusion that Murphy must have been working as a network engineer at the time he formulated his famous law! Split-brains, partitions, device failures, link failures, high rates of packet loss, maintenance and admin issues, and more are all in here – resulting in comedies of errors in the systems built on top of them that would be fit for a christmas pantomine if the consequences weren’t so severe! This reads like a skit from a comedy show:  This 90-second network partition caused file servers using Pacemaker and DRBD (Distributed Replicated Block Device) for HA (high availability) failover to declare each other dead, and to issue STONITH (shoot the other node in the head) messages to one another. The network partition delayed delivery of those messages, causing some file-server pairs to believe they were both active. When the network recovered, both nodes shot each other at the same time. With both nodes dead, files belonging to the pair were unavailable. You really do need to read through the paper to get the overall impression, a summary cannot do it justice. As a consequence of all this:  Split-brain is not an academic concern: it happens to all kinds of systems—sometimes for days on end. Partitions deserve serious consideration… It’s important to consider this risk before a partition occurs, because it’s much easier to make decisions about partition behavior on a whiteboard than to redesign, reengineer, and upgrade a complex system in a production environment—especially when it’s throwing errors at your users. For some applications, failure is an option—but you should characterize and explicitly account for it as a part of your design. Finally, given the additional latency and coordination benefits of partition-aware designs, you might just find that accounting for these partitions delivers benefits in the average case as well. The authors also acknowledge that there might be reliable networks out there:  On the other hand, some networks really are reliable. Engineers at major financial firms have anecdotally reported that despite putting serious effort into designing systems that gracefully tolerate partitions, their networks rarely, if ever, exhibit partition behavior. Cautious engineering and aggressive network advances (along with lots of money) can prevent outages. Moreover, in this article, we have presented failure scenarios; we acknowledge it’s much harder to demonstrate that network failures have not occurred. But if I was thinking about future architectures and cloud deployments, I wouldn’t want to rely on it ;).", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2639988.2655736?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/the-network-is-reliable.json"}
{"id": "49139111", "bin": "1000_1100", "summary_sentences": ["This paper proposes a novel method for conducting multimodal sentiment classification from user-generated videos.", "Multimodal methods comprise of combining various modes of information such as audio, video, and text.", "The framework is mainly based on a long short-term memory (LSTM) model that enables utterances (units of speech bound by breathes or pauses) to capture contextual information.", "What is Sentiment Analysis?", "A sentiment analysis task involves many NLP sub-tasks and most commonly aims to detect polarity (positive/negative sentiment) in text.", "Emotion recognition is a derivative task in which the aim is to predict fine-grained emotions (e.g., fear and joy).", "Why Multimodal information?", "By combining vocal modulations and facial expressions with textual information, it is possible enrich the feature learning process to better understand affective states of opinion holders.", "In other words, there could be other behavioral cues in vocal and visual modalities that could be leveraged.", "Contributions  The proposed framework considers the order, inter-dependencies, and relations that exist among utterances in a video, where others treat them independently.", "In other words, surrounding context should help to better classify the sentiment conveyed by utterances.", "In addition, audio, visual, and textual information are combined to tackle both sentiment and emotion recognition tasks.", "Example  Consider the following utterance found in a review: “The Green Hornet did something similar”.", "Without any context, we may perceive this utterance as conveying negative sentiment.", "What if we included the nearby utterances: “It engages the audience more” and “I just love it”.", "Would the sentiment change to positive?", "You be the judge of that!", "Note that it is highly subjective but we can train a machine to detect these correlations automatically.", "Models  Two main types of feature extraction methods are proposed:  F1: Context-Independent Features (a.k.a unimodal features for each modality)  Textual feature extraction is performed using a convolutional neural network (CNN) where the input is the transcription of each utterance, which is represented by the concatenation of corresponding word2vec word vectors.", "(See paper for more details of CNN)  Audio feature extraction is performed using the openSMILE open-source software, where low-level features, such as voice intensity and pitch, are obtained.", "(See paper for more details on audio features)  Visual feature extraction is performed using a 3D-CNN, where frame-level features are learned.", "(See paper for more details of 3D-CNN)  F2: Contextualized Features  An LSTM-based network is adopted to perform context-dependent feature extraction by modeling relations among utterances.", "Basically, unimodal features are fed as input to a LSTM layer that produces contextualized features as shown in diagram below.", "Different variants of the LSTM model are experimented with, such as sc-LSTM (unidirectional LSTM cells), h-LSTM (dense layer ignored), bc-LSTM (bidirectional LSTMs), and uni-SVM (unimodal features are used directly with SVM for classification).", "Fusing Modalities  There are basically two frameworks for fusing modalities:  Non-hierarchical Framework — unimodal features are concatenated and fed into the various contextual LSTM networks proposed above (e.g., h-LSTM).", "Hierarchical Framework — The difference here is that we don’t concatenate unimodal features, we feed each unimodal feature into the LSTM network proposed above.", "Think of this framework as having some hierarchy.", "In the first level, unimodal features are fed individually to LSTM networks.", "The output of the first level are then concatenated and fed into another LSTM network (i.e., second level).", "(Check diagram below for overview of hierarchy or see paper for all the details)  Datasets  An important consideration in multimodal sentiment analysis is that person-independent datasets must be designed.", "In other words, train/test splits are disjoint with respect to speakers.", "The following datasets were used for the experiments:  MOSI — contains video-based topic reviews annotated by sentiment polarity  MOUD — contains product review videos annotated by sentiment polarity  IEMOCAP — contains scripted affect-related utterances annotated by emotion categories  Main Findings  Hierarchy vs Non-Hierarchy: From the results in the table above we can observe that hierarchical model significantly outperform the non-hierarchical frameworks (highlighted in green).", "LSTM variants: sc-LSTM and bc-LSTM models perform the best out of the LSTM variants, including the uni-SVM model (results highlighted in red).", "These results help to show the importance of considering contextual information when classifying utterances.", "Modalities: In general, unimodal classifiers trained on textual information perform best as compared to other individual modalities (results highlighted in blue).", "The exception was the MOUD dataset, which involved some translation.", "However, combining the modalities tend to boost the performance, indicating that multimodal methods are feasible and effective.", "Generalizability: To test for generalizability, the models were trained on one dataset (MOSI) and tested on another (MOUD).", "Individually, the visual modality caries the more generalized information.", "Overall, fusing the modalities improved the model.", "(See paper for more qualitative analysis on the importance of contextualized information for multimodal sentiment classification.)", "Call for Research  Here are a few ideas you can try to improve the current work:  Currently, this work aims to evaluate methods on benchmark datasets, which are somewhat clean.", "You can try to collect your own datasets and label them automatically, rendering large-scale datasets.", "Also, keep in mind the domain; i.e., you can try to work on a different type of dataset that doesn’t include reviews.", "It would be interesting to see more cases where contextualized information helps with sentiment classification.", "Also, a more advanced idea includes the fusion part of the framework.", "You can try to experiment with more sophisticated fusion techniques, such as those used here .", "Software: openSMILE — Software for extracting acoustic features from audio  Dataset: MOSI  Paper: Context-Dependent Sentiment Analysis in User-Generated Videos  Presentation: Video Clip  Have any other questions regarding this paper?", "Send me a DM @omarsar0 ."], "summary_text": "This paper proposes a novel method for conducting multimodal sentiment classification from user-generated videos. Multimodal methods comprise of combining various modes of information such as audio, video, and text. The framework is mainly based on a long short-term memory (LSTM) model that enables utterances (units of speech bound by breathes or pauses) to capture contextual information. What is Sentiment Analysis? A sentiment analysis task involves many NLP sub-tasks and most commonly aims to detect polarity (positive/negative sentiment) in text. Emotion recognition is a derivative task in which the aim is to predict fine-grained emotions (e.g., fear and joy). Why Multimodal information? By combining vocal modulations and facial expressions with textual information, it is possible enrich the feature learning process to better understand affective states of opinion holders. In other words, there could be other behavioral cues in vocal and visual modalities that could be leveraged. Contributions  The proposed framework considers the order, inter-dependencies, and relations that exist among utterances in a video, where others treat them independently. In other words, surrounding context should help to better classify the sentiment conveyed by utterances. In addition, audio, visual, and textual information are combined to tackle both sentiment and emotion recognition tasks. Example  Consider the following utterance found in a review: “The Green Hornet did something similar”. Without any context, we may perceive this utterance as conveying negative sentiment. What if we included the nearby utterances: “It engages the audience more” and “I just love it”. Would the sentiment change to positive? You be the judge of that! Note that it is highly subjective but we can train a machine to detect these correlations automatically. Models  Two main types of feature extraction methods are proposed:  F1: Context-Independent Features (a.k.a unimodal features for each modality)  Textual feature extraction is performed using a convolutional neural network (CNN) where the input is the transcription of each utterance, which is represented by the concatenation of corresponding word2vec word vectors. (See paper for more details of CNN)  Audio feature extraction is performed using the openSMILE open-source software, where low-level features, such as voice intensity and pitch, are obtained. (See paper for more details on audio features)  Visual feature extraction is performed using a 3D-CNN, where frame-level features are learned. (See paper for more details of 3D-CNN)  F2: Contextualized Features  An LSTM-based network is adopted to perform context-dependent feature extraction by modeling relations among utterances. Basically, unimodal features are fed as input to a LSTM layer that produces contextualized features as shown in diagram below. Different variants of the LSTM model are experimented with, such as sc-LSTM (unidirectional LSTM cells), h-LSTM (dense layer ignored), bc-LSTM (bidirectional LSTMs), and uni-SVM (unimodal features are used directly with SVM for classification). Fusing Modalities  There are basically two frameworks for fusing modalities:  Non-hierarchical Framework — unimodal features are concatenated and fed into the various contextual LSTM networks proposed above (e.g., h-LSTM). Hierarchical Framework — The difference here is that we don’t concatenate unimodal features, we feed each unimodal feature into the LSTM network proposed above. Think of this framework as having some hierarchy. In the first level, unimodal features are fed individually to LSTM networks. The output of the first level are then concatenated and fed into another LSTM network (i.e., second level). (Check diagram below for overview of hierarchy or see paper for all the details)  Datasets  An important consideration in multimodal sentiment analysis is that person-independent datasets must be designed. In other words, train/test splits are disjoint with respect to speakers. The following datasets were used for the experiments:  MOSI — contains video-based topic reviews annotated by sentiment polarity  MOUD — contains product review videos annotated by sentiment polarity  IEMOCAP — contains scripted affect-related utterances annotated by emotion categories  Main Findings  Hierarchy vs Non-Hierarchy: From the results in the table above we can observe that hierarchical model significantly outperform the non-hierarchical frameworks (highlighted in green). LSTM variants: sc-LSTM and bc-LSTM models perform the best out of the LSTM variants, including the uni-SVM model (results highlighted in red). These results help to show the importance of considering contextual information when classifying utterances. Modalities: In general, unimodal classifiers trained on textual information perform best as compared to other individual modalities (results highlighted in blue). The exception was the MOUD dataset, which involved some translation. However, combining the modalities tend to boost the performance, indicating that multimodal methods are feasible and effective. Generalizability: To test for generalizability, the models were trained on one dataset (MOSI) and tested on another (MOUD). Individually, the visual modality caries the more generalized information. Overall, fusing the modalities improved the model. (See paper for more qualitative analysis on the importance of contextualized information for multimodal sentiment classification.) Call for Research  Here are a few ideas you can try to improve the current work:  Currently, this work aims to evaluate methods on benchmark datasets, which are somewhat clean. You can try to collect your own datasets and label them automatically, rendering large-scale datasets. Also, keep in mind the domain; i.e., you can try to work on a different type of dataset that doesn’t include reviews. It would be interesting to see more cases where contextualized information helps with sentiment classification. Also, a more advanced idea includes the fusion part of the framework. You can try to experiment with more sophisticated fusion techniques, such as those used here . Software: openSMILE — Software for extracting acoustic features from audio  Dataset: MOSI  Paper: Context-Dependent Sentiment Analysis in User-Generated Videos  Presentation: Video Clip  Have any other questions regarding this paper? Send me a DM @omarsar0 .", "pdf_url": "https://www.aclweb.org/anthology/P17-1081.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/state-of-the-art-multimodal-sentiment-classification-in-videos-1daa8a481c5a.json"}
{"id": "80897282", "bin": "100_200", "summary_sentences": ["The paper explains how to apply dropout to LSTMs and how it could reduce overfitting in tasks like language modelling, speech recognition, image caption generation and machine translation.", "Dropout  Regularisation method that drops out (or temporarily removes) units in a neural network.", "the network, along with all its incoming and outgoing connections  Conventional dropout does not work well with RNNs as the recurrence amplifies the noise and hurts learning.", "Regularization  The paper proposes to apply dropout to only the non-recurrent connections.", "The dropout operator would corrupt information carried by some units (and not all) forcing them to perform intermediate computations more robustly.", "The information is corrupted L+1 times where L is the number of layers and is independent of timestamps traversed by the information.", "Observation  In the context of language modelling, image caption generation, speech recognition and machine translation, dropout enables training larger networks and reduces the testing error in terms of perplexity and frame accuracy."], "summary_text": "The paper explains how to apply dropout to LSTMs and how it could reduce overfitting in tasks like language modelling, speech recognition, image caption generation and machine translation. Dropout  Regularisation method that drops out (or temporarily removes) units in a neural network. the network, along with all its incoming and outgoing connections  Conventional dropout does not work well with RNNs as the recurrence amplifies the noise and hurts learning. Regularization  The paper proposes to apply dropout to only the non-recurrent connections. The dropout operator would corrupt information carried by some units (and not all) forcing them to perform intermediate computations more robustly. The information is corrupted L+1 times where L is the number of layers and is independent of timestamps traversed by the information. Observation  In the context of language modelling, image caption generation, speech recognition and machine translation, dropout enables training larger networks and reduces the testing error in terms of perplexity and frame accuracy.", "pdf_url": "https://arxiv.org/pdf/1409.2329", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/6245692b276cd0b6dcbaf43e4211db.json"}
{"id": "22546727", "bin": "100_200", "summary_sentences": ["GloVe: Global Vectors for Word Representation – Pennington et al. 2014  Yesterday we looked at some of the amazing properties of word vectors with word2vec .", "Pennington et al. argue that the online scanning approach used by word2vec is suboptimal since it doesn’t fully exploit statistical information regarding word co-occurrences.", "They demonstrate a Global Vectors (GloVe) model which combines the benefits of the word2vec skip-gram model when it comes to word analogy tasks, with the benefits of matrix factorization methods that can exploit global statistical information.", "The GloVe model…  … produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task.", "It also outperforms related models on similarity tasks and named entity recognition.", "The source code for the model, as well as trained word vectors can be found at  [url]"], "summary_text": "GloVe: Global Vectors for Word Representation – Pennington et al. 2014  Yesterday we looked at some of the amazing properties of word vectors with word2vec . Pennington et al. argue that the online scanning approach used by word2vec is suboptimal since it doesn’t fully exploit statistical information regarding word co-occurrences. They demonstrate a Global Vectors (GloVe) model which combines the benefits of the word2vec skip-gram model when it comes to word analogy tasks, with the benefits of matrix factorization methods that can exploit global statistical information. The GloVe model…  … produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition. The source code for the model, as well as trained word vectors can be found at  [url]", "pdf_url": "http://www-nlp.stanford.edu/pubs/glove.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/glove-global-vectors-for-word-representation.json"}
{"id": "23929837", "bin": "1100_1200", "summary_sentences": ["What  They present a hierarchical method for reinforcement learning.", "The method combines \"long\"-term goals with short-term action choices.", "How  They have two components:  Meta-Controller:  Responsible for the \"long\"-term goals.", "Is trained to pick goals (based on the current state) that maximize (extrinsic) rewards, just like you would usually optimize to maximize rewards by picking good actions.", "The Meta-Controller only picks goals when the Controller terminates or achieved the goal.", "Controller:  Receives the current state and the current goal.", "Has to pick a reward maximizing action based on those, just as the agent would usually do (only the goal is added here).", "The reward is intrinsic.", "It comes from the Critic.", "The Critic gives reward whenever the current goal is reached.", "For Montezuma's Revenge:  A goal is to reach a specific object.", "The goal is encoded via a bitmask (as big as the game screen).", "The mask contains 1s wherever the object is.", "They hand-extract the location of a few specific objects.", "So basically:  The Meta-Controller picks the next object to reach via a Q-value function.", "It receives extrinsic reward when objects have been reached in a specific sequence.", "The Controller picks actions that lead to reaching the object based on a Q-value function.", "It iterates action-choosing until it terminates or reached the goal-object.", "The Critic awards intrinsic reward to the Controller whenever the goal-object was reached.", "They use CNNs for the Meta-Controller and the Controller, similar in architecture to the Atari-DQN paper (shallow CNNs).", "They use two replay memories, one for the Meta-Controller (size 40k) and one for the Controller (size 1M).", "Both follow an epsilon-greedy policy (for picking goals/actions).", "Epsilon starts at 1.0 and is annealed down to 0.1.", "They use a discount factor / gamma of 0.9.", "They train with SGD.", "Results  Learns to play Montezuma's Revenge.", "Learns to act well in a more abstract MDP with delayed rewards and where simple Q-learning failed.", "Rough chapter-wise notes  (1) Introduction  Basic problem: Learn goal directed behaviour from sparse feedbacks.", "Challenges:  Explore state space efficiently  Create multiple levels of spatio-temporal abstractions  Their method: Combines deep reinforcement learning with hierarchical value functions.", "Their agent is motivated to solve specific intrinsic goals.", "Goals are defined in the space of entities and relations, which constraints the search space.", "They define their value function as V(s, g) where s is the state and g is a goal.", "First, their agent learns to solve intrinsically generated goals.", "Then it learns to chain these goals together.", "Their model has two hiearchy levels:  Meta-Controller: Selects the current goal based on the current state.", "Controller: Takes state s and goal g, then selects a good action based on s and g. The controller operates until g is achieved, then the meta-controller picks the next goal.", "Meta-Controller gets extrinsic rewards, controller gets intrinsic rewards.", "They use SGD to optimize the whole system (with respect to reward maximization).", "(3) Model  Basic setting: Action a out of all actions A, state s out of S, transition function T(s,a)->s', reward by state F(s)->R.", "epsilon-greedy is good for local exploration, but it's not good at exploring very different areas of the state space.", "They use intrinsically motivated goals to better explore the state space.", "Sequences of goals are arranged to maximize the received extrinsic reward.", "The agent learns one policy per goal.", "Meta-Controller: Receives current state, chooses goal.", "Controller: Receives current state and current goal, chooses action.", "Keeps choosing actions until goal is achieved or a terminal state is reached.", "Has the optimization target of maximizing cumulative reward.", "Critic: Checks if current goal is achieved and if so provides intrinsic reward.", "They use deep Q learning to train their model.", "There are two Q-value functions.", "One for the controller and one for the meta-controller.", "Both formulas are extended by the last chosen goal g.  The Q-value function of the meta-controller does not depend on the chosen action.", "The Q-value function of the controller receives only intrinsic direct reward, not extrinsic direct reward.", "Both Q-value functions are reprsented with DQNs.", "Both are optimized to minimize MSE losses.", "They use separate replay memories for the controller and meta-controller.", "A memory is added for the meta-controller whenever the controller terminates.", "Each new goal is picked by the meta-controller epsilon-greedy (based on the current state).", "The controller picks actions epsilon-greedy (based on the current state and goal).", "Both epsilons are annealed down.", "(4) Experiments  (4.1) Discrete MDP with delayed rewards  Basic MDP setting, following roughly: Several states (s1 to s6) organized in a chain.", "The agent can move left or right.", "It gets high reward if it moves to state s6 and then back to s1, otherwise it gets small reward per reached state.", "They use their hierarchical method, but without neural nets.", "Baseline is Q-learning without a hierarchy/intrinsic rewards.", "Their method performs significantly better than the baseline.", "(4.2) ATARI game with delayed rewards  They play Montezuma's Revenge with their method, because that game has very delayed rewards.", "They use CNNs for the controller and meta-controller (architecture similar to the Atari-DQN paper).", "The critic reacts to (entity1, relation, entity2) relationships.", "The entities are just objects visible in the game.", "The relation is (apparently ?)", "always \"reached\", i.e. whether object1 arrived at object2.", "They extract the objects manually, i.e. assume the existance of a perfect unsupervised object detector.", "They encode the goals apparently not as vectors, but instead just use a bitmask (game screen heightand width), which has 1s at the pixels that show the object.", "Replay memory sizes: 1M for controller, 50k for meta-controller.", "gamma=0.99  They first only train the controller (i.e. meta-controller completely random) and only then train both jointly.", "Their method successfully learns to perform actions which lead to rewards with long delays.", "It starts with easier goals and then learns harder goals."], "summary_text": "What  They present a hierarchical method for reinforcement learning. The method combines \"long\"-term goals with short-term action choices. How  They have two components:  Meta-Controller:  Responsible for the \"long\"-term goals. Is trained to pick goals (based on the current state) that maximize (extrinsic) rewards, just like you would usually optimize to maximize rewards by picking good actions. The Meta-Controller only picks goals when the Controller terminates or achieved the goal. Controller:  Receives the current state and the current goal. Has to pick a reward maximizing action based on those, just as the agent would usually do (only the goal is added here). The reward is intrinsic. It comes from the Critic. The Critic gives reward whenever the current goal is reached. For Montezuma's Revenge:  A goal is to reach a specific object. The goal is encoded via a bitmask (as big as the game screen). The mask contains 1s wherever the object is. They hand-extract the location of a few specific objects. So basically:  The Meta-Controller picks the next object to reach via a Q-value function. It receives extrinsic reward when objects have been reached in a specific sequence. The Controller picks actions that lead to reaching the object based on a Q-value function. It iterates action-choosing until it terminates or reached the goal-object. The Critic awards intrinsic reward to the Controller whenever the goal-object was reached. They use CNNs for the Meta-Controller and the Controller, similar in architecture to the Atari-DQN paper (shallow CNNs). They use two replay memories, one for the Meta-Controller (size 40k) and one for the Controller (size 1M). Both follow an epsilon-greedy policy (for picking goals/actions). Epsilon starts at 1.0 and is annealed down to 0.1. They use a discount factor / gamma of 0.9. They train with SGD. Results  Learns to play Montezuma's Revenge. Learns to act well in a more abstract MDP with delayed rewards and where simple Q-learning failed. Rough chapter-wise notes  (1) Introduction  Basic problem: Learn goal directed behaviour from sparse feedbacks. Challenges:  Explore state space efficiently  Create multiple levels of spatio-temporal abstractions  Their method: Combines deep reinforcement learning with hierarchical value functions. Their agent is motivated to solve specific intrinsic goals. Goals are defined in the space of entities and relations, which constraints the search space. They define their value function as V(s, g) where s is the state and g is a goal. First, their agent learns to solve intrinsically generated goals. Then it learns to chain these goals together. Their model has two hiearchy levels:  Meta-Controller: Selects the current goal based on the current state. Controller: Takes state s and goal g, then selects a good action based on s and g. The controller operates until g is achieved, then the meta-controller picks the next goal. Meta-Controller gets extrinsic rewards, controller gets intrinsic rewards. They use SGD to optimize the whole system (with respect to reward maximization). (3) Model  Basic setting: Action a out of all actions A, state s out of S, transition function T(s,a)->s', reward by state F(s)->R. epsilon-greedy is good for local exploration, but it's not good at exploring very different areas of the state space. They use intrinsically motivated goals to better explore the state space. Sequences of goals are arranged to maximize the received extrinsic reward. The agent learns one policy per goal. Meta-Controller: Receives current state, chooses goal. Controller: Receives current state and current goal, chooses action. Keeps choosing actions until goal is achieved or a terminal state is reached. Has the optimization target of maximizing cumulative reward. Critic: Checks if current goal is achieved and if so provides intrinsic reward. They use deep Q learning to train their model. There are two Q-value functions. One for the controller and one for the meta-controller. Both formulas are extended by the last chosen goal g.  The Q-value function of the meta-controller does not depend on the chosen action. The Q-value function of the controller receives only intrinsic direct reward, not extrinsic direct reward. Both Q-value functions are reprsented with DQNs. Both are optimized to minimize MSE losses. They use separate replay memories for the controller and meta-controller. A memory is added for the meta-controller whenever the controller terminates. Each new goal is picked by the meta-controller epsilon-greedy (based on the current state). The controller picks actions epsilon-greedy (based on the current state and goal). Both epsilons are annealed down. (4) Experiments  (4.1) Discrete MDP with delayed rewards  Basic MDP setting, following roughly: Several states (s1 to s6) organized in a chain. The agent can move left or right. It gets high reward if it moves to state s6 and then back to s1, otherwise it gets small reward per reached state. They use their hierarchical method, but without neural nets. Baseline is Q-learning without a hierarchy/intrinsic rewards. Their method performs significantly better than the baseline. (4.2) ATARI game with delayed rewards  They play Montezuma's Revenge with their method, because that game has very delayed rewards. They use CNNs for the controller and meta-controller (architecture similar to the Atari-DQN paper). The critic reacts to (entity1, relation, entity2) relationships. The entities are just objects visible in the game. The relation is (apparently ?) always \"reached\", i.e. whether object1 arrived at object2. They extract the objects manually, i.e. assume the existance of a perfect unsupervised object detector. They encode the goals apparently not as vectors, but instead just use a bitmask (game screen heightand width), which has 1s at the pixels that show the object. Replay memory sizes: 1M for controller, 50k for meta-controller. gamma=0.99  They first only train the controller (i.e. meta-controller completely random) and only then train both jointly. Their method successfully learns to perform actions which lead to rewards with long delays. It starts with easier goals and then learns harder goals.", "pdf_url": "https://arxiv.org/pdf/1604.06057", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/hierarchical_deep_reinforcement_learning.json"}
{"id": "38469042", "bin": "1100_1200", "summary_sentences": ["Same-different problems strain convolutional neural networks Ricci et al., arXiv 2018  Since we’ve been looking at the idea of adding structured representations and relational reasoning to deep learning systems, I thought it would be interesting to finish off the week with an example of a problem that seems to require it: detecting whether objects in a scene are the same or different.", "This image containing a flute was correctly classified by a CNN trained on millions of photographs.", "On ImageNet the network even surpassed the accuracy of a human observer.", "This image contains two shapes that are the same, a relationship that is immediately obvious to a human observer.", "“Yet, the CNN failed to learn this relation even after seeing millions of training examples.”  The above is an example of a same-different (SD) visual relation problem (output whether the objects in the scene are the same, or different).", "Spatial relation (SR) problems ask whether objects follow a certain spatial relation, e.g. in a line, horizontally stacked, vertically stacked, and so on.", "For example:  The synthetic visual reasoning test (SVRT) contains a collection of 23 binary classification problems along these lines.", "In each case opposing classes differ based on whether their stimuli obey an abstract rule.", "If you train a bunch of CNNs (with different depths, filter sizes, etc.)", "on these tasks an interesting pattern pops out.", "The CNNs really struggled on problems where the abstract rule required detecting whether things were the same or different (congruent up to some transformation), whereas they achieved good accuracy on spatial relation problems.", "The resulting dichotomy across the SVRT problems is striking.", "CNNs fare uniformly worse on SD problems than they do on SR problems.", "Many SR problems were learned satisfactorily, whereas some SD problems (e.g. problems 20 and 7) resulted in accuracy not substantially above chance.", "For SR problems, all the CNNs did pretty well, regardless of network configuration.", "But for SD problems larger networks performed noticeably better than smaller ones.", "This suggests that something about the SD problems is straining the capacity of the CNNs.", "Probing further  To dig deeper into this apparent difference between same-different and spatial-relation problems the authors construct a new visual-relation benchmark called PSVRT.", "The dataset is parameterised so that the size of the items in the scene, the number of scene items, and the size of the whole image can all be controlled.", "Scene items are just binary bit patterns placed on a blank background.", "For any given configuration of parameters, the resulting scene can be used for both an SD problem and an SR problem, simply based on labelling.", "Our goal was to examine how hard it is for a CNN architecture to learn relations for visually different but conceptually equivalent problems.", "If CNNs can truly learn the “rule” underlying these problems, then one would expect the models to learn all problems with more-or-less equal ease.", "However, if the CNN only memorize the distinguishing features of the two image classes, then learning should be affected by the variability of the example images in each category.", "A baseline architecture was established with four convolutional layers, that was able to easily learn both the same-different and spatial-relation PSVRT problems with item size 4, image size 60, and two items in the image.", "This baseline CNN was then trained from scratch on a variety of PSVRT problems, each time using 20 million training images and a batch size of 50.", "There were three sub-experiments:  Fixing item size (m) at 4, number of items (k) at 2, and varying image size (n) between 30 and 180.", "Fixing image size at 60, number of items at 2, and varying item size between 3 and 7.", "Fixing image size at 60, item size at 4, and varying the number of items between 2 and 6.", "In all conditions, we found a strong dichotomy between SD and SR conditions.", "In SR, across all image parameters and in all trials, the model immediately learned at the start of training and quickly approached 100% accuracy, producing consistently high and flat mean ALC curves.", "In SD, however, we found that the overall ALC was significantly lower than SR.  Digging deeper, when learning did occur in SD, increasing item size never strained performance.", "But increasing the overall image size, or increasing the number of items did.", "(Gray bars in the above figures indicate the number of trials in which learning failed).", "The results suggest that straining is not simply a direct outcome of an increase in image variability.", "Using CNNs with more than twice the number of kernels (wide), or twice as many layers (deep) did not change the observed trend.", "What’s going on?", "The authors hypothesise that the CNNs learn ‘subtraction templates’ when tackling SD problems: filters with one positive region and one negative region.", "Each relative arrangement of items requires a different subtraction template since each item must lie in on of the template’s two regions.", "If identical items lie in opposing regions, they are subtracted by the synaptic weights.", "The difference is used to choose the appropriate same/different label.", "A strategy like this doesn’t require memorizing specific items, so item size doesn’t make much of a difference.", "However, image size (the biggest straining factor) exponentially increases the possible number of arrangements of items.", "Our results indicate that visual-relation problems can quickly exceed the representational capacity of feedforward networks.", "While learning templates for individual objects appears to be tractable for today’s deep networks, learning templates for arrangements of objects becomes rapidly intractable because of the combinatorial explosion in the number of features to be stored… Given the vast superiority of humans over modern computers in their ability to detect visual relations, we see the exploration of attentional and grouping mechanisms as an important next step in our computational understanding of visual reasoning."], "summary_text": "Same-different problems strain convolutional neural networks Ricci et al., arXiv 2018  Since we’ve been looking at the idea of adding structured representations and relational reasoning to deep learning systems, I thought it would be interesting to finish off the week with an example of a problem that seems to require it: detecting whether objects in a scene are the same or different. This image containing a flute was correctly classified by a CNN trained on millions of photographs. On ImageNet the network even surpassed the accuracy of a human observer. This image contains two shapes that are the same, a relationship that is immediately obvious to a human observer. “Yet, the CNN failed to learn this relation even after seeing millions of training examples.”  The above is an example of a same-different (SD) visual relation problem (output whether the objects in the scene are the same, or different). Spatial relation (SR) problems ask whether objects follow a certain spatial relation, e.g. in a line, horizontally stacked, vertically stacked, and so on. For example:  The synthetic visual reasoning test (SVRT) contains a collection of 23 binary classification problems along these lines. In each case opposing classes differ based on whether their stimuli obey an abstract rule. If you train a bunch of CNNs (with different depths, filter sizes, etc.) on these tasks an interesting pattern pops out. The CNNs really struggled on problems where the abstract rule required detecting whether things were the same or different (congruent up to some transformation), whereas they achieved good accuracy on spatial relation problems. The resulting dichotomy across the SVRT problems is striking. CNNs fare uniformly worse on SD problems than they do on SR problems. Many SR problems were learned satisfactorily, whereas some SD problems (e.g. problems 20 and 7) resulted in accuracy not substantially above chance. For SR problems, all the CNNs did pretty well, regardless of network configuration. But for SD problems larger networks performed noticeably better than smaller ones. This suggests that something about the SD problems is straining the capacity of the CNNs. Probing further  To dig deeper into this apparent difference between same-different and spatial-relation problems the authors construct a new visual-relation benchmark called PSVRT. The dataset is parameterised so that the size of the items in the scene, the number of scene items, and the size of the whole image can all be controlled. Scene items are just binary bit patterns placed on a blank background. For any given configuration of parameters, the resulting scene can be used for both an SD problem and an SR problem, simply based on labelling. Our goal was to examine how hard it is for a CNN architecture to learn relations for visually different but conceptually equivalent problems. If CNNs can truly learn the “rule” underlying these problems, then one would expect the models to learn all problems with more-or-less equal ease. However, if the CNN only memorize the distinguishing features of the two image classes, then learning should be affected by the variability of the example images in each category. A baseline architecture was established with four convolutional layers, that was able to easily learn both the same-different and spatial-relation PSVRT problems with item size 4, image size 60, and two items in the image. This baseline CNN was then trained from scratch on a variety of PSVRT problems, each time using 20 million training images and a batch size of 50. There were three sub-experiments:  Fixing item size (m) at 4, number of items (k) at 2, and varying image size (n) between 30 and 180. Fixing image size at 60, number of items at 2, and varying item size between 3 and 7. Fixing image size at 60, item size at 4, and varying the number of items between 2 and 6. In all conditions, we found a strong dichotomy between SD and SR conditions. In SR, across all image parameters and in all trials, the model immediately learned at the start of training and quickly approached 100% accuracy, producing consistently high and flat mean ALC curves. In SD, however, we found that the overall ALC was significantly lower than SR.  Digging deeper, when learning did occur in SD, increasing item size never strained performance. But increasing the overall image size, or increasing the number of items did. (Gray bars in the above figures indicate the number of trials in which learning failed). The results suggest that straining is not simply a direct outcome of an increase in image variability. Using CNNs with more than twice the number of kernels (wide), or twice as many layers (deep) did not change the observed trend. What’s going on? The authors hypothesise that the CNNs learn ‘subtraction templates’ when tackling SD problems: filters with one positive region and one negative region. Each relative arrangement of items requires a different subtraction template since each item must lie in on of the template’s two regions. If identical items lie in opposing regions, they are subtracted by the synaptic weights. The difference is used to choose the appropriate same/different label. A strategy like this doesn’t require memorizing specific items, so item size doesn’t make much of a difference. However, image size (the biggest straining factor) exponentially increases the possible number of arrangements of items. Our results indicate that visual-relation problems can quickly exceed the representational capacity of feedforward networks. While learning templates for individual objects appears to be tractable for today’s deep networks, learning templates for arrangements of objects becomes rapidly intractable because of the combinatorial explosion in the number of features to be stored… Given the vast superiority of humans over modern computers in their ability to detect visual relations, we see the exploration of attentional and grouping mechanisms as an important next step in our computational understanding of visual reasoning.", "pdf_url": "https://arxiv.org/pdf/1802.03390", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/same-different-problems-strain-convolutional-neural-networks.json"}
{"id": "67696398", "bin": "1200_1300", "summary_sentences": ["Split-Level IO Scheduling – Yang et al. 2015  The central idea in today’s paper is pretty simple: block-level I/O schedulers (the most common kind) lack the higher level information necessary to perform write-reordering and accurate accounting, whereas system-call  level schedulers have the appropriate context but lack the low-level knowledge needed to build efficient schedulers – so why not create a scheduling framework that can intercept at both the block and system call levels to get the best of both worlds?", "This is what the authors call ‘split-level IO scheduling.’  By implementing a judiciously selected set of handlers at key junctures within the storage stack (namely, at the system-call, page-cache, and block layers), a developer can implement a scheduling discipline with full control over behavior and with no loss in high- or low-level information.", "Split schedulers can determine which processes issued I/O (via graph tags that track causality across levels) and accurately estimate I/O costs.", "Furthermore, memory notiﬁcations make schedulers aware of write work as soon as possible (not tens of seconds later when writeback occurs).", "Finally, split schedulers can prevent ﬁle systems from imposing orderings that are contrary to scheduling goals.", "On top of this split-level framework, the authors build a fair queuing scheduler that reduces priority misallocations by 28x compared to the Linux CFQ scheduler; a deadline-based scheduler that reduces tail latencies by 4x; and a resource limiting scheduler that improves isolation by 6x for some workloads.", "This latter scheduler can improve isolation for virtual machines and for HDFS, and the deadline based scheduler  provides a solution to fsync-induced freezes in database systems.", "[A] block-level framework fails to support correct cause mapping (due to write delegation such as journaling and delayed allocation) or control over reordering (due to ﬁle-system ordering requirements).", "[A] system-call framework solves these two problems, but fails to provide enough information to schedulers for accurate cost estimation because it lacks low-level knowledge.", "These problems are general to many ﬁle systems; even if journals are not used, similar issues arise from the ordering constraints imposed by other mechanisms such as copy-on-write techniques  or soft up- dates.", "Our split framework meets all the needs in Table 1 (cause mapping, cost-estimation, and reordering) by incorporating ideas from the other two frameworks and exposing additional memory-related hooks.", "The ideas apply generally, but the implementation in the paper is integrated with the ext4 and XFS filesystems in Linux.", "Three split-level schedulers are built on top: Actually-Fair Queuing (AFQ) – 950 loc; Split-Deadline – 750 loc; and Split-Token – 950 loc.", "Actually Fair Queuing  AFQ allocates I/O fairly among processes according to their priorties.", "It has performance similar to Linux’s CFQ, while avoiding the priority inversions that can happen with CFQ.", "AFQ employs a two-level scheduling strategy.", "Reads are handled at the block level and writes (and calls that cause writes, such as fsync) are handled at the system-call level.", "This design allows reads to hit the cache while protecting writes from journal entanglement.", "Beneath the journal, low-priority blocks may be prerequisites for high-priority fsync calls, so writes at the block level are dispatched immediately.", "AFQ chooses I/O requests to dequeue at the block and system-call levels using the stride algorithm .", "Whenever a block request is dispatched to disk, AFQ charges the responsible processes for the disk I/O.", "The I/O cost is based on a simple seek model.", "Split-Deadline  [Linux’s] Block-Deadline scheduler does poorly when trying to limit tail latencies, due to its inability to reorder block I/Os in the presence of ﬁlesystem ordering requirements.", "Split-level scheduling, with system-call scheduling capabilities and memory-state knowledge, is better suited to this task.", "We implement the Split-Deadline scheduler by modifying the Linux deadline scheduler (Block-Deadline)…  To show the benefits for real databases, SQLite and PostgreSQL are measured with both Split-Deadline and Block-Deadline.", "…when running on top of Block-Deadline, 4% of transactions fail to meet their latency target, and over 1% take longer than 500ms.", "After further inspection, we found that the latency spikes happen at the end of each checkpoint period, when the system begins to ﬂush a large amount of dirty data to disk using fsync.", "Such data ﬂushing interferes with foreground I/Os, causes long transaction latency and low system throughput.", "The database community has long experienced this “fsync freeze” problem, and has no great solution for it.", "We show next that Split-Deadline provides a simple solution to this problem….", "it effectively eliminates tail latency: 99.99% of the transactions are completed within 15 ms.  Split-Token  [In Split-Token] throttled processes are given tokens at a set rate.", "I/O costs tokens, I/O is blocked if there are no tokens, and the number of tokens that may be held is capped.", "Split-Token throttles a process’s system-call writes and block-level reads if and only if the number of tokens is negative.", "System-call reads are never throttled (to utilize the cache).", "Block writes are never throttled (to avoid entanglement).", "Our implementation uses memory-level and block-level hooks for accounting.", "The scheduler promptly charges tokens as soon as buffers are dirtied, and then revises when the writes are later ﬂushed to the block level (§3.2), charging more tokens (or refunding them) based on ampliﬁcation and sequentiality.", "Tokens represent bytes, so accounting normalizes the cost of an I/O pattern to the equivalent amount of sequential I/O (e.g., 1 MB of random I/O may be counted as 10 MB).", "Under evaluation with QEMU, split-token is much better than the SCS scheduler at eliminating noisy I/O neighbour problems.", "The authors also evaluate split-token in an HDFS context:  To show that local split scheduling is a useful foundation to provide isolation in a distributed environment, we integrate HDFS with Split-Token to provide isolation to HDFS clients.", "We modify the client-to-worker protocol so workers know which account should be billed for disk I/O generated by the handling of a particular RPC call.", "Account information is propagated down to Split-Token and across to other workers (for pipelined writes)… We conclude that local scheduling can be used to meet distributed isolation goals; however, throttled applications may get worse-than-expected performance if the system is not well balanced.", "In conclusion…  While our experiments indicate that simple layering must be abandoned, we need not sacriﬁce modularity.", "In our split framework, the scheduler operates across all layers, but is still abstracted behind a collection of handlers.", "This approach is relatively clean, and enables pluggable scheduling.", "Supporting a new scheduling goal simply involves writing a new scheduler plug-in, not re-engineering the entire storage system.", "Our hope is that split-level scheduling will inspire future vertical integration in storage stacks.", "Our source code is available at  [url]"], "summary_text": "Split-Level IO Scheduling – Yang et al. 2015  The central idea in today’s paper is pretty simple: block-level I/O schedulers (the most common kind) lack the higher level information necessary to perform write-reordering and accurate accounting, whereas system-call  level schedulers have the appropriate context but lack the low-level knowledge needed to build efficient schedulers – so why not create a scheduling framework that can intercept at both the block and system call levels to get the best of both worlds? This is what the authors call ‘split-level IO scheduling.’  By implementing a judiciously selected set of handlers at key junctures within the storage stack (namely, at the system-call, page-cache, and block layers), a developer can implement a scheduling discipline with full control over behavior and with no loss in high- or low-level information. Split schedulers can determine which processes issued I/O (via graph tags that track causality across levels) and accurately estimate I/O costs. Furthermore, memory notiﬁcations make schedulers aware of write work as soon as possible (not tens of seconds later when writeback occurs). Finally, split schedulers can prevent ﬁle systems from imposing orderings that are contrary to scheduling goals. On top of this split-level framework, the authors build a fair queuing scheduler that reduces priority misallocations by 28x compared to the Linux CFQ scheduler; a deadline-based scheduler that reduces tail latencies by 4x; and a resource limiting scheduler that improves isolation by 6x for some workloads. This latter scheduler can improve isolation for virtual machines and for HDFS, and the deadline based scheduler  provides a solution to fsync-induced freezes in database systems. [A] block-level framework fails to support correct cause mapping (due to write delegation such as journaling and delayed allocation) or control over reordering (due to ﬁle-system ordering requirements). [A] system-call framework solves these two problems, but fails to provide enough information to schedulers for accurate cost estimation because it lacks low-level knowledge. These problems are general to many ﬁle systems; even if journals are not used, similar issues arise from the ordering constraints imposed by other mechanisms such as copy-on-write techniques  or soft up- dates. Our split framework meets all the needs in Table 1 (cause mapping, cost-estimation, and reordering) by incorporating ideas from the other two frameworks and exposing additional memory-related hooks. The ideas apply generally, but the implementation in the paper is integrated with the ext4 and XFS filesystems in Linux. Three split-level schedulers are built on top: Actually-Fair Queuing (AFQ) – 950 loc; Split-Deadline – 750 loc; and Split-Token – 950 loc. Actually Fair Queuing  AFQ allocates I/O fairly among processes according to their priorties. It has performance similar to Linux’s CFQ, while avoiding the priority inversions that can happen with CFQ. AFQ employs a two-level scheduling strategy. Reads are handled at the block level and writes (and calls that cause writes, such as fsync) are handled at the system-call level. This design allows reads to hit the cache while protecting writes from journal entanglement. Beneath the journal, low-priority blocks may be prerequisites for high-priority fsync calls, so writes at the block level are dispatched immediately. AFQ chooses I/O requests to dequeue at the block and system-call levels using the stride algorithm . Whenever a block request is dispatched to disk, AFQ charges the responsible processes for the disk I/O. The I/O cost is based on a simple seek model. Split-Deadline  [Linux’s] Block-Deadline scheduler does poorly when trying to limit tail latencies, due to its inability to reorder block I/Os in the presence of ﬁlesystem ordering requirements. Split-level scheduling, with system-call scheduling capabilities and memory-state knowledge, is better suited to this task. We implement the Split-Deadline scheduler by modifying the Linux deadline scheduler (Block-Deadline)…  To show the benefits for real databases, SQLite and PostgreSQL are measured with both Split-Deadline and Block-Deadline. …when running on top of Block-Deadline, 4% of transactions fail to meet their latency target, and over 1% take longer than 500ms. After further inspection, we found that the latency spikes happen at the end of each checkpoint period, when the system begins to ﬂush a large amount of dirty data to disk using fsync. Such data ﬂushing interferes with foreground I/Os, causes long transaction latency and low system throughput. The database community has long experienced this “fsync freeze” problem, and has no great solution for it. We show next that Split-Deadline provides a simple solution to this problem…. it effectively eliminates tail latency: 99.99% of the transactions are completed within 15 ms.  Split-Token  [In Split-Token] throttled processes are given tokens at a set rate. I/O costs tokens, I/O is blocked if there are no tokens, and the number of tokens that may be held is capped. Split-Token throttles a process’s system-call writes and block-level reads if and only if the number of tokens is negative. System-call reads are never throttled (to utilize the cache). Block writes are never throttled (to avoid entanglement). Our implementation uses memory-level and block-level hooks for accounting. The scheduler promptly charges tokens as soon as buffers are dirtied, and then revises when the writes are later ﬂushed to the block level (§3.2), charging more tokens (or refunding them) based on ampliﬁcation and sequentiality. Tokens represent bytes, so accounting normalizes the cost of an I/O pattern to the equivalent amount of sequential I/O (e.g., 1 MB of random I/O may be counted as 10 MB). Under evaluation with QEMU, split-token is much better than the SCS scheduler at eliminating noisy I/O neighbour problems. The authors also evaluate split-token in an HDFS context:  To show that local split scheduling is a useful foundation to provide isolation in a distributed environment, we integrate HDFS with Split-Token to provide isolation to HDFS clients. We modify the client-to-worker protocol so workers know which account should be billed for disk I/O generated by the handling of a particular RPC call. Account information is propagated down to Split-Token and across to other workers (for pipelined writes)… We conclude that local scheduling can be used to meet distributed isolation goals; however, throttled applications may get worse-than-expected performance if the system is not well balanced. In conclusion…  While our experiments indicate that simple layering must be abandoned, we need not sacriﬁce modularity. In our split framework, the scheduler operates across all layers, but is still abstracted behind a collection of handlers. This approach is relatively clean, and enables pluggable scheduling. Supporting a new scheduling goal simply involves writing a new scheduler plug-in, not re-engineering the entire storage system. Our hope is that split-level scheduling will inspire future vertical integration in storage stacks. Our source code is available at  [url]", "pdf_url": "http://sigops.org/sosp/sosp15/current/2015-Monterey/printable/168-yang.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/split-level-io-scheduling.json"}
{"id": "74642936", "bin": "1200_1300", "summary_sentences": ["A design methodology for reliable software systems Liskov 1972  We’ve come to the end of Liskov’s list .", "The final paper is by Barbara Liskov herself, on the question of how best to go about designing software systems so that we can have some confidence they will work.", "The unfortunate fact is that the standard approach to building systems, involving extensive debugging, has not proved successful in producing reliable software, and there is no reason to suppose it ever will.", "So we’re going to need some testing, and for high levels of confidence we’ll need good coverage via:  a complete but minimal set of test cases, and  a system in which the set of relevant test cases is small, such that it is possible to generate every case  It is the system design which determines how many test cases there are and how easily they can be identified, the problems can be solved most effectively during the design process.”  And with that short introduction, the rest of the paper focuses on the questions of ‘What is a good system design?’ and ‘What process will help to ensure we produce one?’  A good system design is one where complexity is tamed by dividing it into modules (called ‘partitions’ in the paper, because the term module had already become very overloaded).", "As we’ve looked at previously , just dividing a system into modules isn’t enough though – it matters very much how you make those divisions.", "In fact,  …the division of a system into modules may introduce additional complexity… if modularity is viewed only as an aid to management, then any ad hoc modularization of the system is acceptable.", "However, the success of modularity depends directly on how well the modules are chosen.", "A good modularity is based on levels of abstraction, and uses structural programming within modules.", "Level of abstraction were first defined by Dijktsra.", "They provide a conceptual framework for achieving a clear and logical design for the system.", "The entire system is conceived as a hierarchy of levels, the lowest levels being those closest to the machine.", "There are two important rules given for levels of abstraction:  Each level has resources which it owns exclusively and which other levels are not permitted to access.", "Lower levels are not aware of the existence of higher levels and therefore may not refer to them in any way.", "With good modularity, the system is broken into a hierarchy of partitions (modules), with each partition representing one level of abstraction and consisting of one or more functions which share common resources.", "The connections between partitions are limited as follows:  Control connections are limited by the rules about the hierarchy of levels of abstraction  Connections in data passed between partitions are limited to the explicit arguments passed from the functions of one partition to the (external) functions of another partition.", "Implicit interaction on common data may only occur among functions within a partition.", "The combined activity of the functions in a partition support its abstraction and nothing more.", "The definition of connections in the above follows Parnas : “The connections between modules are the assumptions which the modules make about each other.”  We know what good modularity looks like when we see it now.", "But how do you arrive at good modularity in the first place?", "The traditional technique for modularization is to analyze the execution-time flow of the system and organize the system structure around each major sequential task.", "This technique leads to a structure which has very simple connections in control, but the connections in data tend to be complex.", "(See Parnas again).", "Select modules to support abstractions or concepts which you find helpful in thinking about the system….", "Abstraction is a very valuable aid to ordering complexity.", "Abstractions are introduced in order to make what the system is doing clearer and more understandable; an abstraction is a conceptual simplification because it expresses what is being done without specifying how it is done.", "What kinds of abstractions should we be on the lookout for?", "Abstractions of resources – modules that map the characteristics of an abstract resource into the real underlying resource or resources  Abstractions that hide data storage representations  Abstractions that limit information:  According to the third requirement for good modularizatio, the functions comprising a partition support only one abstraction and nothing more.", "Sometimes it is difficult to see that this restriction is being violated, or to recognize that the possibility for identification of another abstraction exists.", "One technique for simplification is to limit the amount of information which the functions in the partition need to know (or even have access to).", "One way to limit information is to introduce modules at a lower level, on which the higher-level module depends, which hide that knowledge.", "Abstractions that generalize a function or group of functions.", "“Separating such groups is a common technique in system implementation and is also useful for error avoidance, minimization of work, and standardization.”  Abstractions that encapsulate areas likely to change  The design process proceeds iteratively as follows.", "First determine an initial set of abstractions which represent the eventual system behaviour in a very general way.", "Then establish the data and flow of control connections among the partitions.", "The second phase occurs concurrently with the first; as abstractions are proposed, their utility and practicality are immediately investigated… A partition has been adequately investigated when its connections with the rest of the system are known and when the designers are confident that they understand exactly what its effect on the system will be.", "Varying depths of analysis will be necessary to achieve this confidence.", "When do you start programming modules?", "There is a tendency to think of this as the era of the strict waterfall, but that’s not what Liskov proposes:  It is not clear exactly how early structured programming of the system should begin… The best rule is probably to keep trying to write structured programs; failure will indicate that the system abstractions are not yet sufficiently understood and perhaps this exercise will shed some light on where more effort is needed or where other abstractions are required.", "Finally, a design can be considered ‘finished’ when the following criteria are met:  All major abstractions have been identified and partitions defined for them; the system resources have been distributed among the partitions and their positions in the hierararchy established.", "The system exists as a structured program… this consists of several components, but no component is likely to be completely defined.", "Rather each component is likely to use the names of lower-level components which are not yet defined.", "Sufficient information is available so that a skeleton of a user’s guide to the system could be written.", "(This was an era of much simpler user interfaces remember)."], "summary_text": "A design methodology for reliable software systems Liskov 1972  We’ve come to the end of Liskov’s list . The final paper is by Barbara Liskov herself, on the question of how best to go about designing software systems so that we can have some confidence they will work. The unfortunate fact is that the standard approach to building systems, involving extensive debugging, has not proved successful in producing reliable software, and there is no reason to suppose it ever will. So we’re going to need some testing, and for high levels of confidence we’ll need good coverage via:  a complete but minimal set of test cases, and  a system in which the set of relevant test cases is small, such that it is possible to generate every case  It is the system design which determines how many test cases there are and how easily they can be identified, the problems can be solved most effectively during the design process.”  And with that short introduction, the rest of the paper focuses on the questions of ‘What is a good system design?’ and ‘What process will help to ensure we produce one?’  A good system design is one where complexity is tamed by dividing it into modules (called ‘partitions’ in the paper, because the term module had already become very overloaded). As we’ve looked at previously , just dividing a system into modules isn’t enough though – it matters very much how you make those divisions. In fact,  …the division of a system into modules may introduce additional complexity… if modularity is viewed only as an aid to management, then any ad hoc modularization of the system is acceptable. However, the success of modularity depends directly on how well the modules are chosen. A good modularity is based on levels of abstraction, and uses structural programming within modules. Level of abstraction were first defined by Dijktsra. They provide a conceptual framework for achieving a clear and logical design for the system. The entire system is conceived as a hierarchy of levels, the lowest levels being those closest to the machine. There are two important rules given for levels of abstraction:  Each level has resources which it owns exclusively and which other levels are not permitted to access. Lower levels are not aware of the existence of higher levels and therefore may not refer to them in any way. With good modularity, the system is broken into a hierarchy of partitions (modules), with each partition representing one level of abstraction and consisting of one or more functions which share common resources. The connections between partitions are limited as follows:  Control connections are limited by the rules about the hierarchy of levels of abstraction  Connections in data passed between partitions are limited to the explicit arguments passed from the functions of one partition to the (external) functions of another partition. Implicit interaction on common data may only occur among functions within a partition. The combined activity of the functions in a partition support its abstraction and nothing more. The definition of connections in the above follows Parnas : “The connections between modules are the assumptions which the modules make about each other.”  We know what good modularity looks like when we see it now. But how do you arrive at good modularity in the first place? The traditional technique for modularization is to analyze the execution-time flow of the system and organize the system structure around each major sequential task. This technique leads to a structure which has very simple connections in control, but the connections in data tend to be complex. (See Parnas again). Select modules to support abstractions or concepts which you find helpful in thinking about the system…. Abstraction is a very valuable aid to ordering complexity. Abstractions are introduced in order to make what the system is doing clearer and more understandable; an abstraction is a conceptual simplification because it expresses what is being done without specifying how it is done. What kinds of abstractions should we be on the lookout for? Abstractions of resources – modules that map the characteristics of an abstract resource into the real underlying resource or resources  Abstractions that hide data storage representations  Abstractions that limit information:  According to the third requirement for good modularizatio, the functions comprising a partition support only one abstraction and nothing more. Sometimes it is difficult to see that this restriction is being violated, or to recognize that the possibility for identification of another abstraction exists. One technique for simplification is to limit the amount of information which the functions in the partition need to know (or even have access to). One way to limit information is to introduce modules at a lower level, on which the higher-level module depends, which hide that knowledge. Abstractions that generalize a function or group of functions. “Separating such groups is a common technique in system implementation and is also useful for error avoidance, minimization of work, and standardization.”  Abstractions that encapsulate areas likely to change  The design process proceeds iteratively as follows. First determine an initial set of abstractions which represent the eventual system behaviour in a very general way. Then establish the data and flow of control connections among the partitions. The second phase occurs concurrently with the first; as abstractions are proposed, their utility and practicality are immediately investigated… A partition has been adequately investigated when its connections with the rest of the system are known and when the designers are confident that they understand exactly what its effect on the system will be. Varying depths of analysis will be necessary to achieve this confidence. When do you start programming modules? There is a tendency to think of this as the era of the strict waterfall, but that’s not what Liskov proposes:  It is not clear exactly how early structured programming of the system should begin… The best rule is probably to keep trying to write structured programs; failure will indicate that the system abstractions are not yet sufficiently understood and perhaps this exercise will shed some light on where more effort is needed or where other abstractions are required. Finally, a design can be considered ‘finished’ when the following criteria are met:  All major abstractions have been identified and partitions defined for them; the system resources have been distributed among the partitions and their positions in the hierararchy established. The system exists as a structured program… this consists of several components, but no component is likely to be completely defined. Rather each component is likely to use the names of lower-level components which are not yet defined. Sufficient information is available so that a skeleton of a user’s guide to the system could be written. (This was an era of much simpler user interfaces remember).", "pdf_url": "https://valbonne-consulting.com/papers/classic/Liskov_72-Design_Methodology_for_Reliable_Software_Systems.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/a-design-methodology-for-reliable-software-systems.json"}
{"id": "57114772", "bin": "1300_1400", "summary_sentences": ["Snuba: automating weak supervision to label training data Varma & Ré, VLDB 2019  This week we’re moving on from ICML to start looking at some of the papers from VLDB 2019.", "VLDB is a huge conference, and once again I have a problem because my shortlist of “that looks really interesting, I’d love to read it” papers runs to 54 long at the moment!", "As a special bonus for me, I’m actually going to be at VLDB this year, where no doubt I’ll learn about even more interesting things!", "By the time you get to read this, it should be the first (workshop) day of the conference…  The conference may have changed, but to bridge from ICML to VLDB I’m going to start with a paper on very much the same theme as we’ve been dipping into over the past couple of weeks: how to combine and learn from multiple noisy sources of data and labels.", "Snuba is from the same Stanford line as Snorkel which we looked at last year.", "It’s tackling the same fundamental problem: how to gather enough labeled data to train a model, and how to effectively use it in a weak supervision setting (supervised learning with noisy labels).", "In Snorkel human experts write (noisy) labelling functions, aka heuristics, but in Snuba the system itself generates its own heuristics!", "Here’s the setup:  We have a small labeled dataset, but not big enough to learn an accurate classifier  We use the labeled dataset to learn classifiers that have good-enough accuracy over subsets of the data (features)  We use the learned classifiers to predict labels for a much larger unlabelled dataset  We train a final model on the now noisily labelled large dataset, and this model shows increased performance  It took me quite a while to get my head around this!", "We don’t have enough labeled data to learn a good classifier, but we end up learning a good classifier anyway.", "What magic is this?", "The secret is in the selection, application, and aggregation of those intermediate learned heuristics.", "Snuba automatically generates heuristics that each labels the subset of the data it is accurate for, and iteratively repeats this process until the heuristics together label a large portion of the unlabeled data… Users from research labs, hospitals and industry helped us design Snuba such that it outperforms user-defined heuristics and crowdsourced labels by up to 9.74 F1 point and 13.80 F1 points in terms of end model performance.", "Compared to Snorkel of course, “the key challenge in automating weak supervision lies in replacing the human reasoning that drives heuristic development.”  The big picture  Snuba has three main components: the synthesiser, the pruner, and the verifier.", "It maintains a committed set of heuristics which will be used in labelling, and in each iteration it synthesises new candidate heuristics, selects from among them to add to the committed set, and then verifies the results.", "The synthesiser uses the small labelled dataset to generate new candidate heuristics.", "It associates each heuristic with a labelling pattern to assign labels only where the heuristic has high confidence.", "Allowing heuristics to abstain from labelling datapoints where they have low confidence is a key part of how Snuba achieves its final performance.", "Compared to noisily labelling everything the result is a smaller labelled dataset once the heuristic has been applied to the unlabelled data, but with higher confidence.", "Not every candidate heuristic generated by the synthesiser ends up being used.", "It’s the job of the pruner to select a heuristic to add from among them.", "We want the heuristics in the committed set to be diverse in terms of the datapoints in the unlabeled set they label, but also ensure that it performs well for the datapoints it labels in the labeled dataset.", "A diverse heuristic is defined as one that labels points that have never received a label from any other heuristics.", "The pruner selects for diversity by measuring the Jaccard distance 1 between the set of datapoints labelled by a candidate heuristic and the set of datapoints labelled so far.", "The verifier uses a generative model to learn the accuracies of the heuristics in the committed set and produce a single probabilistic training label for each data point in the unlabeled dataset.", "Generative models are a popular approach to learn and model the accuracies of different labeling sources like user-defined heuristics and knowledge bases when data is labeled by a variety of sources.", "The generative model assumes that each heuristic performs better than random.", "For user-defined heuristics that’s a reasonable assumption, but with machine-generated heuristics that assumption could be violated.", "Snuba use the small labeled dataset to indirectly determine whether the generated heuristics are likely to be worse than random on the unlabeled dataset.", "Snuba keeps iterating (adding new heuristics) until the estimate of the accuracy of the new heuristic suggests it performs worse than random.", "Supported heuristic models  Users can plug in their own heuristic models for heuristic generation, all that’s required is that the model generates heuristics that output a probabilistic label over a subset of the data.", "Snuba comes with three different models out of the box:  Decision stumps are decision trees limited to a certain depth  Logistic regressors learn a single linear decision boundary  K-nearest neighbour relies on the distribution of the labeled datapoints to decide decision boundaries, with confidence based on the distance of an unlabelled point from a cluster.", "Why does the end model performed better than a simple ensemble of the selected heuristics?", "At the end of this process, we have an aggregation of heuristics that assign probabilistic labels to a large portion of the unlabelled dataset.", "Why not just use this aggregation as the final model??", "One of the motivations for designing Snuba is to efficiently label enough training data for training powerful, downstream machine learning models like neural networks.", "Heuristics from Snuba are not used directly for the classification task at hand because (1) they may not label the entire dataset due to abstentions, and (2) they are based only on the user-defined primitives and fail to take advantage of the raw data representation.", "For example, heuristics may be based on features such as measurements of a tumor (images), bag-of-words representations (text), or bounding box coordinates.", "An end model can operate over the entire raw image, sentence, or representation.", "Evaluation  The evaluation demonstrates the following:  Training labels from Snuba outperform labels from automated baseline methods by up to 14.35 F1 points, and to transfer learning from the small labelled dataset by up to 5.74 F1 points  Training labels from Snuba outperform those from user-developed heuristics by up to 9.74 F1 points.", "The heuristics developed by users have very high precision, but Snuba claws back its advantage by improving recall through its diversity measures.", "Each component of Snuba plays its part in boosting overall system performance.", "Experiments are conducted over a variety of different applications and datasets:  In image-based tasks Snuba generated heuristics that used at most 4 primitives, while for text-based tasks it generated heuristics that relied on only a single primitive.", "Our work suggests that there is potential to use a small amount of labeled data to make the process of generating training labels much more efficient.", "The Jaccard distance between two sets A and B is given by  .", "↩"], "summary_text": "Snuba: automating weak supervision to label training data Varma & Ré, VLDB 2019  This week we’re moving on from ICML to start looking at some of the papers from VLDB 2019. VLDB is a huge conference, and once again I have a problem because my shortlist of “that looks really interesting, I’d love to read it” papers runs to 54 long at the moment! As a special bonus for me, I’m actually going to be at VLDB this year, where no doubt I’ll learn about even more interesting things! By the time you get to read this, it should be the first (workshop) day of the conference…  The conference may have changed, but to bridge from ICML to VLDB I’m going to start with a paper on very much the same theme as we’ve been dipping into over the past couple of weeks: how to combine and learn from multiple noisy sources of data and labels. Snuba is from the same Stanford line as Snorkel which we looked at last year. It’s tackling the same fundamental problem: how to gather enough labeled data to train a model, and how to effectively use it in a weak supervision setting (supervised learning with noisy labels). In Snorkel human experts write (noisy) labelling functions, aka heuristics, but in Snuba the system itself generates its own heuristics! Here’s the setup:  We have a small labeled dataset, but not big enough to learn an accurate classifier  We use the labeled dataset to learn classifiers that have good-enough accuracy over subsets of the data (features)  We use the learned classifiers to predict labels for a much larger unlabelled dataset  We train a final model on the now noisily labelled large dataset, and this model shows increased performance  It took me quite a while to get my head around this! We don’t have enough labeled data to learn a good classifier, but we end up learning a good classifier anyway. What magic is this? The secret is in the selection, application, and aggregation of those intermediate learned heuristics. Snuba automatically generates heuristics that each labels the subset of the data it is accurate for, and iteratively repeats this process until the heuristics together label a large portion of the unlabeled data… Users from research labs, hospitals and industry helped us design Snuba such that it outperforms user-defined heuristics and crowdsourced labels by up to 9.74 F1 point and 13.80 F1 points in terms of end model performance. Compared to Snorkel of course, “the key challenge in automating weak supervision lies in replacing the human reasoning that drives heuristic development.”  The big picture  Snuba has three main components: the synthesiser, the pruner, and the verifier. It maintains a committed set of heuristics which will be used in labelling, and in each iteration it synthesises new candidate heuristics, selects from among them to add to the committed set, and then verifies the results. The synthesiser uses the small labelled dataset to generate new candidate heuristics. It associates each heuristic with a labelling pattern to assign labels only where the heuristic has high confidence. Allowing heuristics to abstain from labelling datapoints where they have low confidence is a key part of how Snuba achieves its final performance. Compared to noisily labelling everything the result is a smaller labelled dataset once the heuristic has been applied to the unlabelled data, but with higher confidence. Not every candidate heuristic generated by the synthesiser ends up being used. It’s the job of the pruner to select a heuristic to add from among them. We want the heuristics in the committed set to be diverse in terms of the datapoints in the unlabeled set they label, but also ensure that it performs well for the datapoints it labels in the labeled dataset. A diverse heuristic is defined as one that labels points that have never received a label from any other heuristics. The pruner selects for diversity by measuring the Jaccard distance 1 between the set of datapoints labelled by a candidate heuristic and the set of datapoints labelled so far. The verifier uses a generative model to learn the accuracies of the heuristics in the committed set and produce a single probabilistic training label for each data point in the unlabeled dataset. Generative models are a popular approach to learn and model the accuracies of different labeling sources like user-defined heuristics and knowledge bases when data is labeled by a variety of sources. The generative model assumes that each heuristic performs better than random. For user-defined heuristics that’s a reasonable assumption, but with machine-generated heuristics that assumption could be violated. Snuba use the small labeled dataset to indirectly determine whether the generated heuristics are likely to be worse than random on the unlabeled dataset. Snuba keeps iterating (adding new heuristics) until the estimate of the accuracy of the new heuristic suggests it performs worse than random. Supported heuristic models  Users can plug in their own heuristic models for heuristic generation, all that’s required is that the model generates heuristics that output a probabilistic label over a subset of the data. Snuba comes with three different models out of the box:  Decision stumps are decision trees limited to a certain depth  Logistic regressors learn a single linear decision boundary  K-nearest neighbour relies on the distribution of the labeled datapoints to decide decision boundaries, with confidence based on the distance of an unlabelled point from a cluster. Why does the end model performed better than a simple ensemble of the selected heuristics? At the end of this process, we have an aggregation of heuristics that assign probabilistic labels to a large portion of the unlabelled dataset. Why not just use this aggregation as the final model?? One of the motivations for designing Snuba is to efficiently label enough training data for training powerful, downstream machine learning models like neural networks. Heuristics from Snuba are not used directly for the classification task at hand because (1) they may not label the entire dataset due to abstentions, and (2) they are based only on the user-defined primitives and fail to take advantage of the raw data representation. For example, heuristics may be based on features such as measurements of a tumor (images), bag-of-words representations (text), or bounding box coordinates. An end model can operate over the entire raw image, sentence, or representation. Evaluation  The evaluation demonstrates the following:  Training labels from Snuba outperform labels from automated baseline methods by up to 14.35 F1 points, and to transfer learning from the small labelled dataset by up to 5.74 F1 points  Training labels from Snuba outperform those from user-developed heuristics by up to 9.74 F1 points. The heuristics developed by users have very high precision, but Snuba claws back its advantage by improving recall through its diversity measures. Each component of Snuba plays its part in boosting overall system performance. Experiments are conducted over a variety of different applications and datasets:  In image-based tasks Snuba generated heuristics that used at most 4 primitives, while for text-based tasks it generated heuristics that relied on only a single primitive. Our work suggests that there is potential to use a small amount of labeled data to make the process of generating training labels much more efficient. The Jaccard distance between two sets A and B is given by  . ↩", "pdf_url": "http://www.vldb.org/pvldb/vol12/p223-varma.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/snuba.json"}
{"id": "97381495", "bin": "1300_1400", "summary_sentences": ["Prudent Engineering Practice for Cryptographic Protocols – Abadi & Needham, 1994  Prudent engineering practice for cryptographic protocols for most of us is not to design cryptographic protocols!", "Today’s paper serves to highlight how even the experts can get it wrong, and presents 11 design principles for cryptographic protocols – some of which may be useful in the design of other kinds of protocols too.", "We present principles for the design of cryptographic protocols.", "The principles are not necessary for correctness, nor are they sufficient.", "They are however helpful, in that adherence to them  would have contributed to the simplicity of protocols and avoided a considerable number of published confusions and mistakes.", "We arrived at our principles by noticing some common features among protocols that are difficult to analyze.", "If these features are avoided, it becomes less necessary to resort to formal tools- and also easier to do so if there is good reason to.", "The principles themselves are informal guidelines, and useful independently of any logic.", "The authors observe that formal techniques can help to prove a design is correct, but give you no guidance in coming up with a correct design in the first place.", "A protocol in this paper is simply defined to be a set of rules or conventions defining an exchange of messages among a set of two or more partners.", "Of the 11 principles discussed in the paper, the first two are set above the rest as overarching principles.", "Principle 1  Every message should say what it means:  the interpretation of the message should depend only on its content.", "It should be possible to write down a straightforward English  sentence describing the content-though if  there is a suitable formalism available that  is good too.", "In other words, there is to be no implied context in which the message is to be evaluated.", "Principle 2  The conditions for a message to be acted upon should be clearly set out so that someone reviewing a design may see whether they are acceptable or not.", "For a message to be acted on, you have to trust it.", "What does trust mean?", "You should at least clearly set out what needs to be trusted in order to act on a message.", "Principle 3  If the identity of a principal is essential to  the meaning of a message, it is prudent to  mention the principal’s name explicitly in  the message.", "This follows from the first principle.", "It is ‘obvious and simple, but commonly ignored.’  Principle 4  Be clear as to why encryption is being done.", "Encryption is not wholly cheap, and not asking precisely why it is being done can lead to redundancy.", "Encryption is not synonymous  with security, and its improper use can lead  to errors.", "There are at least four different reasons the authors cite for why a protocol may be using encryption in a particular step.", "Protocol authors should be clear what the intended purpose is at each stage.", "Is it:  To preserve confidentiality?", "To guarantee authenticity (that a message came from a particular sender)?", "To bind together the parts of a message?", "While encryption guarantees confidentiality  and authenticity, it also serves in binding  together the parts of a message: receiving { X ,Y }K is not always the same as receiving { X }K and { Y }K.  When encryption is  used only to bind parts of a message, signature is sufficient.", "The meaning attached  to this binding is rather protocol-dependent,  and often subtle.", "To produce random numbers?", "Principle 5  Don’t confuse the fact that a party signed an encrypted message, with the fact that a party knows the content of an encrypted message…  When a principal signs material that has already been encrypted, it should not be inferred that the principal knows the content  of the message.", "On the other hand, it is  proper to infer that the principal that signs  a message and then encrypts it for privacy  knows the content of the message.", "Principle 6  Be clear what properties you are assuming  about nonces.", "What may do for ensuring  temporal succession may not do for ensuring  association-and perhaps association is best  established by other means.", "Nonces are often used to avoid message replay attacks, as part of a challenge-response exchange.", "“A message is sent which leads to a reply  which could only have been produced in knowl-edge of the first message.", "The objective is to  guarantee that the second message is made after  the first was sent, and sometimes to bind the two together.", "There is sometimes confusion about nonces-are they guaranteed new, random, unpredictable?”  It is not necessary for nonces to be unpredictable (you could use a counter).", "However, predictable nonces should be used with caution.", "This leads to principle 7…  Principle 7  The use of a predictable quantity (such as the value of a counter) can serve in  guaranteeing newness, through a challenge- response exchange.", "But if a predictable  quantity is to be effective, it should be protected so that an intruder cannot simulate a  challenge and later replay a response.", "Principle 8  If timestamps are used as freshness guarantees by reference to absolute time, then  the difference between local clocks at various machines must be much less than the  allowable age of a message deemed to be  valid.", "Furthermore, the time maintenance  mechanism everywhere becomes part of the  trusted computing base.", "Principle 9  A key may have been used recently, for ex-ample to encrypt a nonce, yet be quite old,  and possibly compromised.", "Recent use does  not make the key look any better than it  would otherwise.", "In other words, don’t confuse recency of use with recency of generation.", "Principle 10  If an encoding is used to present the meaning of a message, then it should be possible  to tell which encoding is being used.", "In the  common case where the encoding is protocol  dependent, it should be possible to deduce  that the message belongs to this protocol,  and in fact to a particular run of the protocol, and to know its number in the protocol.", "If you choose to use a compact encoding, make sure there is no loss of information and no chance of confusing messages from different instances of the protocol.", "Principle 11  The final principle builds on the second one:  The protocol designer should know which trust relations his protocol depends on, and  why the dependence is necessary.", "The reasons for particular trust relations being acceptable should be explicit though they will  be founded on judgement and policy rather  than on logic.", "An example is given with respect to timestamps:  The use of timestamps makes explicit for the first  time a question of trust.", "When can a principal  A rely on another principal B putting a correct  timestamp in a message?", "The answer usually  given is that this is acceptable if A trusts B in  relation to timestamps…  Which leads to the general rule…  We may simply say that A trusts  B in regard to some function if a loss of security  to A could follow from B not behaving in the  specified way; it is usually difficult or impossible  for A to verify B’s good behavior.", "There is some measure of trust involved whenever one principal acts on the content of a message from another.", "It is essential that this trust  be properly understood.", "See the full paper for a more detailed exposition of the principles, and for examples in each case of protocols that violated the principle and hence ended up in trouble."], "summary_text": "Prudent Engineering Practice for Cryptographic Protocols – Abadi & Needham, 1994  Prudent engineering practice for cryptographic protocols for most of us is not to design cryptographic protocols! Today’s paper serves to highlight how even the experts can get it wrong, and presents 11 design principles for cryptographic protocols – some of which may be useful in the design of other kinds of protocols too. We present principles for the design of cryptographic protocols. The principles are not necessary for correctness, nor are they sufficient. They are however helpful, in that adherence to them  would have contributed to the simplicity of protocols and avoided a considerable number of published confusions and mistakes. We arrived at our principles by noticing some common features among protocols that are difficult to analyze. If these features are avoided, it becomes less necessary to resort to formal tools- and also easier to do so if there is good reason to. The principles themselves are informal guidelines, and useful independently of any logic. The authors observe that formal techniques can help to prove a design is correct, but give you no guidance in coming up with a correct design in the first place. A protocol in this paper is simply defined to be a set of rules or conventions defining an exchange of messages among a set of two or more partners. Of the 11 principles discussed in the paper, the first two are set above the rest as overarching principles. Principle 1  Every message should say what it means:  the interpretation of the message should depend only on its content. It should be possible to write down a straightforward English  sentence describing the content-though if  there is a suitable formalism available that  is good too. In other words, there is to be no implied context in which the message is to be evaluated. Principle 2  The conditions for a message to be acted upon should be clearly set out so that someone reviewing a design may see whether they are acceptable or not. For a message to be acted on, you have to trust it. What does trust mean? You should at least clearly set out what needs to be trusted in order to act on a message. Principle 3  If the identity of a principal is essential to  the meaning of a message, it is prudent to  mention the principal’s name explicitly in  the message. This follows from the first principle. It is ‘obvious and simple, but commonly ignored.’  Principle 4  Be clear as to why encryption is being done. Encryption is not wholly cheap, and not asking precisely why it is being done can lead to redundancy. Encryption is not synonymous  with security, and its improper use can lead  to errors. There are at least four different reasons the authors cite for why a protocol may be using encryption in a particular step. Protocol authors should be clear what the intended purpose is at each stage. Is it:  To preserve confidentiality? To guarantee authenticity (that a message came from a particular sender)? To bind together the parts of a message? While encryption guarantees confidentiality  and authenticity, it also serves in binding  together the parts of a message: receiving { X ,Y }K is not always the same as receiving { X }K and { Y }K.  When encryption is  used only to bind parts of a message, signature is sufficient. The meaning attached  to this binding is rather protocol-dependent,  and often subtle. To produce random numbers? Principle 5  Don’t confuse the fact that a party signed an encrypted message, with the fact that a party knows the content of an encrypted message…  When a principal signs material that has already been encrypted, it should not be inferred that the principal knows the content  of the message. On the other hand, it is  proper to infer that the principal that signs  a message and then encrypts it for privacy  knows the content of the message. Principle 6  Be clear what properties you are assuming  about nonces. What may do for ensuring  temporal succession may not do for ensuring  association-and perhaps association is best  established by other means. Nonces are often used to avoid message replay attacks, as part of a challenge-response exchange. “A message is sent which leads to a reply  which could only have been produced in knowl-edge of the first message. The objective is to  guarantee that the second message is made after  the first was sent, and sometimes to bind the two together. There is sometimes confusion about nonces-are they guaranteed new, random, unpredictable?”  It is not necessary for nonces to be unpredictable (you could use a counter). However, predictable nonces should be used with caution. This leads to principle 7…  Principle 7  The use of a predictable quantity (such as the value of a counter) can serve in  guaranteeing newness, through a challenge- response exchange. But if a predictable  quantity is to be effective, it should be protected so that an intruder cannot simulate a  challenge and later replay a response. Principle 8  If timestamps are used as freshness guarantees by reference to absolute time, then  the difference between local clocks at various machines must be much less than the  allowable age of a message deemed to be  valid. Furthermore, the time maintenance  mechanism everywhere becomes part of the  trusted computing base. Principle 9  A key may have been used recently, for ex-ample to encrypt a nonce, yet be quite old,  and possibly compromised. Recent use does  not make the key look any better than it  would otherwise. In other words, don’t confuse recency of use with recency of generation. Principle 10  If an encoding is used to present the meaning of a message, then it should be possible  to tell which encoding is being used. In the  common case where the encoding is protocol  dependent, it should be possible to deduce  that the message belongs to this protocol,  and in fact to a particular run of the protocol, and to know its number in the protocol. If you choose to use a compact encoding, make sure there is no loss of information and no chance of confusing messages from different instances of the protocol. Principle 11  The final principle builds on the second one:  The protocol designer should know which trust relations his protocol depends on, and  why the dependence is necessary. The reasons for particular trust relations being acceptable should be explicit though they will  be founded on judgement and policy rather  than on logic. An example is given with respect to timestamps:  The use of timestamps makes explicit for the first  time a question of trust. When can a principal  A rely on another principal B putting a correct  timestamp in a message? The answer usually  given is that this is acceptable if A trusts B in  relation to timestamps…  Which leads to the general rule…  We may simply say that A trusts  B in regard to some function if a loss of security  to A could follow from B not behaving in the  specified way; it is usually difficult or impossible  for A to verify B’s good behavior. There is some measure of trust involved whenever one principal acts on the content of a message from another. It is essential that this trust  be properly understood. See the full paper for a more detailed exposition of the principles, and for examples in each case of protocols that violated the principle and hence ended up in trouble.", "pdf_url": "http://www.cs.utexas.edu/~shmat/courses/cs6431/prudent.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/prudent-cryptography.json"}
{"id": "84480736", "bin": "1400_1500", "summary_sentences": ["Protecting user privacy: an approach for untraceable web browsing history and unambiguous user profiles Beigi et al., WSDM’19  Maybe you’re reading this post online at The Morning Paper , and you came here by clicking a link in your Twitter feed because you follow my paper write-up announcements there .", "It might even be that you fairly frequently visit paper write-ups on The Morning Paper.", "And perhaps there are several other people you follow who also post links that appear in your Twitter feed, and occasionally you click on those links too.", "Given your ‘anonymous’ browsing history I could probably infer that you’re likely to be one of the 20K+ wonderful people with a wide-ranging interest in computer science and the concentration powers needed to follow longer write-ups that follow me on Twitter.", "You’re awesome, thank you!", "Tying other links in the browsing history to other social profiles that have promoted them, we might be able to work out who else our mystery browser probably follows on social media.", "It won’t be long before you’ve been re-identified from your browsing history.", "And that means everything else in that history can be tied back to you too.", "(See ‘ De-anonymizing web browsing data with social networks ’).", "In addition to the web browser, users’ browsing histories are recorded via third-party trackers embedded on web pages to help improve online advertising and web surfing experience.", "Moreover, Internet Service Providers (ISPs) such as AT&T and Verizon, have full access to individuals’ web browsing histories… FCC’s Internet privacy protection has also been removed in late March of 2017.", "This new legislation allows ISPs to monitor, collect, share and sell their customer’s behavior online such as detailed Web browsing histories without their consent and any anonymization.", "(Thank goodness for the GDPR in Europe!)", "Given the lack of protection, it’s now up to individual users to protect their browsing history.", "It’s not possible to remove entries from the history, so the only thing we can do is add noise.", "Adding noise may make it harder to de-anonymise a profile, and it also gives plausible deniability that you actually visited any given link that appears in the history.", "This is a tough one.", "I don’t know about you, but I feel a little uncomfortable with the thought of random links (for who knows what content!)", "appearing in my browsing history.", "Try the opposite thought experiment though, and imagine e.g. a browser plugin that posts every URL you visit to your social media feed.", "I don’t think many people would be comfortable with that either!", "The second issue with polluting your history is that some of the personalisation that goes on is actually useful.", "So you might start seeing content that is less relevant and interesting to you.", "At the heart of today’s paper is an exploration of this privacy-utility trade-off through a system called PBooster.", "In this paper, we aim to study the following problem: how many links and what links should be added to a user’s browsing history to boost user privacy while retaining high utility.", "For privacy without caring about utility, just adding random links is the best strategy (though PBooster gets close).", "The random link strategy seriously degrades the utility of the history though (we’ll get into how that’s measured next), whereas PBooster is able to retain much more utility.", "Measuring privacy and utility  From a privacy perspective, the more uniformly distributed a history is over a set of topics, the harder it will be to infer which topics are of true interest.", "Thus,  … we leverage the entropy of the user’s browsing history distribution over a set of pre-defined topics as a measure of privacy.", "Given some set of m topics, we assign each link in the browsing history to one of those topics, sum counts when grouping by topic, and then normalise them to give the topic probability distribution.", "Using j to range over topics,  to represent the topic probability distribution for a user  , and  to represent the topic probability for user  and topic  , then have have:  The higher this metric is, the greater the privacy.", "For utility, we want the opposite: a similar distribution between the modified history,  , and the true history  .", "For some similarity function sim, the formula used is:  For this work cosine similarity is used:  Navigating the privacy-utility trade-off with PBooster  PBooster seeks to optimise an objective function  , where  controls the desired privacy/utility trade-off.", "It is worthwhile to mention that the search space for this problem is exponential to  , where N is the maximum number of links with respect to a topic.", "That’s about as bad as it gets!", "The solution is to break the search down into two subproblems: first select a subset of topics, and calculate how many links should be added to each (the topic selection phase, and then select the links for each topic (link selection phase).", "An analysis in section 4.4 of the paper shows that the topic selection process is also NP-hard, but there exists a greedy local search algorithm which can achieve at least 1/3 of the value of the optimal solution.", "The algorithm alternates between incrementing the link counts of topics and decrementing them, until no further improvement is possible.", "We emphasize that according to [13], there is no efficient algorithm which could select the best set of links to maximize aggregation of both privacy and utility in polynomial time… the proposed greedy algorithm can select a set with a lower bound of 1/3 of the optimal solution, providing the maximum user privacy and utility in polynomial time.", "When it comes to selecting actual links to hit the desired topic counts, PBooster needs access to the user’s friend list on social media.", "It randomly selects a social media profile from outside of your friend list, and simulates a browsing history from that profile for links in the desired topic areas.", "If the chosen social media profile does not contain enough such links, another profile is chosen at random until all link count targets are hit.", "Evaluation  The evaluation compares PBooster against a strategy of adding the total number of links recommended by the topic selection phase but at random with no consideration for topics, a ‘JustFriends’ strategy that adds links from the profiles of friends during the link selection phase, and an ‘ISPPolluter’ strategy that adds 20,000 links randomly to the history.", "As we saw previously, since random strategies pay no attention to utility, they can achieve the best overall privacy:  Interestingly, ISPPolluter turned out not to work well in practice as a defense against attacks that leverage information from social media feeds.", "For intuition, consider that even adding 20,000 random links, a history with say 50 visits to The Morning Paper is still going to be pretty unique (that is very unlikely to happen by chance).", "Whereas when links are added from a social media profile, this kind of behaviour is generated by design.", "However, as I understand it the end result of using PBooster will be a history that instead of containing influences from n social media profiles, will now contain influences from n+m profiles, where m is the number of profiles used during the link selection phase.", "The measure used for privacy only looks at topic distribution, not the end number of influencing profiles.", "I’d really like to see some analysis of this aspect (for example, if m is small compared to n, then the user is probably still very identifiable, so how should we think about m as it relates to n?).", "When it comes to balancing the trade-off between utility and privacy as measured though, PBooster unsurprisingly comes out best.", "Our experiments demonstrate the efficiency of the proposed model by increasing user privacy and preserving utility of browsing history for future applications."], "summary_text": "Protecting user privacy: an approach for untraceable web browsing history and unambiguous user profiles Beigi et al., WSDM’19  Maybe you’re reading this post online at The Morning Paper , and you came here by clicking a link in your Twitter feed because you follow my paper write-up announcements there . It might even be that you fairly frequently visit paper write-ups on The Morning Paper. And perhaps there are several other people you follow who also post links that appear in your Twitter feed, and occasionally you click on those links too. Given your ‘anonymous’ browsing history I could probably infer that you’re likely to be one of the 20K+ wonderful people with a wide-ranging interest in computer science and the concentration powers needed to follow longer write-ups that follow me on Twitter. You’re awesome, thank you! Tying other links in the browsing history to other social profiles that have promoted them, we might be able to work out who else our mystery browser probably follows on social media. It won’t be long before you’ve been re-identified from your browsing history. And that means everything else in that history can be tied back to you too. (See ‘ De-anonymizing web browsing data with social networks ’). In addition to the web browser, users’ browsing histories are recorded via third-party trackers embedded on web pages to help improve online advertising and web surfing experience. Moreover, Internet Service Providers (ISPs) such as AT&T and Verizon, have full access to individuals’ web browsing histories… FCC’s Internet privacy protection has also been removed in late March of 2017. This new legislation allows ISPs to monitor, collect, share and sell their customer’s behavior online such as detailed Web browsing histories without their consent and any anonymization. (Thank goodness for the GDPR in Europe!) Given the lack of protection, it’s now up to individual users to protect their browsing history. It’s not possible to remove entries from the history, so the only thing we can do is add noise. Adding noise may make it harder to de-anonymise a profile, and it also gives plausible deniability that you actually visited any given link that appears in the history. This is a tough one. I don’t know about you, but I feel a little uncomfortable with the thought of random links (for who knows what content!) appearing in my browsing history. Try the opposite thought experiment though, and imagine e.g. a browser plugin that posts every URL you visit to your social media feed. I don’t think many people would be comfortable with that either! The second issue with polluting your history is that some of the personalisation that goes on is actually useful. So you might start seeing content that is less relevant and interesting to you. At the heart of today’s paper is an exploration of this privacy-utility trade-off through a system called PBooster. In this paper, we aim to study the following problem: how many links and what links should be added to a user’s browsing history to boost user privacy while retaining high utility. For privacy without caring about utility, just adding random links is the best strategy (though PBooster gets close). The random link strategy seriously degrades the utility of the history though (we’ll get into how that’s measured next), whereas PBooster is able to retain much more utility. Measuring privacy and utility  From a privacy perspective, the more uniformly distributed a history is over a set of topics, the harder it will be to infer which topics are of true interest. Thus,  … we leverage the entropy of the user’s browsing history distribution over a set of pre-defined topics as a measure of privacy. Given some set of m topics, we assign each link in the browsing history to one of those topics, sum counts when grouping by topic, and then normalise them to give the topic probability distribution. Using j to range over topics,  to represent the topic probability distribution for a user  , and  to represent the topic probability for user  and topic  , then have have:  The higher this metric is, the greater the privacy. For utility, we want the opposite: a similar distribution between the modified history,  , and the true history  . For some similarity function sim, the formula used is:  For this work cosine similarity is used:  Navigating the privacy-utility trade-off with PBooster  PBooster seeks to optimise an objective function  , where  controls the desired privacy/utility trade-off. It is worthwhile to mention that the search space for this problem is exponential to  , where N is the maximum number of links with respect to a topic. That’s about as bad as it gets! The solution is to break the search down into two subproblems: first select a subset of topics, and calculate how many links should be added to each (the topic selection phase, and then select the links for each topic (link selection phase). An analysis in section 4.4 of the paper shows that the topic selection process is also NP-hard, but there exists a greedy local search algorithm which can achieve at least 1/3 of the value of the optimal solution. The algorithm alternates between incrementing the link counts of topics and decrementing them, until no further improvement is possible. We emphasize that according to [13], there is no efficient algorithm which could select the best set of links to maximize aggregation of both privacy and utility in polynomial time… the proposed greedy algorithm can select a set with a lower bound of 1/3 of the optimal solution, providing the maximum user privacy and utility in polynomial time. When it comes to selecting actual links to hit the desired topic counts, PBooster needs access to the user’s friend list on social media. It randomly selects a social media profile from outside of your friend list, and simulates a browsing history from that profile for links in the desired topic areas. If the chosen social media profile does not contain enough such links, another profile is chosen at random until all link count targets are hit. Evaluation  The evaluation compares PBooster against a strategy of adding the total number of links recommended by the topic selection phase but at random with no consideration for topics, a ‘JustFriends’ strategy that adds links from the profiles of friends during the link selection phase, and an ‘ISPPolluter’ strategy that adds 20,000 links randomly to the history. As we saw previously, since random strategies pay no attention to utility, they can achieve the best overall privacy:  Interestingly, ISPPolluter turned out not to work well in practice as a defense against attacks that leverage information from social media feeds. For intuition, consider that even adding 20,000 random links, a history with say 50 visits to The Morning Paper is still going to be pretty unique (that is very unlikely to happen by chance). Whereas when links are added from a social media profile, this kind of behaviour is generated by design. However, as I understand it the end result of using PBooster will be a history that instead of containing influences from n social media profiles, will now contain influences from n+m profiles, where m is the number of profiles used during the link selection phase. The measure used for privacy only looks at topic distribution, not the end number of influencing profiles. I’d really like to see some analysis of this aspect (for example, if m is small compared to n, then the user is probably still very identifiable, so how should we think about m as it relates to n?). When it comes to balancing the trade-off between utility and privacy as measured though, PBooster unsurprisingly comes out best. Our experiments demonstrate the efficiency of the proposed model by increasing user privacy and preserving utility of browsing history for future applications.", "pdf_url": "https://arxiv.org/pdf/1811.09340", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/protecting-user-privacy-an-approach-for-untraceable-web-browsing-history-and-unambiguous-user-profiles.json"}
{"id": "96747724", "bin": "1400_1500", "summary_sentences": ["“Why Should I Trust You?", "Explaining the Predictions of Any Classifier Ribeiro et al., KDD 2016  You’ve trained a classifier and it’s performing well on the validation set – but does the model exhibit sound judgement or is it making decisions based on spurious criteria?", "Can we trust the model in the real world?", "And can we trust a prediction (classification) it makes well enough to act on it?", "Can we explain why the model made the decision it did, even if the inner workings of the model are not easily understandable by humans?", "These are the questions that Ribeiro et al. pose in this paper, and they answer them by building LIME – an algorithm to explain the predictions of any classifier, and SP-LIME, a method for building trust in the predictions of a model overall.", "Another really nice result is that by explaining to a human how the model made a certain prediction, the human is able to give feedback on whether the reasoning is ‘sound’ and suggest features to remove from the model – this leads to classifiers that generalize much better to real world data.", "Consider two classifiers (Algorithm 1 and Algorithm 2 in the figure below) both trained to determine whether a document is about Christianity or atheism.", "Algorithm 2 performs much better in hold-out tests, but when we see why it is making its decisions, we realise it is actually much worse…  Magenta words are those contributing to the atheism class, green for Christianity.", "The second algorithm is basing its decision on “Posting”, “Host”, “Re” and “nntp” – words that have no connection to either Christianity or atheism, but happen to feature heavily in the headers of newsgroup postings about atheism in the training set.", "What makes a good explanation?", "It must be easily understandable by a human!", "For example, if hundreds or thousands of features significantly contribute to a prediction, it is not reasonable to expect any user to comprehend why the prediction was made, even if individual weights can be inspected.", "And it must meaningfully connect input variables to the response:  ..which is not necessarily tue of the features used by the model, and thus the “input variables” in the explanation may need to be different than the features.", "Furthermore, an explanation must have local fidelity: it should correspond to how the model behaves in the vicinity of the instance being predicted.", "The ideal explainer, should also be able to explain any model, and thus be model-agnostic.", "A key insight – local interpretation  Creating a globally faithful interpreter of a model’s decisions might require a complete description of the model itself.", "But to explain an individual decision we only need to understand how it behaves in a small local region.", "The idea reminds me a little bit of differentiation – overall the shape of the curve may be very complex, but if we look at just a small part we can figure out the gradient in that region.", "Here’s a toy example from the paper – the true decision boundary in the model is represented by the blue/pink background.", "In the immediate vicinity of the decision (the bold red cross) though we can learn a much simpler explanation that is locally faithful even if not globally faithful.", "The LIME algorithm produces Local Interpretable Model-agnostic Explanations.", "The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to the classifier.", "For text classification, an interpretable representation could be a vector indicating the presence or absence of a word, even though the classifier may use more complex word embeddings.", "For image classification an interpretable representation might be an binary vector indicating the ‘presence’ or ‘absence’ of a contiguous patch of similar pixels.", "LIME works by drawing samples in the vicinity of the input to be explained and learning a linear classifier using locally weighted square loss, with a limit K set on the number of interpretable features.", "Since [the algorithm] produces an explanation for an individual prediction, its complexity does not depend on the size of the dataset, but instead on time to compute f(x) [a model prediction] and on the number of samples N. In practice, explaining random forests with 1000 trees using scikit-learn on a laptop with N = 5000 takes under 3 seconds without any optimizations such as using gpus or parallelization.", "Explaining each prediction of the Inception network for image classification takes around 10 minutes.", "From local explanation to model trust  The central idea here is that if we understand and trust the reasoning behind an individual prediction, and we repeat this process for a number of predictions that give good coverage of the input space, then we can start to build global trust in the model itself.", "We propose to give a global understanding of the model by explaining a set of individual instances.", "This approach is still model agnostic, and is complementary to computing summary statistics such as held-out accuracy.", "Even though explanations of multiple instances can be insightful, these instances need to be selected judiciously, since users may not have the time to examine a large number of explanations.", "We represent the time/patience that humans have by a budget B that denotes the number of explanations they are willing to look at in order to understand a model.", "Given a set of instances X, we define the pick step as the task of selecting B instances for the user to inspect.", "Examining the instances X, we know the features that are locally important in making the prediction at X.", "Features that are locally important for many instances are globally important.", "Instances B are picked so as to cover the globally important features first, and to avoid redundancy in explanation between them.", "With a little help from my friends  Using human subjects recruited via Amazon Mechanical Turk – by no means machine learning experts, but with a basic knowledge of religion – the team provided explanations for the predictions of two different models classifying documents as atheist or Christian and asked the subjects which would generalize better (perform the best in the real world).", "Using LIME coupled with the mechanism just described to create representative instances, the human subjects were able to choose the correct model 89% of the time.", "A second experiment asked Amazon Mechanical Turk users to identify which words from the explanations should be removed from subsequent training, for the worst classifier.", "If one notes that a classifier is untrustworthy, a common task in machine learning is feature engineering, i.e. modifying the set of features and retraining in order to improve generalization.", "Explanations can aid in this process by presenting the important features, particularly for removing features that the users feel do not generalize.", "The users are not ML experts, and don’t know anything about the dataset.", "Starting with 10 users, 10 classifiers are trained (one for each subject, with their suggested words removed).", "These are presented to five users each, resulting in another 50 classifiers.", "Each of these are presented to five users, giving 250 final models.", "It is clear… that the crowd workers are able to improve the model by removing features they deem unimportant for the task… Each subject took an average of 3.6 minutes per round of cleaning, resulting in just under 11 minutes to produce a classifier that generalizes much better to real world data.", "High agreement among users on the words to be removed indicated that users are converging to similar correct models.. “This evaluation is an example of how explanations make it easy to improve an untrustworthy classifier – in this case easy enough that machine learning knowledge is not required.”"], "summary_text": "“Why Should I Trust You? Explaining the Predictions of Any Classifier Ribeiro et al., KDD 2016  You’ve trained a classifier and it’s performing well on the validation set – but does the model exhibit sound judgement or is it making decisions based on spurious criteria? Can we trust the model in the real world? And can we trust a prediction (classification) it makes well enough to act on it? Can we explain why the model made the decision it did, even if the inner workings of the model are not easily understandable by humans? These are the questions that Ribeiro et al. pose in this paper, and they answer them by building LIME – an algorithm to explain the predictions of any classifier, and SP-LIME, a method for building trust in the predictions of a model overall. Another really nice result is that by explaining to a human how the model made a certain prediction, the human is able to give feedback on whether the reasoning is ‘sound’ and suggest features to remove from the model – this leads to classifiers that generalize much better to real world data. Consider two classifiers (Algorithm 1 and Algorithm 2 in the figure below) both trained to determine whether a document is about Christianity or atheism. Algorithm 2 performs much better in hold-out tests, but when we see why it is making its decisions, we realise it is actually much worse…  Magenta words are those contributing to the atheism class, green for Christianity. The second algorithm is basing its decision on “Posting”, “Host”, “Re” and “nntp” – words that have no connection to either Christianity or atheism, but happen to feature heavily in the headers of newsgroup postings about atheism in the training set. What makes a good explanation? It must be easily understandable by a human! For example, if hundreds or thousands of features significantly contribute to a prediction, it is not reasonable to expect any user to comprehend why the prediction was made, even if individual weights can be inspected. And it must meaningfully connect input variables to the response:  ..which is not necessarily tue of the features used by the model, and thus the “input variables” in the explanation may need to be different than the features. Furthermore, an explanation must have local fidelity: it should correspond to how the model behaves in the vicinity of the instance being predicted. The ideal explainer, should also be able to explain any model, and thus be model-agnostic. A key insight – local interpretation  Creating a globally faithful interpreter of a model’s decisions might require a complete description of the model itself. But to explain an individual decision we only need to understand how it behaves in a small local region. The idea reminds me a little bit of differentiation – overall the shape of the curve may be very complex, but if we look at just a small part we can figure out the gradient in that region. Here’s a toy example from the paper – the true decision boundary in the model is represented by the blue/pink background. In the immediate vicinity of the decision (the bold red cross) though we can learn a much simpler explanation that is locally faithful even if not globally faithful. The LIME algorithm produces Local Interpretable Model-agnostic Explanations. The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to the classifier. For text classification, an interpretable representation could be a vector indicating the presence or absence of a word, even though the classifier may use more complex word embeddings. For image classification an interpretable representation might be an binary vector indicating the ‘presence’ or ‘absence’ of a contiguous patch of similar pixels. LIME works by drawing samples in the vicinity of the input to be explained and learning a linear classifier using locally weighted square loss, with a limit K set on the number of interpretable features. Since [the algorithm] produces an explanation for an individual prediction, its complexity does not depend on the size of the dataset, but instead on time to compute f(x) [a model prediction] and on the number of samples N. In practice, explaining random forests with 1000 trees using scikit-learn on a laptop with N = 5000 takes under 3 seconds without any optimizations such as using gpus or parallelization. Explaining each prediction of the Inception network for image classification takes around 10 minutes. From local explanation to model trust  The central idea here is that if we understand and trust the reasoning behind an individual prediction, and we repeat this process for a number of predictions that give good coverage of the input space, then we can start to build global trust in the model itself. We propose to give a global understanding of the model by explaining a set of individual instances. This approach is still model agnostic, and is complementary to computing summary statistics such as held-out accuracy. Even though explanations of multiple instances can be insightful, these instances need to be selected judiciously, since users may not have the time to examine a large number of explanations. We represent the time/patience that humans have by a budget B that denotes the number of explanations they are willing to look at in order to understand a model. Given a set of instances X, we define the pick step as the task of selecting B instances for the user to inspect. Examining the instances X, we know the features that are locally important in making the prediction at X. Features that are locally important for many instances are globally important. Instances B are picked so as to cover the globally important features first, and to avoid redundancy in explanation between them. With a little help from my friends  Using human subjects recruited via Amazon Mechanical Turk – by no means machine learning experts, but with a basic knowledge of religion – the team provided explanations for the predictions of two different models classifying documents as atheist or Christian and asked the subjects which would generalize better (perform the best in the real world). Using LIME coupled with the mechanism just described to create representative instances, the human subjects were able to choose the correct model 89% of the time. A second experiment asked Amazon Mechanical Turk users to identify which words from the explanations should be removed from subsequent training, for the worst classifier. If one notes that a classifier is untrustworthy, a common task in machine learning is feature engineering, i.e. modifying the set of features and retraining in order to improve generalization. Explanations can aid in this process by presenting the important features, particularly for removing features that the users feel do not generalize. The users are not ML experts, and don’t know anything about the dataset. Starting with 10 users, 10 classifiers are trained (one for each subject, with their suggested words removed). These are presented to five users each, resulting in another 50 classifiers. Each of these are presented to five users, giving 250 final models. It is clear… that the crowd workers are able to improve the model by removing features they deem unimportant for the task… Each subject took an average of 3.6 minutes per round of cleaning, resulting in just under 11 minutes to produce a classifier that generalizes much better to real world data. High agreement among users on the words to be removed indicated that users are converging to similar correct models.. “This evaluation is an example of how explanations make it easy to improve an untrustworthy classifier – in this case easy enough that machine learning knowledge is not required.”", "pdf_url": "http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/why-should-i-trust-you-explaining-the-predictions-of-any-classifier.json"}
{"id": "82626355", "bin": "1400_1500", "summary_sentences": ["SLOG: serializable, low-latency, geo-replicated transactions Ren et al., VLDB’19  SLOG is another research system motivated by the needs of the application developer (aka, user!).", "Building correct applications is much easier when the system provides strict serializability guarantees.", "Strict serializability reduces application code complexity and bugs, since it behaves like a system that is running on a single machine processing transactions sequentially.", "The challenge with strict serializability (or even just serializability on a regular DBMS) is that it requires coordination, and as we know, coordination kills performance.", "Weaker consistency models can give better performance yet “expose applications to potential race condition bugs, and typically require skilled application programmers.” But developers are the kingmakers (I think it’s been enough time now that we can drop the ‘new’ in that phrase??", ";) ), and thus:  … the demand for systems that support strict serializability has only increased.", "So starting with strict serializability as a given, how do we claw back some of that performance?", "That’s where SLOG (Serializable LOw-latency, Geo-replicated transactions) comes in.", "SLOG achieves high throughput, strictly serializable ACID transactions at geo-replicated scale for all transactions submitted across the world, all the while achieving low latency for transactions that initiate from a location close to the home region for data they access.", "Coordination overhead is bad for performance, but coordination across regions is bad for performance on a whole other level!", "So the central idea behind SLOG is to take advantage of region affinity — the notion that e.g. data related to a user is likely to accessed in their home region— to handle as many transactions as possible using only intra-region coordination.", "Cross-region coordination on every write is not necessary to guarantee strict serializability.", "If every read of a data item is served from the location of the most recent write to that data item, then there is no need to synchronously replicate writes across regions.", "Is my data at home?", "Data is replicated across regions, but for every data item one of these regions is designated as its home replica.", "All writes and linearizable reads of a data item are directed to that replica.", "Transactions can be single homed (all the data items they access have the same home region), or multi-homed (data items from more than one home region).", "Within a region there is a local input log maintained across multiple servers using Paxos.", "Regions periodically send their log updates to other regions, which enables local snapshots reads in remote (non-home) regions, and faster remastering (re-homing) if a data item needs to be migrated.", "If data turns out not to be in the best location, it can be migrated.", "SLOG borrows PNUTs heuristic for when to remaster data: 3 accesses in a row not from the home region (see §3.4 in the paper for details on how migrations are made safe).", "Let’s make a plan  When data for single home transactions is processed within a region, SLOG works similarly to Calvin with data partitioned across servers.", "Key to good intra-region performance is the deterministic nature of the processing.", "All nodes involved in processing a transaction… run an agreement protocol prior to processing a batch of transactions that plans out how to process the batch.", "This plan is deterministic in the sense that all replicas that see the same plan must have only one possible final state after processing transactions according to this plan.", "Once all parties agree to the plan, processing occurs (mostly) independently on each node, with the system relying on the plan’s determinism in order to avoid replica divergence.", "To make the plan we need to know quite a bit about the transactions within a batch: “this makes deterministic database systems a poor fit for ORM tools and other applications that submit transactions to the database in pieces.” SLOG giveth and SLOG taketh away!", "If a transaction that SLOG thought was going to be single-homed turns out not to be (e.g. , data migrated), then that will be detected at runtime, the transaction is aborted, and SLOG restarts it as a multi-home transaction instead.", "Are you feeling lucky?", "SLOG has two different models of operation: SLOG-B and SLOG-HA.", "In SLOG-B the only synchronous replication is internally within a home region.", "If a region fails, data may become unavailable.", "In SLOG-HA data is synchronously replicated to one or more nearby regions.", "Unlike Spanner, Cosmos DB, Aurora, or other previously cited systems that support synchronous cross-region replication, SLOG’s deterministic architecture ensures that its throughput is unaffected by the presence of synchronous cross-region replicas.", "Multi-home transactions  The most technically challenging problem in the design of LOG is the execution of multi-home transactions.", "The goal is to maintain strict serializability guarantees and high throughput in the presence of potentially large numbers of multi-home transactions, even though they may access contended data, and even though they require coordination across physically distant regions.", "For single-homed transactions we know we can end up with a global serializable schedule without coordination.", "But all multi-homed transactions need to be ordered with respect to each other.", "Any strategy that produces a global ordering would do here (e.g. Paxos) – the current implementation achieves this goal by sending all multi-home transactions to the same region to be ordered by the local log there.", "(So in the current implementation, SLOG-HA is region fault-tolerant unless the region that dies happens to also be the nominated global ordering region??).", "Within each region involved in processing a multi-home transaction, a local LockOnlyTxn is generated which locks the local reads and writes for that region.", "LockOnlyTxns are placed in the local log like any other transaction,  securing an ordering relative to other single-home transactions at the same region.", "Evaluation  SLOG is compared to Calvin (a deterministic system, but without the notion of region affinities), to a design based on NuoDB (has region affinities, but is not deterministic, uses 2PL for serializability), and to Cloud Spanner using transactional YCSB and TPC-C NewOrder benchmarks.", "Under low contention (mostly single-home transactions) throughput  is similar across SLOG, Calvin, and NuoDB-2PL, but as the number of multi-home transactions in the mix increases, Calvin shows an advantage.", "SLOG pays an extra cost over Calvin for its LockOnlyTxns, but still does better than NuoDB-2PL.", "However, Calvin’s advantage is reduced somewhat when multi-partition transactions (which could still be single-homed) are in play.", "… we expect most workloads to be in one of two categories: either it is mostly single-partition and mostly single-home, or otherwise both multi-partition and multi-home transactions are common.", "So hold on a minute, did I bring you all this way just to tell you that Calvin is better?!", "Not at all!", "You see, SLOG’s slight disadvantage on multi-home transaction throughput is more than made up for by its latency figures:  …SLOG’s ability to commit single-home transactions as soon as they are processed at their home region allows SLOG to dramatically reduce its latency relative to Calvin.", "Compared to Spanner, SLOG has a very different performance profile.", "In particular, Spanner’s throughput decreases rapidly under contention since it doesn’t allow conflicting transactions to run during 2PC and the Paxos-implemented geo-replication.", "The last word  Current state-of-the-art geo-replicated systems force their users to give up one of: (1) strict serializability, (2) low-latency writes, (3) high transactional throughput.", "Some widely used systems force their users to give up two of them… SLOG leverages physical region locality in an application workload in order to achieve all three, while also supporting online consistent dynamic “re-mastering” of data as application patterns change over time."], "summary_text": "SLOG: serializable, low-latency, geo-replicated transactions Ren et al., VLDB’19  SLOG is another research system motivated by the needs of the application developer (aka, user!). Building correct applications is much easier when the system provides strict serializability guarantees. Strict serializability reduces application code complexity and bugs, since it behaves like a system that is running on a single machine processing transactions sequentially. The challenge with strict serializability (or even just serializability on a regular DBMS) is that it requires coordination, and as we know, coordination kills performance. Weaker consistency models can give better performance yet “expose applications to potential race condition bugs, and typically require skilled application programmers.” But developers are the kingmakers (I think it’s been enough time now that we can drop the ‘new’ in that phrase?? ;) ), and thus:  … the demand for systems that support strict serializability has only increased. So starting with strict serializability as a given, how do we claw back some of that performance? That’s where SLOG (Serializable LOw-latency, Geo-replicated transactions) comes in. SLOG achieves high throughput, strictly serializable ACID transactions at geo-replicated scale for all transactions submitted across the world, all the while achieving low latency for transactions that initiate from a location close to the home region for data they access. Coordination overhead is bad for performance, but coordination across regions is bad for performance on a whole other level! So the central idea behind SLOG is to take advantage of region affinity — the notion that e.g. data related to a user is likely to accessed in their home region— to handle as many transactions as possible using only intra-region coordination. Cross-region coordination on every write is not necessary to guarantee strict serializability. If every read of a data item is served from the location of the most recent write to that data item, then there is no need to synchronously replicate writes across regions. Is my data at home? Data is replicated across regions, but for every data item one of these regions is designated as its home replica. All writes and linearizable reads of a data item are directed to that replica. Transactions can be single homed (all the data items they access have the same home region), or multi-homed (data items from more than one home region). Within a region there is a local input log maintained across multiple servers using Paxos. Regions periodically send their log updates to other regions, which enables local snapshots reads in remote (non-home) regions, and faster remastering (re-homing) if a data item needs to be migrated. If data turns out not to be in the best location, it can be migrated. SLOG borrows PNUTs heuristic for when to remaster data: 3 accesses in a row not from the home region (see §3.4 in the paper for details on how migrations are made safe). Let’s make a plan  When data for single home transactions is processed within a region, SLOG works similarly to Calvin with data partitioned across servers. Key to good intra-region performance is the deterministic nature of the processing. All nodes involved in processing a transaction… run an agreement protocol prior to processing a batch of transactions that plans out how to process the batch. This plan is deterministic in the sense that all replicas that see the same plan must have only one possible final state after processing transactions according to this plan. Once all parties agree to the plan, processing occurs (mostly) independently on each node, with the system relying on the plan’s determinism in order to avoid replica divergence. To make the plan we need to know quite a bit about the transactions within a batch: “this makes deterministic database systems a poor fit for ORM tools and other applications that submit transactions to the database in pieces.” SLOG giveth and SLOG taketh away! If a transaction that SLOG thought was going to be single-homed turns out not to be (e.g. , data migrated), then that will be detected at runtime, the transaction is aborted, and SLOG restarts it as a multi-home transaction instead. Are you feeling lucky? SLOG has two different models of operation: SLOG-B and SLOG-HA. In SLOG-B the only synchronous replication is internally within a home region. If a region fails, data may become unavailable. In SLOG-HA data is synchronously replicated to one or more nearby regions. Unlike Spanner, Cosmos DB, Aurora, or other previously cited systems that support synchronous cross-region replication, SLOG’s deterministic architecture ensures that its throughput is unaffected by the presence of synchronous cross-region replicas. Multi-home transactions  The most technically challenging problem in the design of LOG is the execution of multi-home transactions. The goal is to maintain strict serializability guarantees and high throughput in the presence of potentially large numbers of multi-home transactions, even though they may access contended data, and even though they require coordination across physically distant regions. For single-homed transactions we know we can end up with a global serializable schedule without coordination. But all multi-homed transactions need to be ordered with respect to each other. Any strategy that produces a global ordering would do here (e.g. Paxos) – the current implementation achieves this goal by sending all multi-home transactions to the same region to be ordered by the local log there. (So in the current implementation, SLOG-HA is region fault-tolerant unless the region that dies happens to also be the nominated global ordering region??). Within each region involved in processing a multi-home transaction, a local LockOnlyTxn is generated which locks the local reads and writes for that region. LockOnlyTxns are placed in the local log like any other transaction,  securing an ordering relative to other single-home transactions at the same region. Evaluation  SLOG is compared to Calvin (a deterministic system, but without the notion of region affinities), to a design based on NuoDB (has region affinities, but is not deterministic, uses 2PL for serializability), and to Cloud Spanner using transactional YCSB and TPC-C NewOrder benchmarks. Under low contention (mostly single-home transactions) throughput  is similar across SLOG, Calvin, and NuoDB-2PL, but as the number of multi-home transactions in the mix increases, Calvin shows an advantage. SLOG pays an extra cost over Calvin for its LockOnlyTxns, but still does better than NuoDB-2PL. However, Calvin’s advantage is reduced somewhat when multi-partition transactions (which could still be single-homed) are in play. … we expect most workloads to be in one of two categories: either it is mostly single-partition and mostly single-home, or otherwise both multi-partition and multi-home transactions are common. So hold on a minute, did I bring you all this way just to tell you that Calvin is better?! Not at all! You see, SLOG’s slight disadvantage on multi-home transaction throughput is more than made up for by its latency figures:  …SLOG’s ability to commit single-home transactions as soon as they are processed at their home region allows SLOG to dramatically reduce its latency relative to Calvin. Compared to Spanner, SLOG has a very different performance profile. In particular, Spanner’s throughput decreases rapidly under contention since it doesn’t allow conflicting transactions to run during 2PC and the Paxos-implemented geo-replication. The last word  Current state-of-the-art geo-replicated systems force their users to give up one of: (1) strict serializability, (2) low-latency writes, (3) high transactional throughput. Some widely used systems force their users to give up two of them… SLOG leverages physical region locality in an application workload in order to achieve all three, while also supporting online consistent dynamic “re-mastering” of data as application patterns change over time.", "pdf_url": "http://www.vldb.org/pvldb/vol12/p1747-ren.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/slog.json"}
{"id": "85839366", "bin": "1500_1600", "summary_sentences": ["Large-scale evolution of image classifiers Real et al., 2017  I’m sure you noticed the bewildering array of network architectures in use when we looked at some of the top convolution neural network papers of the last few years last week ( Part 1 , Part2 , Part 3 ).", "With sufficient training data, these networks can achieve amazing feats, but how do you find the best network architecture for your problem in the first place?", "Discovering neural network architectures… remains a laborious task.", "Even within the specific problem of image classification, the state of the the art was attained through many years of focused investigation by hundreds of researchers.", "If you’re an AI researcher and you come up against a difficult problem where it’s hard to encode the best rules (or features) by hand, what do you do?", "Get the machine to learn them for you of course!", "If what you want to learn is a function, and you can optimise it through back propagation, then a network is a good solution (see e.g. ‘ Network in Network ‘, or ‘ Learning to learn by gradient descent by gradient descent ‘).", "If what you want to learn is a policy that plays out over some number of time steps then reinforcement learning would be a good bet.", "But what if you wanted to learn a good network architecture?", "For that the authors turned to a technique from the classical AI toolbox, evolutionary algorithms.", "As you may recall, evolutionary algorithms start out with an initial population, assess the suitability of population members for the task in hand using some kind of fitness function, and then generate a new population for the next iteration by mutations and combinations of the fittest members.", "So we know that we’re going to start out with some population of initial model architectures (say, 1000 of them), define a fitness function based on how well they perform when trained, and come up with a set of appropriate mutation operations over model architectures.", "That’s the big picture, and there’s just one more trick from the deep learning toolbox that we need to bring to bear: brute force!", "If in doubt, overwhelm the problem with bigger models, more data, or in this case, more computation:  We used slightly-modified known evolutionary algorithms and scaled up the computation to unprecedented levels, as far as we know.", "This, together with a set of novel and intuitive mutation operators, allowed us to reach competitive accuracies on the CIFAR-10 dataset.", "This dataset was chosen because it requires large networks to reach high accuracies, thus presenting a computational challenge.", "The initial population consists of very simple (and very poorly performing) linear models, and the end result is a fully trained neural network with no post-processing required.", "Here’s where the evolved models stand in the league table:  That’s a pretty amazing result when you think about it.", "Really top-notch AI researchers are in very short supply, but computation is much more readily available on the open market.", "‘Evolution’ evolved an architecture, with no human guidance, that beats some of our best models from the last few years.", "Let’s take a closer look at the details of the evolutionary algorithm, and then we’ll come back and dig deeper into the evaluation results.", "Evolving models  We start with a population of 1000 very simple linear regression models, and then use tournament selection.", "During each evolutionary step, a worker process (250 of them running in parallel) chooses two individuals at random and compares their fitness.", "The worst of the pair is removed from the population, and the better model is chosen as a parent to help create the next generation.", "A mutation is applied to the parent to create a child.", "The worker then trains the child, evaluates it on the validation set, and puts it back into the population.", "Using this strategy to search large spaces of complex image models requires considerable computation.", "To achieve scale, we developed a massively-parallel, lock-free infrastructure.", "Many workers operate asynchronously on different computers.", "They do not communicate directly with each other.", "Instead, they use a shared file-system, where the population is stored.", "Training and validation takes place on the CIFAR-10 dataset consisting of 50,000 training examples and 10,000 test examples, all labeled with 1 of 10 common object classes.", "Each training runs for 25,600 steps – brief enough so that each individual can be trained somewhere between a few seconds and a few hours, depending on the model size.", "After training, a single evaluation on the validation set provides the accuracy to use as the model’s fitness.", "We need architectures that are trained to completion within an evolutionary experiment… [but] 25,600 steps are not enough to fully train each individual.", "Training a large enough model to completion is prohibitively slow for evolution.", "To resolve this dilemma, we allow the children to inherit the parents’ weights whenever possible.", "The final piece of the puzzle then, is the encoding of model architectures, and the mutation operations defined over them.", "A model architecture is encoded as a graph (its DNA).", "Vertices are tensors or activations (either batch normalisation with ReLUs, or simple linear units).", "Edges in the graph are identity connections (for skipping) or convolutions.", "When multiple edges are incident on a vertex, their spatial scales or numbers of channels may not coincide.", "However, the vertex must have a single size and number of channels for its activations.", "The inconsistent inputs must be resolved.", "Resolution is done by choosing one of the incoming edges as the primary one.", "We pick this primary edge to be the one that is not a skip connection.", "Activation functions are similarly reshaped (using some combination of interpolation, truncation, and padding), and the learning rate is also encoded in the DNA.", "When creating a child, a worker picks a mutation at random from the following set:  Alter learning rate  Identity (effectively gives the individual further training time in the next generation)  Reset weights  Insert convolution (at a random location in the ‘convolutional backbone’).", "Convolutions are 3×3 with strides of 1 or 2.", "Remove convolution  Alter stride (powers of 2 only)  Alter number of channels (of a random convolution)  Filter size (horizontal or vertical at random, on a random convolution, odd values only)  Insert one-to-one (adds a one-to-one / identity connection)  Add skip (identity between random layers)  Remove skip (removes a random skip)  Evaluation results  Here’s an example of an evolution experiment, with selected population members highlighted:  Five experiment runs are done, and although not all models reach the same accuracy, they get pretty close.", "It took 9 x 1019 FLOPs on average per experiment.", "The following chart shows how accuracy improves over time during the experiments:  We observe that populations evolve until they plateau at some local optimum.", "The fitness (i.e. validation accuracy) value at this optimum varies between experiments (Above, inset).", "Since not all experiments reach the highest possible value, some populations are getting “trapped” at inferior local optima.", "This entrapment is affected by two important meta-parameters (i.e. parameters that are not optimized by the algorithm).", "These are the population size and the number of training steps per individual.", "The larger the population size, the more thoroughly the space of models can be explored, which helps to reach better optima.", "More training time means that a model needs to undergo fewer identity mutations to reach a given level of training (remember that the end result of the evolution process is a fully trained model, not just a model architecture).", "Two other approaches to escaping local optima are increasing the mutation rate, and resetting weights.", "When it looks like members of the population are trapped in poor local optima, the team tried applying 5 mutations instead of 1 for a few generations.", "During this period some population members escape the local optimum, and none get worse:  To avoid getting trapped by poorer architectures that just happened to have received more training (e.g. through the identity mutation), the team also tried experiments in which the weights are simultaneously reset across all population members when a plateau is reached.", "The populations suffer a temporary degradation (as to be expected), but ultimately reach a higher optima."], "summary_text": "Large-scale evolution of image classifiers Real et al., 2017  I’m sure you noticed the bewildering array of network architectures in use when we looked at some of the top convolution neural network papers of the last few years last week ( Part 1 , Part2 , Part 3 ). With sufficient training data, these networks can achieve amazing feats, but how do you find the best network architecture for your problem in the first place? Discovering neural network architectures… remains a laborious task. Even within the specific problem of image classification, the state of the the art was attained through many years of focused investigation by hundreds of researchers. If you’re an AI researcher and you come up against a difficult problem where it’s hard to encode the best rules (or features) by hand, what do you do? Get the machine to learn them for you of course! If what you want to learn is a function, and you can optimise it through back propagation, then a network is a good solution (see e.g. ‘ Network in Network ‘, or ‘ Learning to learn by gradient descent by gradient descent ‘). If what you want to learn is a policy that plays out over some number of time steps then reinforcement learning would be a good bet. But what if you wanted to learn a good network architecture? For that the authors turned to a technique from the classical AI toolbox, evolutionary algorithms. As you may recall, evolutionary algorithms start out with an initial population, assess the suitability of population members for the task in hand using some kind of fitness function, and then generate a new population for the next iteration by mutations and combinations of the fittest members. So we know that we’re going to start out with some population of initial model architectures (say, 1000 of them), define a fitness function based on how well they perform when trained, and come up with a set of appropriate mutation operations over model architectures. That’s the big picture, and there’s just one more trick from the deep learning toolbox that we need to bring to bear: brute force! If in doubt, overwhelm the problem with bigger models, more data, or in this case, more computation:  We used slightly-modified known evolutionary algorithms and scaled up the computation to unprecedented levels, as far as we know. This, together with a set of novel and intuitive mutation operators, allowed us to reach competitive accuracies on the CIFAR-10 dataset. This dataset was chosen because it requires large networks to reach high accuracies, thus presenting a computational challenge. The initial population consists of very simple (and very poorly performing) linear models, and the end result is a fully trained neural network with no post-processing required. Here’s where the evolved models stand in the league table:  That’s a pretty amazing result when you think about it. Really top-notch AI researchers are in very short supply, but computation is much more readily available on the open market. ‘Evolution’ evolved an architecture, with no human guidance, that beats some of our best models from the last few years. Let’s take a closer look at the details of the evolutionary algorithm, and then we’ll come back and dig deeper into the evaluation results. Evolving models  We start with a population of 1000 very simple linear regression models, and then use tournament selection. During each evolutionary step, a worker process (250 of them running in parallel) chooses two individuals at random and compares their fitness. The worst of the pair is removed from the population, and the better model is chosen as a parent to help create the next generation. A mutation is applied to the parent to create a child. The worker then trains the child, evaluates it on the validation set, and puts it back into the population. Using this strategy to search large spaces of complex image models requires considerable computation. To achieve scale, we developed a massively-parallel, lock-free infrastructure. Many workers operate asynchronously on different computers. They do not communicate directly with each other. Instead, they use a shared file-system, where the population is stored. Training and validation takes place on the CIFAR-10 dataset consisting of 50,000 training examples and 10,000 test examples, all labeled with 1 of 10 common object classes. Each training runs for 25,600 steps – brief enough so that each individual can be trained somewhere between a few seconds and a few hours, depending on the model size. After training, a single evaluation on the validation set provides the accuracy to use as the model’s fitness. We need architectures that are trained to completion within an evolutionary experiment… [but] 25,600 steps are not enough to fully train each individual. Training a large enough model to completion is prohibitively slow for evolution. To resolve this dilemma, we allow the children to inherit the parents’ weights whenever possible. The final piece of the puzzle then, is the encoding of model architectures, and the mutation operations defined over them. A model architecture is encoded as a graph (its DNA). Vertices are tensors or activations (either batch normalisation with ReLUs, or simple linear units). Edges in the graph are identity connections (for skipping) or convolutions. When multiple edges are incident on a vertex, their spatial scales or numbers of channels may not coincide. However, the vertex must have a single size and number of channels for its activations. The inconsistent inputs must be resolved. Resolution is done by choosing one of the incoming edges as the primary one. We pick this primary edge to be the one that is not a skip connection. Activation functions are similarly reshaped (using some combination of interpolation, truncation, and padding), and the learning rate is also encoded in the DNA. When creating a child, a worker picks a mutation at random from the following set:  Alter learning rate  Identity (effectively gives the individual further training time in the next generation)  Reset weights  Insert convolution (at a random location in the ‘convolutional backbone’). Convolutions are 3×3 with strides of 1 or 2. Remove convolution  Alter stride (powers of 2 only)  Alter number of channels (of a random convolution)  Filter size (horizontal or vertical at random, on a random convolution, odd values only)  Insert one-to-one (adds a one-to-one / identity connection)  Add skip (identity between random layers)  Remove skip (removes a random skip)  Evaluation results  Here’s an example of an evolution experiment, with selected population members highlighted:  Five experiment runs are done, and although not all models reach the same accuracy, they get pretty close. It took 9 x 1019 FLOPs on average per experiment. The following chart shows how accuracy improves over time during the experiments:  We observe that populations evolve until they plateau at some local optimum. The fitness (i.e. validation accuracy) value at this optimum varies between experiments (Above, inset). Since not all experiments reach the highest possible value, some populations are getting “trapped” at inferior local optima. This entrapment is affected by two important meta-parameters (i.e. parameters that are not optimized by the algorithm). These are the population size and the number of training steps per individual. The larger the population size, the more thoroughly the space of models can be explored, which helps to reach better optima. More training time means that a model needs to undergo fewer identity mutations to reach a given level of training (remember that the end result of the evolution process is a fully trained model, not just a model architecture). Two other approaches to escaping local optima are increasing the mutation rate, and resetting weights. When it looks like members of the population are trapped in poor local optima, the team tried applying 5 mutations instead of 1 for a few generations. During this period some population members escape the local optimum, and none get worse:  To avoid getting trapped by poorer architectures that just happened to have received more training (e.g. through the identity mutation), the team also tried experiments in which the weights are simultaneously reset across all population members when a plateau is reached. The populations suffer a temporary degradation (as to be expected), but ultimately reach a higher optima.", "pdf_url": "https://arxiv.org/pdf/1703.01041.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/large-scale-evolution-of-image-classifiers.json"}
{"id": "88246188", "bin": "1500_1600", "summary_sentences": ["Why  Deep plain/ordinary networks usually perform better than shallow networks.", "However, when they get too deep their performance on the training set decreases.", "That should never happen and is a shortcoming of current optimizers.", "If the \"good\" insights of the early layers could be transferred through the network unaltered, while changing/improving the \"bad\" insights, that effect might disappear.", "What residual architectures are  Residual architectures use identity functions to transfer results from previous layers unaltered.", "They change these previous results based on results from convolutional layers.", "So while a plain network might do something like output = convolution(image), a residual network will do output = image + convolution(image).", "If the convolution resorts to just doing nothing, that will make the result a lot worse in the plain network, but not alter it at all in the residual network.", "So in the residual network, the convolution can focus fully on learning what positive changes it has to perform, while in the plain network it first has to learn the identity function and then what positive changes it can perform.", "How it works  Residual architectures can be implemented in most frameworks.", "You only need something like a split layer and an element-wise addition.", "Use one branch with an identity function and one with 2 or more convolutions (1 is also possible, but seems to perform poorly).", "Merge them with the element-wise addition.", "Rough example block (for a 64x32x32 input):  input (64x32x32) --> identity --------------> + --> output (64x32x32)        |                                      ^        ------------> conv 3x3 --> conv 3x3 ---|  An example block when you have to change the dimensionality (e.g. here from 64x32x32 to 128x32x32):  to: 128x32x32 input (64x32x32) --> conv 1x1 -----------------------------> + --> output (128x32x32)        |                                                     ^        ------------> conv 1x1 ----> conv 3x3 --> conv 1x1 ---|                      to: 32x32x32                to: 128x32x32  The authors seem to prefer using either two 3x3 convolutions or the chain of 1x1 then 3x3 then 1x1.", "They use the latter one for their very deep networks.", "The authors also tested:  To use 1x1 convolutions instead of identity functions everywhere.", "Performed a bit better than using 1x1 only for dimensionality changes.", "However, also computation and memory demands.", "To use zero-padding for dimensionality changes (no 1x1 convs, just fill the additional dimensions with zeros).", "Performed only a bit worse than 1x1 convs and a lot better than plain network architectures.", "Pooling can be used as in plain networks.", "No special architectures are necessary.", "Batch normalization can be used as usually (before nonlinearities).", "Results  Residual networks seem to perform generally better than similarly sized plain networks.", "They seem to be able to achieve similar results with less computation.", "They enable well-trainable very deep architectures with up to 1000 layers and more.", "The activations of the residual layers are low compared to plain networks.", "That indicates that the residual networks indeed only learn to make \"good\" changes and default to \"if in doubt, change nothing\".", "Examples of basic building blocks (other architectures are possible).", "The paper doesn't discuss the placement of the ReLU (after add instead of after the layer).", "Activations of layers (after batch normalization, before nonlinearity) throughout the network for plain and residual nets.", "Residual networks have on average lower activations.", "Rough chapter-wise notes  (1) Introduction  In classical architectures, adding more layers can cause the network to perform worse on the training set.", "That shouldn't be the case.", "(E.g. a shallower could be trained and then get a few layers of identity functions on top of it to create a deep network.)", "To combat that problem, they stack residual layers.", "A residual layer is an identity function and can learn to add something on top of that.", "So if x is an input image and f(x) is a convolution, they do something like x + f(x) or even x + f(f(x)).", "The classical architecture would be more like f(f(f(f(x)))).", "Residual architectures can be easily implemented in existing frameworks using skip connections with identity functions (split + merge).", "Residual architecture outperformed other in ILSVRC 2015 and COCO 2015.", "(3) Deep Residual Learning  If some layers have to fit a function H(x) then they should also be able to fit H(x) - x (change between x and H(x)).", "The latter case might be easier to learn than the former one.", "The basic structure of a residual block is y = x + F(x, W), where x is the input image, y is the output image (x + change) and F(x, W) is the residual subnetwork that estimates a good change of x (W are the subnetwork's weights).", "x and F(x, W) are added using element-wise addition.", "x and the output of F(x, W) must be have equal dimensions (channels, height, width).", "If different dimensions are required (mainly change in number of channels) a linear projection V is applied to x: y = F(x, W) + Vx.", "They use a 1x1 convolution for V (without nonlinearity?).", "F(x, W) subnetworks can contain any number of layer.", "They suggest 2+ convolutions.", "Using only 1 layer seems to be useless.", "They run some tests on a network with 34 layers and compare to a 34 layer network without residual blocks and with VGG (19 layers).", "They say that their architecture requires only 18% of the FLOPs of VGG.", "(Though a lot of that probably comes from VGG's 2x4096 fully connected layers?", "They don't use any fully connected layers, only convolutions.)", "A critical part is the change in dimensionality (e.g. from 64 kernels to 128).", "They test (A) adding the new dimensions empty (padding), (B) using the mentioned linear projection with 1x1 convolutions and (C) using the same linear projection, but on all residual blocks (not only for dimensionality changes).", "(A) doesn't add parameters, (B) does (i.e. breaks the pattern of using identity functions).", "They use batch normalization before each nonlinearity.", "Optimizer is SGD.", "They don't use dropout.", "(4) Experiments  When testing on ImageNet an 18 layer plain (i.e. not residual) network has lower training set error than a deep 34 layer plain network.", "They argue that this effect does probably not come from vanishing gradients, because they (a) checked the gradient norms and they looked healthy and (b) use batch normaliaztion.", "They guess that deep plain networks might have exponentially low convergence rates.", "For the residual architectures its the other way round.", "Stacking more layers improves the results.", "The residual networks also perform better (in error %) than plain networks with the same number of parameters and layers.", "(Both for training and validation set.)", "Regarding the previously mentioned handling of dimensionality changes:  (A) Pad new dimensions: Performs worst.", "(Still far better than plain network though.)", "(B) Linear projections for dimensionality changes: Performs better than A.", "(C) Linear projections for all residual blocks: Performs better than B.", "(Authors think that's due to introducing new parameters.)", "They also test on very deep residual networks with 50 to 152 layers.", "For these deep networks their residual block has the form 1x1 conv -> 3x3 conv -> 1x1 conv (i.e. dimensionality reduction, convolution, dimensionality increase).", "These deeper networks perform significantly better.", "In further tests on CIFAR-10 they can observe that the activations of the convolutions in residual networks are lower than in plain networks.", "So the residual networks default to doing nothing and only change (activate) when something needs to be changed.", "They test a network with 1202 layers.", "It is still easily optimizable, but overfits the training set.", "They also test on COCO and get significantly better results than a Faster-R-CNN+VGG implementation."], "summary_text": "Why  Deep plain/ordinary networks usually perform better than shallow networks. However, when they get too deep their performance on the training set decreases. That should never happen and is a shortcoming of current optimizers. If the \"good\" insights of the early layers could be transferred through the network unaltered, while changing/improving the \"bad\" insights, that effect might disappear. What residual architectures are  Residual architectures use identity functions to transfer results from previous layers unaltered. They change these previous results based on results from convolutional layers. So while a plain network might do something like output = convolution(image), a residual network will do output = image + convolution(image). If the convolution resorts to just doing nothing, that will make the result a lot worse in the plain network, but not alter it at all in the residual network. So in the residual network, the convolution can focus fully on learning what positive changes it has to perform, while in the plain network it first has to learn the identity function and then what positive changes it can perform. How it works  Residual architectures can be implemented in most frameworks. You only need something like a split layer and an element-wise addition. Use one branch with an identity function and one with 2 or more convolutions (1 is also possible, but seems to perform poorly). Merge them with the element-wise addition. Rough example block (for a 64x32x32 input):  input (64x32x32) --> identity --------------> + --> output (64x32x32)        |                                      ^        ------------> conv 3x3 --> conv 3x3 ---|  An example block when you have to change the dimensionality (e.g. here from 64x32x32 to 128x32x32):  to: 128x32x32 input (64x32x32) --> conv 1x1 -----------------------------> + --> output (128x32x32)        |                                                     ^        ------------> conv 1x1 ----> conv 3x3 --> conv 1x1 ---|                      to: 32x32x32                to: 128x32x32  The authors seem to prefer using either two 3x3 convolutions or the chain of 1x1 then 3x3 then 1x1. They use the latter one for their very deep networks. The authors also tested:  To use 1x1 convolutions instead of identity functions everywhere. Performed a bit better than using 1x1 only for dimensionality changes. However, also computation and memory demands. To use zero-padding for dimensionality changes (no 1x1 convs, just fill the additional dimensions with zeros). Performed only a bit worse than 1x1 convs and a lot better than plain network architectures. Pooling can be used as in plain networks. No special architectures are necessary. Batch normalization can be used as usually (before nonlinearities). Results  Residual networks seem to perform generally better than similarly sized plain networks. They seem to be able to achieve similar results with less computation. They enable well-trainable very deep architectures with up to 1000 layers and more. The activations of the residual layers are low compared to plain networks. That indicates that the residual networks indeed only learn to make \"good\" changes and default to \"if in doubt, change nothing\". Examples of basic building blocks (other architectures are possible). The paper doesn't discuss the placement of the ReLU (after add instead of after the layer). Activations of layers (after batch normalization, before nonlinearity) throughout the network for plain and residual nets. Residual networks have on average lower activations. Rough chapter-wise notes  (1) Introduction  In classical architectures, adding more layers can cause the network to perform worse on the training set. That shouldn't be the case. (E.g. a shallower could be trained and then get a few layers of identity functions on top of it to create a deep network.) To combat that problem, they stack residual layers. A residual layer is an identity function and can learn to add something on top of that. So if x is an input image and f(x) is a convolution, they do something like x + f(x) or even x + f(f(x)). The classical architecture would be more like f(f(f(f(x)))). Residual architectures can be easily implemented in existing frameworks using skip connections with identity functions (split + merge). Residual architecture outperformed other in ILSVRC 2015 and COCO 2015. (3) Deep Residual Learning  If some layers have to fit a function H(x) then they should also be able to fit H(x) - x (change between x and H(x)). The latter case might be easier to learn than the former one. The basic structure of a residual block is y = x + F(x, W), where x is the input image, y is the output image (x + change) and F(x, W) is the residual subnetwork that estimates a good change of x (W are the subnetwork's weights). x and F(x, W) are added using element-wise addition. x and the output of F(x, W) must be have equal dimensions (channels, height, width). If different dimensions are required (mainly change in number of channels) a linear projection V is applied to x: y = F(x, W) + Vx. They use a 1x1 convolution for V (without nonlinearity?). F(x, W) subnetworks can contain any number of layer. They suggest 2+ convolutions. Using only 1 layer seems to be useless. They run some tests on a network with 34 layers and compare to a 34 layer network without residual blocks and with VGG (19 layers). They say that their architecture requires only 18% of the FLOPs of VGG. (Though a lot of that probably comes from VGG's 2x4096 fully connected layers? They don't use any fully connected layers, only convolutions.) A critical part is the change in dimensionality (e.g. from 64 kernels to 128). They test (A) adding the new dimensions empty (padding), (B) using the mentioned linear projection with 1x1 convolutions and (C) using the same linear projection, but on all residual blocks (not only for dimensionality changes). (A) doesn't add parameters, (B) does (i.e. breaks the pattern of using identity functions). They use batch normalization before each nonlinearity. Optimizer is SGD. They don't use dropout. (4) Experiments  When testing on ImageNet an 18 layer plain (i.e. not residual) network has lower training set error than a deep 34 layer plain network. They argue that this effect does probably not come from vanishing gradients, because they (a) checked the gradient norms and they looked healthy and (b) use batch normaliaztion. They guess that deep plain networks might have exponentially low convergence rates. For the residual architectures its the other way round. Stacking more layers improves the results. The residual networks also perform better (in error %) than plain networks with the same number of parameters and layers. (Both for training and validation set.) Regarding the previously mentioned handling of dimensionality changes:  (A) Pad new dimensions: Performs worst. (Still far better than plain network though.) (B) Linear projections for dimensionality changes: Performs better than A. (C) Linear projections for all residual blocks: Performs better than B. (Authors think that's due to introducing new parameters.) They also test on very deep residual networks with 50 to 152 layers. For these deep networks their residual block has the form 1x1 conv -> 3x3 conv -> 1x1 conv (i.e. dimensionality reduction, convolution, dimensionality increase). These deeper networks perform significantly better. In further tests on CIFAR-10 they can observe that the activations of the convolutions in residual networks are lower than in plain networks. So the residual networks default to doing nothing and only change (activate) when something needs to be changed. They test a network with 1202 layers. It is still easily optimizable, but overfits the training set. They also test on COCO and get significantly better results than a Faster-R-CNN+VGG implementation.", "pdf_url": "http://arxiv.org/pdf/1512.03385", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/deep_residual_learning_for_image_recognition.json"}
{"id": "1873334", "bin": "1500_1600", "summary_sentences": ["Generalized Isolation Level Definitions – Adya et al. 2000  Following on from yesterday’s critique of ANSI SQL isolation levels , today’s paper also gives a precise definition of isolation levels – but does so in a way that is inclusive of optimistic and general multi-version concurrency control strategies instead of being dependent on locking.", "Where Berenson et al. define phenomena P0..P3 in order to explain the ANSI SQL isolation levels (degree 1, 2, and 3), Adya et al.", "define generalised phenomena labelled G0, G1, G2 whose definitions are independent of implementation strategy.", "Based on these generalised phenomena they then provide portable isolation level definitions (PL) that accommodate all implementation strategies, not  just locking based ones.", "We now show that the preventative approach is overly restrictive since it rules out optimistic and multi-version implementations.", "As mentioned, this approach disallows all histories that would not occur in a locking scheme and prevents conflicting operations from executing concurrently… The real problem with the preventative approach is that the phenomena are expressed in terms of single-object histories.", "However, the properties of interest are often multi-object constraints.", "To avoid problems with such constraints, the phenomena need to restrict what can be done with individual objects more than is necessary.", "Our approach avoids this difficulty by using specifications that capture constraints on multiple objects directly.", "Furthermore, the definitions in the preventative approach are not applicable to multi-version systems since they are described in terms of objects rather than in terms of versions.", "On the other hand, our specifications deal with multi-version and single-version histories.", "Following a motivation section, the authors first build up a model of a database system, and then define different types of conflict (read and write dependencies, as well as something called an anti-dependency that we’ll get too) that can occur within that model.", "This leads to the central notion of a Direct Serialization Graph (DSG) for a history.", "The phenomena and isolation levels are then specified as constraints on the allowable graphs.", "How to construct a Direct Serialization Graph  A DSG has one node for every committed transaction.", "(Directed) edges between these nodes represent read/write/anti-dependency conflicts.", "A transaction T2 depends on T1 if there is a path in the graph from T1 to T2.", "It directly depends on T1 if there is an edge from T1 to T2.", "In the description that follows, a delete is modelled as committing a special “dead” version of an object.", "To construct a DSG start with one node for every committed transaction, and for all pairs of distinct nodes T1 and T2, add a read dependency edge from T1 to T2 if any of the following conditions hold:  T1 commits a version xi of object x,  and T2 subsequently reads xi.", "T1 commits a change, and T2 then performs a predicate-based read such that the set of objects matched by the predicate are altered by T1’s commit.", "Furthermore, T1 is the most recent transaction to have committed a change impacting T1’s matches.", "Next add an anti-dependency edge from T1 to T2 if any of the following conditions hold:  T1 reads some version xi of object x, and T2 then commits the next version of x in the version history.", "T1 performs a predicate based read.", "T2 then commits a later (the next?)", "version of some object that would change the matches of T1.", "And finally add a write dependency edge from T1 to T2 if  T1 commits version xi of some object, and T2 then commits the next version of x in the version history.", "The dependency types and corresponding DSG edges are summarised below:  Phenomena and Isolation Levels  Isolation Level PL-1 (Portable definition of Read Uncommitted)  Phenomena P0 prevents dirty writes, and is motivated by a requirement to be able to serialize transactions based on writes, and to simplify recovery from aborts.", "The latter reason is not applicable to all systems, that may use other schemes for recovery.", "Portable Isolation level PL-1 provides for serialized transactions based on writes by disallowing Write Cycles.", "A Write Cycle is phenomenon G0.", "A history exhibits a write cycle if its direct serialization graph contains a cycle consisting entirely of write-dependency edges.", "Note that this definition neatly encompasses write cycles with more than 2 objects involved.", "Isolation Level PL-2 (Portable definition of Read Committed)  The essence of no dirty reads – without being overly restrictive on the reading and writing of objects written by transactions that are still uncommitted – is captured by phenomenon G1, which is comprised of three sub-phenomena G1a, G1b, and G1c.", "G1 subsumes G0, and PL-2 is the isolation level that prohibits G1 (and hence G0 also).", "G1a – Aborted Reads.", "T2 reads some object (including via predicates reads) modified by T1, and T1 aborts.", "To prevent aborted reads, if T2 reads from T1 and T1 aborts, T2 must also abort – known as a cascading abort.", "G1b – Intermediate Reads.", "T2 reads a version of some object (including via predicate reads) modified by T1, and it was not T1’s final modification of that object.", "To prevent intermediate reads, transactions can only be allowed to commit if they have read the final versions of objects from other transactions.", "G1c – Circular Information Flow.", "The Direct Serialization Graph contains a directed cycle consisting entirely of (read and write) dependency edges.", "If T1 is affected by T2, then there is no path by which T2 can also affect T1.", "Proscribing G1 is clearly weaker than proscribing P1 since G1 allows reads from uncommitted transactions….", "Our PL-2 definition treats predicate-based reads like normal reads and provides no extra guarantees for them; we believe this approach is the most useful and flexible.", "Isolation Level PL-2.99 (Portable definition of Repeatable Read)  The level called Repeatable Read… provides less than full serializability with respect to predicates.", "Anti-dependency cycles due to predicates can occur at this level.", "Phenomenon G2-item, Item Anti-Dependency Cycles occurs when a DSG contains a directed cycle having one or more item anti-dependency edges.", "PL-2.99 forbids G2-item anomalies in addition to G1 (and hence G0).", "Isolation Level PL-3 (Portable definition of Serializable)  Consider the following history and DSG:  When T1 performs its query, there are exactly two employees, x and y, both in Sales.", "T1 sums up the salaries of these employees and compares it with the sum-of-salaries maintained for this department.", "However, before it performs the final check, T2 inserts a new employee, z2, in the Sales department, update the sum-of-salaries, and commits.", "Thus when T1 reads the new sum-of-salaries value it finds an inconsistency.", "This history is allowed at isolation level PL-2.99, but forbidden at isolation level PL-3, which prohibits phenomenon G2:  G2 Anti-dependency cycles occurs when a DSG contains a directed cycle with one or more anti-dependency edges (item or predicate).", "Proscribing G2 is weaker than providing P2, since we allow a transaction to modify object x even after another uncommitted transaction has read x….", "our specification for PL-3 provides conflict-serializability (this can be shown using theorems presented in [9]).", "All realistic implementations provide conflict-serializability; thus our PL-3 conditions provide what is normally considered as serializability.", "Mixing Isolation Levels  In a mixed system, each transaction specifies its level when it starts and this information is maintained as part of the history and used to construct a mixed serialization graph or MSG.", "Like a DSG, the MSG contains nodes corresponding to committed transactions and edges corresponding to dependencies, but only dependencies relevant to a transaction’s level or obligatory dependencies show up as edges in the graph… For example, an anti-dependency edge from a PL-3 transaction to a PL-1 transaction is an obligatory edge since overwriting of reads matters at level PL-3.", "If an MSG is acyclic, and phenomena G1a and G1b do not occur for PL-2 and PL-3 transactions then a history is mixing correct.", "Such a history provides each transaction with the guarantees that pertain to its level.", "Broader Applicability  Our approach is applicable to other levels in addition to the ones discussed in the paper.", "We have developed implementation-independent specifications of commercial isolation levels such as Snapshot Isolation and Cursor Stability, and we have defined a new level called PL-2+; the details can be found in [1 – Adya’s PhD thesis].", "PL-2+ is the the weakest level that guarantees consistent reads and causal consistency; it is useful in client-server systems and broadcast environments."], "summary_text": "Generalized Isolation Level Definitions – Adya et al. 2000  Following on from yesterday’s critique of ANSI SQL isolation levels , today’s paper also gives a precise definition of isolation levels – but does so in a way that is inclusive of optimistic and general multi-version concurrency control strategies instead of being dependent on locking. Where Berenson et al. define phenomena P0..P3 in order to explain the ANSI SQL isolation levels (degree 1, 2, and 3), Adya et al. define generalised phenomena labelled G0, G1, G2 whose definitions are independent of implementation strategy. Based on these generalised phenomena they then provide portable isolation level definitions (PL) that accommodate all implementation strategies, not  just locking based ones. We now show that the preventative approach is overly restrictive since it rules out optimistic and multi-version implementations. As mentioned, this approach disallows all histories that would not occur in a locking scheme and prevents conflicting operations from executing concurrently… The real problem with the preventative approach is that the phenomena are expressed in terms of single-object histories. However, the properties of interest are often multi-object constraints. To avoid problems with such constraints, the phenomena need to restrict what can be done with individual objects more than is necessary. Our approach avoids this difficulty by using specifications that capture constraints on multiple objects directly. Furthermore, the definitions in the preventative approach are not applicable to multi-version systems since they are described in terms of objects rather than in terms of versions. On the other hand, our specifications deal with multi-version and single-version histories. Following a motivation section, the authors first build up a model of a database system, and then define different types of conflict (read and write dependencies, as well as something called an anti-dependency that we’ll get too) that can occur within that model. This leads to the central notion of a Direct Serialization Graph (DSG) for a history. The phenomena and isolation levels are then specified as constraints on the allowable graphs. How to construct a Direct Serialization Graph  A DSG has one node for every committed transaction. (Directed) edges between these nodes represent read/write/anti-dependency conflicts. A transaction T2 depends on T1 if there is a path in the graph from T1 to T2. It directly depends on T1 if there is an edge from T1 to T2. In the description that follows, a delete is modelled as committing a special “dead” version of an object. To construct a DSG start with one node for every committed transaction, and for all pairs of distinct nodes T1 and T2, add a read dependency edge from T1 to T2 if any of the following conditions hold:  T1 commits a version xi of object x,  and T2 subsequently reads xi. T1 commits a change, and T2 then performs a predicate-based read such that the set of objects matched by the predicate are altered by T1’s commit. Furthermore, T1 is the most recent transaction to have committed a change impacting T1’s matches. Next add an anti-dependency edge from T1 to T2 if any of the following conditions hold:  T1 reads some version xi of object x, and T2 then commits the next version of x in the version history. T1 performs a predicate based read. T2 then commits a later (the next?) version of some object that would change the matches of T1. And finally add a write dependency edge from T1 to T2 if  T1 commits version xi of some object, and T2 then commits the next version of x in the version history. The dependency types and corresponding DSG edges are summarised below:  Phenomena and Isolation Levels  Isolation Level PL-1 (Portable definition of Read Uncommitted)  Phenomena P0 prevents dirty writes, and is motivated by a requirement to be able to serialize transactions based on writes, and to simplify recovery from aborts. The latter reason is not applicable to all systems, that may use other schemes for recovery. Portable Isolation level PL-1 provides for serialized transactions based on writes by disallowing Write Cycles. A Write Cycle is phenomenon G0. A history exhibits a write cycle if its direct serialization graph contains a cycle consisting entirely of write-dependency edges. Note that this definition neatly encompasses write cycles with more than 2 objects involved. Isolation Level PL-2 (Portable definition of Read Committed)  The essence of no dirty reads – without being overly restrictive on the reading and writing of objects written by transactions that are still uncommitted – is captured by phenomenon G1, which is comprised of three sub-phenomena G1a, G1b, and G1c. G1 subsumes G0, and PL-2 is the isolation level that prohibits G1 (and hence G0 also). G1a – Aborted Reads. T2 reads some object (including via predicates reads) modified by T1, and T1 aborts. To prevent aborted reads, if T2 reads from T1 and T1 aborts, T2 must also abort – known as a cascading abort. G1b – Intermediate Reads. T2 reads a version of some object (including via predicate reads) modified by T1, and it was not T1’s final modification of that object. To prevent intermediate reads, transactions can only be allowed to commit if they have read the final versions of objects from other transactions. G1c – Circular Information Flow. The Direct Serialization Graph contains a directed cycle consisting entirely of (read and write) dependency edges. If T1 is affected by T2, then there is no path by which T2 can also affect T1. Proscribing G1 is clearly weaker than proscribing P1 since G1 allows reads from uncommitted transactions…. Our PL-2 definition treats predicate-based reads like normal reads and provides no extra guarantees for them; we believe this approach is the most useful and flexible. Isolation Level PL-2.99 (Portable definition of Repeatable Read)  The level called Repeatable Read… provides less than full serializability with respect to predicates. Anti-dependency cycles due to predicates can occur at this level. Phenomenon G2-item, Item Anti-Dependency Cycles occurs when a DSG contains a directed cycle having one or more item anti-dependency edges. PL-2.99 forbids G2-item anomalies in addition to G1 (and hence G0). Isolation Level PL-3 (Portable definition of Serializable)  Consider the following history and DSG:  When T1 performs its query, there are exactly two employees, x and y, both in Sales. T1 sums up the salaries of these employees and compares it with the sum-of-salaries maintained for this department. However, before it performs the final check, T2 inserts a new employee, z2, in the Sales department, update the sum-of-salaries, and commits. Thus when T1 reads the new sum-of-salaries value it finds an inconsistency. This history is allowed at isolation level PL-2.99, but forbidden at isolation level PL-3, which prohibits phenomenon G2:  G2 Anti-dependency cycles occurs when a DSG contains a directed cycle with one or more anti-dependency edges (item or predicate). Proscribing G2 is weaker than providing P2, since we allow a transaction to modify object x even after another uncommitted transaction has read x…. our specification for PL-3 provides conflict-serializability (this can be shown using theorems presented in [9]). All realistic implementations provide conflict-serializability; thus our PL-3 conditions provide what is normally considered as serializability. Mixing Isolation Levels  In a mixed system, each transaction specifies its level when it starts and this information is maintained as part of the history and used to construct a mixed serialization graph or MSG. Like a DSG, the MSG contains nodes corresponding to committed transactions and edges corresponding to dependencies, but only dependencies relevant to a transaction’s level or obligatory dependencies show up as edges in the graph… For example, an anti-dependency edge from a PL-3 transaction to a PL-1 transaction is an obligatory edge since overwriting of reads matters at level PL-3. If an MSG is acyclic, and phenomena G1a and G1b do not occur for PL-2 and PL-3 transactions then a history is mixing correct. Such a history provides each transaction with the guarantees that pertain to its level. Broader Applicability  Our approach is applicable to other levels in addition to the ones discussed in the paper. We have developed implementation-independent specifications of commercial isolation levels such as Snapshot Isolation and Cursor Stability, and we have defined a new level called PL-2+; the details can be found in [1 – Adya’s PhD thesis]. PL-2+ is the the weakest level that guarantees consistent reads and causal consistency; it is useful in client-server systems and broadcast environments.", "pdf_url": "http://bnrg.cs.berkeley.edu/~adj/cs262/papers/icde00.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/generalized-isolation-level-definitions.json"}
{"id": "31021586", "bin": "1500_1600", "summary_sentences": ["The why and how of nonnegative matrix factorization Gillis, arXiv 2014 from: ‘ Regularization, Optimization, Kernels, and Support Vector Machines .’  Last week we looked at the paper ‘ Beyond news content ,’ which made heavy use of nonnegative matrix factorisation.", "Today we’ll be looking at that technique in a little more detail.", "As the name suggests, ‘The Why and How of Nonnegative matrix factorisation’ describes both why NMF is interesting (the intuition for how it works), and how to compute an NMF.", "I’m mostly interested in the intuition (and also out of my depth for some of the how!", "), but I’ll give you a sketch of the implementation approaches.", "Nonnegative matrix factorization (NMF) has become a widely used tool for the analysis of high dimensional data as it automatically extracts sparse and meaningful features from a set of nonnegative data vectors.", "NMF was first introduced by Paatero andTapper in 1994, and popularised in a article by Lee and Seung in 1999.", "Since then, the number of publications referencing the technique has grown rapidly:  What is NMF?", "NMF approximates a matrix  with a low-rank matrix approximation such that  .", "For the discussion in this paper, we’ll assume that  is set up so that there are  data points each with  dimensions, and every column of  is a data point, i.e. .", "We want to reduce the  original dimensions to  (aka, create a rank  approximation).", "So we’ll have  and  .", "The interpretation of  is that each column is a basis element.", "By basis element we mean some component that crops up again and again in all of the  original data points.", "These are the fundamental building blocks from which we can reconstruct approximations to all of the original data points.", "The interpretation of  is that each column gives the ‘coordinates of a data point’ in the basis  .", "In other words, it tells you how to reconstruct an approximation to the original data point from a linear combination of the building blocks in  A popular way of measuring how good the approximation  actually is, is the Frobenius norm (denoted by the F subscript you may have noticed).", "The Frobenius norm is:  .", "An optimal approximation to the Frobenius norm can be computed through truncated Singular Value Decomposition (SVD).", "Why does it work?", "The intuition.", "The reason why NMF has become so popular is because of its ability to automatically extract sparse and easily interpretable factors.", "The authors give three examples of NMF at work: in image processing, text mining, and hyperspectral imaging.", "Image processing  Say we take a gray-level image of a face containing p pixels, and squash the data into a single vector such that the ith entry represents the value of the ith pixel.", "Let the rows of  represent the p pixels, and the n columns each represent one image.", "NMF will produce two matrices W and H. The columns of W can be interpreted as images (the basis images), and H tells us how to sum up the basis images in order to reconstruct an approximation to a given face.", "In the case of facial images, the basis images are features such as eyes, noses, moustaches, and lips, while the columns of H indicate which feature is present in which image.", "Text mining  In text mining consider the bag-of-words matrix representation where each row corresponds to a word, and each column to a document (for the attentive reader, that’s the transpose of the bag-of-words matrix we looked at in ‘ Beyond news content ’9).", "NMF will produce two matrices W and H. The columns of W can be interpreted as basis documents (bags of words).", "What interpretation can we give to such a basis document in this case?", "They represent topics!", "Sets of words found simultaneously in different documents.", "H tells us how to sum contributions from different topics to reconstruct the word mix of a given original document.", "Therefore, given a set of documents, NMF identifies topics and simultaneously classifies the documents among these different topics.", "Hyperspectral unmixing  A hyperspectral image typically has 100 to 200 wavelength-indexed bands showing the fraction of incident light being reflected by the pixel at each of those wavelengths.", "Given such an image we want to identify the different materials present in it (e.g. grass, roads, metallic surfaces) – these are called the endmembers.", "Then we want to know which endmembers are present in each pixel, and in what proportion.", "For example, a pixel might be reflecting 0.3 x the spectral signal of grass, and 0.7 x the spectral signal of a road surface.", "NMF will produce two matrices W and H. The columns of W can be interpreted as basis endmembers.", "H tells us how to sum contributions from different endmembers to reconstruct the spectral signal observed at a pixel.", "…given a hyperspectral image, NMF is able to compute the spectral signatures of the endmembers, and simultaneously the abundance of each endmember in each pixel.", "Implementing NMF  For a rank r factorisation, we have the following optimisation problem:  Though note that the Frobenius norm show here assumes Gaussian noise, and other norms may be used in practice depending on the distribution (e.g., Kullback-Leibler divergence for text-mining, the Itakura-Saito distance for music analysis, or the  norm to improve robustness against outliers).", "So far everything to do with NMF sounds pretty good, until you reach the key moment in section 3:  There are many issues when using NMF in practice.", "In particular, NMF is NP-hard.", "Unfortunately, as opposed to the unconstrained problem which can be solved efficiently using the SVD, NMF is NP-hard in general.", "Fortunately there are heuristic approximations which have been proven to work well in many applications.", "Another issue with NMF is that there is not guaranteed to be a single unique decomposition (in general, there might be many schemes for defining sets of basis elements).", "For example, in text mining you would end up with different topics and classifications.", "“In practice, this issue is tackled using other priors on the factors W and H and adding proper regularization terms in the objective function.”  Finally, it’s hard to know how to choose the factorisation rank, r. Some approaches include trial and error, estimation using SVD based of the decay of the singular values, and insights from experts (e.g., there are roughly so many endmembers you might expect to find in a hyperspectral image).", "Almost all NMF algorithms use a two-block coordinate descent scheme (exact or inexact), that is, they optimize alternatively over one of the two factors, W or H, while keeping the other fixed.", "The reason is that the subproblem in one factor is convex.", "More precisely, it is a nonnegative least squares problem (NNLS).", "Many algorithms exist to solve the NNLS problem; and NMF algorithms based on two-block coordinate descent differ by which NNLS algorithm is used.", "Some NNLS algorithms that can be plugged in include multiplicative updates, alternating least squares, alternating nonnegative least squares, and hierarchical alternating least squares.", "The following charts show the performance of these algorithms on a dense data set (left), and a sparse data set (right).", "You can initialise W and H randomly, but there are also alternate strategies designed to give better initial estimates in the hope of converging more rapidly to a good solution:  Use some clustering method, and make the cluster means of the top r clusters as the columns of W, and H as a scaling of the cluster indicator matrix (which elements belong to which cluster).", "Finding the best rank-r approximation of X using SVD and using this to initialise W and H (see section 3.1.8)  Picking r columns of X and just using those as the initial values for W.  Section 3.2 in the paper discusses an emerging class of polynomial time algorithms for NMF in the special case where the matrix X is r-separable.", "That is, there exist a subset of r columns such that all other columns of X can be reconstructed from them.", "In the text mining example for instance this would mean that each topic has at least one document focused solely on that topic.", "… we believe NMF has a bright future…"], "summary_text": "The why and how of nonnegative matrix factorization Gillis, arXiv 2014 from: ‘ Regularization, Optimization, Kernels, and Support Vector Machines .’  Last week we looked at the paper ‘ Beyond news content ,’ which made heavy use of nonnegative matrix factorisation. Today we’ll be looking at that technique in a little more detail. As the name suggests, ‘The Why and How of Nonnegative matrix factorisation’ describes both why NMF is interesting (the intuition for how it works), and how to compute an NMF. I’m mostly interested in the intuition (and also out of my depth for some of the how! ), but I’ll give you a sketch of the implementation approaches. Nonnegative matrix factorization (NMF) has become a widely used tool for the analysis of high dimensional data as it automatically extracts sparse and meaningful features from a set of nonnegative data vectors. NMF was first introduced by Paatero andTapper in 1994, and popularised in a article by Lee and Seung in 1999. Since then, the number of publications referencing the technique has grown rapidly:  What is NMF? NMF approximates a matrix  with a low-rank matrix approximation such that  . For the discussion in this paper, we’ll assume that  is set up so that there are  data points each with  dimensions, and every column of  is a data point, i.e. . We want to reduce the  original dimensions to  (aka, create a rank  approximation). So we’ll have  and  . The interpretation of  is that each column is a basis element. By basis element we mean some component that crops up again and again in all of the  original data points. These are the fundamental building blocks from which we can reconstruct approximations to all of the original data points. The interpretation of  is that each column gives the ‘coordinates of a data point’ in the basis  . In other words, it tells you how to reconstruct an approximation to the original data point from a linear combination of the building blocks in  A popular way of measuring how good the approximation  actually is, is the Frobenius norm (denoted by the F subscript you may have noticed). The Frobenius norm is:  . An optimal approximation to the Frobenius norm can be computed through truncated Singular Value Decomposition (SVD). Why does it work? The intuition. The reason why NMF has become so popular is because of its ability to automatically extract sparse and easily interpretable factors. The authors give three examples of NMF at work: in image processing, text mining, and hyperspectral imaging. Image processing  Say we take a gray-level image of a face containing p pixels, and squash the data into a single vector such that the ith entry represents the value of the ith pixel. Let the rows of  represent the p pixels, and the n columns each represent one image. NMF will produce two matrices W and H. The columns of W can be interpreted as images (the basis images), and H tells us how to sum up the basis images in order to reconstruct an approximation to a given face. In the case of facial images, the basis images are features such as eyes, noses, moustaches, and lips, while the columns of H indicate which feature is present in which image. Text mining  In text mining consider the bag-of-words matrix representation where each row corresponds to a word, and each column to a document (for the attentive reader, that’s the transpose of the bag-of-words matrix we looked at in ‘ Beyond news content ’9). NMF will produce two matrices W and H. The columns of W can be interpreted as basis documents (bags of words). What interpretation can we give to such a basis document in this case? They represent topics! Sets of words found simultaneously in different documents. H tells us how to sum contributions from different topics to reconstruct the word mix of a given original document. Therefore, given a set of documents, NMF identifies topics and simultaneously classifies the documents among these different topics. Hyperspectral unmixing  A hyperspectral image typically has 100 to 200 wavelength-indexed bands showing the fraction of incident light being reflected by the pixel at each of those wavelengths. Given such an image we want to identify the different materials present in it (e.g. grass, roads, metallic surfaces) – these are called the endmembers. Then we want to know which endmembers are present in each pixel, and in what proportion. For example, a pixel might be reflecting 0.3 x the spectral signal of grass, and 0.7 x the spectral signal of a road surface. NMF will produce two matrices W and H. The columns of W can be interpreted as basis endmembers. H tells us how to sum contributions from different endmembers to reconstruct the spectral signal observed at a pixel. …given a hyperspectral image, NMF is able to compute the spectral signatures of the endmembers, and simultaneously the abundance of each endmember in each pixel. Implementing NMF  For a rank r factorisation, we have the following optimisation problem:  Though note that the Frobenius norm show here assumes Gaussian noise, and other norms may be used in practice depending on the distribution (e.g., Kullback-Leibler divergence for text-mining, the Itakura-Saito distance for music analysis, or the  norm to improve robustness against outliers). So far everything to do with NMF sounds pretty good, until you reach the key moment in section 3:  There are many issues when using NMF in practice. In particular, NMF is NP-hard. Unfortunately, as opposed to the unconstrained problem which can be solved efficiently using the SVD, NMF is NP-hard in general. Fortunately there are heuristic approximations which have been proven to work well in many applications. Another issue with NMF is that there is not guaranteed to be a single unique decomposition (in general, there might be many schemes for defining sets of basis elements). For example, in text mining you would end up with different topics and classifications. “In practice, this issue is tackled using other priors on the factors W and H and adding proper regularization terms in the objective function.”  Finally, it’s hard to know how to choose the factorisation rank, r. Some approaches include trial and error, estimation using SVD based of the decay of the singular values, and insights from experts (e.g., there are roughly so many endmembers you might expect to find in a hyperspectral image). Almost all NMF algorithms use a two-block coordinate descent scheme (exact or inexact), that is, they optimize alternatively over one of the two factors, W or H, while keeping the other fixed. The reason is that the subproblem in one factor is convex. More precisely, it is a nonnegative least squares problem (NNLS). Many algorithms exist to solve the NNLS problem; and NMF algorithms based on two-block coordinate descent differ by which NNLS algorithm is used. Some NNLS algorithms that can be plugged in include multiplicative updates, alternating least squares, alternating nonnegative least squares, and hierarchical alternating least squares. The following charts show the performance of these algorithms on a dense data set (left), and a sparse data set (right). You can initialise W and H randomly, but there are also alternate strategies designed to give better initial estimates in the hope of converging more rapidly to a good solution:  Use some clustering method, and make the cluster means of the top r clusters as the columns of W, and H as a scaling of the cluster indicator matrix (which elements belong to which cluster). Finding the best rank-r approximation of X using SVD and using this to initialise W and H (see section 3.1.8)  Picking r columns of X and just using those as the initial values for W.  Section 3.2 in the paper discusses an emerging class of polynomial time algorithms for NMF in the special case where the matrix X is r-separable. That is, there exist a subset of r columns such that all other columns of X can be reconstructed from them. In the text mining example for instance this would mean that each topic has at least one document focused solely on that topic. … we believe NMF has a bright future…", "pdf_url": "https://arxiv.org/pdf/1401.5226", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/the-why-and-how-of-nonnegative-matrix-factorization.json"}
{"id": "25809281", "bin": "200_300", "summary_sentences": ["The paper proposes two variants of Word2Vec model so that it may account for syntactic properties of words and perform better on syntactic tasks like POS tagging and dependency parsing.", "In the original Skip-Gram setting, the model predicts the 2c words in the context window (c is the size of the context window).", "But it uses the same set of parameters whether predicting the word next to the centre word or the word farthest away, thus losing all information about the word order.", "Similarly, the CBOW (Continuous Bas Of Words) model just adds the embedding of all the surrounding words thereby losing the word order information.", "The paper proposes to use a set of 2c matrices each for a different word in the context window for both Skip-Gram and CBOW models.", "This simple trick allows for accounting of syntactic properties in the word vectors and improves the performance of dependency parsing task and POS tagging.", "The downside of using this is that now the model has far more parameters than before which increases the training time and needs a large enough corpus to avoid sparse representation."], "summary_text": "The paper proposes two variants of Word2Vec model so that it may account for syntactic properties of words and perform better on syntactic tasks like POS tagging and dependency parsing. In the original Skip-Gram setting, the model predicts the 2c words in the context window (c is the size of the context window). But it uses the same set of parameters whether predicting the word next to the centre word or the word farthest away, thus losing all information about the word order. Similarly, the CBOW (Continuous Bas Of Words) model just adds the embedding of all the surrounding words thereby losing the word order information. The paper proposes to use a set of 2c matrices each for a different word in the context window for both Skip-Gram and CBOW models. This simple trick allows for accounting of syntactic properties in the word vectors and improves the performance of dependency parsing task and POS tagging. The downside of using this is that now the model has far more parameters than before which increases the training time and needs a large enough corpus to avoid sparse representation.", "pdf_url": "http://www.cs.cmu.edu/~lingwang/papers/naacl2015.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/two-too-simple-adaptations-of-word2vec-for-syntax-problems.json"}
{"id": "6799048", "bin": "200_300", "summary_sentences": ["The paper discusses neural module network trees (NMN-trees).", "Here modules are composed in a tree structure to answer a question/task and modules are trained in different configurations to ensure they learn more core concepts and can generalize.", "Longer summary:  How to perform systematic generalization?", "First we need to ask how good current models are at understanding language.", "Adversarial examples show how fragile these models can be.", "This leads us to conclude that systematic generalization is an issue that requires specific attention.", "Maybe we should rethink the modeling assumptions being made.", "We can think that samples can come from different data domains but are generated by some set of shared rules.", "If we correctly learned these rules then domain shift in the test data would not hurt model performance.", "Currently we can construct an experiment to introduce systematic bias in the data which causes the performance to suffer.", "From this experiment we can start to determine what the issue is.", "A recent new idea is to force a model to have more independent units is neural module network trees (NMN-trees).", "Here modules are composed in a tree structure to answer a question/task and modules are trained in different configurations to ensure they learn more core concepts and can generalize."], "summary_text": "The paper discusses neural module network trees (NMN-trees). Here modules are composed in a tree structure to answer a question/task and modules are trained in different configurations to ensure they learn more core concepts and can generalize. Longer summary:  How to perform systematic generalization? First we need to ask how good current models are at understanding language. Adversarial examples show how fragile these models can be. This leads us to conclude that systematic generalization is an issue that requires specific attention. Maybe we should rethink the modeling assumptions being made. We can think that samples can come from different data domains but are generated by some set of shared rules. If we correctly learned these rules then domain shift in the test data would not hurt model performance. Currently we can construct an experiment to introduce systematic bias in the data which causes the performance to suffer. From this experiment we can start to determine what the issue is. A recent new idea is to force a model to have more independent units is neural module network trees (NMN-trees). Here modules are composed in a tree structure to answer a question/task and modules are trained in different configurations to ensure they learn more core concepts and can generalize.", "pdf_url": "http://arxiv.org/pdf/1811.12889v3", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/1811.12889.json"}
{"id": "93218559", "bin": "200_300", "summary_sentences": ["The goal of this method is to create a feature representation $f$ of an input $x$ that is domain invariant over some domain $d$.", "The feature vector $f$ is obtained from $x$ using an encoder network (e.g. $f = G_f(x)$).", "The reason this is an issue is that the input $x$ is correlated with $d$ and this can confuse the model to extract features that capture differences in domains instead of differences in classes.", "Here I will recast the problem differently from in the paper:  **Problem:** Given a conditional probability $p(x|d=0)$ that may be different from $p(x|d=1)$:  $$p(x|d=0) \\stackrel{?", "}{\\ne} p(x|d=1)$$  we would like it to be the case that these distributions are equal.", "$$p(G_f(x) |d=0) = p(G_f(x)|d=1)$$  aka:  $$p(f|d=0) = p(f|d=1)$$  Of course this is an issue if some class label $y$ is correlated with $d$ meaning that we may hurt the performance of a classifier that now may not be able to predict $y$ as well as before.", "[url]"], "summary_text": "The goal of this method is to create a feature representation $f$ of an input $x$ that is domain invariant over some domain $d$. The feature vector $f$ is obtained from $x$ using an encoder network (e.g. $f = G_f(x)$). The reason this is an issue is that the input $x$ is correlated with $d$ and this can confuse the model to extract features that capture differences in domains instead of differences in classes. Here I will recast the problem differently from in the paper:  **Problem:** Given a conditional probability $p(x|d=0)$ that may be different from $p(x|d=1)$:  $$p(x|d=0) \\stackrel{? }{\\ne} p(x|d=1)$$  we would like it to be the case that these distributions are equal. $$p(G_f(x) |d=0) = p(G_f(x)|d=1)$$  aka:  $$p(f|d=0) = p(f|d=1)$$  Of course this is an issue if some class label $y$ is correlated with $d$ meaning that we may hurt the performance of a classifier that now may not be able to predict $y$ as well as before. [url]", "pdf_url": "http://arxiv.org/pdf/1409.7495v2", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/1409.7495.json"}
{"id": "22127536", "bin": "300_400", "summary_sentences": ["The paper presents gradient computation based techniques to visualise image classification models.", "Experimental Setup  Single deep convNet trained on ILSVRC-2013 dataset (1.2M training images and 1000 classes).", "Weight layer configuration is: conv64-conv256-conv256-conv256-conv256-full4096-full4096-full1000.", "Class Model Visualisation  Given a learnt ConvNet and a class (of interest), start with the zero image and perform optimisation by back propagating with respect to the input image (keeping the ConvNet weights constant).", "Add the mean image (for training set) to the resulting image.", "The paper used unnormalised class scores so that optimisation focuses on increasing the score of target class and not decreasing the score of other classes.", "Image-Specific Class Saliency Visualisation  Given an image, class of interest, and trained ConvNet, rank the pixels of the input image based on their influence on class scores.", "Derivative of the class score with respect to image gives an estimate of the importance of different pixels for the class.", "The magnitude of derivative also indicated how much each pixel needs to be changed to improve the class score.", "Class Saliency Extraction  Find the derivative of the class score with respect with respect to the input image.", "This would result in one single saliency map per colour channel.", "To obtain a single saliency map, take the maximum magnitude of derivative across all colour channels.", "Weakly Supervised Object Localisation  The saliency map for an image provides a rough encoding of the location of the object of the class of interest.", "Given an image and its saliency map, an object segmentation map can be computed using GraphCut colour segmentation.", "Color continuity cues are needed as saliency maps might capture only the most dominant part of the object in the image.", "This weakly supervised approach achieves 46.4% top-5 error on the test set of ILSVRC-2013.", "Relation to Deconvolutional Networks  DeconvNet-based reconstruction of the n-th layer input is similar to computing the gradient of the visualised neuron activity f with respect to the input layer.", "One difference is in the way RELU neurons are treated:  In DeconvNet, the sign indicator (for the derivative of RELU) is computed on output reconstruction while in this paper, the sign indicator is computed on the layer input."], "summary_text": "The paper presents gradient computation based techniques to visualise image classification models. Experimental Setup  Single deep convNet trained on ILSVRC-2013 dataset (1.2M training images and 1000 classes). Weight layer configuration is: conv64-conv256-conv256-conv256-conv256-full4096-full4096-full1000. Class Model Visualisation  Given a learnt ConvNet and a class (of interest), start with the zero image and perform optimisation by back propagating with respect to the input image (keeping the ConvNet weights constant). Add the mean image (for training set) to the resulting image. The paper used unnormalised class scores so that optimisation focuses on increasing the score of target class and not decreasing the score of other classes. Image-Specific Class Saliency Visualisation  Given an image, class of interest, and trained ConvNet, rank the pixels of the input image based on their influence on class scores. Derivative of the class score with respect to image gives an estimate of the importance of different pixels for the class. The magnitude of derivative also indicated how much each pixel needs to be changed to improve the class score. Class Saliency Extraction  Find the derivative of the class score with respect with respect to the input image. This would result in one single saliency map per colour channel. To obtain a single saliency map, take the maximum magnitude of derivative across all colour channels. Weakly Supervised Object Localisation  The saliency map for an image provides a rough encoding of the location of the object of the class of interest. Given an image and its saliency map, an object segmentation map can be computed using GraphCut colour segmentation. Color continuity cues are needed as saliency maps might capture only the most dominant part of the object in the image. This weakly supervised approach achieves 46.4% top-5 error on the test set of ILSVRC-2013. Relation to Deconvolutional Networks  DeconvNet-based reconstruction of the n-th layer input is similar to computing the gradient of the visualised neuron activity f with respect to the input layer. One difference is in the way RELU neurons are treated:  In DeconvNet, the sign indicator (for the derivative of RELU) is computed on output reconstruction while in this paper, the sign indicator is computed on the layer input.", "pdf_url": "https://arxiv.org/pdf/1312.6034", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/8da7f77418aa22751ffed115779126.json"}
{"id": "39357436", "bin": "300_400", "summary_sentences": ["Word based language models suffer from the problem of rare or Out of Vocabulary (OOV) words.", "Learning representations for OOV words directly on the end task often results in poor representation.", "The alternative is to replace all the rare words with a single, unique representation (loss of information) or use character level models to obtain word representations (they tend to miss on the semantic relationship).", "The paper proposes to learn a network that can predict the representations of words using auxiliary data (referred to as definitions) such as dictionary definitions, Wikipedia infoboxes, the spelling of the word etc.", "The auxiliary data encoders are trained jointly with the end task to ensure that word representations align with the requirements of the end task.", "Approach  Given a rare word w, let d(w) = <x1, x2…> denote its defination where xi are words.", "d(w) is fed to a defination reader network f (LSTM) and its last state is used as the defination embedding ed(w)  In case w has multiple definitions, the embeddings are combined using mean pooling.", "The approach can be extended to in-vocabulary words as well by using the definition embedding of such words to update their original embeddings.", "Experiments  Auxiliary data sources  Word definitions from WordNet  Spelling of words  The proposed approach was tested on following tasks:  Extractive Question Answering over SQuAD  Base model from Xiong et al. 2016  Entailment Prediction over SNLI corpus  Base models from Bowman et al.", "2015 and Chen et al. 2016  One Billion Words Language Modelling  For all the tasks, models using both spelling and dictionary (SD) outperformed the model using just one.", "While SD does not outperform the Glove model (with full vocabulary), it does bridge the performance gap significantly.", "Future Work  Multi-token words like “San Francisco” are not accounted for now.", "The model does not handle the rare words which appear in the definition and just replaces them by the  token.", "Making the model recursive would be a useful addition."], "summary_text": "Word based language models suffer from the problem of rare or Out of Vocabulary (OOV) words. Learning representations for OOV words directly on the end task often results in poor representation. The alternative is to replace all the rare words with a single, unique representation (loss of information) or use character level models to obtain word representations (they tend to miss on the semantic relationship). The paper proposes to learn a network that can predict the representations of words using auxiliary data (referred to as definitions) such as dictionary definitions, Wikipedia infoboxes, the spelling of the word etc. The auxiliary data encoders are trained jointly with the end task to ensure that word representations align with the requirements of the end task. Approach  Given a rare word w, let d(w) = <x1, x2…> denote its defination where xi are words. d(w) is fed to a defination reader network f (LSTM) and its last state is used as the defination embedding ed(w)  In case w has multiple definitions, the embeddings are combined using mean pooling. The approach can be extended to in-vocabulary words as well by using the definition embedding of such words to update their original embeddings. Experiments  Auxiliary data sources  Word definitions from WordNet  Spelling of words  The proposed approach was tested on following tasks:  Extractive Question Answering over SQuAD  Base model from Xiong et al. 2016  Entailment Prediction over SNLI corpus  Base models from Bowman et al. 2015 and Chen et al. 2016  One Billion Words Language Modelling  For all the tasks, models using both spelling and dictionary (SD) outperformed the model using just one. While SD does not outperform the Glove model (with full vocabulary), it does bridge the performance gap significantly. Future Work  Multi-token words like “San Francisco” are not accounted for now. The model does not handle the rare words which appear in the definition and just replaces them by the  token. Making the model recursive would be a useful addition.", "pdf_url": "https://arxiv.org/pdf/1506.03134", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/learning-to-compute-word-embeddings-on-the-fly.json"}
{"id": "98781755", "bin": "300_400", "summary_sentences": ["The paper introduces a simple data augmentation protocol that provides a good compositional inductive bias for sequential models.", "Synthetic examples are created by taking real sequences and replacing the fragments in sequences which appear in similar environments.", "This operation is referred to as GECA (Good Enough Compositional Augmentation).", "The underlying idea is that if two fragments of training examples occur in some environment, then any environment where the first fragment appears is also a valid environment for the second fragment.", "Approach  Discover substitutable fragments (ie pairs of fragments that co-occur with a common fragment) and use them to generate new sequences by swapping fragments.", "The current work uses very simple criteria to decide if fragments are substitutable - fragments should occur in at least one lexical environment that is exactly the same.", "A lexical environment is the k-word window around each span of the fragment.", "Though the idea can be motivated by work in generative syntax and distributional semantics, it would not hold like a physical law when applied to the real data.", "The authors view this tradeoff as a balance between the shortage of training data vs relative frequency of mistake in the proposed data augmentation approach.", "Results  The approach is evaluated on the SCAN dataset when the model is trained on the short sequence of English commands.", "Though the dataset augmentation helps the baseline models, it is not surprising given the nature of the SCAN dataset.", "More challenging tasks (for evaluating the proposed approach) are semantic parsing (where the query is represented in the form of λ calculus or SQL and low resource language modeling.", "While the improvement (in terms of metrics) is sometimes limited, the gains are consistent across different datasets.", "Given that the proposed approach is relatively simple and straightforward, it appears to be quite promising."], "summary_text": "The paper introduces a simple data augmentation protocol that provides a good compositional inductive bias for sequential models. Synthetic examples are created by taking real sequences and replacing the fragments in sequences which appear in similar environments. This operation is referred to as GECA (Good Enough Compositional Augmentation). The underlying idea is that if two fragments of training examples occur in some environment, then any environment where the first fragment appears is also a valid environment for the second fragment. Approach  Discover substitutable fragments (ie pairs of fragments that co-occur with a common fragment) and use them to generate new sequences by swapping fragments. The current work uses very simple criteria to decide if fragments are substitutable - fragments should occur in at least one lexical environment that is exactly the same. A lexical environment is the k-word window around each span of the fragment. Though the idea can be motivated by work in generative syntax and distributional semantics, it would not hold like a physical law when applied to the real data. The authors view this tradeoff as a balance between the shortage of training data vs relative frequency of mistake in the proposed data augmentation approach. Results  The approach is evaluated on the SCAN dataset when the model is trained on the short sequence of English commands. Though the dataset augmentation helps the baseline models, it is not surprising given the nature of the SCAN dataset. More challenging tasks (for evaluating the proposed approach) are semantic parsing (where the query is represented in the form of λ calculus or SQL and low resource language modeling. While the improvement (in terms of metrics) is sometimes limited, the gains are consistent across different datasets. Given that the proposed approach is relatively simple and straightforward, it appears to be quite promising.", "pdf_url": "https://arxiv.org/pdf/1904.09545", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/good-enough-compositional-data-augmentation.json"}
{"id": "12279648", "bin": "300_400", "summary_sentences": ["“Optimism in the face of uncertainty”, the mantra of the Upper-Confidence Bound 1 algorithm, becomes impractical to follow when the action space is continuous.", "Hence, most approaches default to using epsilon-greedy exploration.", "This paper proposes a scalable and efficient method for assigning exploration bonuses in large RL problems with complex observations.", "A model of the task dynamics is learned to assess the novelty of a new state.", "As the ability to model the dynamics of a particular state-action pair improves, the “understanding” of the state is thus better and hence its novelty is lower.", "This circumvents the need to explicitly maintain visitation frequencies for states and state-action pairs in a table.", "When a state-action pair is not understood well enough to make accurate predictions, it is assumed that more knowledge is needed and hence a higher “novelty” value is assigned to that reward signal.", "Evidence  This approach was evaluated on 14 games in the Arcade Learning Environment (ALE)  The reinforcement learning algorithm that was employed was DQN, and performance was evaluated against DQN with epsilon-greedy exploration, Boltzman exploration, and Thompson Sampling  Not clear that this approach outperforms other state-of-the-art methods consistently  Strengths  The paper references methods that were attempted but ultimately failed, such as learning a dynamics model that would predict raw frames (next states) for the Atari simulation  Weaknesses  Need to see this method tested on other environments and scenarios  Interesting related works  Thompson Sampling  Boltzman exploration  Notes  PAC-MDP algorithms such as MBIE-EB and Bayesian algorithms such as Bayesian Exploration Bonuses manage the exploration versus exploitation tradeoff by assigning bonuses to novel states.", "(What are thooose).", "These sound similar to the UCB1 exploration strategy  An autoencoder was used to obtain the function sigma that encodes the state prediction model.", "The choice of autoencoder was for dimensionality reduction of the state space  “The hidden layers are reduced in dimension until maximal compression occurs with 128 units”  An MLP with 2 layers was used to predict model dynamics.", "The sixth layer of the auto-encoder produces the state with reduced dimensionality"], "summary_text": "“Optimism in the face of uncertainty”, the mantra of the Upper-Confidence Bound 1 algorithm, becomes impractical to follow when the action space is continuous. Hence, most approaches default to using epsilon-greedy exploration. This paper proposes a scalable and efficient method for assigning exploration bonuses in large RL problems with complex observations. A model of the task dynamics is learned to assess the novelty of a new state. As the ability to model the dynamics of a particular state-action pair improves, the “understanding” of the state is thus better and hence its novelty is lower. This circumvents the need to explicitly maintain visitation frequencies for states and state-action pairs in a table. When a state-action pair is not understood well enough to make accurate predictions, it is assumed that more knowledge is needed and hence a higher “novelty” value is assigned to that reward signal. Evidence  This approach was evaluated on 14 games in the Arcade Learning Environment (ALE)  The reinforcement learning algorithm that was employed was DQN, and performance was evaluated against DQN with epsilon-greedy exploration, Boltzman exploration, and Thompson Sampling  Not clear that this approach outperforms other state-of-the-art methods consistently  Strengths  The paper references methods that were attempted but ultimately failed, such as learning a dynamics model that would predict raw frames (next states) for the Atari simulation  Weaknesses  Need to see this method tested on other environments and scenarios  Interesting related works  Thompson Sampling  Boltzman exploration  Notes  PAC-MDP algorithms such as MBIE-EB and Bayesian algorithms such as Bayesian Exploration Bonuses manage the exploration versus exploitation tradeoff by assigning bonuses to novel states. (What are thooose). These sound similar to the UCB1 exploration strategy  An autoencoder was used to obtain the function sigma that encodes the state prediction model. The choice of autoencoder was for dimensionality reduction of the state space  “The hidden layers are reduced in dimension until maximal compression occurs with 128 units”  An MLP with 2 layers was used to predict model dynamics. The sixth layer of the auto-encoder produces the state with reduced dimensionality", "pdf_url": "http://arxiv.org/pdf/1507.00814", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/incentivizing-exploraton-in-rl.json"}
{"id": "67930407", "bin": "400_500", "summary_sentences": ["The paper presents the task of abductive NLP (pronounced as alpha NLP) where the model needs to perform abductive reasoning.", "Abductive reasoning is the inference to the most plausible explanation.", "Even though it is considered to be an important component for understanding narratives, the work in this domain is sparse.", "A new dataset called as Abstractive Reasoning in narrative Text (ART) consisting of 20K narrative contexts and 200k explanations is also provided.", "The dataset models the task as multiple-choice questions to make the evaluation process easy.", "Task Setup  Given a pair of observations O1 and O2 and two hypothesis h1 and h2, the task is to select the most plausible hypothesis.", "In general, P(h | O1, O2) is propotional to P(h |O1)P(O2|h, O1).", "Different independence assumptions can be imposed on the structure of the problem eg one assumption could be that the hypothesis is independent of the observations or the “fully connected” assumption would jointly model both the observations and the hypothesis.", "Dataset  Along with crowdsourcing several plausible hypotheses for each observation instance pair, an adversarial filtering algorithm (AF) is used to remove weak pairs of hypothesis.", "Observation pairs are created using the ROCStories dataset which is a collection of short, manually crafted stories of 5 sentences.", "The average word length for both the content and the hypothesis is between 8 to 9.", "To collect plausible hypothesis, the crowd workers were asked to fill in a plausible “in-between” sentence in natural language.", "Given the plausible hypothesis, the crowd workers were asked to create an implausible hypothesis by editing fewer than 6 words.", "Adversarial filtering approach from Zellers et al. is used with BERT as the adversary.", "A temperature parameter is introduced to control the maximum number of instances that can be changed in each adversarial filtering iteration.", "Key Observations  Human performance: 91.4%  Baselines like SVM classifier, the bag-of-words classifier (using Glove) and max-pooling overt BiLSTM representation: approx 50%  Entailment NLI baseline: 59%.", "This highlights the additional complexity of abductive NLI as compared to entailment NLI.", "BERT: 68.9%  GPT: 63.1%  Numerical and spatial knowledge-based data points are particularly hard.", "The model is more likely to fail when the narrative created by the incorrect hypothesis is plausible"], "summary_text": "The paper presents the task of abductive NLP (pronounced as alpha NLP) where the model needs to perform abductive reasoning. Abductive reasoning is the inference to the most plausible explanation. Even though it is considered to be an important component for understanding narratives, the work in this domain is sparse. A new dataset called as Abstractive Reasoning in narrative Text (ART) consisting of 20K narrative contexts and 200k explanations is also provided. The dataset models the task as multiple-choice questions to make the evaluation process easy. Task Setup  Given a pair of observations O1 and O2 and two hypothesis h1 and h2, the task is to select the most plausible hypothesis. In general, P(h | O1, O2) is propotional to P(h |O1)P(O2|h, O1). Different independence assumptions can be imposed on the structure of the problem eg one assumption could be that the hypothesis is independent of the observations or the “fully connected” assumption would jointly model both the observations and the hypothesis. Dataset  Along with crowdsourcing several plausible hypotheses for each observation instance pair, an adversarial filtering algorithm (AF) is used to remove weak pairs of hypothesis. Observation pairs are created using the ROCStories dataset which is a collection of short, manually crafted stories of 5 sentences. The average word length for both the content and the hypothesis is between 8 to 9. To collect plausible hypothesis, the crowd workers were asked to fill in a plausible “in-between” sentence in natural language. Given the plausible hypothesis, the crowd workers were asked to create an implausible hypothesis by editing fewer than 6 words. Adversarial filtering approach from Zellers et al. is used with BERT as the adversary. A temperature parameter is introduced to control the maximum number of instances that can be changed in each adversarial filtering iteration. Key Observations  Human performance: 91.4%  Baselines like SVM classifier, the bag-of-words classifier (using Glove) and max-pooling overt BiLSTM representation: approx 50%  Entailment NLI baseline: 59%. This highlights the additional complexity of abductive NLI as compared to entailment NLI. BERT: 68.9%  GPT: 63.1%  Numerical and spatial knowledge-based data points are particularly hard. The model is more likely to fail when the narrative created by the incorrect hypothesis is plausible", "pdf_url": "https://arxiv.org/pdf/1908.05739", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/abductive-commonsense-reasoning.json"}
{"id": "50438213", "bin": "400_500", "summary_sentences": ["This paper presents a feed-forward neural network architecture for processing graphs as inputs, inspired from previous work on Graph Neural Networks.", "In brief, the architecture of the GG-NN corresponds to $T$ steps of GRU-like (gated recurrent units) updates, where T is a hyper-parameter.", "At each step, a vector representation is computed for all nodes in the graph, where a node's representation at step t is computed from the representation of nodes at step $t-1$.", "Specifically, the representation of a node will be updated based on the representation of its neighbors in the graph.", "Incoming and outgoing edges in the graph are treated differently by the neural network, by using different parameter matrices for each.", "Moreover, if edges have labels, separate parameters can be learned for the different types of edges (meaning that edge labels determine the configuration of parameter sharing in the model).", "Finally, GG-NNs can incorporate node-level attributes, by using them in the initialization (time step 0) of the nodes' representations.", "GG-NNs can be used to perform a variety of tasks on graphs.", "The per-node representations can be used to make per-node predictions by feeding them to a neural network (shared across nodes).", "A graph-level predictor can also be obtained using a soft attention architecture, where per-node outputs are used as scores into a softmax in order to pool the representations across the graph, and feed this graph-level representation to a neural network.", "The attention mechanism can be conditioned on a \"question\" (e.g. on a task to predict the shortest path in a graph, the question would be the identity of the beginning and end nodes of the path to find), which is fed to the node scorer of the soft attention mechanism.", "Moreover, the authors describe how to chain GG-NNs to go beyond predicting individual labels and predict sequences.", "Experiments on several datasets are presented.", "These include tasks where a single output is required (on a few bAbI tasks) as well as tasks where a sequential output is required, such as outputting the shortest path or the Eulerian circuit of a graph.", "Moreover, experiments on a much more complex and interesting program verification task are presented."], "summary_text": "This paper presents a feed-forward neural network architecture for processing graphs as inputs, inspired from previous work on Graph Neural Networks. In brief, the architecture of the GG-NN corresponds to $T$ steps of GRU-like (gated recurrent units) updates, where T is a hyper-parameter. At each step, a vector representation is computed for all nodes in the graph, where a node's representation at step t is computed from the representation of nodes at step $t-1$. Specifically, the representation of a node will be updated based on the representation of its neighbors in the graph. Incoming and outgoing edges in the graph are treated differently by the neural network, by using different parameter matrices for each. Moreover, if edges have labels, separate parameters can be learned for the different types of edges (meaning that edge labels determine the configuration of parameter sharing in the model). Finally, GG-NNs can incorporate node-level attributes, by using them in the initialization (time step 0) of the nodes' representations. GG-NNs can be used to perform a variety of tasks on graphs. The per-node representations can be used to make per-node predictions by feeding them to a neural network (shared across nodes). A graph-level predictor can also be obtained using a soft attention architecture, where per-node outputs are used as scores into a softmax in order to pool the representations across the graph, and feed this graph-level representation to a neural network. The attention mechanism can be conditioned on a \"question\" (e.g. on a task to predict the shortest path in a graph, the question would be the identity of the beginning and end nodes of the path to find), which is fed to the node scorer of the soft attention mechanism. Moreover, the authors describe how to chain GG-NNs to go beyond predicting individual labels and predict sequences. Experiments on several datasets are presented. These include tasks where a single output is required (on a few bAbI tasks) as well as tasks where a sequential output is required, such as outputting the shortest path or the Eulerian circuit of a graph. Moreover, experiments on a much more complex and interesting program verification task are presented.", "pdf_url": "http://arxiv.org/pdf/1511.05493", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/litbz15.json"}
{"id": "23242120", "bin": "400_500", "summary_sentences": ["SGD is a widely used optimization method for training the parameters of some model f on some given task.", "Since the convergence of SGD is related to the variance of the stochastic gradient estimate, there's been a lot of work on trying to come up with such stochastic estimates with smaller variance.", "This paper does it using an importance sampling (IS) Monte Carlo estimate of the gradient, and learning the proposal distribution $q$ of the IS estimate.", "The proposal distribution $q$ is parametrized in some way, and is trained to minimize the variance of the gradient estimate.", "It is trained simultaneously while the model $f$ that SGD (i.e. the SGD that uses IS to get its gradient) is training.", "To make this whole story more recursive, the proposal distribution $q$ is also trained with SGD :-) This makes sense, since one expects the best proposal to depend on the value of the parameters of model $f$, so the best proposal $q$ should vary as $f$ is trained.", "One application of this idea is in optimizing a classification model over a distribution that is imbalanced class-wise (e.g. there are classes with much fewer examples).", "In this case, the proposal distribution determines how frequently we sample examples from each class (conditioned on the class, training examples are chosen uniformly).", "#### My two cents  This is a really cool idea.", "I particularly like the application to training on an imbalanced classification problem.", "People have mostly been using heuristics to tackle this problem, such as initially sampling each class equally as often, and then fine-tuning/calibrating the model using the real class proportions.", "This approach instead proposes a really elegant, coherent, solution to this problem.", "I would have liked to see a comparison with that aforementioned heuristic (for mainly selfish reasons :-) ).", "They instead compare with an importance sampling approach with proposal that assigns the same probability to each class, which is a reasonable alternative (though I don't know if it's used as often as the more heuristic approach).", "There are other applications, to matrix factorization and reinforcement learning, that are presented in the paper and seem neat, though I haven't gone through those as much.", "Overall, one of my favorite paper this year: it's original, tackles a problem for which I've always hated the heuristic solution I'm using now, proposes an elegant solution to it, and is applicable even more widely than that setting."], "summary_text": "SGD is a widely used optimization method for training the parameters of some model f on some given task. Since the convergence of SGD is related to the variance of the stochastic gradient estimate, there's been a lot of work on trying to come up with such stochastic estimates with smaller variance. This paper does it using an importance sampling (IS) Monte Carlo estimate of the gradient, and learning the proposal distribution $q$ of the IS estimate. The proposal distribution $q$ is parametrized in some way, and is trained to minimize the variance of the gradient estimate. It is trained simultaneously while the model $f$ that SGD (i.e. the SGD that uses IS to get its gradient) is training. To make this whole story more recursive, the proposal distribution $q$ is also trained with SGD :-) This makes sense, since one expects the best proposal to depend on the value of the parameters of model $f$, so the best proposal $q$ should vary as $f$ is trained. One application of this idea is in optimizing a classification model over a distribution that is imbalanced class-wise (e.g. there are classes with much fewer examples). In this case, the proposal distribution determines how frequently we sample examples from each class (conditioned on the class, training examples are chosen uniformly). #### My two cents  This is a really cool idea. I particularly like the application to training on an imbalanced classification problem. People have mostly been using heuristics to tackle this problem, such as initially sampling each class equally as often, and then fine-tuning/calibrating the model using the real class proportions. This approach instead proposes a really elegant, coherent, solution to this problem. I would have liked to see a comparison with that aforementioned heuristic (for mainly selfish reasons :-) ). They instead compare with an importance sampling approach with proposal that assigns the same probability to each class, which is a reasonable alternative (though I don't know if it's used as often as the more heuristic approach). There are other applications, to matrix factorization and reinforcement learning, that are presented in the paper and seem neat, though I haven't gone through those as much. Overall, one of my favorite paper this year: it's original, tackles a problem for which I've always hated the heuristic solution I'm using now, proposes an elegant solution to it, and is applicable even more widely than that setting.", "pdf_url": "http://arxiv.org/pdf/1506.09016", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/bouchardtpg15.json"}
{"id": "80262838", "bin": "400_500", "summary_sentences": ["What  They suggest a factorization of standard 3x3 convolutions that is more efficient.", "They build a model based on that factorization.", "The model has hyperparameters to choose higher performance or higher accuracy.", "How  Factorization  They factorize the standard 3x3 convolution into one depthwise 3x3 convolution, followed by a pointwise convoluton.", "Normal 3x3 convolution:  Computes per filter and location a weighted average over all filters.", "For kernel height kH, width kW and number of input filters/planes Fin, it requires kH*kW*Fin computations per location.", "Depthwise 3x3 convolution:  Computes per filter and location a weighted average over one input filter.", "E.g. the 13th filter would only computed weighted averages over the 13th input filter/plane and ignore all the other input filters/planes.", "This requires kH*kW*1 computations per location, i.e. drastically less than a normal convolution.", "Pointwise convolution:  This is just another name for a normal 1x1 convolution.", "This is placed after a depthwise convolution in order to compensate the fact that every (depthwise) filter only sees a single input plane.", "As the kernel size is 1, this is rather fast to compute.", "Visualization of normal vs factorized convolution:  Models  They use two hyperparameters for their models.", "alpha: Multiplier for the width in the range (0, 1].", "A value of 0.5 means that every layer has half as many filters.", "roh: Multiplier for the resolution.", "In practice this is simply the input image size, having a value of {224, 192, 160, 128}.", "Results  ImageNet  Compared to VGG16, they achieve 1 percentage point less accuracy, while using only about 4% of VGG's multiply and additions (mult-adds) and while using only about 3% of the parameters.", "Compared to GoogleNet, they achieve about 1 percentage point more accuracy, while using only about 36% of the mult-adds and 61% of the parameters.", "Note that they don't compare to ResNet.", "Results for architecture choices vs. accuracy on ImageNet:  Relation between mult-adds and accuracy on ImageNet:  Object Detection  Their mAP is a bit on COCO when combining MobileNet with SSD (as opposed to using VGG or Inception v2).", "Their mAP is quite a bit worse on COCO when combining MobileNet with Faster R-CNN.", "Reducing the number of filters (alpha) influences the results more than reducing the input image resolution (roh).", "Making the models shallower influences the results more than making them thinner."], "summary_text": "What  They suggest a factorization of standard 3x3 convolutions that is more efficient. They build a model based on that factorization. The model has hyperparameters to choose higher performance or higher accuracy. How  Factorization  They factorize the standard 3x3 convolution into one depthwise 3x3 convolution, followed by a pointwise convoluton. Normal 3x3 convolution:  Computes per filter and location a weighted average over all filters. For kernel height kH, width kW and number of input filters/planes Fin, it requires kH*kW*Fin computations per location. Depthwise 3x3 convolution:  Computes per filter and location a weighted average over one input filter. E.g. the 13th filter would only computed weighted averages over the 13th input filter/plane and ignore all the other input filters/planes. This requires kH*kW*1 computations per location, i.e. drastically less than a normal convolution. Pointwise convolution:  This is just another name for a normal 1x1 convolution. This is placed after a depthwise convolution in order to compensate the fact that every (depthwise) filter only sees a single input plane. As the kernel size is 1, this is rather fast to compute. Visualization of normal vs factorized convolution:  Models  They use two hyperparameters for their models. alpha: Multiplier for the width in the range (0, 1]. A value of 0.5 means that every layer has half as many filters. roh: Multiplier for the resolution. In practice this is simply the input image size, having a value of {224, 192, 160, 128}. Results  ImageNet  Compared to VGG16, they achieve 1 percentage point less accuracy, while using only about 4% of VGG's multiply and additions (mult-adds) and while using only about 3% of the parameters. Compared to GoogleNet, they achieve about 1 percentage point more accuracy, while using only about 36% of the mult-adds and 61% of the parameters. Note that they don't compare to ResNet. Results for architecture choices vs. accuracy on ImageNet:  Relation between mult-adds and accuracy on ImageNet:  Object Detection  Their mAP is a bit on COCO when combining MobileNet with SSD (as opposed to using VGG or Inception v2). Their mAP is quite a bit worse on COCO when combining MobileNet with Faster R-CNN. Reducing the number of filters (alpha) influences the results more than reducing the input image resolution (roh). Making the models shallower influences the results more than making them thinner.", "pdf_url": "https://arxiv.org/pdf/1704.04861", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/mobilenets.json"}
{"id": "63758896", "bin": "400_500", "summary_sentences": ["The paper proposes a very cool idea at the intersection of deep learning and physics.", "The idea is to train a neural network architecture that builds on the concept of Hamiltonian Mechanics (from Physics) to learn physical conservation laws in an unsupervised manner.", "Link to the code  Link to author’s blog  Hamiltonian Mechanics  It is a branch of physics that can describe systems which follow some conservation laws and invariants.", "Consider a set of N pair of coordinates [(q1, p1), …, (qN, pN)] where q = [q1, …, qN] dnotes the position of the set of objects while p = [p1, …, pN] denotes the momentum of the set of variables.", "Together these N pairs completely describe the system.", "A scalar function H(q, p), called as the Hamiltonian is defined such that the partial derivative of H with respect to p is equal to derivative of q with respect to time t and the negative of partial derivative of H with respect to q is equal to derivative of p with respect to time t.  This can be expressed in the form of the equation as follows:  The Hamiltonian can be tied to the total energy of the system and can be used in any system where the total energy is conserved.", "Hamiltonian Neural Network (HNN)  The Hamiltonian H can be parameterized using a neural network and can learn conserved quantities from the data in an unsupervised manner.", "The loss function looks as follows:  The partial derivatives can be obtained by computing the in-graph gradient of the output variables with respect to the input variables.", "Observations  For setups where the energy must be conserved exactly, (eg ideal mass-spring and ideal pendulum), the HNN learn to preserve an energy-like scalar.", "For setups where the energy need not be conserved exactly, the HNNs still learn to preserve the energy thus highlighting a limitation of HNNs.", "In case of two body problems, the HNN model is shown to be much more robust when making predictions over longer time horizons as compared to the baselines.", "In the final experiment, the model is trained on pixel observations and not state observations.", "In this case, two auxiliary losses are added: auto-encoder reconstruction loss and a loss on the latent space representations.", "Similar to the previous experiments, the HNN model makes robust predictions over much longer time horizons."], "summary_text": "The paper proposes a very cool idea at the intersection of deep learning and physics. The idea is to train a neural network architecture that builds on the concept of Hamiltonian Mechanics (from Physics) to learn physical conservation laws in an unsupervised manner. Link to the code  Link to author’s blog  Hamiltonian Mechanics  It is a branch of physics that can describe systems which follow some conservation laws and invariants. Consider a set of N pair of coordinates [(q1, p1), …, (qN, pN)] where q = [q1, …, qN] dnotes the position of the set of objects while p = [p1, …, pN] denotes the momentum of the set of variables. Together these N pairs completely describe the system. A scalar function H(q, p), called as the Hamiltonian is defined such that the partial derivative of H with respect to p is equal to derivative of q with respect to time t and the negative of partial derivative of H with respect to q is equal to derivative of p with respect to time t.  This can be expressed in the form of the equation as follows:  The Hamiltonian can be tied to the total energy of the system and can be used in any system where the total energy is conserved. Hamiltonian Neural Network (HNN)  The Hamiltonian H can be parameterized using a neural network and can learn conserved quantities from the data in an unsupervised manner. The loss function looks as follows:  The partial derivatives can be obtained by computing the in-graph gradient of the output variables with respect to the input variables. Observations  For setups where the energy must be conserved exactly, (eg ideal mass-spring and ideal pendulum), the HNN learn to preserve an energy-like scalar. For setups where the energy need not be conserved exactly, the HNNs still learn to preserve the energy thus highlighting a limitation of HNNs. In case of two body problems, the HNN model is shown to be much more robust when making predictions over longer time horizons as compared to the baselines. In the final experiment, the model is trained on pixel observations and not state observations. In this case, two auxiliary losses are added: auto-encoder reconstruction loss and a loss on the latent space representations. Similar to the previous experiments, the HNN model makes robust predictions over much longer time horizons.", "pdf_url": "https://arxiv.org/pdf/1906.01563", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/hamiltonian-neural-networks.json"}
{"id": "24510148", "bin": "500_600", "summary_sentences": ["Learning to prove theorems via interacting with proof assistants Yang & Deng, ICML’19  Something a little different to end the week: deep learning meets theorem proving!", "It’s been a while since we gave formal methods some love on The Morning Paper, and this paper piqued my interest.", "You’ve probably heard of Coq , a proof management system backed by about 30 years of research and developed out of INRIA.", "Here’s how the Coq home page introduces it:  Coq is a formal proof management system.", "It provides a formal language to write mathematical definitions, executable algorithms and theorems together with an environment for semi-interactive development of machine-checked proofs.", "Certified programs can then be extracted to languages like OCaml, Haskell, and Scheme.", "In fully automated theorem proving (ATP), after providing a suitable formalism and a theorem, the goal is to be able to push a button and have the system prove that the theorem is true (or false!).", "The state-of-the-art in ATP is still some way behind human experts though it two key areas: the scale of systems it can tackle, and the interpretability of the generated proofs.", "What a typical theorem prover does… is to prove by resolution refutation: it converts the premises and the negation of the theorem into first-order clauses in conjunctive normal form.", "It then keeps generating new clauses by applying the resolution rule until an empty clause emerges, yielding a proof consisting of a long sequence of CNFs and resolutions.", "Enter interactive theorem proving (ITP).", "In ITP a human expert guides the structure of the proof, telling the system which proof tactics to use at each step:  The tactics capture high-level proof techniques such as induction, leaving low-level details to the software referred to as proof assistants.", "( Enlarge )  It would be nice if we could learn from all the ITP proofs that humans have generated, and use those learnings to improve the proof steps that a system can take automatically:  … human experts have written a large amount of ITP code, which provides an opportunity to develop machine learning systems to imitate humans for interacting with the proof assistant.", "Such a combination would get us closer to fully automated human-like proofs.", "To pull all this together we need three things:  A large ITP dataset we can learn from  A learning environment for training and evaluating auto-ITP agents.", "Agents start with a set of premises and a theorem to prove, and then they interact with the proof assistant by issuing a sequence of tactics.", "A way to learn and generate new tactics which can be used in the construction of those proofs.", "The first two requirements are satisfied by CoqGym, and ASTactic handles tactic generation.", "CoqGym  CoqGym (  [url]"], "summary_text": "Learning to prove theorems via interacting with proof assistants Yang & Deng, ICML’19  Something a little different to end the week: deep learning meets theorem proving! It’s been a while since we gave formal methods some love on The Morning Paper, and this paper piqued my interest. You’ve probably heard of Coq , a proof management system backed by about 30 years of research and developed out of INRIA. Here’s how the Coq home page introduces it:  Coq is a formal proof management system. It provides a formal language to write mathematical definitions, executable algorithms and theorems together with an environment for semi-interactive development of machine-checked proofs. Certified programs can then be extracted to languages like OCaml, Haskell, and Scheme. In fully automated theorem proving (ATP), after providing a suitable formalism and a theorem, the goal is to be able to push a button and have the system prove that the theorem is true (or false!). The state-of-the-art in ATP is still some way behind human experts though it two key areas: the scale of systems it can tackle, and the interpretability of the generated proofs. What a typical theorem prover does… is to prove by resolution refutation: it converts the premises and the negation of the theorem into first-order clauses in conjunctive normal form. It then keeps generating new clauses by applying the resolution rule until an empty clause emerges, yielding a proof consisting of a long sequence of CNFs and resolutions. Enter interactive theorem proving (ITP). In ITP a human expert guides the structure of the proof, telling the system which proof tactics to use at each step:  The tactics capture high-level proof techniques such as induction, leaving low-level details to the software referred to as proof assistants. ( Enlarge )  It would be nice if we could learn from all the ITP proofs that humans have generated, and use those learnings to improve the proof steps that a system can take automatically:  … human experts have written a large amount of ITP code, which provides an opportunity to develop machine learning systems to imitate humans for interacting with the proof assistant. Such a combination would get us closer to fully automated human-like proofs. To pull all this together we need three things:  A large ITP dataset we can learn from  A learning environment for training and evaluating auto-ITP agents. Agents start with a set of premises and a theorem to prove, and then they interact with the proof assistant by issuing a sequence of tactics. A way to learn and generate new tactics which can be used in the construction of those proofs. The first two requirements are satisfied by CoqGym, and ASTactic handles tactic generation. CoqGym  CoqGym (  [url]", "pdf_url": "http://proceedings.mlr.press/v97/yang19a/yang19a.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/learning-to-prove-theorems-via-interacting-with-proof-assistants.json"}
{"id": "43687503", "bin": "500_600", "summary_sentences": ["This paper suggests a method (NoBackTrack) for training recurrent neural networks in an online way, i.e. without having to do backprop through time.", "One way of understanding the method is that it applies the [forward method for automatic differentiation](//en.wikipedia.org/wiki/Automatic_differentiation#Forward_accumulation), but since it requires maintaining a large Jacobian matrix (nb.", "of hidden units times nb.", "of parameters), they propose a way of obtaining a stochastic (but unbiased!)", "estimate of that matrix.", "Moreover, the method is improved by using Kalman filtering on that estimate, effectively smoothing the estimate over time.", "#### My two cents  Online training of RNNs is a big, unsolved problem.", "The current approach people use is to truncate backprop to only a few steps in the past, which is more of a heuristic.", "This paper makes progress towards a more principled approach.", "I really like the \"rank-one trick\" of Equation 7, really cute!", "And it is quite central to this method too, so good job on connecting those dots!", "The authors present this work as being preliminary, and indeed they do not compare with truncated backprop.", "I really hope they do in a future version of this work.", "Also, I don't think I buy their argument that the \"theory of stochastic gradient descent applies\".", "Here's the reason.", "So the method tracks the Jacobian of the hidden state wrt the parameter, which they note $G(t)$.", "It is update into $G(t+1)$, using a recursion which is based on the chain rule.", "However, between computing $G(t)$ and $G(t+1)$, a gradient step is performed during training.", "This means that $G(t)$ is now slightly stale, and corresponds to the gradient with respect to old value of the parameters, not the current value.", "As far as I understand, this implies that $G(t+1)$ (more specifically, its stochastic estimate as proposed in this paper) isn't unbiased anymore.", "So, unless I'm missing something (which I might!", "), I don't think we can invoke the theory of SGD as they suggest.", "But frankly, that last issue seems pretty unavoidable in the online setting.", "I suspect this will never be solved, and future research will have to somehow have to design learning algorithms that are robust to this issue (or develop new theory that shows it isn't one).", "So overall, kudos to the authors, and I'm really looking forward to read more about where this research goes!"], "summary_text": "This paper suggests a method (NoBackTrack) for training recurrent neural networks in an online way, i.e. without having to do backprop through time. One way of understanding the method is that it applies the [forward method for automatic differentiation](//en.wikipedia.org/wiki/Automatic_differentiation#Forward_accumulation), but since it requires maintaining a large Jacobian matrix (nb. of hidden units times nb. of parameters), they propose a way of obtaining a stochastic (but unbiased!) estimate of that matrix. Moreover, the method is improved by using Kalman filtering on that estimate, effectively smoothing the estimate over time. #### My two cents  Online training of RNNs is a big, unsolved problem. The current approach people use is to truncate backprop to only a few steps in the past, which is more of a heuristic. This paper makes progress towards a more principled approach. I really like the \"rank-one trick\" of Equation 7, really cute! And it is quite central to this method too, so good job on connecting those dots! The authors present this work as being preliminary, and indeed they do not compare with truncated backprop. I really hope they do in a future version of this work. Also, I don't think I buy their argument that the \"theory of stochastic gradient descent applies\". Here's the reason. So the method tracks the Jacobian of the hidden state wrt the parameter, which they note $G(t)$. It is update into $G(t+1)$, using a recursion which is based on the chain rule. However, between computing $G(t)$ and $G(t+1)$, a gradient step is performed during training. This means that $G(t)$ is now slightly stale, and corresponds to the gradient with respect to old value of the parameters, not the current value. As far as I understand, this implies that $G(t+1)$ (more specifically, its stochastic estimate as proposed in this paper) isn't unbiased anymore. So, unless I'm missing something (which I might! ), I don't think we can invoke the theory of SGD as they suggest. But frankly, that last issue seems pretty unavoidable in the online setting. I suspect this will never be solved, and future research will have to somehow have to design learning algorithms that are robust to this issue (or develop new theory that shows it isn't one). So overall, kudos to the authors, and I'm really looking forward to read more about where this research goes!", "pdf_url": "http://arxiv.org/pdf/1507.07680", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/ollivierc15.json"}
{"id": "11836666", "bin": "500_600", "summary_sentences": ["It is desirable for an agent to symbolically reason about its environment, in order to expediate the process of learning optimal behaviors.", "However, “classic” symbolic AI suffers from the symbol grounding problem; symbolic elements have traditionally been hand-crafted, and hence, are brittle.", "On the other hand, Deep Learning can be used to automatically learn ~optimal “symbols”, upon which a reinforcement learning agent could learn behaviors motivated by these learned symbols.", "By forcing a Deep RL agent to operate in a symbolic domain, the decisions made by the agent are naturally more interpretable.", "The aspects of AI that this work focuses on are closely related to those proposed in the manifesto written by Lake et al. .", "There are also nods to Douglas Hofstadter’s work on analogy as being the main driving force behind intelligence, as well as Marcus Hutter’s Universal Artificial Intelligence work.", "Conceptual abstraction - agents can use abstractions to make analogies and hence learn optimal behaviors faster  Compositional structure - probabilistic first-order logic provides the underlying framework for a representation medium that is compositional  Common sense priors - priors from simple object tracking!", "Persistence, kinematics (constant velocity models), etc.", "Causal reasoning - the proposed architecture attempts to discover causal structure in the environment to accelerate learning by explicitly maintaining sets of causal rules  A Q-learning agent is designed for their toy example.", "The state space consists of the different interactions between the learned symbolic abstractions of the environment.", "A tracking process is carried out separately from the agent for keeping track of the different symbolic abstractions.", "Main result: on random toy problem instances, DQN is not able to better than chance… Hypothesis: DQN relies on the fact that you should be able to internally learn a model for $p(s_{t+1}|s_{t},a_{t})$ after going through a lot of examples.", "When this distribution is non-stationary, it can’t!", "However, the proposed architecture doesn’t seem to care about this.", "It instead directly is learning about object interactions.", "Methodology  Their environment is a simple B&W grid upon which shapes (O’s, X’s, and +’s) can be randomly positioned.", "Low-level symbol generation: Uses a convolutional autoencoder to do simple dimensionality reduction and learn relevant features.", "Then, they do a form of unsupervised clustering by finding the maximally activated feature corresponding to each pixel, then thresholding these values.", "Once they have a sparse list of salient pixels, they form a spectrum with the activations across all features, and define a distance metric on the spectra via the sum of squared distances.", "This allows them to cluster pixels (objects) into certain categories.", "Representation building: notions of spatial proximity, type-transitions between frames (including birth and death), neighborhood (number of neighbors), relative distances and positions  Reinforcement Learning: Agent is the ‘+’, separate Q for interactions between different pairs of object types.", "State space describes different possible relations between two objects types.", "Seek to learn how to interact via (U, D, L, R) actions."], "summary_text": "It is desirable for an agent to symbolically reason about its environment, in order to expediate the process of learning optimal behaviors. However, “classic” symbolic AI suffers from the symbol grounding problem; symbolic elements have traditionally been hand-crafted, and hence, are brittle. On the other hand, Deep Learning can be used to automatically learn ~optimal “symbols”, upon which a reinforcement learning agent could learn behaviors motivated by these learned symbols. By forcing a Deep RL agent to operate in a symbolic domain, the decisions made by the agent are naturally more interpretable. The aspects of AI that this work focuses on are closely related to those proposed in the manifesto written by Lake et al. . There are also nods to Douglas Hofstadter’s work on analogy as being the main driving force behind intelligence, as well as Marcus Hutter’s Universal Artificial Intelligence work. Conceptual abstraction - agents can use abstractions to make analogies and hence learn optimal behaviors faster  Compositional structure - probabilistic first-order logic provides the underlying framework for a representation medium that is compositional  Common sense priors - priors from simple object tracking! Persistence, kinematics (constant velocity models), etc. Causal reasoning - the proposed architecture attempts to discover causal structure in the environment to accelerate learning by explicitly maintaining sets of causal rules  A Q-learning agent is designed for their toy example. The state space consists of the different interactions between the learned symbolic abstractions of the environment. A tracking process is carried out separately from the agent for keeping track of the different symbolic abstractions. Main result: on random toy problem instances, DQN is not able to better than chance… Hypothesis: DQN relies on the fact that you should be able to internally learn a model for $p(s_{t+1}|s_{t},a_{t})$ after going through a lot of examples. When this distribution is non-stationary, it can’t! However, the proposed architecture doesn’t seem to care about this. It instead directly is learning about object interactions. Methodology  Their environment is a simple B&W grid upon which shapes (O’s, X’s, and +’s) can be randomly positioned. Low-level symbol generation: Uses a convolutional autoencoder to do simple dimensionality reduction and learn relevant features. Then, they do a form of unsupervised clustering by finding the maximally activated feature corresponding to each pixel, then thresholding these values. Once they have a sparse list of salient pixels, they form a spectrum with the activations across all features, and define a distance metric on the spectra via the sum of squared distances. This allows them to cluster pixels (objects) into certain categories. Representation building: notions of spatial proximity, type-transitions between frames (including birth and death), neighborhood (number of neighbors), relative distances and positions  Reinforcement Learning: Agent is the ‘+’, separate Q for interactions between different pairs of object types. State space describes different possible relations between two objects types. Seek to learn how to interact via (U, D, L, R) actions.", "pdf_url": "https://arxiv.org/pdf/1609.05518v2", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/deep-symbolic-rl.json"}
{"id": "5674972", "bin": "500_600", "summary_sentences": ["This paper presents a novel approach to replace the classic epsilon-greedy exploration strategy.", "The main idea is to encourage deep exploration by creating a new Deep Q-Learning architecture that supports selecting actions from randomized Q-functions that are trained on bootstrapped data.", "This is a quick look at the proposed architecture.", "Bootstrapped DQN architecture  Each head represents a different Q-function that is trained on a subset of the data.", "The shared network learns a joint feature representation across all the data; it can be thought of as a data-dependent dropout.", "For DRL, samples stored in a replay buffer contain a flag marking which of the K Q-functions it came from.", "The algorithm speeds up learning compared to other exploration tactics for DRL since it encourages deep exploration; at the beginning of each episode, a different Q-function is randomly sampled from a uniform distribution and it is used until the end of that episode.", "Another key component of the Bootstrapped DQN algorithm is the bootstrap mask.", "The mask decides, for each Q-value function, whether or not it should train upon the experience generated at step t. Each individual experience is given a randomly sampled mask m ~ M, where M is Bernoulli, Poission, etc.", "Then, when training the network on a minibatch sampled from the replay buffer, the mask m decides whether or not a specific Q-value function is to be trained upon that experience.", "The authors show that the effect of this on the learning process is akin to dropout.", "This heuristic, plus the randomized Q-value functions, help Bootstrapped DQN deal with learning from noisy data and exploring complex state/action spaces efficiently.", "Strengths  The authors based their idea on sound statistical principles and conducted numerous experiments to back up their claims.", "Their results show that Bootstrapped DQN can learn faster (but not necessarily with higher long-term rewards) than state-of-the-art DQN.", "The authors also compare their work with Stadie, Levine, and Abeel’s paper on Incentivizing Exploration in RL.", "See my previous post for details .", "The authors show that Bootstrapped DQN outperforms Stadie’s methods, as Stadie’s methods attempt the more ambitious task of learning a model of the task dynamics and using how well the agent has learned said model to inform the exploration.", "Weaknesses  The paper is a bit hard to follow at times, and you have to go all the way to the appendix to get a good understanding of how the entire algorithm comes together and works.", "The step-by-step algorithm description could be more complete (there are steps of the training process left out, albeit they are not unique to Bootstrap DQN) and should not be hidden down in the appendix.", "This should probably be in Section 3.", "The MDP examples in Section 5 were not explained well; it feels like it doesn’t contribute too much to the overall impact of the paper."], "summary_text": "This paper presents a novel approach to replace the classic epsilon-greedy exploration strategy. The main idea is to encourage deep exploration by creating a new Deep Q-Learning architecture that supports selecting actions from randomized Q-functions that are trained on bootstrapped data. This is a quick look at the proposed architecture. Bootstrapped DQN architecture  Each head represents a different Q-function that is trained on a subset of the data. The shared network learns a joint feature representation across all the data; it can be thought of as a data-dependent dropout. For DRL, samples stored in a replay buffer contain a flag marking which of the K Q-functions it came from. The algorithm speeds up learning compared to other exploration tactics for DRL since it encourages deep exploration; at the beginning of each episode, a different Q-function is randomly sampled from a uniform distribution and it is used until the end of that episode. Another key component of the Bootstrapped DQN algorithm is the bootstrap mask. The mask decides, for each Q-value function, whether or not it should train upon the experience generated at step t. Each individual experience is given a randomly sampled mask m ~ M, where M is Bernoulli, Poission, etc. Then, when training the network on a minibatch sampled from the replay buffer, the mask m decides whether or not a specific Q-value function is to be trained upon that experience. The authors show that the effect of this on the learning process is akin to dropout. This heuristic, plus the randomized Q-value functions, help Bootstrapped DQN deal with learning from noisy data and exploring complex state/action spaces efficiently. Strengths  The authors based their idea on sound statistical principles and conducted numerous experiments to back up their claims. Their results show that Bootstrapped DQN can learn faster (but not necessarily with higher long-term rewards) than state-of-the-art DQN. The authors also compare their work with Stadie, Levine, and Abeel’s paper on Incentivizing Exploration in RL. See my previous post for details . The authors show that Bootstrapped DQN outperforms Stadie’s methods, as Stadie’s methods attempt the more ambitious task of learning a model of the task dynamics and using how well the agent has learned said model to inform the exploration. Weaknesses  The paper is a bit hard to follow at times, and you have to go all the way to the appendix to get a good understanding of how the entire algorithm comes together and works. The step-by-step algorithm description could be more complete (there are steps of the training process left out, albeit they are not unique to Bootstrap DQN) and should not be hidden down in the appendix. This should probably be in Section 3. The MDP examples in Section 5 were not explained well; it feels like it doesn’t contribute too much to the overall impact of the paper.", "pdf_url": "http://arxiv.org/pdf/1602.04621v3.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/deep-exploration.json"}
{"id": "56472821", "bin": "500_600", "summary_sentences": ["The Google File System – Ghemawat, Gobioff & Leung, 2003  Here’s a paper with a lot to answer for!", "Back in 2003 Ghemawat et al reported that  We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications.", "It provides fault-tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients.", "Google’s workloads and use of commodity hardware led to the following observations:  Component failures are the norm rather than the exception  The files Google worked with were huge by traditional standards – multi-GB files were very common.", "Managing a large number of small (Kb) files became unwieldy causing block size and I/O to be revisited.", "Most Google workloads only mutated files by appending to them.", "Random writes were practically non-existent.", "Files were often only read sequentially  Google were able to co-design their applications and the API to their file system (i.e. abandon the requirement for POSIX compliance) for better flexibility  The result was the Google File System, designed to store a modest (few million) number of large files, with workloads consisting of large streaming reads and small random reads, large sequential writes that append data to files, and semantics for supporting multiple clients concurrently appending to the same file.", "In addition:  High sustained bandwidth is more important than low latency.", "The implementation details probably sound very familiar: a master and multiple chunkservers, with files divided into replicated chunks of 64MB.", "We keep the overall system highly available with two simple yet effective strategies: fast recovery and replication.", "We always talk a lot about replication, it’s good to remember the fast-recovery part of the equation too.", "Also of note in the paper is the empasis on providing good diagnostic tools.", "In summary:  The Google File System demonstrates the qualities essential for supporting large-scale data processing workloads on commodity hardware.", "While some design decisions are specific to our unique setting, many apply to data processing tasks of a similar magnitude and cost consciousness.", "GFS of course went on to inspire Hadoop’s HDFS, and the rest is history.", "It’s good to go back and look at the workload assumptions that inspired GFS, as a sanity check that your use-cases match.", "Deploying HDFS just because 67% of other companies install HDFS is not a good strategy!", "It’s also interesting to wonder how things would have played out if Google themselves had decided to open source GFS.", "Google of course subsequently went on to introduce Caffeine and  Colossus (aka GFS 2) as they needed to respond in more of a real-time than a batch mode.", "HDFS evolved to….", "?"], "summary_text": "The Google File System – Ghemawat, Gobioff & Leung, 2003  Here’s a paper with a lot to answer for! Back in 2003 Ghemawat et al reported that  We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault-tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. Google’s workloads and use of commodity hardware led to the following observations:  Component failures are the norm rather than the exception  The files Google worked with were huge by traditional standards – multi-GB files were very common. Managing a large number of small (Kb) files became unwieldy causing block size and I/O to be revisited. Most Google workloads only mutated files by appending to them. Random writes were practically non-existent. Files were often only read sequentially  Google were able to co-design their applications and the API to their file system (i.e. abandon the requirement for POSIX compliance) for better flexibility  The result was the Google File System, designed to store a modest (few million) number of large files, with workloads consisting of large streaming reads and small random reads, large sequential writes that append data to files, and semantics for supporting multiple clients concurrently appending to the same file. In addition:  High sustained bandwidth is more important than low latency. The implementation details probably sound very familiar: a master and multiple chunkservers, with files divided into replicated chunks of 64MB. We keep the overall system highly available with two simple yet effective strategies: fast recovery and replication. We always talk a lot about replication, it’s good to remember the fast-recovery part of the equation too. Also of note in the paper is the empasis on providing good diagnostic tools. In summary:  The Google File System demonstrates the qualities essential for supporting large-scale data processing workloads on commodity hardware. While some design decisions are specific to our unique setting, many apply to data processing tasks of a similar magnitude and cost consciousness. GFS of course went on to inspire Hadoop’s HDFS, and the rest is history. It’s good to go back and look at the workload assumptions that inspired GFS, as a sanity check that your use-cases match. Deploying HDFS just because 67% of other companies install HDFS is not a good strategy! It’s also interesting to wonder how things would have played out if Google themselves had decided to open source GFS. Google of course subsequently went on to introduce Caffeine and  Colossus (aka GFS 2) as they needed to respond in more of a real-time than a batch mode. HDFS evolved to…. ?", "pdf_url": "https://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/gfs-sosp2003.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/the-google-file-system.json"}
{"id": "37171167", "bin": "600_700", "summary_sentences": ["In the domain of machine comprehension, making multiple passes over the given document is an effective technique to extract the relation between the given passage, question and answer.", "Unlike previous approaches, which perform a fixed number of passes over the passage, Reasoning Network (ReasoNet) uses reinforcement learning (RL) to decide how many times a document should be read.", "Every time the document is read, ReasoNet determines whether the document should be read again or has the termination state been reached.", "If termination state is reached, the answer module is triggered to generate the answer.", "Since the termination state is discrete and not connected to the final output, RL approach is used.", "Datasets  CNN, DailyMail Dataset  SQuAD  Graph Reachability Dataset  2 synthetic datasets to test if the network can answer questions like “Is node_1 connected to node_12”?", "Architecture  Memory (M) - Comprises of the vector representation of the document and the question (encoded using GRU or other RNNs).", "Attention - Attention vector (xt) is a function of current internal state st and external memory M. The state and memory are passed through FCs and fed to a similarity function.", "Internal State (st) - Vector representation of the question state computed by a RNN using the previous internal state and the attention vector xt  Termination Gate (Tt) - Uses a logistic regression model to generate a random binary variable using the current internal state st.  Answer - Answer module is triggered when Tt = 1.", "For CNN and DailyMail, a linear projection of GRU outputs is used to predict the answer from candidate entities.", "For SQuAD, the position of the first and the last word from the answer span are predicted.", "For Graph Reachability, a logistic regression module is used to predict yes/no as the answer.", "Reinforcement Learning - For the RL setting, reward at time t, rt = 1 if Tt = 1 and answer is correct.", "Otherwise rt = 0  Workflow - Given a passage p, query q and answer a:  Extract memory using p  Extract initial hidden state using q  ReasoNet executes all possible episodes that can be enumerated by setting an upper limit on the number of passes.", "These episodes generate actions and answers that are used to train the ReasoNet.", "Result  CNN, DailyMail Corpus  ReasoNet outperforms all the baselines which use fixed number of reasoning steps and could benefit by capturing the word alignment signals between query and passage.", "SQuAD  At the time of submission, ReasoNet was ranked 2nd on the SQuAD leaderboard and as of 9th July 2017, it is ranked 4th.", "Graph Reachability Dataset  ReasoNet - Standard ReasoNet as described above.", "ReasoNet-Last - Use the prediction from the Tmax  ReasoNet > ReasoNet-Last > Deep LSTM Reader  ReasoNet converges faster than ReasoNet-Last indicating that the terminate gate is useful.", "Notes  As such there is nothing discouraging the ReasoNet to make unnecessary passes over the passage.", "In fact, the modal value of the number of passes = upper bound on the number of passes.", "This effect is more prominent for large graph indicating that the ReasoNet may try to play safe by performing extra passes.", "It would be interesting to see if the network can be discouraged from making unnecessary passed by awarding a small negative reward for each pass."], "summary_text": "In the domain of machine comprehension, making multiple passes over the given document is an effective technique to extract the relation between the given passage, question and answer. Unlike previous approaches, which perform a fixed number of passes over the passage, Reasoning Network (ReasoNet) uses reinforcement learning (RL) to decide how many times a document should be read. Every time the document is read, ReasoNet determines whether the document should be read again or has the termination state been reached. If termination state is reached, the answer module is triggered to generate the answer. Since the termination state is discrete and not connected to the final output, RL approach is used. Datasets  CNN, DailyMail Dataset  SQuAD  Graph Reachability Dataset  2 synthetic datasets to test if the network can answer questions like “Is node_1 connected to node_12”? Architecture  Memory (M) - Comprises of the vector representation of the document and the question (encoded using GRU or other RNNs). Attention - Attention vector (xt) is a function of current internal state st and external memory M. The state and memory are passed through FCs and fed to a similarity function. Internal State (st) - Vector representation of the question state computed by a RNN using the previous internal state and the attention vector xt  Termination Gate (Tt) - Uses a logistic regression model to generate a random binary variable using the current internal state st.  Answer - Answer module is triggered when Tt = 1. For CNN and DailyMail, a linear projection of GRU outputs is used to predict the answer from candidate entities. For SQuAD, the position of the first and the last word from the answer span are predicted. For Graph Reachability, a logistic regression module is used to predict yes/no as the answer. Reinforcement Learning - For the RL setting, reward at time t, rt = 1 if Tt = 1 and answer is correct. Otherwise rt = 0  Workflow - Given a passage p, query q and answer a:  Extract memory using p  Extract initial hidden state using q  ReasoNet executes all possible episodes that can be enumerated by setting an upper limit on the number of passes. These episodes generate actions and answers that are used to train the ReasoNet. Result  CNN, DailyMail Corpus  ReasoNet outperforms all the baselines which use fixed number of reasoning steps and could benefit by capturing the word alignment signals between query and passage. SQuAD  At the time of submission, ReasoNet was ranked 2nd on the SQuAD leaderboard and as of 9th July 2017, it is ranked 4th. Graph Reachability Dataset  ReasoNet - Standard ReasoNet as described above. ReasoNet-Last - Use the prediction from the Tmax  ReasoNet > ReasoNet-Last > Deep LSTM Reader  ReasoNet converges faster than ReasoNet-Last indicating that the terminate gate is useful. Notes  As such there is nothing discouraging the ReasoNet to make unnecessary passes over the passage. In fact, the modal value of the number of passes = upper bound on the number of passes. This effect is more prominent for large graph indicating that the ReasoNet may try to play safe by performing extra passes. It would be interesting to see if the network can be discouraged from making unnecessary passed by awarding a small negative reward for each pass.", "pdf_url": "https://arxiv.org/pdf/1609.05284", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/reasonet-learning-to-stop-reading-in-machine-comprehension.json"}
{"id": "6777051", "bin": "600_700", "summary_sentences": ["The paper presents I2A (Imagination Augmented Agent) that combines the model-based and model-free approaches leading to data efficiency and robustness even with imperfect models.", "I2A agent uses the predictions from a learned environment model as an additional context in deep policy networks.", "This leads to improved data efficiency and robustness to imperfect models.", "I2A agent has two main modules - Imagination module and the Policy module.", "Imagination Module  Environment Model  This is a recurrent model, trained in an unsupervised manner using the agent trajectories.", "It can be used to predict the future state given the current state and action.", "The environment model can be rolled out multiple times to obtain a simulated trajectory or an “imagined” trajectory.", "During each rollout, the actions are chosen using a rollout policy πr.", "Rollout Encoder  A rollout encoder E (LSTM) is used to process the entire imagined rollout.", "The imagination module is used to generate n trajectories.", "Each trajectory is a sequence of outputs of the environment model.", "These n trajectories are concatenated into a single “imagination” vector.", "The training data for the environment model is generated from trajectories of a partially trained model-free agent.", "Pretraining the environment model (instead of joint training with policy) leads to faster runtime.", "Policy Module  This module uses the output of both model-based path and model-free path as its input.", "It generates the policy vector and value function.", "Rollout Strategy  One rollout is performed for each possible action in the environment ie, the first action in the ith rollout is the ith action in the action set.", "Subsequent actions are generated using a shared rollout policy π’  An effective strategy was to create a small model-free network π’(ot) and then add a KL loss component that encourages π’(ot)to be similar to the imagination augmented policy π(ot).", "Baselines  Model-free agent  Copy-model agent - same as I2A but the environment model is replaced by a “copy” model that just returns the input observations.", "Environments  Sokoban  Task is to push a number of boxes onto given target locations.", "I2A outperforms the baselines and gains in performance as the number of unrolling steps increases (though at a diminishing rate).", "In case of poor environment models, the agent seems to be able to ignore the later part of the rollout when the error starts to accumulate.", "Monte Carlo search algorithm (without an explicit rollout encoder) performed poorly as compared to the model using rollout encoder.", "Predicting the reward along with value function and action seems to speed up training.", "If a near-perfect model is available, I2A agent’s performance can be improved by performing Monte Carlo search with the trained I2A agent for the rollout policy.", "The agent plays entire episodes in simulation and tries to find a successful action sequence within 10 retries.", "MiniPacman  I2A agent is evaluated to see if a single model can be used to solve multiple tasks.", "A new environment is designed to define multiple tasks in an environment with shared state transitions.", "Each task is specified by a 5-dimensional reward vector that associates a reward with moving, eating food, eating a pill, eating a ghost and being eaten by a ghost.", "A single environment model is trained to predict both observations (frames) and events (eg “eating a pill”).", "This way, the environment model is shared across all tasks.", "Baseline agents and I2As are trained on each task separately.", "I2A architecture outperforms the standard agent in all tasks and the copy-model baseline in all but one task.", "The improvement in performance is higher for tasks where rewards are sparse and where the anticipation of ghost dynamics is especially important indicating that the I2A agent can use the environment model to explore the environment more effectively."], "summary_text": "The paper presents I2A (Imagination Augmented Agent) that combines the model-based and model-free approaches leading to data efficiency and robustness even with imperfect models. I2A agent uses the predictions from a learned environment model as an additional context in deep policy networks. This leads to improved data efficiency and robustness to imperfect models. I2A agent has two main modules - Imagination module and the Policy module. Imagination Module  Environment Model  This is a recurrent model, trained in an unsupervised manner using the agent trajectories. It can be used to predict the future state given the current state and action. The environment model can be rolled out multiple times to obtain a simulated trajectory or an “imagined” trajectory. During each rollout, the actions are chosen using a rollout policy πr. Rollout Encoder  A rollout encoder E (LSTM) is used to process the entire imagined rollout. The imagination module is used to generate n trajectories. Each trajectory is a sequence of outputs of the environment model. These n trajectories are concatenated into a single “imagination” vector. The training data for the environment model is generated from trajectories of a partially trained model-free agent. Pretraining the environment model (instead of joint training with policy) leads to faster runtime. Policy Module  This module uses the output of both model-based path and model-free path as its input. It generates the policy vector and value function. Rollout Strategy  One rollout is performed for each possible action in the environment ie, the first action in the ith rollout is the ith action in the action set. Subsequent actions are generated using a shared rollout policy π’  An effective strategy was to create a small model-free network π’(ot) and then add a KL loss component that encourages π’(ot)to be similar to the imagination augmented policy π(ot). Baselines  Model-free agent  Copy-model agent - same as I2A but the environment model is replaced by a “copy” model that just returns the input observations. Environments  Sokoban  Task is to push a number of boxes onto given target locations. I2A outperforms the baselines and gains in performance as the number of unrolling steps increases (though at a diminishing rate). In case of poor environment models, the agent seems to be able to ignore the later part of the rollout when the error starts to accumulate. Monte Carlo search algorithm (without an explicit rollout encoder) performed poorly as compared to the model using rollout encoder. Predicting the reward along with value function and action seems to speed up training. If a near-perfect model is available, I2A agent’s performance can be improved by performing Monte Carlo search with the trained I2A agent for the rollout policy. The agent plays entire episodes in simulation and tries to find a successful action sequence within 10 retries. MiniPacman  I2A agent is evaluated to see if a single model can be used to solve multiple tasks. A new environment is designed to define multiple tasks in an environment with shared state transitions. Each task is specified by a 5-dimensional reward vector that associates a reward with moving, eating food, eating a pill, eating a ghost and being eaten by a ghost. A single environment model is trained to predict both observations (frames) and events (eg “eating a pill”). This way, the environment model is shared across all tasks. Baseline agents and I2As are trained on each task separately. I2A architecture outperforms the standard agent in all tasks and the copy-model baseline in all but one task. The improvement in performance is higher for tasks where rewards are sparse and where the anticipation of ghost dynamics is especially important indicating that the I2A agent can use the environment model to explore the environment more effectively.", "pdf_url": "https://arxiv.org/pdf/1707.06203", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/imagination-augmented-agents-for-deep-reinforcement-learning.json"}
{"id": "30189879", "bin": "600_700", "summary_sentences": ["The paper provides a multi-agent learning environment and proposes a learning approach that facilitates the emergence of a basic compositional language.", "The language is quite rudimentary and is essentially a sequence of abstract discrete symbols.", "But it does comprise of a defined vocabulary and syntax.", "Setup  Cooperative, partially observable Markov game (multi-agent extension of MDP).", "All agents have identical action and observation spaces, use the same policy and receive a shared reward.", "Grounded Communication Environment  Physically simulated 2-D environment in continuous space and discrete time with N agents and M landmarks.", "The agents and the landmarks would occupy some location and would have some attributes (colour, shape).", "Within the environment, the agents can go to a location, look at a location or do nothing.", "Additionally, they can utter communication symbols c (from a shared vocabulary C).", "Agents themselves learn to assign a meaning to the symbols.", "Each agent has an internal goal (which could require interaction with other agents to complete) which the other agents cannot see.", "Goal for agent i consists of an action to perform, a landmark location where to perform the action and another agent who should be performing the action.", "Since the agent is continuously emitting symbols, a memory module is provided and simple additive memory updates are done.", "For interaction, the agents could use verbal utterances, non-verbal signals (gaze) or non-communicative strategies (pushing other agents).", "Approach  A model of all agent and environment state dynamics is created over time and the return gradient is computed.", "Gumbel-Softmax distribution is used to obtain categorical word emission c.  A multi-layer perceptron is used to model the policy which returns action, communication symbol and the memory update for each agent.", "Since the number of agents (and hence the number of communication streams etc) can vary across instantiations, an identical model is instantiated per agent and per communication stream.", "The output of individual processing modules are pooled into feature vectors corresponding to communication and physical observations.", "These pooled features and the goal vectors are fed to the final processing module from which actions and categorical symbols are sampled.", "In practice, using an additional task (each agent predicts the goal for another agent) encouraged more meaningful communication utterances.", "Compositionality and Vocabulary Size  Authors recommend using a large vocabulary with a soft penalty that discourages use of too many words.", "This leads to use of a large vocabulary in the intermediate state which converges to a small vocabulary.", "Along the lines of rich gets richer dynamics, the communication symbol c’s are modelled as being generated by a Dirichlet process.", "The resulting reward across all agents is the log-likelihood of all communication utterances to have been generated by a Dirichlet process.", "Since the agents can only communicate in discrete symbols and do not have a global positioning reference, they need to unambiguously communicate landmark references to other agents.", "Case I - Agents can not see each other  Non-verbal communication is not possible.", "When trained with just 2 agents, symbols are assigned for each landmark and action.", "As the number of agents is increased, additional symbols are used to refer to agents.", "If the agents of the same colour are asked to perform conflicting tasks, they perform the average of conflicting tasks.", "If distractor locations are added, the agents learn to ignore them.", "Non-verbal communication  Agents are allowed to observe other agents’ position, gaze etc.", "Now the location can be pointed to using gaze.", "If gaze is disabled, the agent could indicate the goal landmark by moving to it.", "Basically even when the communication is disabled the agents can come up with strategies to complete the task."], "summary_text": "The paper provides a multi-agent learning environment and proposes a learning approach that facilitates the emergence of a basic compositional language. The language is quite rudimentary and is essentially a sequence of abstract discrete symbols. But it does comprise of a defined vocabulary and syntax. Setup  Cooperative, partially observable Markov game (multi-agent extension of MDP). All agents have identical action and observation spaces, use the same policy and receive a shared reward. Grounded Communication Environment  Physically simulated 2-D environment in continuous space and discrete time with N agents and M landmarks. The agents and the landmarks would occupy some location and would have some attributes (colour, shape). Within the environment, the agents can go to a location, look at a location or do nothing. Additionally, they can utter communication symbols c (from a shared vocabulary C). Agents themselves learn to assign a meaning to the symbols. Each agent has an internal goal (which could require interaction with other agents to complete) which the other agents cannot see. Goal for agent i consists of an action to perform, a landmark location where to perform the action and another agent who should be performing the action. Since the agent is continuously emitting symbols, a memory module is provided and simple additive memory updates are done. For interaction, the agents could use verbal utterances, non-verbal signals (gaze) or non-communicative strategies (pushing other agents). Approach  A model of all agent and environment state dynamics is created over time and the return gradient is computed. Gumbel-Softmax distribution is used to obtain categorical word emission c.  A multi-layer perceptron is used to model the policy which returns action, communication symbol and the memory update for each agent. Since the number of agents (and hence the number of communication streams etc) can vary across instantiations, an identical model is instantiated per agent and per communication stream. The output of individual processing modules are pooled into feature vectors corresponding to communication and physical observations. These pooled features and the goal vectors are fed to the final processing module from which actions and categorical symbols are sampled. In practice, using an additional task (each agent predicts the goal for another agent) encouraged more meaningful communication utterances. Compositionality and Vocabulary Size  Authors recommend using a large vocabulary with a soft penalty that discourages use of too many words. This leads to use of a large vocabulary in the intermediate state which converges to a small vocabulary. Along the lines of rich gets richer dynamics, the communication symbol c’s are modelled as being generated by a Dirichlet process. The resulting reward across all agents is the log-likelihood of all communication utterances to have been generated by a Dirichlet process. Since the agents can only communicate in discrete symbols and do not have a global positioning reference, they need to unambiguously communicate landmark references to other agents. Case I - Agents can not see each other  Non-verbal communication is not possible. When trained with just 2 agents, symbols are assigned for each landmark and action. As the number of agents is increased, additional symbols are used to refer to agents. If the agents of the same colour are asked to perform conflicting tasks, they perform the average of conflicting tasks. If distractor locations are added, the agents learn to ignore them. Non-verbal communication  Agents are allowed to observe other agents’ position, gaze etc. Now the location can be pointed to using gaze. If gaze is disabled, the agent could indicate the goal landmark by moving to it. Basically even when the communication is disabled the agents can come up with strategies to complete the task.", "pdf_url": "https://arxiv.org/pdf/1703.04908", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/emergence-of-grounded-compositional-language-in-multi-agent-populations.json"}
{"id": "26277001", "bin": "600_700", "summary_sentences": ["The paper introduces a novel technique to explain the predictions of any classifier in an interpretable and faithful manner.", "It also proposes a method to explain models by obtaining representative individual predictions and their explanations.", "Demo  Desired Characteristics for Explanations  Interpretable  Take into account user limitations.", "Since features in a machine learning model need not be interpretable, the input to the explanations may have to be different from input to the model.", "Local Fidelity  Explanation should be locally faithful, ie it should correspond to how the model behaves in the vicinity of the instance being predicted.", "Model Agnostic  Treat the original, given model as a black box.", "Global Perspective  Select a few predictions such that they represent the entire model.", "LIME  Local Interpretable Model-agnostic Explanations  Interpretable Data Representations  For text classification, an interpretable representation could be a binary vector indicating the presence or absence of a word (or bag or words).", "For image classification, an interpretable representation may be a binary vector indicating the \"presence\" of a super-pixel.", "x ∈ Rd is the original representation of an instance being explained while x` ∈ {0, 1}d denotes a binary vector for its representation.", "Fidelity-Interpretability Trade-off  Define an explanation as a model g ∈ G, where G is a class of potentially interpretable models and g acts over absence/presence of the interpretable components  Define Ω(g) as a measure of complexity (as opposed to interpretability) of the explanation g ∈ G.  Define f to be the model being explained.", "Define πx(z) as a proximity measure between an instance z to x (to define locality around x).", "Define L(f, g, πx) as a measure of how unfaithful g is in approximating f in the locality defined by πx.", "To ensure both interpretability and local fidelity, we minimise L(f, g, πx) while having Ω(g) be low enough to be interpretable.", "Sampling for Local Exploration  Since f is treated as a black box, the local behaviour of L(f, g, πx) is approximated by drawing samples weighted by πx.", "Given an instance x`, generate a dataset of perturbed samples Z and optimise the LIME model loss, L(f, g, πx).", "The paper proposes to use sparse linear explanations with the locally weighted square loss as L. This could be a problem in the case of highly non-linear models.", "Submodular Pick for Explaining Models  Global understanding of the model by explaining a set of individual instances.", "Define B to the number of explanations to be generated.", "Pick Step - the task of selecting B instances for the user to inspect.", "Aim to obtain non-redundant explanations that represent how the model behaves globally.", "Given a matrix of n explanations, using d features (also called explanation matrix), rank the features such that the feature which explains more instances gets a higher score.", "When selecting instances, avoid instances with similar explanation and try to increase coverage.", "Conclusion  The paper evaluates its approach on a series of simulated and human-in-the-loop tasks to check:  Are explanations faithful to the model.", "Could the predictions be trusted.", "Can the model be trusted.", "Can users select the best classifier given the explanations.", "Can user (non-experts) improve the classifier by means of feature selection.", "Can explanations lead to insights about the model itself.", "Future Work  Need to define a way of finding (and ranking) compatible features across images for SP-LIME.", "It could be difficult to define the relevant features for model explanation in certain cases - for example, single words may not be a good feature in sentiment analysis models."], "summary_text": "The paper introduces a novel technique to explain the predictions of any classifier in an interpretable and faithful manner. It also proposes a method to explain models by obtaining representative individual predictions and their explanations. Demo  Desired Characteristics for Explanations  Interpretable  Take into account user limitations. Since features in a machine learning model need not be interpretable, the input to the explanations may have to be different from input to the model. Local Fidelity  Explanation should be locally faithful, ie it should correspond to how the model behaves in the vicinity of the instance being predicted. Model Agnostic  Treat the original, given model as a black box. Global Perspective  Select a few predictions such that they represent the entire model. LIME  Local Interpretable Model-agnostic Explanations  Interpretable Data Representations  For text classification, an interpretable representation could be a binary vector indicating the presence or absence of a word (or bag or words). For image classification, an interpretable representation may be a binary vector indicating the \"presence\" of a super-pixel. x ∈ Rd is the original representation of an instance being explained while x` ∈ {0, 1}d denotes a binary vector for its representation. Fidelity-Interpretability Trade-off  Define an explanation as a model g ∈ G, where G is a class of potentially interpretable models and g acts over absence/presence of the interpretable components  Define Ω(g) as a measure of complexity (as opposed to interpretability) of the explanation g ∈ G.  Define f to be the model being explained. Define πx(z) as a proximity measure between an instance z to x (to define locality around x). Define L(f, g, πx) as a measure of how unfaithful g is in approximating f in the locality defined by πx. To ensure both interpretability and local fidelity, we minimise L(f, g, πx) while having Ω(g) be low enough to be interpretable. Sampling for Local Exploration  Since f is treated as a black box, the local behaviour of L(f, g, πx) is approximated by drawing samples weighted by πx. Given an instance x`, generate a dataset of perturbed samples Z and optimise the LIME model loss, L(f, g, πx). The paper proposes to use sparse linear explanations with the locally weighted square loss as L. This could be a problem in the case of highly non-linear models. Submodular Pick for Explaining Models  Global understanding of the model by explaining a set of individual instances. Define B to the number of explanations to be generated. Pick Step - the task of selecting B instances for the user to inspect. Aim to obtain non-redundant explanations that represent how the model behaves globally. Given a matrix of n explanations, using d features (also called explanation matrix), rank the features such that the feature which explains more instances gets a higher score. When selecting instances, avoid instances with similar explanation and try to increase coverage. Conclusion  The paper evaluates its approach on a series of simulated and human-in-the-loop tasks to check:  Are explanations faithful to the model. Could the predictions be trusted. Can the model be trusted. Can users select the best classifier given the explanations. Can user (non-experts) improve the classifier by means of feature selection. Can explanations lead to insights about the model itself. Future Work  Need to define a way of finding (and ranking) compatible features across images for SP-LIME. It could be difficult to define the relevant features for model explanation in certain cases - for example, single words may not be a good feature in sentiment analysis models.", "pdf_url": "https://arxiv.org/pdf/1602.04938", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/744ab6c17a2289ca139ea586d1d65e.json"}
{"id": "93598871", "bin": "700_800", "summary_sentences": ["This paper presents an extensive evaluation of variants of LSTM networks.", "Specifically, they start from what they consider to be the vanilla architecture and, from it, also consider 8 variants which correspond to small modifications on the vanilla case.", "The vanilla architecture is the one described in Graves & Schmidhuber (2005)  [ref] , and the variants consider removing single parts of it (input,forget,output gates or activation functions), coupling the input and forget gate (which is inspired from GRU) or having full recurrence between all gates (which comes from the original LSTM formulation).", "In their experimental setup, they consider 3 datasets: TIMIT (speech recognition), IAM Online Handwriting Database (character recognition) and JSB Chorales (polyphonic music modeling).", "For each, they tune the hyper-parameters of each of the 9 architectures, using random search based on 200 samples.", "Then, they keep the 20 best hyper-parameters and use the statistics of those as a basis for comparing the architectures.", "#### My two cents  This was a very useful ready.", "I'd make it a required read for anyone that wants to start using LSTMs.", "First, I found the initial historical description of the developments surrounding LSTMs very interesting and clarifying.", "But more importantly, it presents a really useful picture of LSTMs that can both serve as a good basis for starting to use LSTMs and also an insightful (backed with data) exposition of the importance of each part in the LSTM.", "The analysis based on an fANOVA (which I didn't know about until now) is quite neat.", "Perhaps the most surprising observation is that momentum actually doesn't seem to help that much.", "Investigating second order interaction between hyper-parameters was a smart thing to do (showing that tuning the learning rate and hidden layer jointly might not be that important, which is a useful insight).The illustrations in Figure 4, layout out the estimated relationship (with uncertainty) between learning rate / hidden layer size / input noise variance and performance / training time is also full of useful information.", "I wont repeat here the main observations of the paper, which are laid out clearly in the conclusion (section 6).", "Additionally, my personal take-away point is that, in an LSTM implementation, it might still be useful to support the removal peepholes or having coupled input and forget gates, since they both yielded the ultimate best test set performance on at least one of the datasets (I'm assuming it was also best on the validation set, though this might not be the case...)  The fANOVE analysis makes it clear that the learning rate is the most critical hyper-parameter to tune (can be \"make or break\").", "That said, this is already well known.", "And the fact that it explains so much of the variance might reflect a bias of the analysis towards a situation where the learning rate isn't tuned as well as it could be in practice (this is afterall THE hyper-parameter that neural net researcher spend the most time tuning in practice).", "So, as future work, this suggests perhaps doing another round of the same analysis (which is otherwise really neatly setup), where more effort is always put on tuning the learning rate, individually for each of the other hyper-parameters.", "In other words, we'd try to ignore the regions of hyper-parameter space that correspond to bad learning rates, in order to \"marginalize out\" its effect.", "This would thus explore the perhaps more realistic setup that assumes one always tunes the learning rate as best as possible.", "Also, considering a less aggressive gradient clipping into the hyper-parameter search would be interesting since, as the authors admit, clipping within [-1,1] might have been too much and could explain why it didn't help   Otherwise, a really great and useful read!"], "summary_text": "This paper presents an extensive evaluation of variants of LSTM networks. Specifically, they start from what they consider to be the vanilla architecture and, from it, also consider 8 variants which correspond to small modifications on the vanilla case. The vanilla architecture is the one described in Graves & Schmidhuber (2005)  [ref] , and the variants consider removing single parts of it (input,forget,output gates or activation functions), coupling the input and forget gate (which is inspired from GRU) or having full recurrence between all gates (which comes from the original LSTM formulation). In their experimental setup, they consider 3 datasets: TIMIT (speech recognition), IAM Online Handwriting Database (character recognition) and JSB Chorales (polyphonic music modeling). For each, they tune the hyper-parameters of each of the 9 architectures, using random search based on 200 samples. Then, they keep the 20 best hyper-parameters and use the statistics of those as a basis for comparing the architectures. #### My two cents  This was a very useful ready. I'd make it a required read for anyone that wants to start using LSTMs. First, I found the initial historical description of the developments surrounding LSTMs very interesting and clarifying. But more importantly, it presents a really useful picture of LSTMs that can both serve as a good basis for starting to use LSTMs and also an insightful (backed with data) exposition of the importance of each part in the LSTM. The analysis based on an fANOVA (which I didn't know about until now) is quite neat. Perhaps the most surprising observation is that momentum actually doesn't seem to help that much. Investigating second order interaction between hyper-parameters was a smart thing to do (showing that tuning the learning rate and hidden layer jointly might not be that important, which is a useful insight).The illustrations in Figure 4, layout out the estimated relationship (with uncertainty) between learning rate / hidden layer size / input noise variance and performance / training time is also full of useful information. I wont repeat here the main observations of the paper, which are laid out clearly in the conclusion (section 6). Additionally, my personal take-away point is that, in an LSTM implementation, it might still be useful to support the removal peepholes or having coupled input and forget gates, since they both yielded the ultimate best test set performance on at least one of the datasets (I'm assuming it was also best on the validation set, though this might not be the case...)  The fANOVE analysis makes it clear that the learning rate is the most critical hyper-parameter to tune (can be \"make or break\"). That said, this is already well known. And the fact that it explains so much of the variance might reflect a bias of the analysis towards a situation where the learning rate isn't tuned as well as it could be in practice (this is afterall THE hyper-parameter that neural net researcher spend the most time tuning in practice). So, as future work, this suggests perhaps doing another round of the same analysis (which is otherwise really neatly setup), where more effort is always put on tuning the learning rate, individually for each of the other hyper-parameters. In other words, we'd try to ignore the regions of hyper-parameter space that correspond to bad learning rates, in order to \"marginalize out\" its effect. This would thus explore the perhaps more realistic setup that assumes one always tunes the learning rate as best as possible. Also, considering a less aggressive gradient clipping into the hyper-parameter search would be interesting since, as the authors admit, clipping within [-1,1] might have been too much and could explain why it didn't help   Otherwise, a really great and useful read!", "pdf_url": "http://arxiv.org/pdf/1503.04069", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/greffskss15.json"}
{"id": "6726365", "bin": "700_800", "summary_sentences": ["The paper looks at the problem of learning structured exploration policies for training RL agents.", "Structured Exploration  Consider a stochastic, parameterized policy πθ(a|s) where θ represents the policy-parameters.", "To encourage exploration, noise can be added to the policy at each time step t. But the noise added in such a manner does not have any notion of temporal coherence.", "Another issue is that if the policy is represented by a simple distribution (say parameterized unimodal Gaussian), it can not model complex time-correlated stochastic processes.", "The paper proposes to condition the policy on per-episode random variables (z) which are sampled from a learned latent distribution.", "Consider a distibution over the tasks p(T).", "At the start of any episode of the ith task, a latent variable zi is sampled from the distribution N(μi, σi) where μi and σi are the learned parameters of the distribution and are referred to as the variation parameters.", "Once sampled, the same zi is used to condition the policy for as long as the current episode lasts and the action is sampled from then distribution πθ(a|s, zi).", "The intuition is that the latent variable zi would encode the notion of a task or goal that does not change arbitrarily during the episode.", "Model Agnostic Exploration with Structured Noise  The paper focuses on the setting where the structured exploration policies are to be learned while leveraging the learning from prior tasks.", "A meta-learning approach, called as model agnostic exploration with structured noise (MAESN) is proposed to learn a good initialization of the policy-parameters and to learn a latent space (for sampling the z from) that can inject structured stochasticity in the policy.", "General meta-RL approaches have two limitations when it comes to “learning to explore”:  Casting meta-RL problems as RL problems lead to policies that do not exhibit sufficient variability to explore effectively.", "Many current approaches try to meta-learn the entire learning algorithm which limits the asymptotic performance of the model.", "Idea behind MAESN is to meta-train policy-parameters so that they learn to use the task-specific latent variables for exploration and can quickly adapt to a new task.", "An important detail is that the parameters are optimized to maximize the expected rewards after one step of gradient update to ensure that the policy uses the latent variables for exploration.", "For every iteration of meta-training, an “inner” gradient update is performed on the variational parameters and the post-inner-update parameters are used to perform the meta-update.", "The authors report that performing the “inner” gradient update on the policy-parameters does not help the overall learning objective and that the step size for each parameter had to be meta-learned.", "The variation parameters have the usual KL divergence loss which encourages them to be close to the prior distribution (unit Gaussian in this case).", "After training, the variational parameters for each task are quite close to the prior probably because the training objective optimizes for the expected reward after one step of gradient descent on the variational parameters.", "Another implementation detail is that reward shaping is used to ensure that the policy gets useful signal during meta-training.", "To be fair to the baselines, reward shaping is used while training baselines as well.", "Moreover, the policies trained with reward shaping generalizes to sparse reward setup as well (during meta-test time).", "Experiments  Three tasks distributions: Robotic Manipulation, Wheeled Locomotion, and Legged Locomotion.", "Each task distribution has 100 meta-training tasks.", "In the Manipulation task distribution, the learner has to push different blocks from different positions to different goal positions.", "In the Locomotion task distributions, the different tasks correspond to the different goal positions.", "The experiments show that the proposed approach can adapt to new tasks quickly and the learn coherent exploration strategy.", "• In some cases, learning from scratch also provides a strong asymptotic performance although learning from scratch takes much longer."], "summary_text": "The paper looks at the problem of learning structured exploration policies for training RL agents. Structured Exploration  Consider a stochastic, parameterized policy πθ(a|s) where θ represents the policy-parameters. To encourage exploration, noise can be added to the policy at each time step t. But the noise added in such a manner does not have any notion of temporal coherence. Another issue is that if the policy is represented by a simple distribution (say parameterized unimodal Gaussian), it can not model complex time-correlated stochastic processes. The paper proposes to condition the policy on per-episode random variables (z) which are sampled from a learned latent distribution. Consider a distibution over the tasks p(T). At the start of any episode of the ith task, a latent variable zi is sampled from the distribution N(μi, σi) where μi and σi are the learned parameters of the distribution and are referred to as the variation parameters. Once sampled, the same zi is used to condition the policy for as long as the current episode lasts and the action is sampled from then distribution πθ(a|s, zi). The intuition is that the latent variable zi would encode the notion of a task or goal that does not change arbitrarily during the episode. Model Agnostic Exploration with Structured Noise  The paper focuses on the setting where the structured exploration policies are to be learned while leveraging the learning from prior tasks. A meta-learning approach, called as model agnostic exploration with structured noise (MAESN) is proposed to learn a good initialization of the policy-parameters and to learn a latent space (for sampling the z from) that can inject structured stochasticity in the policy. General meta-RL approaches have two limitations when it comes to “learning to explore”:  Casting meta-RL problems as RL problems lead to policies that do not exhibit sufficient variability to explore effectively. Many current approaches try to meta-learn the entire learning algorithm which limits the asymptotic performance of the model. Idea behind MAESN is to meta-train policy-parameters so that they learn to use the task-specific latent variables for exploration and can quickly adapt to a new task. An important detail is that the parameters are optimized to maximize the expected rewards after one step of gradient update to ensure that the policy uses the latent variables for exploration. For every iteration of meta-training, an “inner” gradient update is performed on the variational parameters and the post-inner-update parameters are used to perform the meta-update. The authors report that performing the “inner” gradient update on the policy-parameters does not help the overall learning objective and that the step size for each parameter had to be meta-learned. The variation parameters have the usual KL divergence loss which encourages them to be close to the prior distribution (unit Gaussian in this case). After training, the variational parameters for each task are quite close to the prior probably because the training objective optimizes for the expected reward after one step of gradient descent on the variational parameters. Another implementation detail is that reward shaping is used to ensure that the policy gets useful signal during meta-training. To be fair to the baselines, reward shaping is used while training baselines as well. Moreover, the policies trained with reward shaping generalizes to sparse reward setup as well (during meta-test time). Experiments  Three tasks distributions: Robotic Manipulation, Wheeled Locomotion, and Legged Locomotion. Each task distribution has 100 meta-training tasks. In the Manipulation task distribution, the learner has to push different blocks from different positions to different goal positions. In the Locomotion task distributions, the different tasks correspond to the different goal positions. The experiments show that the proposed approach can adapt to new tasks quickly and the learn coherent exploration strategy. • In some cases, learning from scratch also provides a strong asymptotic performance although learning from scratch takes much longer.", "pdf_url": "https://arxiv.org/pdf/1904.06387", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/meta-reinforcement-learning-of-structured-exploration-strategies.json"}
{"id": "16057781", "bin": "700_800", "summary_sentences": ["The paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies.", "Given a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner.", "The framework is called as Model Primitive Hierarchical Reinforcement Learning (MPHRL).", "Idea  Instead of learning a single transition model of the environment (aka world model) that can model the transitions very well, it is sufficient to learn several (say k) suboptimal models (aka model primitives).", "Each model primitive will be good in only a small part of the state space (aka region of specialization).", "These model primitives can then be used to train a gating mechanism for selecting sub-policies to solve a given task.", "Since these model primitives are sub-optimal, they are not directly used with model-based RL but are used to obtain useful functional decompositions and sub-policies are trained with model-free approaches.", "Single Task Learning  A gating controller is trained to choose the sub-policy whose model primitive makes the best prediction.", "This requires modeling p(Mk | st, at, st+1) where p(Mk) denotes the probability of selecting the kth model primitive.", "This is hard to compute as the system does not have access to st+1 and at at time t before it has choosen the sub-policy.", "Properly marginalizing st+1 and at would require expensive MC sampling.", "Hence an approximation is used and the gating controller is modeled as a categorical distribution - to produce p(Mk | st).", "This is trained via a conditional cross entropy loss where the ground truth distribution is obtained from transitions sampled in a rollout.", "The paper notes that technique is biased but reports that it still works for the downstream tasks.", "The gating controller composes the sub-policies as a mixture of Gaussians.", "For learning, PPO algorithm is used with each model primitives gradient weighted by the probability from the gating controller.", "Lifelong Learning  Different tasks could share common subtasks but may require a different composition of subtasks.", "Hence, the learned sub-policies are transferred across tasks but not the gating controller or the baseline estimator (from PPO).", "Experiments  Domains:  Mujoco ant navigating different mazes.", "Stacker arm picking up and placing different boxes.", "Implementation Details:  Gaussian subpolicies  PPO as the baseline  Model primitives are hand-crafted using the true next state provided by the environment simulator.", "Single Task  Only maze task is considered with the start position (of the ant) and the goal position is fixed.", "Observation includes distance from the goal.", "Forcing the agent to decompose the problem, when a more direct solution may be available, causes the sample complexity to increase on one task.", "Lifelong Learning  Maze  10 random Mujoco ant mazes used as the task distribution.", "MPHRL takes almost twice the number of steps (as compared to PPO baseline) to solve the first task but this cost gets amortized over the distribution and the model takes half the number of steps as compared to the baseline (summed over the 10 tasks).", "Pick and Place  8 Pick and Place tasks are created with max 3 goal locations.", "Observation includes the position of the goal.", "Ablations  Overlapping model primitives can degrade the performance (to some extent).", "Similarly, the performance suffers when redundant primitives are introduced indicating that the gating mechanism is not very robust.", "Sub-policies could quickly adapt to the previous tasks (on which they were trained initially) despite being finetuned on subsequent tasks.", "The order of tasks (in the 10-Mazz task) does not degrage the performance.", "Transfering the gating controller leads to negative transfer.", "Notes  I think the biggest strength of the work is that accurate dynamics model are not needed (which are hard to train anyways!)", "through the experimental results are not conclusive given the limited number of domains on which the approach is tested."], "summary_text": "The paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies. Given a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner. The framework is called as Model Primitive Hierarchical Reinforcement Learning (MPHRL). Idea  Instead of learning a single transition model of the environment (aka world model) that can model the transitions very well, it is sufficient to learn several (say k) suboptimal models (aka model primitives). Each model primitive will be good in only a small part of the state space (aka region of specialization). These model primitives can then be used to train a gating mechanism for selecting sub-policies to solve a given task. Since these model primitives are sub-optimal, they are not directly used with model-based RL but are used to obtain useful functional decompositions and sub-policies are trained with model-free approaches. Single Task Learning  A gating controller is trained to choose the sub-policy whose model primitive makes the best prediction. This requires modeling p(Mk | st, at, st+1) where p(Mk) denotes the probability of selecting the kth model primitive. This is hard to compute as the system does not have access to st+1 and at at time t before it has choosen the sub-policy. Properly marginalizing st+1 and at would require expensive MC sampling. Hence an approximation is used and the gating controller is modeled as a categorical distribution - to produce p(Mk | st). This is trained via a conditional cross entropy loss where the ground truth distribution is obtained from transitions sampled in a rollout. The paper notes that technique is biased but reports that it still works for the downstream tasks. The gating controller composes the sub-policies as a mixture of Gaussians. For learning, PPO algorithm is used with each model primitives gradient weighted by the probability from the gating controller. Lifelong Learning  Different tasks could share common subtasks but may require a different composition of subtasks. Hence, the learned sub-policies are transferred across tasks but not the gating controller or the baseline estimator (from PPO). Experiments  Domains:  Mujoco ant navigating different mazes. Stacker arm picking up and placing different boxes. Implementation Details:  Gaussian subpolicies  PPO as the baseline  Model primitives are hand-crafted using the true next state provided by the environment simulator. Single Task  Only maze task is considered with the start position (of the ant) and the goal position is fixed. Observation includes distance from the goal. Forcing the agent to decompose the problem, when a more direct solution may be available, causes the sample complexity to increase on one task. Lifelong Learning  Maze  10 random Mujoco ant mazes used as the task distribution. MPHRL takes almost twice the number of steps (as compared to PPO baseline) to solve the first task but this cost gets amortized over the distribution and the model takes half the number of steps as compared to the baseline (summed over the 10 tasks). Pick and Place  8 Pick and Place tasks are created with max 3 goal locations. Observation includes the position of the goal. Ablations  Overlapping model primitives can degrade the performance (to some extent). Similarly, the performance suffers when redundant primitives are introduced indicating that the gating mechanism is not very robust. Sub-policies could quickly adapt to the previous tasks (on which they were trained initially) despite being finetuned on subsequent tasks. The order of tasks (in the 10-Mazz task) does not degrage the performance. Transfering the gating controller leads to negative transfer. Notes  I think the biggest strength of the work is that accurate dynamics model are not needed (which are hard to train anyways!) through the experimental results are not conclusive given the limited number of domains on which the approach is tested.", "pdf_url": "https://arxiv.org/pdf/1903.01567", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/model-primitive-hierarchical-lifelong-reinforcement-learning.json"}
{"id": "47686356", "bin": "800_900", "summary_sentences": ["TuckER is a simple, yet powerful linear model that uses Tucker decomposition for the task of link prediction in knowledge graphs.", "Paper  Implementation  Knowledge Graph as a Tensor  Let E be the set of all the entities and R be the set of all the relations in a given knowledge graph (KG).", "The KG can be represented as a list of triples of the form (source entity, relation, object entity) or (es, r, eo).", "The list of triples can be represented as a third-order tensor (of binary values) where each element corresponds to a triple and each element’s value corresponds to ether that element is present in the KG or not.", "The link prediction task can be formulated as - given a set of all triples, learn a scoring function that assigns a score to each triple.", "The score indicates whether the triple is actually present in the KG or not.", "TuckER Decomposition  Tucker decomposition factorizes a tensor into a set of factor matrices and a smaller core tensor.", "In the specific case of three-mode tensors (alternate representation of a KG), the given original tensor X (of shape IxJxK) can be factorized into a core tensor W (of shape PxQxR) and 3 factor matrics - A (of shape IxP), B (of shape JxQ) and C (of shape KxR) such that X is approximately W x1 A x2 B x3 C, where Xn denotes the tensor product along the nth mode.", "Generally, P, Q, R are smaller than I, J K (respectively) and W can be seen as a compressed version of X.  TuckER Decomposition for Link Prediction  Two embedding matrics are used for embedding the entities and the relations respectively.", "Entity embedding matrix E is shared for both subject and the object ie E = A = B.", "The scoring function is gives as W x1 es x2 wr x3 e0 where es, wr and eo are the embedding vectors corresonding to es, er and eo respectively.", "Note that both the core tensor and the factor matrices are to be learnt.", "Model is trained with the standard negative log-likelihood loss given as (for one triple):  y * log(p) + (1-y) * log(1-p)  To speed up training and increase accuracy, 1-N scoring is used.", "A given (es, r) is simultaneously scored for all the entities using the local-closed world assumption (knowledge graph is only locally complete).", "Handling asymmetric relations is straightforward by learning a relation embedding alongside a relation-agnostic core tensor which enables knowledge sharing across relations.", "Theoretical Analysis  One important consideration would be the expressive power of TuckER models, especially in relation to other models like ComplEx and SimplE.", "It can be shown the TuckER is fully expressive ie give any ground truth over E and R, there exists a TuckER model which can perfectly represent the data - using 1-hot entity and relation embedding.", "For full expressiveness, dimensionality of entity (relation) is nE (nR) where nE (nR) are the number of entities (relations).", "In comparsion, the required dimensionality for ComplEx is nE * nR (for both entity and relations) and for SimplE, it is min(E * nR, number of facts + 1) (for both entity and relations).", "Many existing models like RESCAL, DistMult, ComplEx, SimplE etc can be seen as special cases of TuckER.", "Experiments  Datasets  FB15k, FB15k-237, WN18, WN18RR  The max number of entities is around 41K and max number of relations is around 1.3K  Implementation  BatchNorm, Dropout and Learning rate decay are used.", "Metrics  Mean Reciprocal Rank (MRR) - the average of the inverse of mean rank assigned to the true triple overall ne generated triples.", "hits@k (k = 1, 3, 10) - percentage of times the true triple is ranked in the top k of the ne generated triples.", "Higher is better for both the metrics.", "Results  TuckER outperforms all the baseline models on all but one task.", "Dropout is an important factor with higher dropout rates (0, 3, 0.4, 0.5) needed for datasets with fewer training examples per relation (hence more prone to overfitting).", "TuckER improves performance more significantly when the number of relations is large.", "Even with lower embedding dimensions, TuckER’s performance does not deteriorate as much as other models."], "summary_text": "TuckER is a simple, yet powerful linear model that uses Tucker decomposition for the task of link prediction in knowledge graphs. Paper  Implementation  Knowledge Graph as a Tensor  Let E be the set of all the entities and R be the set of all the relations in a given knowledge graph (KG). The KG can be represented as a list of triples of the form (source entity, relation, object entity) or (es, r, eo). The list of triples can be represented as a third-order tensor (of binary values) where each element corresponds to a triple and each element’s value corresponds to ether that element is present in the KG or not. The link prediction task can be formulated as - given a set of all triples, learn a scoring function that assigns a score to each triple. The score indicates whether the triple is actually present in the KG or not. TuckER Decomposition  Tucker decomposition factorizes a tensor into a set of factor matrices and a smaller core tensor. In the specific case of three-mode tensors (alternate representation of a KG), the given original tensor X (of shape IxJxK) can be factorized into a core tensor W (of shape PxQxR) and 3 factor matrics - A (of shape IxP), B (of shape JxQ) and C (of shape KxR) such that X is approximately W x1 A x2 B x3 C, where Xn denotes the tensor product along the nth mode. Generally, P, Q, R are smaller than I, J K (respectively) and W can be seen as a compressed version of X.  TuckER Decomposition for Link Prediction  Two embedding matrics are used for embedding the entities and the relations respectively. Entity embedding matrix E is shared for both subject and the object ie E = A = B. The scoring function is gives as W x1 es x2 wr x3 e0 where es, wr and eo are the embedding vectors corresonding to es, er and eo respectively. Note that both the core tensor and the factor matrices are to be learnt. Model is trained with the standard negative log-likelihood loss given as (for one triple):  y * log(p) + (1-y) * log(1-p)  To speed up training and increase accuracy, 1-N scoring is used. A given (es, r) is simultaneously scored for all the entities using the local-closed world assumption (knowledge graph is only locally complete). Handling asymmetric relations is straightforward by learning a relation embedding alongside a relation-agnostic core tensor which enables knowledge sharing across relations. Theoretical Analysis  One important consideration would be the expressive power of TuckER models, especially in relation to other models like ComplEx and SimplE. It can be shown the TuckER is fully expressive ie give any ground truth over E and R, there exists a TuckER model which can perfectly represent the data - using 1-hot entity and relation embedding. For full expressiveness, dimensionality of entity (relation) is nE (nR) where nE (nR) are the number of entities (relations). In comparsion, the required dimensionality for ComplEx is nE * nR (for both entity and relations) and for SimplE, it is min(E * nR, number of facts + 1) (for both entity and relations). Many existing models like RESCAL, DistMult, ComplEx, SimplE etc can be seen as special cases of TuckER. Experiments  Datasets  FB15k, FB15k-237, WN18, WN18RR  The max number of entities is around 41K and max number of relations is around 1.3K  Implementation  BatchNorm, Dropout and Learning rate decay are used. Metrics  Mean Reciprocal Rank (MRR) - the average of the inverse of mean rank assigned to the true triple overall ne generated triples. hits@k (k = 1, 3, 10) - percentage of times the true triple is ranked in the top k of the ne generated triples. Higher is better for both the metrics. Results  TuckER outperforms all the baseline models on all but one task. Dropout is an important factor with higher dropout rates (0, 3, 0.4, 0.5) needed for datasets with fewer training examples per relation (hence more prone to overfitting). TuckER improves performance more significantly when the number of relations is large. Even with lower embedding dimensions, TuckER’s performance does not deteriorate as much as other models.", "pdf_url": "https://arxiv.org/pdf/1903.01567", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/tucker-tensor-factorization-for-knowledge-graph-completion.json"}
{"id": "57983162", "bin": "800_900", "summary_sentences": ["What  They present a variation of Faster R-CNN, i.e. a model that predicts bounding boxes in images and classifies them.", "In contrast to Faster R-CNN, their model is fully convolutional.", "In contrast to Faster R-CNN, the computation per bounding box candidate (region proposal) is very low.", "How  The basic architecture is the same as in Faster R-CNN:  A base network transforms an image to a feature map.", "Here they use ResNet-101 to do that.", "A region proposal network (RPN) uses the feature map to locate bounding box candidates (\"region proposals\") in the image.", "A classifier uses the feature map and the bounding box candidates and classifies each one of them into C+1 classes, where C is the number of object classes to spot (e.g. \"person\", \"chair\", \"bottle\", ...) and 1 is added for the background.", "During that process, small subregions of the feature maps (those that match the bounding box candidates) must be extracted and converted to fixed-sizes matrices.", "The method to do that is called \"Region of Interest Pooling\" (RoI-Pooling) and is based on max pooling.", "It is mostly the same as in Faster R-CNN.", "Visualization of the basic architecture:  Position-sensitive classification  Fully convolutional bounding box detectors tend to not work well.", "The authors argue, that the problems come from the translation-invariance of convolutions, which is a desirable property in the case of classification but not when precise localization of objects is required.", "They tackle that problem by generating multiple heatmaps per object class, each one being slightly shifted (\"position-sensitive score maps\").", "More precisely:  The classifier generates per object class c a total of k*k heatmaps.", "In the simplest form k is equal to 1.", "Then only one heatmap is generated, which signals whether a pixel is part of an object of class c.  They use k=3*3.", "The first of those heatmaps signals, whether a pixel is part of the top left corner of a bounding box of class c. The second heatmap signals, whether a pixel is part of the top center of a bounding box of class c (and so on).", "The RoI-Pooling is applied to these heatmaps.", "For k=3*3, each bounding box candidate is converted to 3*3 values.", "The first one resembles the top left corner of the bounding box candidate.", "Its value is generated by taking the average of the values in that area in the first heatmap.", "Once the 3*3 values are generated, the final score of class c for that bounding box candidate is computed by averaging the values.", "That process is repeated for all classes and a softmax is used to determine the final class.", "The graphic below shows examples for that:  The above described RoI-Pooling uses only averages and hence is almost (computationally) free.", "They make use of that during the training by sampling many candidates and only backpropagating on those with high losses (online hard example mining, OHEM).", "À trous trick  In order to increase accuracy for small bounding boxes they use the à trous trick.", "That means that they use a pretrained base network (here ResNet-101), then remove a pooling layer and set the à trous rate (aka dilation) of all convolutions after the removed pooling layer to 2.", "The á trous rate describes the distance of sampling locations of a convolution.", "Usually that is 1 (sampled locations are right next to each other).", "If it is set to 2, there is one value \"skipped\" between each pair of neighbouring sampling location.", "By doing that, the convolutions still behave as if the pooling layer existed (and therefore their weights can be reused).", "At the same time, they work at an increased resolution, making them more capable of classifying small objects.", "(Runtime increases though.)", "Training of R-FCN happens similarly to Faster R-CNN.", "Results  Similar accuracy as the most accurate Faster R-CNN configurations at a lower runtime of roughly 170ms per image.", "Switching to ResNet-50 decreases accuracy by about 2 percentage points mAP (at faster runtime).", "Switching to ResNet-152 seems to provide no measureable benefit.", "OHEM improves mAP by roughly 2 percentage points.", "À trous trick improves mAP by roughly 2 percentage points.", "Training on k=1 (one heatmap per class) results in a failure, i.e. a model that fails to predict bounding boxes.", "k=7 is slightly more accurate than k=3."], "summary_text": "What  They present a variation of Faster R-CNN, i.e. a model that predicts bounding boxes in images and classifies them. In contrast to Faster R-CNN, their model is fully convolutional. In contrast to Faster R-CNN, the computation per bounding box candidate (region proposal) is very low. How  The basic architecture is the same as in Faster R-CNN:  A base network transforms an image to a feature map. Here they use ResNet-101 to do that. A region proposal network (RPN) uses the feature map to locate bounding box candidates (\"region proposals\") in the image. A classifier uses the feature map and the bounding box candidates and classifies each one of them into C+1 classes, where C is the number of object classes to spot (e.g. \"person\", \"chair\", \"bottle\", ...) and 1 is added for the background. During that process, small subregions of the feature maps (those that match the bounding box candidates) must be extracted and converted to fixed-sizes matrices. The method to do that is called \"Region of Interest Pooling\" (RoI-Pooling) and is based on max pooling. It is mostly the same as in Faster R-CNN. Visualization of the basic architecture:  Position-sensitive classification  Fully convolutional bounding box detectors tend to not work well. The authors argue, that the problems come from the translation-invariance of convolutions, which is a desirable property in the case of classification but not when precise localization of objects is required. They tackle that problem by generating multiple heatmaps per object class, each one being slightly shifted (\"position-sensitive score maps\"). More precisely:  The classifier generates per object class c a total of k*k heatmaps. In the simplest form k is equal to 1. Then only one heatmap is generated, which signals whether a pixel is part of an object of class c.  They use k=3*3. The first of those heatmaps signals, whether a pixel is part of the top left corner of a bounding box of class c. The second heatmap signals, whether a pixel is part of the top center of a bounding box of class c (and so on). The RoI-Pooling is applied to these heatmaps. For k=3*3, each bounding box candidate is converted to 3*3 values. The first one resembles the top left corner of the bounding box candidate. Its value is generated by taking the average of the values in that area in the first heatmap. Once the 3*3 values are generated, the final score of class c for that bounding box candidate is computed by averaging the values. That process is repeated for all classes and a softmax is used to determine the final class. The graphic below shows examples for that:  The above described RoI-Pooling uses only averages and hence is almost (computationally) free. They make use of that during the training by sampling many candidates and only backpropagating on those with high losses (online hard example mining, OHEM). À trous trick  In order to increase accuracy for small bounding boxes they use the à trous trick. That means that they use a pretrained base network (here ResNet-101), then remove a pooling layer and set the à trous rate (aka dilation) of all convolutions after the removed pooling layer to 2. The á trous rate describes the distance of sampling locations of a convolution. Usually that is 1 (sampled locations are right next to each other). If it is set to 2, there is one value \"skipped\" between each pair of neighbouring sampling location. By doing that, the convolutions still behave as if the pooling layer existed (and therefore their weights can be reused). At the same time, they work at an increased resolution, making them more capable of classifying small objects. (Runtime increases though.) Training of R-FCN happens similarly to Faster R-CNN. Results  Similar accuracy as the most accurate Faster R-CNN configurations at a lower runtime of roughly 170ms per image. Switching to ResNet-50 decreases accuracy by about 2 percentage points mAP (at faster runtime). Switching to ResNet-152 seems to provide no measureable benefit. OHEM improves mAP by roughly 2 percentage points. À trous trick improves mAP by roughly 2 percentage points. Training on k=1 (one heatmap per class) results in a failure, i.e. a model that fails to predict bounding boxes. k=7 is slightly more accurate than k=3.", "pdf_url": "https://arxiv.org/pdf/1605.06409", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/r-fcn.json"}
{"id": "8264189", "bin": "900_1000", "summary_sentences": ["Overview  This paper addresses a key NLP problem known as sarcasm detection using a combination of models based on convolutional neural networks (CNNs).", "Detection of sarcasm is important in other areas such as affective computing and sentiment analysis because such expressions can flip the polarity of a sentence.", "Example  Sarcasm can be considered as expressing a bitter gibe or taunt.", "Examples include statements such as “Is it time for your medication or mine?” and “I work 40 hours a week to be this poor”.", "(Find more fun examples here )  Challenges  To understand and detect sarcasm it is important to understand the facts related to an event.", "This allows for detection of contradiction between the objective polarity (usually negative) and the sarcastic characteristics conveyed by the author (usually positive).", "Consider the example, “I love the pain of breakup”, it is difficult to extract the knowledge needed to detect if there is sarcasm in this statement.", "In the example, “I love the pain” provides knowledge of the sentiment expressed by the author (in this case positive), and “breakup” describes a contradicting sentiment (that of negative).", "Other challenges that exist in understanding sarcastic statements is the reference to multiple events and the need to extract a large amount of facts, commonsense knowledge, anaphora resolution, and logical reasoning.", "The authors avoid automatic feature extraction and rely on CNNs to automatically learn features from a sarcasm dataset.", "Contributions  Apply deep learning to sarcasm detection  Leverage user profiling, emotion, and sentiment features for sarcasm detection  Apply pre-trained models for automatic feature extraction  Model  Sentiment shifting is prevalent in sarcasm-related communication; thus, the authors propose to first train a sentiment model (based on a CNN) for learning sentiment-specific feature extraction.", "The model learns local features in lower layers which are then converted into global features in the higher layers.", "The authors observe that sarcastic expressions are user-specific — some users post more sarcasm than others.", "In the proposed framework, personality-based features, sentiment features, and emotion-based features are incorporated into the sarcasm detection framework.", "Each set of features are learned by separate models, becoming pre-trained models used to extract sarcasm-related features from a dataset.", "CNN Framework  CNNs are effective at modeling hierarchy of local features to learn more global features, which is essential to learn context.", "Sentences are represented using word vectors (embeddings) and provided as input.", "Google’s word2vec vectors are employed as input.", "Non-static representations are used, therefore, parameters for these word vectors are learned during the training phase.", "Max pooling is then applied to the feature maps to generate features.", "A fully connected layer is applied followed by a softmax layer for outputting the final prediction.", "(See diagram of the CNN-based architecture below)  To obtain the other features — sentiment (S), emotion (E), and personality (P) — CNN models are pre-trained and used to extract features from the sarcasm datasets.", "Different training datasets were used to train each model.", "(Refer to paper for more details)  Two classifiers are tested — a pure CNN classifier (CNN) and CNN-extracted features fed to an SVM classifier (CNN-SVM).", "A separate baseline classifier (B) — consisting of only the CNN model without the incorporation of the other models (e.g., emotion and sentiment) — is trained as well.", "Experiments  Data — Balanced and imbalanced sarcastic tweets datasets were obtained from ( Ptacek et al., 2014 ) and The Sarcasm Detector .", "Usernames, URLs, and hashtags are removed, and the NLTK Twitter Tokenizer was used for tokenization.", "(See paper for more details)  The performances of both the CNN and CNN-SVM classifier, when applied to all datasets, are shown in the table below.", "We can observe that when the models (specifically CNN-SVM) combine sarcasm features, emotion features, sentiment features, and personality traits features, it outperforms all the other models with the exception of the baseline model (B).", "The table below shows comparison results of the the state-of-the-art model (method 1), other well-known sarcasm detection research (method 2), and the proposed model (method 3).", "The proposed model consistently outperforms all the other models.", "Generalizability capabilities of the models were tested and the main finding was that if the datasets differed in nature, this significantly impacted the results.", "(See visualization of the datasets rendered via PCA below).", "For instance, training was done on Dataset 1 and tested on Dataset 2; the F1-score of the model was 33.05%, significantly dropping in accuracy.", "Conclusion and Future Work  Overall, the authors found that sarcasm is very topic-dependent and highly contextual, therefore, sentiment and other contextual clues help to detect sarcasm from text.", "Pre-trained sentiment, emotion, and personality models are used to capture contextualized information from text.", "Hand-crafted features (e.g., n-grams), though somewhat useful for sarcasm detection, will produce very sparse feature vector representations.", "For those reasons, word embeddings are used as input features.", "References  Ref:  [url]"], "summary_text": "Overview  This paper addresses a key NLP problem known as sarcasm detection using a combination of models based on convolutional neural networks (CNNs). Detection of sarcasm is important in other areas such as affective computing and sentiment analysis because such expressions can flip the polarity of a sentence. Example  Sarcasm can be considered as expressing a bitter gibe or taunt. Examples include statements such as “Is it time for your medication or mine?” and “I work 40 hours a week to be this poor”. (Find more fun examples here )  Challenges  To understand and detect sarcasm it is important to understand the facts related to an event. This allows for detection of contradiction between the objective polarity (usually negative) and the sarcastic characteristics conveyed by the author (usually positive). Consider the example, “I love the pain of breakup”, it is difficult to extract the knowledge needed to detect if there is sarcasm in this statement. In the example, “I love the pain” provides knowledge of the sentiment expressed by the author (in this case positive), and “breakup” describes a contradicting sentiment (that of negative). Other challenges that exist in understanding sarcastic statements is the reference to multiple events and the need to extract a large amount of facts, commonsense knowledge, anaphora resolution, and logical reasoning. The authors avoid automatic feature extraction and rely on CNNs to automatically learn features from a sarcasm dataset. Contributions  Apply deep learning to sarcasm detection  Leverage user profiling, emotion, and sentiment features for sarcasm detection  Apply pre-trained models for automatic feature extraction  Model  Sentiment shifting is prevalent in sarcasm-related communication; thus, the authors propose to first train a sentiment model (based on a CNN) for learning sentiment-specific feature extraction. The model learns local features in lower layers which are then converted into global features in the higher layers. The authors observe that sarcastic expressions are user-specific — some users post more sarcasm than others. In the proposed framework, personality-based features, sentiment features, and emotion-based features are incorporated into the sarcasm detection framework. Each set of features are learned by separate models, becoming pre-trained models used to extract sarcasm-related features from a dataset. CNN Framework  CNNs are effective at modeling hierarchy of local features to learn more global features, which is essential to learn context. Sentences are represented using word vectors (embeddings) and provided as input. Google’s word2vec vectors are employed as input. Non-static representations are used, therefore, parameters for these word vectors are learned during the training phase. Max pooling is then applied to the feature maps to generate features. A fully connected layer is applied followed by a softmax layer for outputting the final prediction. (See diagram of the CNN-based architecture below)  To obtain the other features — sentiment (S), emotion (E), and personality (P) — CNN models are pre-trained and used to extract features from the sarcasm datasets. Different training datasets were used to train each model. (Refer to paper for more details)  Two classifiers are tested — a pure CNN classifier (CNN) and CNN-extracted features fed to an SVM classifier (CNN-SVM). A separate baseline classifier (B) — consisting of only the CNN model without the incorporation of the other models (e.g., emotion and sentiment) — is trained as well. Experiments  Data — Balanced and imbalanced sarcastic tweets datasets were obtained from ( Ptacek et al., 2014 ) and The Sarcasm Detector . Usernames, URLs, and hashtags are removed, and the NLTK Twitter Tokenizer was used for tokenization. (See paper for more details)  The performances of both the CNN and CNN-SVM classifier, when applied to all datasets, are shown in the table below. We can observe that when the models (specifically CNN-SVM) combine sarcasm features, emotion features, sentiment features, and personality traits features, it outperforms all the other models with the exception of the baseline model (B). The table below shows comparison results of the the state-of-the-art model (method 1), other well-known sarcasm detection research (method 2), and the proposed model (method 3). The proposed model consistently outperforms all the other models. Generalizability capabilities of the models were tested and the main finding was that if the datasets differed in nature, this significantly impacted the results. (See visualization of the datasets rendered via PCA below). For instance, training was done on Dataset 1 and tested on Dataset 2; the F1-score of the model was 33.05%, significantly dropping in accuracy. Conclusion and Future Work  Overall, the authors found that sarcasm is very topic-dependent and highly contextual, therefore, sentiment and other contextual clues help to detect sarcasm from text. Pre-trained sentiment, emotion, and personality models are used to capture contextualized information from text. Hand-crafted features (e.g., n-grams), though somewhat useful for sarcasm detection, will produce very sparse feature vector representations. For those reasons, word embeddings are used as input features. References  Ref:  [url]", "pdf_url": "https://arxiv.org/pdf/1610.08815", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/detecting-sarcasm-with-deep-convolutional-neural-networks-4a0657f79e80.json"}
{"id": "82821959", "bin": "900_1000", "summary_sentences": ["Scaling Concurrent Log-Structured Data Stores – Golan-Gueta et al. 2015  Key-value stores based on log-structured merge trees are everywhere.", "The original design was intended to mitigate slow disk I/O.", "Once this is achieved, as we scale to more and more cores the authors find that in-memory contention now becomes the bottleneck (see yesterday’s piece on the Universal Scalability Law ).", "By replacing the in-memory component of the LSM-tree with a concurrent hash map data structure it is possible to get much better scalability.", "Disk access is a principal bottleneck in storage systems, and remains a bottleneck even with today’s SSDs.", "Since reads are often effectively masked by caching, significant emphasis is placed on improving write throughput and latency.", "It is therefore not surprising that log-structured merge solutions, which batch writes in memory and merge them with on-disk storage in the background, have become the de facto choice for today’s leading key-value stores…  (Side note: – will storage access still be the principal bottleneck when we have NVMM ?)", "Google’s LevelDB is the state-of-the-art implementation of a single machine LSM that serves as the backbone in many of such key-value stores.", "It applies coarse-grained synchronization that forces all puts to be executed sequentially, and a single threaded merge process.", "These two design choices significantly reduce the system throughput in multicore environment.", "This effect is mitigated by HyperLevelDB, the data storage engine that powers HyperDex.", "It improves on LevelDB in two key ways: (1) by using fine-grained locking to increase concurrency, and (2) by using a different merging strategy… Facebook’s key-value store RocksDB also builds on LevelDB.", "Links: LevelDB , HyperLevelDB , HyperDex , RocksDB .", "One way of dealing with the challenge of multiple cores is to further partition the data and run multiple Log Structured Merge Data Stores (LSM-DS) on the same machine.", "A fine-grained partitioning mechanism is recommended by Dean et al. in The Tail at Scale .", "Golan-Gueta et al. put forward two counter-arguments to this approach: (a) consistent snapshot scans do not span multiple partitions, instead requiring transactions across shards, and (b) you need a system-level mechanism for managing partitions, which can itself become a bottleneck.", "The following chart shows the performance of the authors’ concurrent-LSM (cLSM) implementation with one large partition, vs LevelDB and HyperLevelDB handling the same amount of overall data but divided into four partitions.", "cLSM achieves much better throughput as the number of concurrent threads increases, but…  A first glance at the RHS of the chart makes cLSM look impressive.", "Note a couple of things though: the performance at 16 threads is quite close for all systems (oh, and the hardware used for the test can support 16 hardware threads ;) ); and above 16 threads although cLSM performs much better than the others, its absolute gains in throughput are relatively modest.", "You can also just start to see a nice curve as predicted but the USL in the cLSM throughput numbers.", "More convincing is the data from the evaluation of workloads logged in production by a key-value store supporting ‘some of the major personalized content and advertising systems on the web.’ See figure 10 from the paper, reproduced below:  cLSM also manages to maintain lower latency numbers than the competition as throughput scales.", "How is it done?", "First off, the in-memory component of the LSM tree is replaced by a ‘thread-safe map data structure.’  We assume a thread-safe map data structure for the in- memory component, i.e., its operations can be executed by multiple threads concurrently.", "Numerous data structure implementations, provide this functionality in a non-blocking and atomic manner.", "This is then integrated into the normal LSM merge operation.", "Snapshot scans are implemented on top of the in-memory map on the assumption that the map provides iterators with weak consistency (if an element is included in the map for the duration of the scan, it will be included in the scan results).", "Finally, an atomic read-modify-write operation is introduced:  We now introduce a general read-modify-write operation, RMW(k,f), which atomically applies an arbitrary function f to the current value v associated with key k and stores f(v) in its place.", "Such operations are useful for many applications, ranging from simple vector clock update and validation to implementing full-scale transactions.", "The implementation assumes a linked-list or derivative implementation type for the map structure.", "On this basis it uses optimistic concurrency control, and the contents of the prev and succ nodes in the chain to detect conflicts.", "At its heart though, the improvement seems to boil down to the simple idea of ‘why don’t we use an existing efficient in-memory structure for the in-memory part of the LSM-tree’.", "That seems so simple and obvious (the benefit of hindsight perhaps?)", "that I’m left with the nagging feeling maybe I’m missing something here…?"], "summary_text": "Scaling Concurrent Log-Structured Data Stores – Golan-Gueta et al. 2015  Key-value stores based on log-structured merge trees are everywhere. The original design was intended to mitigate slow disk I/O. Once this is achieved, as we scale to more and more cores the authors find that in-memory contention now becomes the bottleneck (see yesterday’s piece on the Universal Scalability Law ). By replacing the in-memory component of the LSM-tree with a concurrent hash map data structure it is possible to get much better scalability. Disk access is a principal bottleneck in storage systems, and remains a bottleneck even with today’s SSDs. Since reads are often effectively masked by caching, significant emphasis is placed on improving write throughput and latency. It is therefore not surprising that log-structured merge solutions, which batch writes in memory and merge them with on-disk storage in the background, have become the de facto choice for today’s leading key-value stores…  (Side note: – will storage access still be the principal bottleneck when we have NVMM ?) Google’s LevelDB is the state-of-the-art implementation of a single machine LSM that serves as the backbone in many of such key-value stores. It applies coarse-grained synchronization that forces all puts to be executed sequentially, and a single threaded merge process. These two design choices significantly reduce the system throughput in multicore environment. This effect is mitigated by HyperLevelDB, the data storage engine that powers HyperDex. It improves on LevelDB in two key ways: (1) by using fine-grained locking to increase concurrency, and (2) by using a different merging strategy… Facebook’s key-value store RocksDB also builds on LevelDB. Links: LevelDB , HyperLevelDB , HyperDex , RocksDB . One way of dealing with the challenge of multiple cores is to further partition the data and run multiple Log Structured Merge Data Stores (LSM-DS) on the same machine. A fine-grained partitioning mechanism is recommended by Dean et al. in The Tail at Scale . Golan-Gueta et al. put forward two counter-arguments to this approach: (a) consistent snapshot scans do not span multiple partitions, instead requiring transactions across shards, and (b) you need a system-level mechanism for managing partitions, which can itself become a bottleneck. The following chart shows the performance of the authors’ concurrent-LSM (cLSM) implementation with one large partition, vs LevelDB and HyperLevelDB handling the same amount of overall data but divided into four partitions. cLSM achieves much better throughput as the number of concurrent threads increases, but…  A first glance at the RHS of the chart makes cLSM look impressive. Note a couple of things though: the performance at 16 threads is quite close for all systems (oh, and the hardware used for the test can support 16 hardware threads ;) ); and above 16 threads although cLSM performs much better than the others, its absolute gains in throughput are relatively modest. You can also just start to see a nice curve as predicted but the USL in the cLSM throughput numbers. More convincing is the data from the evaluation of workloads logged in production by a key-value store supporting ‘some of the major personalized content and advertising systems on the web.’ See figure 10 from the paper, reproduced below:  cLSM also manages to maintain lower latency numbers than the competition as throughput scales. How is it done? First off, the in-memory component of the LSM tree is replaced by a ‘thread-safe map data structure.’  We assume a thread-safe map data structure for the in- memory component, i.e., its operations can be executed by multiple threads concurrently. Numerous data structure implementations, provide this functionality in a non-blocking and atomic manner. This is then integrated into the normal LSM merge operation. Snapshot scans are implemented on top of the in-memory map on the assumption that the map provides iterators with weak consistency (if an element is included in the map for the duration of the scan, it will be included in the scan results). Finally, an atomic read-modify-write operation is introduced:  We now introduce a general read-modify-write operation, RMW(k,f), which atomically applies an arbitrary function f to the current value v associated with key k and stores f(v) in its place. Such operations are useful for many applications, ranging from simple vector clock update and validation to implementing full-scale transactions. The implementation assumes a linked-list or derivative implementation type for the map structure. On this basis it uses optimistic concurrency control, and the contents of the prev and succ nodes in the chain to detect conflicts. At its heart though, the improvement seems to boil down to the simple idea of ‘why don’t we use an existing efficient in-memory structure for the in-memory part of the LSM-tree’. That seems so simple and obvious (the benefit of hindsight perhaps?) that I’m left with the nagging feeling maybe I’m missing something here…?", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2741948.2741973?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/scaling-concurrent-log-structured-data-stores.json"}
