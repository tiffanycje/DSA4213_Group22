{"id": "93106503", "bin": "0_100", "summary_sentences": ["A Critical Paper Review by Alex Lamb:   [url]"], "summary_text": "A Critical Paper Review by Alex Lamb:   [url]", "pdf_url": "http://arxiv.org/pdf/1702.08396", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/zhaose17.json"}
{"id": "73705154", "bin": "0_100", "summary_sentences": ["This model called Med2Vec is inspired by Word2Vec.", "It is Word2Vec for time series patient visits with ICD codes.", "The model learns embeddings for medical codes as well as the demographics of patients.", "[url]"], "summary_text": "This model called Med2Vec is inspired by Word2Vec. It is Word2Vec for time series patient visits with ICD codes. The model learns embeddings for medical codes as well as the demographics of patients. [url]", "pdf_url": "http://arxiv.org/pdf/1602.05568v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/1602.05568.json"}
{"id": "72992946", "bin": "0_100", "summary_sentences": ["This paper presents a recurrent neural network architecture in which some of the recurrent weights dynamically change during the forward pass, using a hebbian-like rule.", "They correspond to the matrices $A(t)$ in the figure below:  !", "[Fast weights RNN figure]( [url]"], "summary_text": "This paper presents a recurrent neural network architecture in which some of the recurrent weights dynamically change during the forward pass, using a hebbian-like rule. They correspond to the matrices $A(t)$ in the figure below:  ! [Fast weights RNN figure]( [url]", "pdf_url": "http://arxiv.org/pdf/1610.06258v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/1610.06258.json"}
{"id": "1077676", "bin": "0_100", "summary_sentences": ["Academic Torrents is a BitTorrent service that aims to make it easy for academics to share data via BitTorrent.", "Specific use cases are during competitions where everyone needs access to data quickly.", "Also, when a dataset is not available anymore the data can be shared from simple desktop computers and become available globally."], "summary_text": "Academic Torrents is a BitTorrent service that aims to make it easy for academics to share data via BitTorrent. Specific use cases are during competitions where everyone needs access to data quickly. Also, when a dataset is not available anymore the data can be shared from simple desktop computers and become available globally.", "pdf_url": "https://dl.acm.org/doi/pdf/abs/10.1145/2616498.2616528.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/cohenl14.json"}
{"id": "53851426", "bin": "0_100", "summary_sentences": ["The paper looks at approaches to predicting individual survival time distributions (isd).", "The motivation is shown in the figure below.", "Between two patients the survival time varies greatly so we should be able to predict a distribution like the red curve.", "[url]"], "summary_text": "The paper looks at approaches to predicting individual survival time distributions (isd). The motivation is shown in the figure below. Between two patients the survival time varies greatly so we should be able to predict a distribution like the red curve. [url]", "pdf_url": "http://arxiv.org/pdf/1811.11347v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/1811.11347.json"}
{"id": "70176908", "bin": "0_100", "summary_sentences": ["Here is a video overview:  [url]"], "summary_text": "Here is a video overview:  [url]", "pdf_url": "http://arxiv.org/pdf/1702.00071v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/1702.00071.json"}
{"id": "4741660", "bin": "0_100", "summary_sentences": ["Basically they observe a pattern they call The Filter Lottery (TFL) where the random seed causes a high variance  in the training accuracy:  !", "[]( [url]"], "summary_text": "Basically they observe a pattern they call The Filter Lottery (TFL) where the random seed causes a high variance  in the training accuracy:  ! []( [url]", "pdf_url": "http://arxiv.org/pdf/1602.05931", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/cohenl016.json"}
{"id": "65847395", "bin": "0_100", "summary_sentences": ["This paper presents a novel neural network approach (though see [here]( [url]"], "summary_text": "This paper presents a novel neural network approach (though see [here]( [url]", "pdf_url": "http://arxiv.org/pdf/1605.08803v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/1605.08803.json"}
{"id": "95858167", "bin": "0_100", "summary_sentences": ["This idea is so badass!", "It uses Simple Tree Matching  [ref]  and extends it to work with HTML and then recursively searches an unseen document to align it with previously seen examples.", "An overview of the problem of *shift* can be seen on the left of the figure below and  the alignment is shown on the right.", "[url]"], "summary_text": "This idea is so badass! It uses Simple Tree Matching  [ref]  and extends it to work with HTML and then recursively searches an unseen document to align it with previously seen examples. An overview of the problem of *shift* can be seen on the left of the figure below and  the alignment is shown on the right. [url]", "pdf_url": "http://arxiv.org/pdf/1505.01303", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/cohendb15.json"}
{"id": "43813190", "bin": "0_100", "summary_sentences": ["The basic approach is an RNN applied to text to predict a medical event such as an ICD code.", "It is unclear if the complicated Bi-RNN model is required.", "This has some useful applications such as  - Adapt old databases - Correct errors - Upgrade ICD versions  A simple diagram of an RNN applied to medical next is shown below:    [url]"], "summary_text": "The basic approach is an RNN applied to text to predict a medical event such as an ICD code. It is unclear if the complicated Bi-RNN model is required. This has some useful applications such as  - Adapt old databases - Correct errors - Upgrade ICD versions  A simple diagram of an RNN applied to medical next is shown below:    [url]", "pdf_url": "http://aclweb.org/anthology/N/N16/N16-1056.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/jagannathay16.json"}
{"id": "63993374", "bin": "0_100", "summary_sentences": ["Summary by [brannondorsey]( [url]"], "summary_text": "Summary by [brannondorsey]( [url]", "pdf_url": "http://arxiv.org/pdf/1611.07004v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/1611.07004.json"}
{"id": "43514018", "bin": "0_100", "summary_sentences": ["Playing FPS games with deep reinforcement learning Lample et al. arXiv preprint, 2016  When I wrote up ‘ Asynchronous methods for deep learning ’ last month, I made a throwaway remark that after Go the next challenge for deep learning systems would be to win an esports competition against the best human teams.", "Can you imagine the theatre!", "Source: ‘League of Legends’ video game championship is like the World Cup, Super Bowl combined – Fortune:  [url]"], "summary_text": "Playing FPS games with deep reinforcement learning Lample et al. arXiv preprint, 2016  When I wrote up ‘ Asynchronous methods for deep learning ’ last month, I made a throwaway remark that after Go the next challenge for deep learning systems would be to win an esports competition against the best human teams. Can you imagine the theatre! Source: ‘League of Legends’ video game championship is like the World Cup, Super Bowl combined – Fortune:  [url]", "pdf_url": "https://arxiv.org/pdf/1609.05521.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/playing-fps-games-with-deep-reinforcement-learning.json"}
{"id": "27264933", "bin": "0_100", "summary_sentences": ["Adam is like RMSProp with momentum.", "The (simplified) update [[Stanford CS231n]]( [url]"], "summary_text": "Adam is like RMSProp with momentum. The (simplified) update [[Stanford CS231n]]( [url]", "pdf_url": "http://arxiv.org/pdf/1412.6980", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/kingmab14.json"}
{"id": "24431610", "bin": "0_100", "summary_sentences": ["One core aspect of this attention approach is that it provides the ability to debug the learned representation by visualizing the softmax output (later called $\\alpha_{ij}$) over the input words for each output word as shown below.", "[url]"], "summary_text": "One core aspect of this attention approach is that it provides the ability to debug the learned representation by visualizing the softmax output (later called $\\alpha_{ij}$) over the input words for each output word as shown below. [url]", "pdf_url": "http://arxiv.org/pdf/1409.0473", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/bahdanaucb14.json"}
{"id": "74977893", "bin": "0_100", "summary_sentences": ["This summary is as ridiculous as this network is long.", "A good implementation of the network is here:  [url]"], "summary_text": "This summary is as ridiculous as this network is long. A good implementation of the network is here:  [url]", "pdf_url": "http://arxiv.org/pdf/1512.03385", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/hezrs15.json"}
{"id": "74971632", "bin": "0_100", "summary_sentences": ["This paper introduces the GoogLeNet Inception Architecture The major part of this paper is the *Inception Module* which takes convolutions at multiple layers and provides a good receptive field as well as reducing the overall number of parameters.", "!", "[Inception Module]( [url]"], "summary_text": "This paper introduces the GoogLeNet Inception Architecture The major part of this paper is the *Inception Module* which takes convolutions at multiple layers and provides a good receptive field as well as reducing the overall number of parameters. ! [Inception Module]( [url]", "pdf_url": "http://arxiv.org/pdf/1409.4842", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/szegedyljsraevr14.json"}
{"id": "81034618", "bin": "0_100", "summary_sentences": ["Generalized data structure synthesis Loncaric et al., ICSE’18  Many systems have a few key data structures at their heart.", "Finding correct and efficient implementations for these data structures is not always easy.", "Today’s paper introduces Cozy (  [url]"], "summary_text": "Generalized data structure synthesis Loncaric et al., ICSE’18  Many systems have a few key data structures at their heart. Finding correct and efficient implementations for these data structures is not always easy. Today’s paper introduces Cozy (  [url]", "pdf_url": "https://homes.cs.washington.edu/~mernst/pubs/generalized-synthesis-icse2018.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/generalized-data-structure-synthesis.json"}
{"id": "13237217", "bin": "0_100", "summary_sentences": ["GraphIt: a high-performance graph DSL Zhang et al., OOPSLA’18  See also:  [url]"], "summary_text": "GraphIt: a high-performance graph DSL Zhang et al., OOPSLA’18  See also:  [url]", "pdf_url": "https://arxiv.org/pdf/1805.00923.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/graphit-a-high-performance-graph-dsl.json"}
{"id": "59963771", "bin": "0_100", "summary_sentences": ["This paper discusses an important bias in evaluation of methods using cross-validation.", "A method that makes decisions based of cross validation can appear to increase overall performance by simply dealing with the bias of cross-validation and not the real problem."], "summary_text": "This paper discusses an important bias in evaluation of methods using cross-validation. A method that makes decisions based of cross validation can appear to increase overall performance by simply dealing with the bias of cross-validation and not the real problem.", "pdf_url": "http://www.jmlr.org/papers/volume3/reunanen03a/reunanen03a.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/reunanen03.json"}
{"id": "79091650", "bin": "0_100", "summary_sentences": ["This is Adagrad.", "Adagrad is an adaptive learning rate method.", "Some sample code from  [[Stanford CS231n]]( [url]"], "summary_text": "This is Adagrad. Adagrad is an adaptive learning rate method. Some sample code from  [[Stanford CS231n]]( [url]", "pdf_url": "http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/duchihs10.json"}
{"id": "48091191", "bin": "1000_1100", "summary_sentences": ["What bugs cause production cloud incidents?", "Liu et al., HotOS’19  Last time out we looked at SLOs for cloud platforms , today we’re looking at what causes them to be broken!", "This is a study of every high severity production incident at Microsoft Azure services over a span of six months, where the root cause of that incident was a software bug.", "In total, there were 112 such incidents over the period March – September 2018 (not all of them affecting external customers).", "Software bugs are the most common cause of incidents during this period, accounting for around 40% of all incidents (so we can infer there were around 280 incidents total in the pool).", "The 112 incidents caused by software bugs are further broken down into categories, with data-format bugs, fault-related bugs, timing bugs, and constant_value bugs being the largest categories.", "Interestingly, outages caused by configuration errors represented only a small number of incidents in this study.", "This could be an artefact of that data set in some way, or it might be due to the tool chain that Microsoft uses:  The types of bugs we observed in production are biased by the fact that Microsoft uses effective tools to mostly eliminate many types oft bugs before they can manifest in production, and hence our study includes zero or few of such bugs.", "For example, we observed only a small number of configuration bugs caused by mis-specification of configuration entries in configuration files, even though such bugs were reported to be common in other settings.", "Most Azure code is written in .Net managed languages such as C#, reducing memory leak bugs.", "Tools like CHESS and PCT are used to expose shared-memory concurrency bugs.", "TLA+ is used to model concurrent and distributed system protocols helping to eliminate high level design and semantic bugs.", "In addition, Azure’s Fault Analysis Service supports various types of fault injections during testing, such as node restart, data migration,  and random faults.", "Microsoft is also using fuzz testing, cloud contract checking, and network configuration verification tools.", "Data formats  Of the software bugs that survive all of this and end up causing high severity incidents, one of.", "the most common causes are data format change (21%).", "Different components of cloud services interact with each other through various types of “data”, including inter-process/node messages, persistent files, and so on.", "At the same time, cloud software goes through frequent updates.", "As a result, different software components in the cloud could hold conflicting assumptions about the format of certain data, leading to service incidents.", "It looks like data validation isn’t only useful in a machine learning context !", "All but one of the data format bugs involved multiple processes or nodes.", "In 40% of cases different parties assume different formats for shared files or database tables.", "For example, an upgrade has been deployed to a component ‘owning’ the table which changes the schema.", "The other 60% of cases are caused by a service changing the interface of its external message APIs.", "For example, a service that used to return 200 together with an empty list when no results were found changes to returning a 404, and breaks existing clients.", "The large scale, frequent updates, and long running nature of cloud services likely have facilitated the occurrence of these bugs.", "I’d expect this class of bugs to also surface in microservices systems.", "Does that match your experience?", "Fault related  Next up is an old chestnut: error and exception handling faults (31%).", "Components that fail and report an error that can’t be handled (e.g., missing exception handlers) (43% of this category)  Unresponsive components that hang and are not picked up by fault-detection mechanisms, leading to user-visible timeouts (29% of this category)  Silent corruption of data with no error detection code in place, leading to incorrect results returned to users (17% of this category)  Exception / error handlers contribute to the incidents by either ignoring error reports (35%), over-reacting (35%), or containing bugs within the handlers themselves leading to infinite loops, timing issues, and so on (30%).", "Timing incidents  13% of the incidents are related to timing, with only 14% of the timing incidents actually recorded as deadlocks.", "Compared to classic timing bugs racing between threads in the same process, here many of these bugs are about race conditions between multiple nodes and many of them are racing on persistent data like cached firewall rules, configuration, database tables, and so on.", "Constant-value setting incidents  7% of all software bug incidents are caused by mistakes in constants: hard-code configuration, special purpose strings such as URLs, and enum-typed values.", "Bug resolution  Facing tight time pressure, more often than not, software bug incidents were resolved through a variety of mitigation techniques (56%) without patching the buggy code (44%), providing quick solutions to users and maximizing service availability.", "(Mitigated incidents may well have been followed up later with code changes, but these aren’t recorded in the respective incident reports).", "When mitigating there are three main types of mitigation uncovered:  Code mitigation involves rolling back to an earlier version of the software  Data mitigation involves manually restoring, cleaning up, or deleting data in a file, table, etc.", "Environment mitigation involves killing and restarting processes, migrating workloads, adding fail-over resources, etc..", "When mitigating, environment mitigations are the most common.", "Although the kind of mitigate employed does vary based on root cause:  Much recent work looked at how to automatically generate new patches.", "In comparison, automatically generating mitigation steps has not been well studied and is worth more attention in the future."], "summary_text": "What bugs cause production cloud incidents? Liu et al., HotOS’19  Last time out we looked at SLOs for cloud platforms , today we’re looking at what causes them to be broken! This is a study of every high severity production incident at Microsoft Azure services over a span of six months, where the root cause of that incident was a software bug. In total, there were 112 such incidents over the period March – September 2018 (not all of them affecting external customers). Software bugs are the most common cause of incidents during this period, accounting for around 40% of all incidents (so we can infer there were around 280 incidents total in the pool). The 112 incidents caused by software bugs are further broken down into categories, with data-format bugs, fault-related bugs, timing bugs, and constant_value bugs being the largest categories. Interestingly, outages caused by configuration errors represented only a small number of incidents in this study. This could be an artefact of that data set in some way, or it might be due to the tool chain that Microsoft uses:  The types of bugs we observed in production are biased by the fact that Microsoft uses effective tools to mostly eliminate many types oft bugs before they can manifest in production, and hence our study includes zero or few of such bugs. For example, we observed only a small number of configuration bugs caused by mis-specification of configuration entries in configuration files, even though such bugs were reported to be common in other settings. Most Azure code is written in .Net managed languages such as C#, reducing memory leak bugs. Tools like CHESS and PCT are used to expose shared-memory concurrency bugs. TLA+ is used to model concurrent and distributed system protocols helping to eliminate high level design and semantic bugs. In addition, Azure’s Fault Analysis Service supports various types of fault injections during testing, such as node restart, data migration,  and random faults. Microsoft is also using fuzz testing, cloud contract checking, and network configuration verification tools. Data formats  Of the software bugs that survive all of this and end up causing high severity incidents, one of. the most common causes are data format change (21%). Different components of cloud services interact with each other through various types of “data”, including inter-process/node messages, persistent files, and so on. At the same time, cloud software goes through frequent updates. As a result, different software components in the cloud could hold conflicting assumptions about the format of certain data, leading to service incidents. It looks like data validation isn’t only useful in a machine learning context ! All but one of the data format bugs involved multiple processes or nodes. In 40% of cases different parties assume different formats for shared files or database tables. For example, an upgrade has been deployed to a component ‘owning’ the table which changes the schema. The other 60% of cases are caused by a service changing the interface of its external message APIs. For example, a service that used to return 200 together with an empty list when no results were found changes to returning a 404, and breaks existing clients. The large scale, frequent updates, and long running nature of cloud services likely have facilitated the occurrence of these bugs. I’d expect this class of bugs to also surface in microservices systems. Does that match your experience? Fault related  Next up is an old chestnut: error and exception handling faults (31%). Components that fail and report an error that can’t be handled (e.g., missing exception handlers) (43% of this category)  Unresponsive components that hang and are not picked up by fault-detection mechanisms, leading to user-visible timeouts (29% of this category)  Silent corruption of data with no error detection code in place, leading to incorrect results returned to users (17% of this category)  Exception / error handlers contribute to the incidents by either ignoring error reports (35%), over-reacting (35%), or containing bugs within the handlers themselves leading to infinite loops, timing issues, and so on (30%). Timing incidents  13% of the incidents are related to timing, with only 14% of the timing incidents actually recorded as deadlocks. Compared to classic timing bugs racing between threads in the same process, here many of these bugs are about race conditions between multiple nodes and many of them are racing on persistent data like cached firewall rules, configuration, database tables, and so on. Constant-value setting incidents  7% of all software bug incidents are caused by mistakes in constants: hard-code configuration, special purpose strings such as URLs, and enum-typed values. Bug resolution  Facing tight time pressure, more often than not, software bug incidents were resolved through a variety of mitigation techniques (56%) without patching the buggy code (44%), providing quick solutions to users and maximizing service availability. (Mitigated incidents may well have been followed up later with code changes, but these aren’t recorded in the respective incident reports). When mitigating there are three main types of mitigation uncovered:  Code mitigation involves rolling back to an earlier version of the software  Data mitigation involves manually restoring, cleaning up, or deleting data in a file, table, etc. Environment mitigation involves killing and restarting processes, migrating workloads, adding fail-over resources, etc.. When mitigating, environment mitigations are the most common. Although the kind of mitigate employed does vary based on root cause:  Much recent work looked at how to automatically generate new patches. In comparison, automatically generating mitigation steps has not been well studied and is worth more attention in the future.", "pdf_url": "https://people.cs.uchicago.edu/~shanlu/paper/hotos19_azure.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/what-bugs-cause-cloud-production-incidents.json"}
{"id": "88700162", "bin": "1000_1100", "summary_sentences": ["The Semantic Elegance of Applicative Languages – Turner ’81.", "Here’s a paper you can enjoy simply for its prose!", "In what does the alleged superiority of applicative languages consist?", "In what indeed!", "And while we’re at it, what’s an applicative language?", "I looked up a few definitions; if we call it a functional language I don’t think we’ll go too far wrong.", "Let us resume…  In what does the alleged superiority of applicative languages consist?", "In the last analysis the answer must be in terms of the reduction in the time required to produce a correct program to solve a given problem.", "On reflection I decided that the best way to demonstrate this would be to take some reasonably non-trivial problem and show how, by proceeding within a certain kind of applicative language framework it was possible to develop a  working solution with a fraction of the effort that would have been necessary in a conventional imperative language.", "The cynical among you might be thinking: he’s picked a problem that’s well suited to be solved with a functional programming approach, and then shown that the functional approach is a good way of solving it!", "Even if that is true, it’s still a joy to get an insight into the mind of an advanced functional programmer.", "It reminds me of one of those chess books where a great player explains what they were thinking move by move in an annotated game.", "The problem is to enumerate all of the possible paraffin molecules.", "These are made of carbon C and hydrogen H atoms.", "Each C makes 4 bonds, and paraffins don’t allow double bonds or cycles.", "H                H   H     |                |   | H - C - H   ,    H - C - C - H    , ...     |                |   |     H                H   H  See the problem statement and example in the paper – it is succintly explained, and for maximum value you should pause and spend some time thinking how you would solve it before reading on.", "The tricky parts come when the ‘C’s themselves start to make complex shapes (T’s etc.", "), and you need to weed out duplicates through symmetries.", "from the point of view  of a programmer who had not tried it before, the problem seemed difficult enough to be interesting.", "At least several competent programmers I know reported to me that they had found it so.", "Central to the solution is figuring out when two molecules are equivalent.", "Turner defines operations ‘invert’, ‘rotate’ and ‘swap’ which make structure-preserving changes to the shape of a molecule.", "There then follows a very elegant solution based on determining a ‘closure under laws.’  -- a and b are equivalent if b is a member of the set of  -- all equivalent representations of a...   equiv a b = member (equivclass a) b  equivclass a = closure_under_laws [rotate, invert, swap] [a]  The key idea is embodied in the function “closure under laws” which takes a set of functions and a set of objects and finds the closure of the latter under repeated applications of the members of the former.", "With another neat (and commonly used) functional technique of generating an infinite list of molecules from which we can display as many as we like, the initial version of the program is complete.", "This solution, however, runs with appalling slowness (I tried it) mainly because of easily removable inefficiencies in our definition of “para”.", "There is a minor problem and a major problem…  The major problem is due to repeated calculation of the same sub-molecules many times over.", "Failure to memoise leads to an exponential deterioration in performance!", "For a recursive function, like “para”, memoisation leads to an exponential improvement in performance.", "(or to put it another way, failure to memoise leads an exponential deterioration in performance!)", "There follows some discussion of the author’s lessons learned during this exercise:  the effort needed to derive a solution can be reduced to a small fraction of that required in a traditional programming language  an applicative language (even if it’s not the language used for the ultimate implementation) can be an extremely valuable tool for the development and testing of algorithms  the language supports very nicely a separation of concerns in which you first make things correct without worrying about efficiency, and then repair efficiency by applying transformations known to preserve correctness:  In a surprising large number of cases it turns out that a small number of standard optimisations are sufficient to bring about the necessary improvement in performance.", "Two in particular seem to be of such general applicability as to deserve special mention in a next (and final) section of this paper…  And what are these two methods?", "Memoisation (discussed earlier), and filter promotion.", "Filter promotion is the idea that instead of generating a list containing terms we don’t want, and then filtering them out (e.g. filter pred xs), push the filter predicate down into the list generator so that we don’t generate the unwanted terms in the first place.", "We can get a considerable improvement in performance, however, by pushing the filter inside the generator “partitions” so that the unwanted lists are not created in the first place  It’s a short read, and hard to communicate the value in a summary.", "I hope I have whetted your appetite to go and read the whole paper.", "If you want to play around with the code, I ported the solution to Haskell here .", "Though I bet you can improve on my code given I’m not especially fluent in Haskell…"], "summary_text": "The Semantic Elegance of Applicative Languages – Turner ’81. Here’s a paper you can enjoy simply for its prose! In what does the alleged superiority of applicative languages consist? In what indeed! And while we’re at it, what’s an applicative language? I looked up a few definitions; if we call it a functional language I don’t think we’ll go too far wrong. Let us resume…  In what does the alleged superiority of applicative languages consist? In the last analysis the answer must be in terms of the reduction in the time required to produce a correct program to solve a given problem. On reflection I decided that the best way to demonstrate this would be to take some reasonably non-trivial problem and show how, by proceeding within a certain kind of applicative language framework it was possible to develop a  working solution with a fraction of the effort that would have been necessary in a conventional imperative language. The cynical among you might be thinking: he’s picked a problem that’s well suited to be solved with a functional programming approach, and then shown that the functional approach is a good way of solving it! Even if that is true, it’s still a joy to get an insight into the mind of an advanced functional programmer. It reminds me of one of those chess books where a great player explains what they were thinking move by move in an annotated game. The problem is to enumerate all of the possible paraffin molecules. These are made of carbon C and hydrogen H atoms. Each C makes 4 bonds, and paraffins don’t allow double bonds or cycles. H                H   H     |                |   | H - C - H   ,    H - C - C - H    , ...     |                |   |     H                H   H  See the problem statement and example in the paper – it is succintly explained, and for maximum value you should pause and spend some time thinking how you would solve it before reading on. The tricky parts come when the ‘C’s themselves start to make complex shapes (T’s etc. ), and you need to weed out duplicates through symmetries. from the point of view  of a programmer who had not tried it before, the problem seemed difficult enough to be interesting. At least several competent programmers I know reported to me that they had found it so. Central to the solution is figuring out when two molecules are equivalent. Turner defines operations ‘invert’, ‘rotate’ and ‘swap’ which make structure-preserving changes to the shape of a molecule. There then follows a very elegant solution based on determining a ‘closure under laws.’  -- a and b are equivalent if b is a member of the set of  -- all equivalent representations of a...   equiv a b = member (equivclass a) b  equivclass a = closure_under_laws [rotate, invert, swap] [a]  The key idea is embodied in the function “closure under laws” which takes a set of functions and a set of objects and finds the closure of the latter under repeated applications of the members of the former. With another neat (and commonly used) functional technique of generating an infinite list of molecules from which we can display as many as we like, the initial version of the program is complete. This solution, however, runs with appalling slowness (I tried it) mainly because of easily removable inefficiencies in our definition of “para”. There is a minor problem and a major problem…  The major problem is due to repeated calculation of the same sub-molecules many times over. Failure to memoise leads to an exponential deterioration in performance! For a recursive function, like “para”, memoisation leads to an exponential improvement in performance. (or to put it another way, failure to memoise leads an exponential deterioration in performance!) There follows some discussion of the author’s lessons learned during this exercise:  the effort needed to derive a solution can be reduced to a small fraction of that required in a traditional programming language  an applicative language (even if it’s not the language used for the ultimate implementation) can be an extremely valuable tool for the development and testing of algorithms  the language supports very nicely a separation of concerns in which you first make things correct without worrying about efficiency, and then repair efficiency by applying transformations known to preserve correctness:  In a surprising large number of cases it turns out that a small number of standard optimisations are sufficient to bring about the necessary improvement in performance. Two in particular seem to be of such general applicability as to deserve special mention in a next (and final) section of this paper…  And what are these two methods? Memoisation (discussed earlier), and filter promotion. Filter promotion is the idea that instead of generating a list containing terms we don’t want, and then filtering them out (e.g. filter pred xs), push the filter predicate down into the list generator so that we don’t generate the unwanted terms in the first place. We can get a considerable improvement in performance, however, by pushing the filter inside the generator “partitions” so that the unwanted lists are not created in the first place  It’s a short read, and hard to communicate the value in a summary. I hope I have whetted your appetite to go and read the whole paper. If you want to play around with the code, I ported the solution to Haskell here . Though I bet you can improve on my code given I’m not especially fluent in Haskell…", "pdf_url": "http://nsl.com/misc/sasl/paraffins-turner.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/the-semantic-elegance-of-applicative-languages.json"}
{"id": "58308538", "bin": "1000_1100", "summary_sentences": ["This article summarizes a novel technique for a very complex task in NLP known as noun compound classification.", "Paper Title  Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds using Paraphrases in a Neural Model — Vered Shwartz and Chris Waterson  Basic Overview  This paper addresses an important NLP task — the automatic interpretation of the relation between the constituents of a noun compound.", "Motivation  Consider the following noun compound examples: olive oil and baby oil.", "You can observe that the word “olive” in the phrase “olive oil” describes a SOURCE relation, and the word “baby” in “baby oil” describes a PURPOSE relation.", "In other words, babies should never be put in the same context as olives in terms of what they represent in the real world.", "This distinction is important because it can be used for various applications that require complex text understanding capabilities.", "Example  Imagine you asked Google search what olive oil is made up of.", "If Google search is smart it should respond “olive”.", "Now imagine you asked Google what baby oil is made up of.", "Definitely not babies!", "The answer should be other ingredients of oil or the main ingredient of oil.", "This is a very important distinction!", "It’s a challenging task because the meaning of both types of oils is not easy to interpret given the meaning of its constituent words.", "Can you think of more examples?", "Try and you will see why this area of NLP research is important.", "As an NLP researcher, I can even see how this is useful for disambiguating between sentiment phrases.", "(More on this in another article.)", "Literature Review  Two very common approaches are used to address this issue: paraphrases and noun-compound representations.", "The first approach maps relationships between constituents and the latter approach makes use of distributional representations of the individual constituents.", "(Don’t panic, I will explain what they mean in a little bit.)", "A more recent work (Dima, 2016) showed that constituent embeddings are effective to represent the noun-compounds (hereinafter also referred to as NCs.", "The main reason for why it works is attributed to a phenomenon referred to as lexical memorization .", "Contribution  This paper proposes a neural paraphrasing approach that combines path embeddings (which represents the relationships between NCs) and distributional information (obtained directly from word embeddings) to conduct the NC classification task.", "The authors also experiment with settings where lexical memorization is avoided to show that their method is more robust and their results are not attributed to this phenomenon.", "Model  The authors used HypeNET to learn patterns connecting the joint occurrences of instances of constituents in a corpus.", "These are also referred to as path embeddings.", "Three models were combined to perform the NCs relation classification: path-based, integrated, and integrated-NC.", "Each model incrementally adds new features (in this case different distributional inputs) which essentially adds more contextualized information to the overall input vector.", "This process can be seen more clearly in the diagram below:  The path embeddings (colored purple in the diagram) are learned using a vanilla LSTM with input vectors representing a concatenation of the following vectors: lemma, part-of-speech tag, dependency label, and direction vectors.", "(See paper for more details).", "NC labels (relations) are obtained using a distant supervision approach via the output of the LSTM.", "Evaluation  Two datasets, obtained from Tratz (2011) , were used to evaluate the proposed neural paraphrasing model.", "Several comparison models were proposed, including several baseline models and re-trained models adopted from previous work and the state-of-the-art method.", "(See paper for more details on the experimental setup).", "Table 1 shows that for most cases the integrated models (Int and Int-NC) outperform all other models on the using different data splitting strategies (shown in Split column).", "There is very little difference between the results obtained from the Int model and Int-NC model, indicating that the NC embeddings did not contribute much to the classification task.", "Analysis  Further analysis was conducted on the random splitting strategy to analyze variations in results of the different models.", "In Table 3, you can observe some of the relations that yielded reasonable performance (e.g., MEASURE and PERSONAL TITLE)  The authors also discovered that complex relations perform poorly such as LEXICALIZED for the NC, “soap opera” and OBJECTIVE for NC, “recovery plan”.", "(See more interesting examples in the paper).", "Table 4 below provides examples of NC embeddings obtained from the test set (left) and an example of the most similar NC in the embeddings (right).", "The authors observed that only 27.61% of the NCs were mostly similar to NCs with the same label.", "They attributed this behavior to inconsistent annotations rather than quality of embeddings.", "My Further Ideas and Conclusion  Visualize NC vector embeddings to observe relation clusters and patterns that may have similar properties.", "Study more closely the lexical memorization phenomena, which the paper helps to “slightly” address using the path-based component of the whole model.", "Overall, the performance was improved but there is still a lot more room for improvement in terms of data quality and modeling.", "The NC embeddings are not helping the current model, so it provides a feasible future research direction without changing the overall structure of the model.", "Resources  The original paper | Code Repository (Tensorflow implementation)   [url]"], "summary_text": "This article summarizes a novel technique for a very complex task in NLP known as noun compound classification. Paper Title  Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds using Paraphrases in a Neural Model — Vered Shwartz and Chris Waterson  Basic Overview  This paper addresses an important NLP task — the automatic interpretation of the relation between the constituents of a noun compound. Motivation  Consider the following noun compound examples: olive oil and baby oil. You can observe that the word “olive” in the phrase “olive oil” describes a SOURCE relation, and the word “baby” in “baby oil” describes a PURPOSE relation. In other words, babies should never be put in the same context as olives in terms of what they represent in the real world. This distinction is important because it can be used for various applications that require complex text understanding capabilities. Example  Imagine you asked Google search what olive oil is made up of. If Google search is smart it should respond “olive”. Now imagine you asked Google what baby oil is made up of. Definitely not babies! The answer should be other ingredients of oil or the main ingredient of oil. This is a very important distinction! It’s a challenging task because the meaning of both types of oils is not easy to interpret given the meaning of its constituent words. Can you think of more examples? Try and you will see why this area of NLP research is important. As an NLP researcher, I can even see how this is useful for disambiguating between sentiment phrases. (More on this in another article.) Literature Review  Two very common approaches are used to address this issue: paraphrases and noun-compound representations. The first approach maps relationships between constituents and the latter approach makes use of distributional representations of the individual constituents. (Don’t panic, I will explain what they mean in a little bit.) A more recent work (Dima, 2016) showed that constituent embeddings are effective to represent the noun-compounds (hereinafter also referred to as NCs. The main reason for why it works is attributed to a phenomenon referred to as lexical memorization . Contribution  This paper proposes a neural paraphrasing approach that combines path embeddings (which represents the relationships between NCs) and distributional information (obtained directly from word embeddings) to conduct the NC classification task. The authors also experiment with settings where lexical memorization is avoided to show that their method is more robust and their results are not attributed to this phenomenon. Model  The authors used HypeNET to learn patterns connecting the joint occurrences of instances of constituents in a corpus. These are also referred to as path embeddings. Three models were combined to perform the NCs relation classification: path-based, integrated, and integrated-NC. Each model incrementally adds new features (in this case different distributional inputs) which essentially adds more contextualized information to the overall input vector. This process can be seen more clearly in the diagram below:  The path embeddings (colored purple in the diagram) are learned using a vanilla LSTM with input vectors representing a concatenation of the following vectors: lemma, part-of-speech tag, dependency label, and direction vectors. (See paper for more details). NC labels (relations) are obtained using a distant supervision approach via the output of the LSTM. Evaluation  Two datasets, obtained from Tratz (2011) , were used to evaluate the proposed neural paraphrasing model. Several comparison models were proposed, including several baseline models and re-trained models adopted from previous work and the state-of-the-art method. (See paper for more details on the experimental setup). Table 1 shows that for most cases the integrated models (Int and Int-NC) outperform all other models on the using different data splitting strategies (shown in Split column). There is very little difference between the results obtained from the Int model and Int-NC model, indicating that the NC embeddings did not contribute much to the classification task. Analysis  Further analysis was conducted on the random splitting strategy to analyze variations in results of the different models. In Table 3, you can observe some of the relations that yielded reasonable performance (e.g., MEASURE and PERSONAL TITLE)  The authors also discovered that complex relations perform poorly such as LEXICALIZED for the NC, “soap opera” and OBJECTIVE for NC, “recovery plan”. (See more interesting examples in the paper). Table 4 below provides examples of NC embeddings obtained from the test set (left) and an example of the most similar NC in the embeddings (right). The authors observed that only 27.61% of the NCs were mostly similar to NCs with the same label. They attributed this behavior to inconsistent annotations rather than quality of embeddings. My Further Ideas and Conclusion  Visualize NC vector embeddings to observe relation clusters and patterns that may have similar properties. Study more closely the lexical memorization phenomena, which the paper helps to “slightly” address using the path-based component of the whole model. Overall, the performance was improved but there is still a lot more room for improvement in terms of data quality and modeling. The NC embeddings are not helping the current model, so it provides a feasible future research direction without changing the overall structure of the model. Resources  The original paper | Code Repository (Tensorflow implementation)   [url]", "pdf_url": "https://arxiv.org/pdf/1803.08073", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/olive-oil-is-made-of-olives-baby-oil-is-made-for-babies-paper-summary-a6f9b5544761.json"}
{"id": "92908954", "bin": "1000_1100", "summary_sentences": ["A Language-based Approach to Unifying Events and Threads – Li and Zdancewic, 2006  So far in this mini-series we’ve seen that thread and event-based models are duals , that threads are a bad idea – you should really be using events, and that events are a bad idea – you should really be using threads.", "What if we could combine the two models and bring together the deadlocks, data races and corruption of threads with the hard to understand control flow of events?", ";) Or, if we applied the techniques wisely, the easy to follow control flow of the thread-based approach, with the natural way that events model some domains.", "I think the latter is more what the authors had in mind!", "The goal is to design a software library that provides two different abstractions for application-level threads: the thread view, which allows per-thread code be written in the natural, imperative, multi-threaded style, and the event view, which allows the threads be passively manipulated by the underlying scheduler in an abstract, type-safe way.", "…  This dualized model gives the best of two worlds: the expressiveness of threads and the customizability of events.", "After briefly comparing event and thread-based models, the authors go on to present a unified model implemented in  Haskell: “Our experience shows that Haskell is a reasonable language for building scalable systems software; it is expressive, succinct, efficient and type safe; it also interacts well with C libraries and APIs.”  The primary advantage of the thread model is that the programmer can reason about the series of actions taken by a thread in the familiar way, just as for a sequential program….", "Event-driven programming, in contrast, is hard.", "The control flow graph has to be decomposed to multiple event handlers and represented as some form of state machines with explicit message passing or in continuation-passing style (CPS).", "Event-driven programs however are ‘usually more flexible and customizable,’ and are a good match for interrupt-driven systems.", "Note that the description of ‘event handlers, state machines, and explicit message passing’ is very reminiscent of the actor model.", "The CPS style evokes callbacks and promises.", "So how do Li and Zdancewic bring the two together?", "Monads provide the thread abstraction by defining an imperative sub-language of Haskell with system calls and thread control primitives  Higher-order functions provide the internal representation of threads in continuation-passing style  Lazy data structures provide the event abstraction, which is a lazy tree that represents the trace of system calls generated by threads.", "The goal is to design a monad that provides a thread abstraction, so the programmer can write multi-threaded code using the overloaded “do”-syntax with a set of system calls.", "The implementation of this monad is tricky, but the details are hidden from the programmer.", "For example:  sock_accept server_fd do {   new_fd <- sys_nbio (accept server_fd);   if new_fd > 0      then return new_fd      else do {         sys_epoll_wait fd EPOLL_READ;         sock_accept server_fd;      } }  (If you’re not familiar with reading Haskell code, the semi-colon separated statements in the do blocks provide elegant syntactic sugar for sequential composition of functions).", "By designing thread control primitives as monadic combinators, the monad interface can be used as an abstraction for multi-threaded programming, because it provides a way of hiding the ‘internal plumbing’ needed to write programs in CPS style.", "Here’s an example of a simple recursive event loop:  worker_epoll sched =   do {         results <- epoll_wait;         mapM (writeChan (ready_queue sched)) results;         worker_epoll sched;   }  (case statements can be used to select among multiple possible events).", "This event-driven architecture is similar to that in SEDA, but our events are finer-grained: instead of requiring the programmer manually decompose a computation into stages and specify what stages can be performed in parallel, the event-driven scheduler automatically decomposes a threaded computation into fine-grained segments separated by system calls.", "Haskell’s type system ensures that each segment is a purely functional computation without I/O, so such segments can be safely executed in parallel.", "A web server is implemented to assess the programming model and performance of the solution.", "Not only is the multi-threaded programming style natural and elegant, but the event-driven architecture also makes the scheduler clean.", "The scheduler, including the CPS monad, system call implementations, event loops and queues for AIO, epoll, mutexes, file-opening and exception handling (but not counting the wrapper interfaces for C library functions), is only 220 lines of well-structured code.", "Performance-wise the authors report that ‘in our experience Haskell programs, while slower that C programs, are not orders of magnitude slower.’ In their tests, the implementation performed favourably when compared to the Linux Native-POSIX Thread Library (NPTL) and against Apache for disk-bound workloads.", "Introducing Haskell STM transactions made the solution slower than C, but this disadvantage becomes less significant as more processors are introduced.", "In exchange for performance, Haskell delivers many features that simplify program development, including a very expressive static type system, type inference, lightweight closures, garbage collection, and convenient syntax overloading.", "We heavily use these features in our thread library implementation; it might be possible to implement the unified concurrency model in a general-purpose language lacking some of these features, but the results would likely be cumbersome to use.", "Nevertheless, it is worth investigating how to apply our approach in more mainstream languages like Java.", "Tomorrow we’ll look at a paper that reports on experiences trying to do exactly that.", "The bottom line:  Events and threads should be combined into an unified programming model in general-purpose programming languages.", "With proper language support, application-level threads can be made extremely lightweight and easy to use.", "Our experiments demonstrate that this approach is practical and our programming experience suggests that this is a very appealing way of writing scalable, massively concurrent software."], "summary_text": "A Language-based Approach to Unifying Events and Threads – Li and Zdancewic, 2006  So far in this mini-series we’ve seen that thread and event-based models are duals , that threads are a bad idea – you should really be using events, and that events are a bad idea – you should really be using threads. What if we could combine the two models and bring together the deadlocks, data races and corruption of threads with the hard to understand control flow of events? ;) Or, if we applied the techniques wisely, the easy to follow control flow of the thread-based approach, with the natural way that events model some domains. I think the latter is more what the authors had in mind! The goal is to design a software library that provides two different abstractions for application-level threads: the thread view, which allows per-thread code be written in the natural, imperative, multi-threaded style, and the event view, which allows the threads be passively manipulated by the underlying scheduler in an abstract, type-safe way. …  This dualized model gives the best of two worlds: the expressiveness of threads and the customizability of events. After briefly comparing event and thread-based models, the authors go on to present a unified model implemented in  Haskell: “Our experience shows that Haskell is a reasonable language for building scalable systems software; it is expressive, succinct, efficient and type safe; it also interacts well with C libraries and APIs.”  The primary advantage of the thread model is that the programmer can reason about the series of actions taken by a thread in the familiar way, just as for a sequential program…. Event-driven programming, in contrast, is hard. The control flow graph has to be decomposed to multiple event handlers and represented as some form of state machines with explicit message passing or in continuation-passing style (CPS). Event-driven programs however are ‘usually more flexible and customizable,’ and are a good match for interrupt-driven systems. Note that the description of ‘event handlers, state machines, and explicit message passing’ is very reminiscent of the actor model. The CPS style evokes callbacks and promises. So how do Li and Zdancewic bring the two together? Monads provide the thread abstraction by defining an imperative sub-language of Haskell with system calls and thread control primitives  Higher-order functions provide the internal representation of threads in continuation-passing style  Lazy data structures provide the event abstraction, which is a lazy tree that represents the trace of system calls generated by threads. The goal is to design a monad that provides a thread abstraction, so the programmer can write multi-threaded code using the overloaded “do”-syntax with a set of system calls. The implementation of this monad is tricky, but the details are hidden from the programmer. For example:  sock_accept server_fd do {   new_fd <- sys_nbio (accept server_fd);   if new_fd > 0      then return new_fd      else do {         sys_epoll_wait fd EPOLL_READ;         sock_accept server_fd;      } }  (If you’re not familiar with reading Haskell code, the semi-colon separated statements in the do blocks provide elegant syntactic sugar for sequential composition of functions). By designing thread control primitives as monadic combinators, the monad interface can be used as an abstraction for multi-threaded programming, because it provides a way of hiding the ‘internal plumbing’ needed to write programs in CPS style. Here’s an example of a simple recursive event loop:  worker_epoll sched =   do {         results <- epoll_wait;         mapM (writeChan (ready_queue sched)) results;         worker_epoll sched;   }  (case statements can be used to select among multiple possible events). This event-driven architecture is similar to that in SEDA, but our events are finer-grained: instead of requiring the programmer manually decompose a computation into stages and specify what stages can be performed in parallel, the event-driven scheduler automatically decomposes a threaded computation into fine-grained segments separated by system calls. Haskell’s type system ensures that each segment is a purely functional computation without I/O, so such segments can be safely executed in parallel. A web server is implemented to assess the programming model and performance of the solution. Not only is the multi-threaded programming style natural and elegant, but the event-driven architecture also makes the scheduler clean. The scheduler, including the CPS monad, system call implementations, event loops and queues for AIO, epoll, mutexes, file-opening and exception handling (but not counting the wrapper interfaces for C library functions), is only 220 lines of well-structured code. Performance-wise the authors report that ‘in our experience Haskell programs, while slower that C programs, are not orders of magnitude slower.’ In their tests, the implementation performed favourably when compared to the Linux Native-POSIX Thread Library (NPTL) and against Apache for disk-bound workloads. Introducing Haskell STM transactions made the solution slower than C, but this disadvantage becomes less significant as more processors are introduced. In exchange for performance, Haskell delivers many features that simplify program development, including a very expressive static type system, type inference, lightweight closures, garbage collection, and convenient syntax overloading. We heavily use these features in our thread library implementation; it might be possible to implement the unified concurrency model in a general-purpose language lacking some of these features, but the results would likely be cumbersome to use. Nevertheless, it is worth investigating how to apply our approach in more mainstream languages like Java. Tomorrow we’ll look at a paper that reports on experiences trying to do exactly that. The bottom line:  Events and threads should be combined into an unified programming model in general-purpose programming languages. With proper language support, application-level threads can be made extremely lightweight and easy to use. Our experiments demonstrate that this approach is practical and our programming experience suggests that this is a very appealing way of writing scalable, massively concurrent software.", "pdf_url": "http://thelackthereof.org/docs/library/LZ06b.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/a-language-based-approach-to-unifying-events-and-threads.json"}
{"id": "33126132", "bin": "1000_1100", "summary_sentences": ["Learning the structure of generative models without labeled data Bach et al., ICML’17  For the last couple of posts we’ve been looking at Snorkel and BabbleLabble which both depend on data programming – the ability to intelligently combine the outputs of a set of labelling functions.", "The core of data programming is developed in two papers, ‘ Data programming: creating large training sets, quickly ’ (Ratner 2016) and today’s paper choice, ‘ Learning the structure of generative models without labeled data ’ (Bach 2017).", "The original data programming paper works explicitly with input pairs (x,y) (e.g. the chemical and disease word pairs we saw from the disease task in Snorkel) which (for me at least) confuses the presentation a little compared to the latter ICML paper which just assumes inputs  (which could of course have pair structure, but we don’t care about that at this level of detail).", "Also in the original paper dependencies between labelling functions are explicitly specified by end users (as one of four types: similar, fixing, reinforcing, and exclusive) and built into a factor graph.", "In the ICML paper dependencies are learned.", "So I’m going to work mostly from ‘Learning the structure of generative models without labeled data’ in this post, and concentrate just on the core development of the theory.", "A generative probabilistic model for accuracy estimation under independence  We have input data points  , and for each  a latent variable  that is its true label.", "We don’t know  , but we do have  labelling functions  .", "Each labelling functions takes as input an data point  and produces as output a value  which is either true, abstain, or false (  ).", "(The approach generalises to the multi-class case).", "We start out by assuming that the outputs of the labelling functions are conditionally independent given the true label.", "Consider a labelling function  , which gives output  on input  (with true label  ).", "We can model the accuracy of this labelling function with  .", "We’ll say the accuracy function has value 1 if  correctly labels an example, and 0 otherwise.", "In our setting this can be succinctly expressed as:  The accuracy function itself is controlled by some parameter  modelling how accurate each labelling function is.", "Now for  input examples and  labelling functions, we have a conditionally independent model  where  is the set of true labels  .", "The unknown parameters  modelling the accuracy of the labelling functions can now be estimated by minimising the negative log marginal likelihood of failing to give the correct label (guessing a wrong label, or abstaining).", "We can do this using standard stochastic gradient descent, where the gradient for parameter  is given by the number of examples  correctly labels minus the number of examples it fails to label correctly.", "Structure learning  The conditionally independent model is a common assumption, and using a more sophisticated generative model currently requires users to specify its structure.", "As we know from the Snorkel paper, unfortunately labelling functions are often not independent.", "To address this issue, we generalize the conditionally independent model as a factor graph with additional dependencies, including higher-order factors that connect multiple labeling function outputs for each data point  and label  .", "Suppose there is a set  of dependency types of interest, where a dependency type represents a form of dependency between labelling functions, such as correlation.", "Unknown to us, there is also a set  of index tuples which indicate the set of labelling functions that participate in each dependency of type  .", "This gives us a general model of the form  and we want to learn the parameters  .", "Estimating the structure of the distribution  is challenging because Y is latent; we never observe its value, even during training.", "We must therefore work with the marginal likelihood  .", "Learning the parameters of the generative model requires Gibbs sampling to estimate gradients.", "As the number of possible dependencies increases at least quadratically in the number of labeling functions, this heavyweight approach to learning does not scale.", "The way out is to consider each labelling function in turn, optimising the log marginal pseudolikelihood of its outputs conditioned on the outputs of all the other labelling functions.", "where  is a hyperparameter.", "By conditioning on all other labeling functions in each term…, we ensue that the gradient can be computed in polynomial time with respect to the number of labeling functions, data points, and possible dependencies; without requiring any sampling or variational approximations.", "It all comes together in the following algorithm:  The hyperparameter  controls the threshold and regularization strength and requires tuning.", "For the other parameters the authors fix these at step size =  , epoch count = 10, and truncation frequency = 10.", "Theoretical guarantees  Assuming that there exist some set of parameters which can capture the true dependencies within the model we are using, and that all non-zero parameters are bounded away from zero (have at least a minimum magnitude  ), then we can uncover the exact dependency structure with probability at least  given  inputs, where  Where  is the maximum number of possible dependencies a single labelling function (or true label) can be involved in, and  is a measure of how much better our dependency estimates are with each labeling function than without (my loose interpretation of  – see eqn 7 in section 4 of the paper for the official definition).", "If we further assume that the only dependency types are accuracy and correlation dependencies then we need an input dataset of size  Practical results  Our analysis… guarantees that the sample complexity grows at worst on the order  for  labeling functions.", "In practice, we find that structure learning performs better than this guaranteed rate, depending linearly on the number of true correlations and logarithmically on the number of possible correlations.", "Compared to estimating structures via parameter learning over all possible dependencies, algorithm one is 100x faster and selects only 1/4 of the extraneous correlations."], "summary_text": "Learning the structure of generative models without labeled data Bach et al., ICML’17  For the last couple of posts we’ve been looking at Snorkel and BabbleLabble which both depend on data programming – the ability to intelligently combine the outputs of a set of labelling functions. The core of data programming is developed in two papers, ‘ Data programming: creating large training sets, quickly ’ (Ratner 2016) and today’s paper choice, ‘ Learning the structure of generative models without labeled data ’ (Bach 2017). The original data programming paper works explicitly with input pairs (x,y) (e.g. the chemical and disease word pairs we saw from the disease task in Snorkel) which (for me at least) confuses the presentation a little compared to the latter ICML paper which just assumes inputs  (which could of course have pair structure, but we don’t care about that at this level of detail). Also in the original paper dependencies between labelling functions are explicitly specified by end users (as one of four types: similar, fixing, reinforcing, and exclusive) and built into a factor graph. In the ICML paper dependencies are learned. So I’m going to work mostly from ‘Learning the structure of generative models without labeled data’ in this post, and concentrate just on the core development of the theory. A generative probabilistic model for accuracy estimation under independence  We have input data points  , and for each  a latent variable  that is its true label. We don’t know  , but we do have  labelling functions  . Each labelling functions takes as input an data point  and produces as output a value  which is either true, abstain, or false (  ). (The approach generalises to the multi-class case). We start out by assuming that the outputs of the labelling functions are conditionally independent given the true label. Consider a labelling function  , which gives output  on input  (with true label  ). We can model the accuracy of this labelling function with  . We’ll say the accuracy function has value 1 if  correctly labels an example, and 0 otherwise. In our setting this can be succinctly expressed as:  The accuracy function itself is controlled by some parameter  modelling how accurate each labelling function is. Now for  input examples and  labelling functions, we have a conditionally independent model  where  is the set of true labels  . The unknown parameters  modelling the accuracy of the labelling functions can now be estimated by minimising the negative log marginal likelihood of failing to give the correct label (guessing a wrong label, or abstaining). We can do this using standard stochastic gradient descent, where the gradient for parameter  is given by the number of examples  correctly labels minus the number of examples it fails to label correctly. Structure learning  The conditionally independent model is a common assumption, and using a more sophisticated generative model currently requires users to specify its structure. As we know from the Snorkel paper, unfortunately labelling functions are often not independent. To address this issue, we generalize the conditionally independent model as a factor graph with additional dependencies, including higher-order factors that connect multiple labeling function outputs for each data point  and label  . Suppose there is a set  of dependency types of interest, where a dependency type represents a form of dependency between labelling functions, such as correlation. Unknown to us, there is also a set  of index tuples which indicate the set of labelling functions that participate in each dependency of type  . This gives us a general model of the form  and we want to learn the parameters  . Estimating the structure of the distribution  is challenging because Y is latent; we never observe its value, even during training. We must therefore work with the marginal likelihood  . Learning the parameters of the generative model requires Gibbs sampling to estimate gradients. As the number of possible dependencies increases at least quadratically in the number of labeling functions, this heavyweight approach to learning does not scale. The way out is to consider each labelling function in turn, optimising the log marginal pseudolikelihood of its outputs conditioned on the outputs of all the other labelling functions. where  is a hyperparameter. By conditioning on all other labeling functions in each term…, we ensue that the gradient can be computed in polynomial time with respect to the number of labeling functions, data points, and possible dependencies; without requiring any sampling or variational approximations. It all comes together in the following algorithm:  The hyperparameter  controls the threshold and regularization strength and requires tuning. For the other parameters the authors fix these at step size =  , epoch count = 10, and truncation frequency = 10. Theoretical guarantees  Assuming that there exist some set of parameters which can capture the true dependencies within the model we are using, and that all non-zero parameters are bounded away from zero (have at least a minimum magnitude  ), then we can uncover the exact dependency structure with probability at least  given  inputs, where  Where  is the maximum number of possible dependencies a single labelling function (or true label) can be involved in, and  is a measure of how much better our dependency estimates are with each labeling function than without (my loose interpretation of  – see eqn 7 in section 4 of the paper for the official definition). If we further assume that the only dependency types are accuracy and correlation dependencies then we need an input dataset of size  Practical results  Our analysis… guarantees that the sample complexity grows at worst on the order  for  labeling functions. In practice, we find that structure learning performs better than this guaranteed rate, depending linearly on the number of true correlations and logarithmically on the number of possible correlations. Compared to estimating structures via parameter learning over all possible dependencies, algorithm one is 100x faster and selects only 1/4 of the extraneous correlations.", "pdf_url": "https://arxiv.org/pdf/1703.00854", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/learning-the-structure-of-generative-models-without-labeled-data.json"}
{"id": "597051", "bin": "1000_1100", "summary_sentences": ["What  They propose a method to pretrain jointly on video and corresponding audio data without labels and then use the results to:  perform sound source logalicization (estimate what in an image causes the current sound),  perform audio-visual action recognition,  perform audio-separation (e.g. remove speech from person not visible in an image, remove speech from a specific visible person).", "How  Basic method  They train their network to differentiate between real frames (4.2 seconds) and their corresponding audio data on the one hand and real frames but misaligned audio data on the other hand.", "Misaligned here means that the audio is shifted by a few seconds (by 2.0 to 5.8 seconds to be precise).", "To solve this task, the network has to learn to understand the scene in order to spot small misalignments.", "The network can learn this task with self-supervision, i.e. without annotations.", "Architecture  Their network starts with two branches.", "One for frames from videos.", "One for audio data.", "Both branches are then merged and finally end in a fully convolutional layer.", "Visualization:  Sound Source Localization  They train their model as described.", "The features maps of the final convolutional layer then contain information about the spatial location of the sound source.", "They convert these feature maps to a heatmap visualization by first weighting each map (based on its weight to produce the final binary output), then summing the maps and lastly normalizing them to e.g. 0 to 1.0.", "Action Recognition  They first pretrain using their described method, then fine-tune on the UCF-101 dataset (using 2.56 second videos with augmentation and small audio-shifts down to one frame).", "On/off-screen audio-visual source separation  Their goal is to separate sound from (a) sources visible in the video and (b) sources not visible in the video.", "Sources in (b) can be outside of the camera view, but can also be sources intentionally hidden, e.g. by setting the image area of a known source in all frames to zero.", "They focus here in speech by people (i.e. the model has to learn to separate between speeches by people visible in the video and those not visible).", "They first pretrain a model as described above.", "Then they create a new model:  It has one subnetwork for video+audio processing (can reuse the weights of the pretrained network).", "It has a second subnetwork to process the audio spectrogram in a U-Net form.", "The features of the first subnetwork get merged into the second subnetwork at various resolutions.", "The output are two spectrograms: one for the on-screen audio, one for the off-screen audio.", "Visualization:  They train this by pairing video sequences from VoxCeleb with their audio (on-screen) and other audio from a random clip (off-screen).", "They use a loss with two L1-based regression losses:  where x_F is the on-screen (foreground) audio, f_F(.)", "its prediction and analogously x_B and f_B(.)", "the background (off-screen) audio and its prediction.", "One can make this loss permutation invariant by using min(L(x_M, I), L(I, x_M)).", "Results  Sound Source Localization  They train on 750k videos from the AudioSet dataset.", "They reached 59.9% accuracy during pretraining (i.e. when discriminating between non-shifted and shifted audio).", "Humans reached around 66.6% (under easier test conditions, e.g. stronger audio shifts).", "Qualitative results, showing via CAM method the test examples with strongest activation maps:  Note that AudioSet did not only include human speech.", "The model just seemed to react strongly to that.", "Same as before, but test examples with the weakest activation maps:  Strongest activation maps, separated for three example classes (trained on Kinetics):  Action Recognition  They used the UCF-101 dataset, as mentioned above.", "They achieve significantly higher scores than other self-supervised methods.", "They are still quite a bit behind the best method (82.1% vs 94.5%), though that one is a supervised method and was pretrained on two large datasets (ImageNet + Kinetics).", "They test here pairing videos with audio from random other videos (instead of shifted audio from the same video).", "This reduced performance from 84.2% to 78.7%, likely because it is a too easy task that often does not require precise understanding of e.g. the motions in the video.", "Setting the audio branch's activation after pretraining to zero and then training on UCF-101 resulted in an accuracy drop of 5 percentage points.", "Using no self-supervised pretraining resulted in an accuracy drop of 14 percentage points.", "Using spectrograms as audio input performed comparable to raw wave inputs.", "On/off-screen audio-visual source separation  They trained on the VoxCeleb dataset, as described above.", "They made sure that the speaker identities were disjoint between train and test sets.", "They sampled 2.1s clips from each 5s video.", "Results are shown in the video (see at top of this file).", "Training the model only on a single frame instead of the full video worsened the loss from 11.4 to 14.8.", "This drop was especially pronounced in videos with two persons of the same gender, where lip motion is an important clue.", "Using the state-of-the-art I3D net pretrained on Kinect as audio-visual feature inputs (instead of their pretrained net) worsened the loss from 11.4 to 12.3.", "They suspect that this is because action recognition can work decently with just single-frame analysis, while their pretraining (and this source seperation task) require in-depth motion analysis."], "summary_text": "What  They propose a method to pretrain jointly on video and corresponding audio data without labels and then use the results to:  perform sound source logalicization (estimate what in an image causes the current sound),  perform audio-visual action recognition,  perform audio-separation (e.g. remove speech from person not visible in an image, remove speech from a specific visible person). How  Basic method  They train their network to differentiate between real frames (4.2 seconds) and their corresponding audio data on the one hand and real frames but misaligned audio data on the other hand. Misaligned here means that the audio is shifted by a few seconds (by 2.0 to 5.8 seconds to be precise). To solve this task, the network has to learn to understand the scene in order to spot small misalignments. The network can learn this task with self-supervision, i.e. without annotations. Architecture  Their network starts with two branches. One for frames from videos. One for audio data. Both branches are then merged and finally end in a fully convolutional layer. Visualization:  Sound Source Localization  They train their model as described. The features maps of the final convolutional layer then contain information about the spatial location of the sound source. They convert these feature maps to a heatmap visualization by first weighting each map (based on its weight to produce the final binary output), then summing the maps and lastly normalizing them to e.g. 0 to 1.0. Action Recognition  They first pretrain using their described method, then fine-tune on the UCF-101 dataset (using 2.56 second videos with augmentation and small audio-shifts down to one frame). On/off-screen audio-visual source separation  Their goal is to separate sound from (a) sources visible in the video and (b) sources not visible in the video. Sources in (b) can be outside of the camera view, but can also be sources intentionally hidden, e.g. by setting the image area of a known source in all frames to zero. They focus here in speech by people (i.e. the model has to learn to separate between speeches by people visible in the video and those not visible). They first pretrain a model as described above. Then they create a new model:  It has one subnetwork for video+audio processing (can reuse the weights of the pretrained network). It has a second subnetwork to process the audio spectrogram in a U-Net form. The features of the first subnetwork get merged into the second subnetwork at various resolutions. The output are two spectrograms: one for the on-screen audio, one for the off-screen audio. Visualization:  They train this by pairing video sequences from VoxCeleb with their audio (on-screen) and other audio from a random clip (off-screen). They use a loss with two L1-based regression losses:  where x_F is the on-screen (foreground) audio, f_F(.) its prediction and analogously x_B and f_B(.) the background (off-screen) audio and its prediction. One can make this loss permutation invariant by using min(L(x_M, I), L(I, x_M)). Results  Sound Source Localization  They train on 750k videos from the AudioSet dataset. They reached 59.9% accuracy during pretraining (i.e. when discriminating between non-shifted and shifted audio). Humans reached around 66.6% (under easier test conditions, e.g. stronger audio shifts). Qualitative results, showing via CAM method the test examples with strongest activation maps:  Note that AudioSet did not only include human speech. The model just seemed to react strongly to that. Same as before, but test examples with the weakest activation maps:  Strongest activation maps, separated for three example classes (trained on Kinetics):  Action Recognition  They used the UCF-101 dataset, as mentioned above. They achieve significantly higher scores than other self-supervised methods. They are still quite a bit behind the best method (82.1% vs 94.5%), though that one is a supervised method and was pretrained on two large datasets (ImageNet + Kinetics). They test here pairing videos with audio from random other videos (instead of shifted audio from the same video). This reduced performance from 84.2% to 78.7%, likely because it is a too easy task that often does not require precise understanding of e.g. the motions in the video. Setting the audio branch's activation after pretraining to zero and then training on UCF-101 resulted in an accuracy drop of 5 percentage points. Using no self-supervised pretraining resulted in an accuracy drop of 14 percentage points. Using spectrograms as audio input performed comparable to raw wave inputs. On/off-screen audio-visual source separation  They trained on the VoxCeleb dataset, as described above. They made sure that the speaker identities were disjoint between train and test sets. They sampled 2.1s clips from each 5s video. Results are shown in the video (see at top of this file). Training the model only on a single frame instead of the full video worsened the loss from 11.4 to 14.8. This drop was especially pronounced in videos with two persons of the same gender, where lip motion is an important clue. Using the state-of-the-art I3D net pretrained on Kinect as audio-visual feature inputs (instead of their pretrained net) worsened the loss from 11.4 to 12.3. They suspect that this is because action recognition can work decently with just single-frame analysis, while their pretraining (and this source seperation task) require in-depth motion analysis.", "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Andrew_Owens_Audio-Visual_Scene_Analysis_ECCV_2018_paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/audio-visual_scene_analysis_with_self-supervised_multisensory_features.json"}
{"id": "80719489", "bin": "1000_1100", "summary_sentences": ["What  Generative Moment Matching Networks (GMMN) are generative models that use maximum mean discrepancy (MMD) for their objective function.", "MMD is a measure of how similar two datasets are (here: generated dataset and training set).", "GMMNs are similar to GANs, but they replace the Discriminator with the MMD measure, making their optimization more stable.", "How  MMD calculates a similarity measure by comparing statistics of two datasets with each other.", "MMD is calculated based on samples from the training set and the generated dataset.", "A kernel function is applied to pairs of these samples (thus the statistics are acutally calculated in high-dimensional spaces).", "The authors use Gaussian kernels.", "MMD can be approximated using a small number of samples.", "MMD is differentiable and therefor can be used as a standard loss function.", "They train two models:  GMMN: Noise vector input (as in GANs), several ReLU layers into one sigmoid layer.", "MMD as the loss function.", "GMMN+AE: Same as GMMN, but the sigmoid output is not an image, but instead the code that gets fed into an autoencoder's (AE) decoder.", "The AE is trained separately on the dataset.", "MMD is backpropagated through the decoder and then the GMMN.", "I.e. the GMMN learns to produce codes that let the decoder generate good looking images.", "MMD formula, where x_i is a training set example and y_i a generated example.", "Architectures of GMMN (left) and GMMN+AE (right).", "Results  They tested only on MNIST and TFD (i.e. datasets that are well suited for AEs...).", "Their GMMN achieves similar log likelihoods compared to other models.", "Their GMMN+AE achieves better log likelihoods than other models.", "GMMN+AE produces good looking images.", "GMMN+AE produces smooth interpolations between images.", "Generated TFD images and interpolations between them.", "Rough chapter-wise notes  (1) Introduction  Sampling in GMMNs is fast.", "GMMNs are similar to GANs.", "While the training objective in GANs is a minimax problem, in GMMNs it is a simple loss function.", "GMMNs are based on maximum mean discrepancy.", "They use that (implemented via the kernel trick) as the loss function.", "GMMNs try to generate data so that the moments in the generated data are as similar as possible to the moments in the training data.", "They combine GMMNs with autoencoders.", "That is, they first train an autoencoder to generate images.", "Then they train a GMMN to produce sound code inputs to the decoder of the autoencoder.", "(2) Maximum Mean Discrepancy  Maximum mean discrepancy (MMD) is a frequentist estimator to tell whether two datasets X and Y come from the same probability distribution.", "MMD estimates basic statistics values (i.e. mean and higher order statistics) of both datasets and compares them with each other.", "MMD can be formulated so that examples from the datasets are only used for scalar products.", "Then the kernel trick can be applied.", "It can be shown that minimizing MMD with gaussian kernels is equivalent to matching all moments between the probability distributions of the datasets.", "(4) Generative Moment Matching Networks  Data Space Networks  Just like GANs, GMMNs start with a noise vector that has N values sampled uniformly from [-1, 1].", "The noise vector is then fed forward through several fully connected ReLU layers.", "The MMD is differentiable and therefor can be used for backpropagation.", "Auto-Encoder Code Sparse Networks  AEs can be used to reconstruct high-dimensional data, which is a simpler task than to learn to generate new data from scratch.", "Advantages of using the AE code space:  Dimensionality can be explicitly chosen.", "Disentangling factors of variation.", "They suggest a combination of GMMN and AE.", "They first train an AE, then they train a GMMN to generate good codes for the AE's decoder (based on MMD loss).", "For some reason they use greedy layer-wise pretraining with later fine-tuning for the AE, but don't explain why.", "(That training method is outdated?)", "They add dropout to their AE's encoder to get a smoother code manifold.", "Practical Considerations  MMD has a bandwidth parameter (as its based on RBFs).", "Instead of chosing a single fixed bandwidth, they instead use multiple kernels with different bandwidths (1, 5, 10, ...), apply them all and then sum the results.", "Instead of MMD^2 loss they use sqrt(MMD^2), which does not go as fast to zero as raw MMD, thereby creating stronger gradients.", "Per minibatch they generate a small number of samples und they pick a small number of samples from the training set.", "They then compute MMD for these samples.", "I.e. they don't run MMD over the whole training set as that would be computationally prohibitive.", "(5) Experiments  They trained on MNIST and TFD.", "They used an GMMN with 4 ReLU layers and autoencoders with either 2/2 (encoder, decoder) hidden sigmoid layers (MNIST) or 3/3 (TFD).", "They used dropout on the encoder layers.", "They used layer-wise pretraining and finetuning for the AEs.", "They tuned most of the hyperparameters using bayesian optimization.", "They use minibatch sizes of 1000 and compute MMD based on those (i.e. based on 2000 points total).", "Their GMMN+AE model achieves better log likelihood values than all competitors.", "The raw GMMN model performs roughly on par with the competitors.", "Nearest neighbor evaluation indicates that it did not just memorize the training set.", "The model learns smooth interpolations between digits (MNIST) and faces (TFD)."], "summary_text": "What  Generative Moment Matching Networks (GMMN) are generative models that use maximum mean discrepancy (MMD) for their objective function. MMD is a measure of how similar two datasets are (here: generated dataset and training set). GMMNs are similar to GANs, but they replace the Discriminator with the MMD measure, making their optimization more stable. How  MMD calculates a similarity measure by comparing statistics of two datasets with each other. MMD is calculated based on samples from the training set and the generated dataset. A kernel function is applied to pairs of these samples (thus the statistics are acutally calculated in high-dimensional spaces). The authors use Gaussian kernels. MMD can be approximated using a small number of samples. MMD is differentiable and therefor can be used as a standard loss function. They train two models:  GMMN: Noise vector input (as in GANs), several ReLU layers into one sigmoid layer. MMD as the loss function. GMMN+AE: Same as GMMN, but the sigmoid output is not an image, but instead the code that gets fed into an autoencoder's (AE) decoder. The AE is trained separately on the dataset. MMD is backpropagated through the decoder and then the GMMN. I.e. the GMMN learns to produce codes that let the decoder generate good looking images. MMD formula, where x_i is a training set example and y_i a generated example. Architectures of GMMN (left) and GMMN+AE (right). Results  They tested only on MNIST and TFD (i.e. datasets that are well suited for AEs...). Their GMMN achieves similar log likelihoods compared to other models. Their GMMN+AE achieves better log likelihoods than other models. GMMN+AE produces good looking images. GMMN+AE produces smooth interpolations between images. Generated TFD images and interpolations between them. Rough chapter-wise notes  (1) Introduction  Sampling in GMMNs is fast. GMMNs are similar to GANs. While the training objective in GANs is a minimax problem, in GMMNs it is a simple loss function. GMMNs are based on maximum mean discrepancy. They use that (implemented via the kernel trick) as the loss function. GMMNs try to generate data so that the moments in the generated data are as similar as possible to the moments in the training data. They combine GMMNs with autoencoders. That is, they first train an autoencoder to generate images. Then they train a GMMN to produce sound code inputs to the decoder of the autoencoder. (2) Maximum Mean Discrepancy  Maximum mean discrepancy (MMD) is a frequentist estimator to tell whether two datasets X and Y come from the same probability distribution. MMD estimates basic statistics values (i.e. mean and higher order statistics) of both datasets and compares them with each other. MMD can be formulated so that examples from the datasets are only used for scalar products. Then the kernel trick can be applied. It can be shown that minimizing MMD with gaussian kernels is equivalent to matching all moments between the probability distributions of the datasets. (4) Generative Moment Matching Networks  Data Space Networks  Just like GANs, GMMNs start with a noise vector that has N values sampled uniformly from [-1, 1]. The noise vector is then fed forward through several fully connected ReLU layers. The MMD is differentiable and therefor can be used for backpropagation. Auto-Encoder Code Sparse Networks  AEs can be used to reconstruct high-dimensional data, which is a simpler task than to learn to generate new data from scratch. Advantages of using the AE code space:  Dimensionality can be explicitly chosen. Disentangling factors of variation. They suggest a combination of GMMN and AE. They first train an AE, then they train a GMMN to generate good codes for the AE's decoder (based on MMD loss). For some reason they use greedy layer-wise pretraining with later fine-tuning for the AE, but don't explain why. (That training method is outdated?) They add dropout to their AE's encoder to get a smoother code manifold. Practical Considerations  MMD has a bandwidth parameter (as its based on RBFs). Instead of chosing a single fixed bandwidth, they instead use multiple kernels with different bandwidths (1, 5, 10, ...), apply them all and then sum the results. Instead of MMD^2 loss they use sqrt(MMD^2), which does not go as fast to zero as raw MMD, thereby creating stronger gradients. Per minibatch they generate a small number of samples und they pick a small number of samples from the training set. They then compute MMD for these samples. I.e. they don't run MMD over the whole training set as that would be computationally prohibitive. (5) Experiments  They trained on MNIST and TFD. They used an GMMN with 4 ReLU layers and autoencoders with either 2/2 (encoder, decoder) hidden sigmoid layers (MNIST) or 3/3 (TFD). They used dropout on the encoder layers. They used layer-wise pretraining and finetuning for the AEs. They tuned most of the hyperparameters using bayesian optimization. They use minibatch sizes of 1000 and compute MMD based on those (i.e. based on 2000 points total). Their GMMN+AE model achieves better log likelihood values than all competitors. The raw GMMN model performs roughly on par with the competitors. Nearest neighbor evaluation indicates that it did not just memorize the training set. The model learns smooth interpolations between digits (MNIST) and faces (TFD).", "pdf_url": "http://arxiv.org/pdf/1502.02761", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/generative_moment_matching_networks.json"}
{"id": "91451160", "bin": "1000_1100", "summary_sentences": ["The MADlib Analytics Library – MAD Skills, the SQL – Hellerstein et al. 2012  The way that we use large databases has evolved from being primarily in support of accounting and financial record-keeping, to primarily in support of predictive analytics over a wide range of potentially noisy data.", "Analytics at scale requires the marriage of scalable data platforms with analytic libraries.", "The section on related work contains a nice overview of the main approaches taken, together with representative systems in each category.", "I’ve sketched out this ‘family tree’ below.", "This was a 2012 paper – what new systems should we also include for a 2015 update?", "The analytics use case looks different to the system-of-record use case:  Data scientists make use of database engines in a very different way than traditional data warehousing professionals.", "Rather than carefully designing global schemas and “repelling” data until it is integrated, they load data into private schemas in whatever form is convenient.", "Rather than focusing on simple OLAP-style drill-down reports, they implement rich statistical models and algorithms in the database, using extensible SQL as a language for orchestrating data movement between disk, memory, and multiple parallel machines.", "In short, for data scientists a DBMS is a scalable analytics runtime — one that is conveniently compatible with the database systems widely used for transactions and accounting.", "From these observations came the MAD acronynm to describe the characteristics of this new platform (Magnetic, Agile, and Deep):  Magnetic indicates that the platform attracts all kinds of data to it, rather than repelling data that doesn’t strictly conform to pre-conceived ideals  Agile to represent the fact that data scientists use iterative, interactive processess for modeling, loading, and iterating on data, and  Deep to reflect the statisticals models and algorithms being used.", "MADlib is a library of analytic methods that can be installed and executed within a relational database engine that supports extensible SQL.", "It is also an open source project with a website at MADlib.net .", "MADlib has continued to see healthy growth and development since the publication of this paper, with the 1.7 release coming out less than a month ago (Dec. 31st 2014).", "In a first for The Morning Paper, you can even watch a video of Joe Hellerstein and Chris Ré explaining the background of the project and research:  One standard methodology in the domain of statistical analytics is called SEMMA , which stands for Sample, Explore, Modify, Model, Assess.", "The ‘EMMA’ portion of this cycle identifies a set of fundamental tasks that an analyst needs to perform, but the first, “S” step makes less and less sense in many settings today….", "Winning requires extracting advantages in the long tail of “special interests”.", "While the Hadoop system has been evolving, we’ve also seen a resurgence of interest (and acknowledgement) of the important role that SQL has to play.", "“For these cases, it would be helpful to push statistical methods into the DBMS.", "And as we will see, massively parallel databases form a surprisingly useful platform for sophisticated analytics.”  Ideally, we would like MADlib methods to be written entirely in a straightforward and portable SQL.", "Unfortunately, the portable core of “vanilla” SQL is often not quite enough to express the kinds of algorithms needed for advanced analytics.", "You need to be able to intelligently partition large matrices, and to quickly invoke well-tuned linear algebra methods.", "MADlib exploits user-defined aggregates (UDAs), user-defined functions (UDFs), and a sparse matrix C library to provide efficient representations on disk and in memory.", "The most basic building block in the macro-programming of MADlib is the use of user-defined aggregates (UDAs).", "In general, aggregates—and the related window functions—are the natural way  in SQL to implement mathematical functions that take as input the values of an arbitrary number of rows (tuples).", "DBMSs typically implement aggregates as data-parallel streaming algorithms.", "And there is a large body of recent work on online learning algorithms and model-averaging techniques that fit the computational model of aggregates well.", "Many statistical methods are iterative – i.e. they make many passes over a data set.", "Several SQL workarounds are described (virtual tables, window aggregates, recursive queries) but none quite met the needs of MADlib.", "So a driver UDF was written in Python to control iteration in such a way that all large data movement is done within the database engine and its buffer pool.", "What does all this look like to the end user?", "Here’s a linear regression example:  psql# SELECT (linregr(y, x)).", "* FROM data; -[ RECORD 1     ]+--------------------------------------------  coef         | {1.7307,2.2428} r2           | 0.9475 std_err      | {0.3258,0.0533} t_stats      | {5.3127,42.0640} p_values     | {6.7681e-07,4.4409e-16} condition_no | 169.5093  The paper contains examples showing how linear regression, logistic regression, and k-Means are implemented in MADlib, as well as a discussion of contributions for convex optimisation and statistical text analytics.", "The authors conclude:  Scalable analytics are a clear priority for the research and industrial communities.", "MADlib was designed to fill a vacuum for scalable analytics in SQL DBMSs, and connect database research to market needs.", "In our experience, a parallel DBMS provides a very efficient and flexible dataflow substrate for implementing statistical and analytic methods at scale.", "If you want to see what MADlib can do now three years on, take a look at the list of modules in the MADlib documentation.", "You can even install MADlib in PostgreSQL and have a play!"], "summary_text": "The MADlib Analytics Library – MAD Skills, the SQL – Hellerstein et al. 2012  The way that we use large databases has evolved from being primarily in support of accounting and financial record-keeping, to primarily in support of predictive analytics over a wide range of potentially noisy data. Analytics at scale requires the marriage of scalable data platforms with analytic libraries. The section on related work contains a nice overview of the main approaches taken, together with representative systems in each category. I’ve sketched out this ‘family tree’ below. This was a 2012 paper – what new systems should we also include for a 2015 update? The analytics use case looks different to the system-of-record use case:  Data scientists make use of database engines in a very different way than traditional data warehousing professionals. Rather than carefully designing global schemas and “repelling” data until it is integrated, they load data into private schemas in whatever form is convenient. Rather than focusing on simple OLAP-style drill-down reports, they implement rich statistical models and algorithms in the database, using extensible SQL as a language for orchestrating data movement between disk, memory, and multiple parallel machines. In short, for data scientists a DBMS is a scalable analytics runtime — one that is conveniently compatible with the database systems widely used for transactions and accounting. From these observations came the MAD acronynm to describe the characteristics of this new platform (Magnetic, Agile, and Deep):  Magnetic indicates that the platform attracts all kinds of data to it, rather than repelling data that doesn’t strictly conform to pre-conceived ideals  Agile to represent the fact that data scientists use iterative, interactive processess for modeling, loading, and iterating on data, and  Deep to reflect the statisticals models and algorithms being used. MADlib is a library of analytic methods that can be installed and executed within a relational database engine that supports extensible SQL. It is also an open source project with a website at MADlib.net . MADlib has continued to see healthy growth and development since the publication of this paper, with the 1.7 release coming out less than a month ago (Dec. 31st 2014). In a first for The Morning Paper, you can even watch a video of Joe Hellerstein and Chris Ré explaining the background of the project and research:  One standard methodology in the domain of statistical analytics is called SEMMA , which stands for Sample, Explore, Modify, Model, Assess. The ‘EMMA’ portion of this cycle identifies a set of fundamental tasks that an analyst needs to perform, but the first, “S” step makes less and less sense in many settings today…. Winning requires extracting advantages in the long tail of “special interests”. While the Hadoop system has been evolving, we’ve also seen a resurgence of interest (and acknowledgement) of the important role that SQL has to play. “For these cases, it would be helpful to push statistical methods into the DBMS. And as we will see, massively parallel databases form a surprisingly useful platform for sophisticated analytics.”  Ideally, we would like MADlib methods to be written entirely in a straightforward and portable SQL. Unfortunately, the portable core of “vanilla” SQL is often not quite enough to express the kinds of algorithms needed for advanced analytics. You need to be able to intelligently partition large matrices, and to quickly invoke well-tuned linear algebra methods. MADlib exploits user-defined aggregates (UDAs), user-defined functions (UDFs), and a sparse matrix C library to provide efficient representations on disk and in memory. The most basic building block in the macro-programming of MADlib is the use of user-defined aggregates (UDAs). In general, aggregates—and the related window functions—are the natural way  in SQL to implement mathematical functions that take as input the values of an arbitrary number of rows (tuples). DBMSs typically implement aggregates as data-parallel streaming algorithms. And there is a large body of recent work on online learning algorithms and model-averaging techniques that fit the computational model of aggregates well. Many statistical methods are iterative – i.e. they make many passes over a data set. Several SQL workarounds are described (virtual tables, window aggregates, recursive queries) but none quite met the needs of MADlib. So a driver UDF was written in Python to control iteration in such a way that all large data movement is done within the database engine and its buffer pool. What does all this look like to the end user? Here’s a linear regression example:  psql# SELECT (linregr(y, x)). * FROM data; -[ RECORD 1     ]+--------------------------------------------  coef         | {1.7307,2.2428} r2           | 0.9475 std_err      | {0.3258,0.0533} t_stats      | {5.3127,42.0640} p_values     | {6.7681e-07,4.4409e-16} condition_no | 169.5093  The paper contains examples showing how linear regression, logistic regression, and k-Means are implemented in MADlib, as well as a discussion of contributions for convex optimisation and statistical text analytics. The authors conclude:  Scalable analytics are a clear priority for the research and industrial communities. MADlib was designed to fill a vacuum for scalable analytics in SQL DBMSs, and connect database research to market needs. In our experience, a parallel DBMS provides a very efficient and flexible dataflow substrate for implementing statistical and analytic methods at scale. If you want to see what MADlib can do now three years on, take a look at the list of modules in the MADlib documentation. You can even install MADlib in PostgreSQL and have a play!", "pdf_url": "http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-38.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/the-madlib-analytics-library.json"}
{"id": "24753674", "bin": "1000_1100", "summary_sentences": ["This paper presents a method to train a neural network to make predictions for *counterfactual* questions.", "In short, such questions are questions about what the result of an intervention would have been, had a different choice for the intervention been made (e.g. *Would this patient have lower blood sugar had she received a different medication?*).", "One approach to tackle this problem is to collect data of the form $(x_i, t_i, y_i^F)$ where $x_i$ describes a situation (e.g. a patient), $t_i$ describes the intervention made (in this paper $t_i$ is binary, e.g.", "$t_i = 1$ if a new treatment is used while $t_i = 0$ would correspond to using the current treatment) and $y_i^F$ is the factual outcome of the intervention $t_i$ for $x_i$.", "From this training data, a predictor $h(x,t)$ taking the pair $(x_i, t_i)$ as input and outputting a prediction for $y_i^F$ could be trained.", "From this predictor, one could imagine answering counterfactual questions by feeding $(x_i, 1-t_i)$ (i.e. a description of the same situation $x_i$ but with the opposite intervention $1-t_i$) to our predictor and comparing the prediction $h(x_i, 1-t_i)$ with $y_i^F$.", "This would give us an estimate of the change in the outcome, had a different intervention been made, thus providing an answer to our counterfactual question.", "The authors point out that this scenario is related to that of domain adaptation (more specifically to the special case of covariate shift) in which the input training distribution (here represented by inputs $(x_i,t_i)$) is different from the distribution of inputs that will be fed at test time to our predictor (corresponding to the inputs $(x_i, 1-t_i)$).", "If the choice of intervention $t_i$ is evenly spread and chosen independently from $x_i$, the distributions become the same.", "However, in observational studies, the choice of $t_i$ for some given $x_i$ is often not independent of $x_i$ and made according to some unknown policy.", "This is the situation of interest in this paper.", "Thus, the authors propose an approach inspired by the domain adaptation literature.", "Specifically, they propose to have the predictor $h(x,t)$ learn a representation of $x$ that is indiscriminate of the intervention $t$ (see Figure 2 for the proposed neural network architecture).", "Indeed, this is a notion that is [well established][1] in the domain adaptation literature and has been exploited previously using regularization terms based on [adversarial learning][2] and [maximum mean discrepancy][3].", "In this paper, the authors used instead a regularization (noted in the paper as $disc(\\Phi_{t=0},\\Phi_ {t=1})$) based on the so-called discrepancy distance of [Mansour et al. ][4], adapting its use to the case of a neural network.", "As an example, imagine that in our dataset, a new treatment ($t=1$) was much more frequently used than not ($t=0$) for men.", "Thus, for men, relatively insufficient evidence for counterfactual inference is expected to be found in our training dataset.", "Intuitively, we would thus want our predictor to not rely as much on that \"feature\" of patients when inferring the impact of the treatment.", "In addition to this term, the authors also propose incorporating an additional regularizer where the prediction $h(x_i,1-t_i)$ on counterfactual inputs is pushed to be as close as possible to the target $y_{j}^F$ of the observation $x_j$ that is closest to $x_i$ **and** actually had the counterfactual intervention $t_j = 1-t_i$.", "The paper first shows a bound relating the counterfactual generalization error to the discrepancy distance.", "Moreover, experiments simulating counterfactual inference tasks are presented, in which performance is measured by comparing the predicted treatment effects (as estimated by the difference between the observed effect $y_i^F$ for the observed treatment and the predicted effect $h(x_i, 1-t_i)$ for the opposite treatment) with the real effect (known here because the data is simulated).", "The paper shows that the proposed approach using neural networks outperforms several baselines on this task.", "**My two cents**  The connection with domain adaptation presented here is really clever and enlightening.", "This sounds like a very compelling approach to counterfactual inference, which can exploit a lot of previous work on domain adaptation.", "The paper mentions that selecting the hyper-parameters (such as the regularization terms weights) in this scenario is not a trivial task.", "Indeed, measuring performance here requires knowing the true difference in intervention outcomes, which in practice usually cannot be known (e.g. two treatments usually cannot be given to the same patient once).", "In the paper, they somewhat \"cheat\" by using the ground truth difference in outcomes to measure out-of-sample performance, which the authors admit is unrealistic.", "Thus, an interesting avenue for future work would be to design practical hyper-parameter selection procedures for this scenario.", "I wonder whether the *reverse cross-validation* approach we used in our work on our adversarial approach to domain adaptation (see [Section 5.1.2][5]) could successfully be used here.", "Finally, I command the authors for presenting such a nicely written description of counterfactual inference problem setup in general, I really enjoyed it!", "[1]:  [url]"], "summary_text": "This paper presents a method to train a neural network to make predictions for *counterfactual* questions. In short, such questions are questions about what the result of an intervention would have been, had a different choice for the intervention been made (e.g. *Would this patient have lower blood sugar had she received a different medication?*). One approach to tackle this problem is to collect data of the form $(x_i, t_i, y_i^F)$ where $x_i$ describes a situation (e.g. a patient), $t_i$ describes the intervention made (in this paper $t_i$ is binary, e.g. $t_i = 1$ if a new treatment is used while $t_i = 0$ would correspond to using the current treatment) and $y_i^F$ is the factual outcome of the intervention $t_i$ for $x_i$. From this training data, a predictor $h(x,t)$ taking the pair $(x_i, t_i)$ as input and outputting a prediction for $y_i^F$ could be trained. From this predictor, one could imagine answering counterfactual questions by feeding $(x_i, 1-t_i)$ (i.e. a description of the same situation $x_i$ but with the opposite intervention $1-t_i$) to our predictor and comparing the prediction $h(x_i, 1-t_i)$ with $y_i^F$. This would give us an estimate of the change in the outcome, had a different intervention been made, thus providing an answer to our counterfactual question. The authors point out that this scenario is related to that of domain adaptation (more specifically to the special case of covariate shift) in which the input training distribution (here represented by inputs $(x_i,t_i)$) is different from the distribution of inputs that will be fed at test time to our predictor (corresponding to the inputs $(x_i, 1-t_i)$). If the choice of intervention $t_i$ is evenly spread and chosen independently from $x_i$, the distributions become the same. However, in observational studies, the choice of $t_i$ for some given $x_i$ is often not independent of $x_i$ and made according to some unknown policy. This is the situation of interest in this paper. Thus, the authors propose an approach inspired by the domain adaptation literature. Specifically, they propose to have the predictor $h(x,t)$ learn a representation of $x$ that is indiscriminate of the intervention $t$ (see Figure 2 for the proposed neural network architecture). Indeed, this is a notion that is [well established][1] in the domain adaptation literature and has been exploited previously using regularization terms based on [adversarial learning][2] and [maximum mean discrepancy][3]. In this paper, the authors used instead a regularization (noted in the paper as $disc(\\Phi_{t=0},\\Phi_ {t=1})$) based on the so-called discrepancy distance of [Mansour et al. ][4], adapting its use to the case of a neural network. As an example, imagine that in our dataset, a new treatment ($t=1$) was much more frequently used than not ($t=0$) for men. Thus, for men, relatively insufficient evidence for counterfactual inference is expected to be found in our training dataset. Intuitively, we would thus want our predictor to not rely as much on that \"feature\" of patients when inferring the impact of the treatment. In addition to this term, the authors also propose incorporating an additional regularizer where the prediction $h(x_i,1-t_i)$ on counterfactual inputs is pushed to be as close as possible to the target $y_{j}^F$ of the observation $x_j$ that is closest to $x_i$ **and** actually had the counterfactual intervention $t_j = 1-t_i$. The paper first shows a bound relating the counterfactual generalization error to the discrepancy distance. Moreover, experiments simulating counterfactual inference tasks are presented, in which performance is measured by comparing the predicted treatment effects (as estimated by the difference between the observed effect $y_i^F$ for the observed treatment and the predicted effect $h(x_i, 1-t_i)$ for the opposite treatment) with the real effect (known here because the data is simulated). The paper shows that the proposed approach using neural networks outperforms several baselines on this task. **My two cents**  The connection with domain adaptation presented here is really clever and enlightening. This sounds like a very compelling approach to counterfactual inference, which can exploit a lot of previous work on domain adaptation. The paper mentions that selecting the hyper-parameters (such as the regularization terms weights) in this scenario is not a trivial task. Indeed, measuring performance here requires knowing the true difference in intervention outcomes, which in practice usually cannot be known (e.g. two treatments usually cannot be given to the same patient once). In the paper, they somewhat \"cheat\" by using the ground truth difference in outcomes to measure out-of-sample performance, which the authors admit is unrealistic. Thus, an interesting avenue for future work would be to design practical hyper-parameter selection procedures for this scenario. I wonder whether the *reverse cross-validation* approach we used in our work on our adversarial approach to domain adaptation (see [Section 5.1.2][5]) could successfully be used here. Finally, I command the authors for presenting such a nicely written description of counterfactual inference problem setup in general, I really enjoyed it! [1]:  [url]", "pdf_url": "http://arxiv.org/pdf/1605.03661", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/johanssonss16.json"}
{"id": "18958324", "bin": "1000_1100", "summary_sentences": ["What  They suggest a new method to generate images which maximize the activation of a specific neuron in a (trained) target network (abbreviated with \"DNN\").", "E.g. if your DNN contains a neuron that is active whenever there is a car in an image, the method should generate images containing cars.", "Such methods can be used to investigate what exactly a network has learned.", "There are plenty of methods like this one.", "They usually differ from each other by using different natural image priors.", "A natural image prior is a restriction on the generated images.", "Such a prior pushes the generated images towards realistic looking ones.", "Without such a prior it is easy to generate images that lead to high activations of specific neurons, but don't look realistic at all (e.g. they might look psychodelic or like white noise).", "That's because the space of possible images is extremely high-dimensional and can therefore hardly be covered reliably by a single network.", "Note also that training datasets usually only show a very limited subset of all possible images.", "Their work introduces a new natural image prior.", "How  Usually, if one wants to generate images that lead to high activations, the basic/naive method is to:  Start with a noise image,  Feed that image through DNN,  Compute an error that is high if the activation of the specified neuron is low (analogous for high activation),  Backpropagate the error through DNN,  Change the noise image according to the gradient,  Repeat.", "So, the noise image is basically treated like weights in the network.", "Their alternative method is based on a Generator network G.  That G is trained according to the method described in Generating Images with Perceptual Similarity Metrics based on Deep Networks .", "Very rough outline of that method:  First, a pretrained network E is given (they picked CaffeNet, which is a variation of AlexNet).", "G then has to learn to inverse E, i.e. G receives per image the features extracted by a specific layer in E (e.g.", "the last fully connected layer before the output) and has to generate (recreate) the image from these features.", "Their modified steps are:  (New step) Start with a noise vector,  (New step) Feed that vector through G resulting in an image,  (Same) Feed that image through DNN,  (Same) Compute an error that is low if the activation of the specified neuron is high (analogous for low activations),  (Same) Backpropagate the error through DNN,  (Modified) Change the noise vector according to the gradient,  (Same) Repeat.", "Visualization of their architecture:  Additionally they do:  Apply an L2 norm to the noise vector, which adds pressure to each component to take low values.", "They say that this improved the results.", "Clip each component of the noise vector to a range [0, a], which improved the results significantly.", "The range starts at 0, because the network (E) inverted by their Generator (G) is based on ReLUs.", "a is derived from test images fed through E and set to 3 standard diviations of the mean activation of that component (recall that the \"noise\" vector mirrors a specific layer in E).", "They argue that this clipping is similar to a prior on the noise vector components.", "That prior reflects likely values of the layer in E that is used for the noise vector.", "Results  Examples of generated images:  Early vs. late layers  For G they have to pick a specific layer from E that G has to invert.", "They found that using \"later\" layers (e.g. the fully connected layers at the end) produced images with more reasonable overall structure than using \"early\" layers (e.g.", "first convolutional layers).", "Early layers led to repeating structures.", "Datasets and architectures  Both G and DNN have to be trained on datasets.", "They found that these networks can actually be trained on different datasets, the results will still look good.", "However, they found that the architectures of DNN and E should be similar to create the best looking images (though this might also be down to depth of the tested networks).", "Verification that the prior can generate any image  They tested whether the generated images really show what the DNN-neurons prefer and not what the Generator/prior prefers.", "To do that, they retrained DNNs on images that were both directly from the dataset as well as images that were somehow modified.", "Those modifications were:  Treated RGB images as if they were BGR (creating images with weird colors).", "Copy-pasted areas in the images around (creating mosaics).", "Blurred the images (with gaussian blur).", "The DNNs were then trained to classify the \"normal\" images into 1000 classes and the modified images into 1000 other classes (2000 total).", "So at the end there were (in the same DNN) neurons reacting strongly to specific classes of unmodified images and other neurons that reacted strongly to specific classes of modified images.", "When generating images to maximize activations of specific neurons, the Generator was able to create both modified and unmodified images.", "Though it seemed to have some trouble with blurring.", "That shows that the generated images probably indeed show what the DNN has learned and not just what G has learned.", "Uncanonical images  The method can sometimes generate uncanonical images (e.g. instead of a full dog just blobs of texture).", "They found that this seems to be mostly the case when the dataset images have uncanonical pose, i.e. are very diverse/multi-modal."], "summary_text": "What  They suggest a new method to generate images which maximize the activation of a specific neuron in a (trained) target network (abbreviated with \"DNN\"). E.g. if your DNN contains a neuron that is active whenever there is a car in an image, the method should generate images containing cars. Such methods can be used to investigate what exactly a network has learned. There are plenty of methods like this one. They usually differ from each other by using different natural image priors. A natural image prior is a restriction on the generated images. Such a prior pushes the generated images towards realistic looking ones. Without such a prior it is easy to generate images that lead to high activations of specific neurons, but don't look realistic at all (e.g. they might look psychodelic or like white noise). That's because the space of possible images is extremely high-dimensional and can therefore hardly be covered reliably by a single network. Note also that training datasets usually only show a very limited subset of all possible images. Their work introduces a new natural image prior. How  Usually, if one wants to generate images that lead to high activations, the basic/naive method is to:  Start with a noise image,  Feed that image through DNN,  Compute an error that is high if the activation of the specified neuron is low (analogous for high activation),  Backpropagate the error through DNN,  Change the noise image according to the gradient,  Repeat. So, the noise image is basically treated like weights in the network. Their alternative method is based on a Generator network G.  That G is trained according to the method described in Generating Images with Perceptual Similarity Metrics based on Deep Networks . Very rough outline of that method:  First, a pretrained network E is given (they picked CaffeNet, which is a variation of AlexNet). G then has to learn to inverse E, i.e. G receives per image the features extracted by a specific layer in E (e.g. the last fully connected layer before the output) and has to generate (recreate) the image from these features. Their modified steps are:  (New step) Start with a noise vector,  (New step) Feed that vector through G resulting in an image,  (Same) Feed that image through DNN,  (Same) Compute an error that is low if the activation of the specified neuron is high (analogous for low activations),  (Same) Backpropagate the error through DNN,  (Modified) Change the noise vector according to the gradient,  (Same) Repeat. Visualization of their architecture:  Additionally they do:  Apply an L2 norm to the noise vector, which adds pressure to each component to take low values. They say that this improved the results. Clip each component of the noise vector to a range [0, a], which improved the results significantly. The range starts at 0, because the network (E) inverted by their Generator (G) is based on ReLUs. a is derived from test images fed through E and set to 3 standard diviations of the mean activation of that component (recall that the \"noise\" vector mirrors a specific layer in E). They argue that this clipping is similar to a prior on the noise vector components. That prior reflects likely values of the layer in E that is used for the noise vector. Results  Examples of generated images:  Early vs. late layers  For G they have to pick a specific layer from E that G has to invert. They found that using \"later\" layers (e.g. the fully connected layers at the end) produced images with more reasonable overall structure than using \"early\" layers (e.g. first convolutional layers). Early layers led to repeating structures. Datasets and architectures  Both G and DNN have to be trained on datasets. They found that these networks can actually be trained on different datasets, the results will still look good. However, they found that the architectures of DNN and E should be similar to create the best looking images (though this might also be down to depth of the tested networks). Verification that the prior can generate any image  They tested whether the generated images really show what the DNN-neurons prefer and not what the Generator/prior prefers. To do that, they retrained DNNs on images that were both directly from the dataset as well as images that were somehow modified. Those modifications were:  Treated RGB images as if they were BGR (creating images with weird colors). Copy-pasted areas in the images around (creating mosaics). Blurred the images (with gaussian blur). The DNNs were then trained to classify the \"normal\" images into 1000 classes and the modified images into 1000 other classes (2000 total). So at the end there were (in the same DNN) neurons reacting strongly to specific classes of unmodified images and other neurons that reacted strongly to specific classes of modified images. When generating images to maximize activations of specific neurons, the Generator was able to create both modified and unmodified images. Though it seemed to have some trouble with blurring. That shows that the generated images probably indeed show what the DNN has learned and not just what G has learned. Uncanonical images  The method can sometimes generate uncanonical images (e.g. instead of a full dog just blobs of texture). They found that this seems to be mostly the case when the dataset images have uncanonical pose, i.e. are very diverse/multi-modal.", "pdf_url": "http://arxiv.org/pdf/1605.09304v3", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/synthesizing_the_preferred_inputs_for_neurons_in_neural_networks_via_deep_generator_networks.json"}
{"id": "6827132", "bin": "1000_1100", "summary_sentences": ["Tail attacks on web applications Shan et al., CCS’17  This paper introduces a stealthy DDoS attack on classic n-tier web applications.", "It is designed to push the tail latency high while simultaneously being very hard to detect using traditional monitoring tools.", "The attack exploits ‘millibottlenecks’ — caused by buffers in the system that fill up when subjected to a short burst of traffic — by sending timed pulses of attack traffic.", "Tail attacks aim to create very short (hundreds of milliseconds) resource contention (e.g., CPU or disk I/O) with dependencies among distributed nodes, while giving an “unsaturated illusion” for state-of-the-art IDS/IPS tools leading to a higher level of stealthiness.", "The attacker sends short “ON” bursts of attack traffic, where the duration of a burst is typically on the order of milliseconds.", "These are followed longer “OFF” periods in which the target system can cool down, clearing up the queued requests and returning to a lower utilisation state.", "When an attack pulse is sent, resource millibottlenecks occur in one of the tiers (assuming the pulse parameters are set accordingly).", "A millibottleneck stops the saturated tier processing for a short time (order of milliseconds), leading to the filling up of the message queues and thread pools in the bottleneck tier, and quickly propagating the queue overflow to all the upstream tiers of the n-tier system.", "Once things are backed up all the way to the front-end, incoming packets for new requests will be dropped, and end-users may see very long response times (on the order of seconds).", "Testing across EC2, Azure, and an academic cloud platform “NSF Cloudlab”, tail attacks show a 4x-8x increase in 95th percentile latencies when testing a variety of configurations of the RUBBoS n-tier web application benchmark.", "Tail attack analysis  Tail attacks can be modelled using queueing networks.", "The following table shows the parameters in the model:  With reference to the parameters above, the attack volume during a burst is given by:  And the period of damage during which requests will be dropped is given by:  End-users with dropped requests perceive a long response time, which can be approximated by:  An attacker needs to choose attack parameters  , and in theory the optimal parameters can be found by solving a nonlinear optimisation problem, or explored via simulation using e.g., the Java Modelling Tools (JMT) open source modelling toolkit for queueing networks.", "The authors demonstrate both of these approaches in sections 3.2 and 3.3 respectively.", "However, they both fail to address the dynamics of real systems.", "For that, we need to introduce a feedback control loop.", "Maintaining optimal attack parameters using feedback control  Since baseline workloads vary (and so might system capacity), we need to dynamically adjust attack parameters to keep them in the sweet spot.", "Too little and the system will shrug the attacks off.", "Too much and detection mechanisms may be triggered.", "Enter the feedback controller:  The controller is a Kalman filter which executes recursively for each new observation.", "First it creates an a priori estimate of the system state and error matrix, and then these are refined using the current measurement.", "Using the Kalman filter, the Controller can predict the required attack parameters at the k-th burst given the historical results of all k-1 bursts, dynamically command the new parameters to the bots, and automatically and effectively launch Tail Attacks.", "The observations needed by the filter are produced by the estimator.", "A “Prober” monitors attacks and is used to infer  (the damage period).", "It sends lightweight requests with short expect services times and uses the request-response intervals to monitor the impact of attacks without causing too much load during the “OFF” periods.", "The millibottleneck length  is estimated by the attack bots themselves.", "These send a burst of heavyweight requests designed to make it through all the tiers of the application, and estimate the bottleneck length by measuring request-response intervals.", "Tail attacks in action  The RUBBoS n-tier web application benchmark modelled on Slashdot is deployed in a variety of configurations across three cloud services.", "In the table below, the four digit (or three digit) notation #W#A#L#D represents the number of web servers, app servers, load balancers (may not be present) and database servers.", "Columns 2 to 5 (in the table above) show the corresponding model parameters in our real cloud production setting experiments controlled by our attack framework.", "It clearly shows that our attacks controlled by our algorithm can achieve the predefined targets (5% drop ratio, damage length less than 100ms, millibottleneck length less than 500ms).", "Here’s the same story plotted as a sequence of charts:  Detection and defence  Detecting and defending against Tail Attacks requires a three-pronged strategy:  Fine-grained monitoring, with granularity less than the millibottlenecks period.", "Burst detection looking for potential attack bursts where requests are dropped, cross-tier queues are overflown, millibottlenecks (on e.g. CPU or I/O) occur, and there is a burst of requests.", "IP-based statistical analysis to try and separate the bots from legitimate users.", "For case 3, we can define a suspicion index for each IP address as:  Where  and  are the number of attacks for each IP during “ON” bursts and the attack interval T (including both “ON” and “OFF” periods) respectively.", "Plotting this for the RUBBoS experiment for example shows a clear separation between normal users and attackers.", "Welcome to the emerging world of low-volume application DDoS attacks!"], "summary_text": "Tail attacks on web applications Shan et al., CCS’17  This paper introduces a stealthy DDoS attack on classic n-tier web applications. It is designed to push the tail latency high while simultaneously being very hard to detect using traditional monitoring tools. The attack exploits ‘millibottlenecks’ — caused by buffers in the system that fill up when subjected to a short burst of traffic — by sending timed pulses of attack traffic. Tail attacks aim to create very short (hundreds of milliseconds) resource contention (e.g., CPU or disk I/O) with dependencies among distributed nodes, while giving an “unsaturated illusion” for state-of-the-art IDS/IPS tools leading to a higher level of stealthiness. The attacker sends short “ON” bursts of attack traffic, where the duration of a burst is typically on the order of milliseconds. These are followed longer “OFF” periods in which the target system can cool down, clearing up the queued requests and returning to a lower utilisation state. When an attack pulse is sent, resource millibottlenecks occur in one of the tiers (assuming the pulse parameters are set accordingly). A millibottleneck stops the saturated tier processing for a short time (order of milliseconds), leading to the filling up of the message queues and thread pools in the bottleneck tier, and quickly propagating the queue overflow to all the upstream tiers of the n-tier system. Once things are backed up all the way to the front-end, incoming packets for new requests will be dropped, and end-users may see very long response times (on the order of seconds). Testing across EC2, Azure, and an academic cloud platform “NSF Cloudlab”, tail attacks show a 4x-8x increase in 95th percentile latencies when testing a variety of configurations of the RUBBoS n-tier web application benchmark. Tail attack analysis  Tail attacks can be modelled using queueing networks. The following table shows the parameters in the model:  With reference to the parameters above, the attack volume during a burst is given by:  And the period of damage during which requests will be dropped is given by:  End-users with dropped requests perceive a long response time, which can be approximated by:  An attacker needs to choose attack parameters  , and in theory the optimal parameters can be found by solving a nonlinear optimisation problem, or explored via simulation using e.g., the Java Modelling Tools (JMT) open source modelling toolkit for queueing networks. The authors demonstrate both of these approaches in sections 3.2 and 3.3 respectively. However, they both fail to address the dynamics of real systems. For that, we need to introduce a feedback control loop. Maintaining optimal attack parameters using feedback control  Since baseline workloads vary (and so might system capacity), we need to dynamically adjust attack parameters to keep them in the sweet spot. Too little and the system will shrug the attacks off. Too much and detection mechanisms may be triggered. Enter the feedback controller:  The controller is a Kalman filter which executes recursively for each new observation. First it creates an a priori estimate of the system state and error matrix, and then these are refined using the current measurement. Using the Kalman filter, the Controller can predict the required attack parameters at the k-th burst given the historical results of all k-1 bursts, dynamically command the new parameters to the bots, and automatically and effectively launch Tail Attacks. The observations needed by the filter are produced by the estimator. A “Prober” monitors attacks and is used to infer  (the damage period). It sends lightweight requests with short expect services times and uses the request-response intervals to monitor the impact of attacks without causing too much load during the “OFF” periods. The millibottleneck length  is estimated by the attack bots themselves. These send a burst of heavyweight requests designed to make it through all the tiers of the application, and estimate the bottleneck length by measuring request-response intervals. Tail attacks in action  The RUBBoS n-tier web application benchmark modelled on Slashdot is deployed in a variety of configurations across three cloud services. In the table below, the four digit (or three digit) notation #W#A#L#D represents the number of web servers, app servers, load balancers (may not be present) and database servers. Columns 2 to 5 (in the table above) show the corresponding model parameters in our real cloud production setting experiments controlled by our attack framework. It clearly shows that our attacks controlled by our algorithm can achieve the predefined targets (5% drop ratio, damage length less than 100ms, millibottleneck length less than 500ms). Here’s the same story plotted as a sequence of charts:  Detection and defence  Detecting and defending against Tail Attacks requires a three-pronged strategy:  Fine-grained monitoring, with granularity less than the millibottlenecks period. Burst detection looking for potential attack bursts where requests are dropped, cross-tier queues are overflown, millibottlenecks (on e.g. CPU or I/O) occur, and there is a burst of requests. IP-based statistical analysis to try and separate the bots from legitimate users. For case 3, we can define a suspicion index for each IP address as:  Where  and  are the number of attacks for each IP during “ON” bursts and the attack interval T (including both “ON” and “OFF” periods) respectively. Plotting this for the RUBBoS experiment for example shows a clear separation between normal users and attackers. Welcome to the emerging world of low-volume application DDoS attacks!", "pdf_url": "http://iisp.gatech.edu/sites/default/files/images/tail_attack_-_calton_pu.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/tail-attacks-on-web-applications.json"}
{"id": "29796623", "bin": "1000_1100", "summary_sentences": ["Combining static model checking with dynamic enforcement using the Statecall Policy Language – Madhavapeddy 2009  We know that getting distributed systems right is hard, and subtle, ‘deep’ bugs can lurk in both algorithms and implementations.", "Can we do better than informal reasoning coupled with some unit and integration tests?", "Evidence suggests we have to do better!", "We’ve previously looked at Amazon’s use of TLA+ , and today’s selection is the first of three papers I’ve selected to probe deeper into this issue.", "Today’s choice looks at the Statecall Policy Language (SPL) that was used by Howard et al. in Raft refloated to validate their Raft implementation.", "In the next two days we’ll also be looking at distributed model checking made practical with SAMC, and pitting our wits against Molly with Peter Alvaro’s ‘Lineage Driven Fault Injection.’  The Statecall Policy Language (SPL) was designed to make model checking more accessible to regular programmers.", "Models are specified in terms of allowable sequences of program events, and the SPL model can then be translated by a compiler into a variety of forms.", "These include:  A translation into PROMELA ,  which can then be used with SPIN to check static properties of the model.", "A graphical visualization using GraphViz.", "Generated code in your target implementation language (OCaml backend is currently implemented, but the design allows for others) that enables run-time validation (i.e. ensures that the real-world behaviour is not deviating from the model).", "Also known as a safety monitor.", "Debugging stubs to support an HTML and Javascript debugging view giving a real-time window into all the automata embedded in the program.", "Writing as of 2009, Madhavapeddy states:  None of the major implementations of protocols such as HTTP (Apache), SMTP (Sendmail/Postfix), or DNS (BIND) are regularly model-checked by their development teams.", "All of them regularly suffer from serious security flaws ranging from low-level buffer overflows to subtle high-level protocol errors, some of which could have been caught by using model checking.", "(We’ll see more examples of verification techniques finding meaningful real-world bugs over the next two days).", "Here’s an SPL model for ping that supports the -c n (only send n packets in total) and -w (wait instead of timing out) flags/behaviours.", "01 automaton ping (int max_count, int count, bool can_timeout) { 02   Initialize; 03   during { 04     count = 0; 05     do { 06       Transmit_Ping; 07       either { 08         Receive_Ping; 09       } or (can_timeout) { 10         Timeout_Ping; 11       }; 12       count = count + 1; 13     } until (count &gt;= max_count); 14   } handle { 15     SIGINFO; 16     Print_Summary; 17   }; 18 }  The Statecalls begin with an initial capital letter: Initialize, Transmit_Ping, Receive_Ping, Timeout_Ping, and Print_Summary.", "The automaton defines the allowable sequences of statecalls.", "Signal handlers are often a source of bugs due to their extremely asynchronous nature — SPL provides a during/handle construct (used in the example above, see the lines 03 and 14) which models them by permitting a state transition into alternative statement blocks during normal execution of an SPL specification.", "Once you are satisfied with the SPL model you can run the SPL compiler.", "The generated code for the executable model can be linked with the real ping implementation.", "You can ‘even do this manually’ if you really want to!", "This code is linked in with the main ping application, and appropriate calls to initialize the automaton and invoke statecalls are inserted in the code.", "Crucially, we do not mandate a single style of invoking statecalls; instead the programmer can choose between automatic mechanisms (e.g. MPL packet parsing code can automatically invoke statecalls when transmitting or receiving packets), language-assisted means (e.g.", "functional combinators, object inheritance, or pre-processors such as cpp), or even careful manual insertion in places where other methods are inconvenient.", "Underneath the covers, SPL translates models into an intermediate form based on a Control Flow Automata (CFA) graph.", "For more complex protocols, it is possible to ‘divide-and-conquer’ :  It is often more convenient and readable to break down a complex protocol into smaller blocks which express the same protocol but with certain aspects factored out into simpler state machines.", "Accordingly, SPL specifications can define multiple automata, but the external interface hides this abstraction and only exposes a single, flat set of statecalls.", "Each automaton then executes in parallel, received statecalls are only dispatched to automata which include the statecall in their alphabet.", "The debugging support is a nice touch (see screenshot below):  This page contains a real-time graphical view of all the automata embedded in the program, along with the set of valid states they can transition to next.", "Since the granularity of the SPL automata are chosen by the programmer, this is much more useful than the “raw” models obtained through static code analysis which often include a lot of superfluous information.", "Figure 5 shows a screen capture of the SPL AJAX debugger single-stepping through the global SPL automaton for theMelange SSH server.", "The mlssh server is blocked waiting for password authentication, having previously attempted to authenticate via null and public-key authentication.", "In our experience, the debugger was a valuable tool to debug complex protocol bugs in our implementation, as the single-stepping view via this debugger is significantly higher level than the alternative provided by either the native OCaml debugger or gdb."], "summary_text": "Combining static model checking with dynamic enforcement using the Statecall Policy Language – Madhavapeddy 2009  We know that getting distributed systems right is hard, and subtle, ‘deep’ bugs can lurk in both algorithms and implementations. Can we do better than informal reasoning coupled with some unit and integration tests? Evidence suggests we have to do better! We’ve previously looked at Amazon’s use of TLA+ , and today’s selection is the first of three papers I’ve selected to probe deeper into this issue. Today’s choice looks at the Statecall Policy Language (SPL) that was used by Howard et al. in Raft refloated to validate their Raft implementation. In the next two days we’ll also be looking at distributed model checking made practical with SAMC, and pitting our wits against Molly with Peter Alvaro’s ‘Lineage Driven Fault Injection.’  The Statecall Policy Language (SPL) was designed to make model checking more accessible to regular programmers. Models are specified in terms of allowable sequences of program events, and the SPL model can then be translated by a compiler into a variety of forms. These include:  A translation into PROMELA ,  which can then be used with SPIN to check static properties of the model. A graphical visualization using GraphViz. Generated code in your target implementation language (OCaml backend is currently implemented, but the design allows for others) that enables run-time validation (i.e. ensures that the real-world behaviour is not deviating from the model). Also known as a safety monitor. Debugging stubs to support an HTML and Javascript debugging view giving a real-time window into all the automata embedded in the program. Writing as of 2009, Madhavapeddy states:  None of the major implementations of protocols such as HTTP (Apache), SMTP (Sendmail/Postfix), or DNS (BIND) are regularly model-checked by their development teams. All of them regularly suffer from serious security flaws ranging from low-level buffer overflows to subtle high-level protocol errors, some of which could have been caught by using model checking. (We’ll see more examples of verification techniques finding meaningful real-world bugs over the next two days). Here’s an SPL model for ping that supports the -c n (only send n packets in total) and -w (wait instead of timing out) flags/behaviours. 01 automaton ping (int max_count, int count, bool can_timeout) { 02   Initialize; 03   during { 04     count = 0; 05     do { 06       Transmit_Ping; 07       either { 08         Receive_Ping; 09       } or (can_timeout) { 10         Timeout_Ping; 11       }; 12       count = count + 1; 13     } until (count &gt;= max_count); 14   } handle { 15     SIGINFO; 16     Print_Summary; 17   }; 18 }  The Statecalls begin with an initial capital letter: Initialize, Transmit_Ping, Receive_Ping, Timeout_Ping, and Print_Summary. The automaton defines the allowable sequences of statecalls. Signal handlers are often a source of bugs due to their extremely asynchronous nature — SPL provides a during/handle construct (used in the example above, see the lines 03 and 14) which models them by permitting a state transition into alternative statement blocks during normal execution of an SPL specification. Once you are satisfied with the SPL model you can run the SPL compiler. The generated code for the executable model can be linked with the real ping implementation. You can ‘even do this manually’ if you really want to! This code is linked in with the main ping application, and appropriate calls to initialize the automaton and invoke statecalls are inserted in the code. Crucially, we do not mandate a single style of invoking statecalls; instead the programmer can choose between automatic mechanisms (e.g. MPL packet parsing code can automatically invoke statecalls when transmitting or receiving packets), language-assisted means (e.g. functional combinators, object inheritance, or pre-processors such as cpp), or even careful manual insertion in places where other methods are inconvenient. Underneath the covers, SPL translates models into an intermediate form based on a Control Flow Automata (CFA) graph. For more complex protocols, it is possible to ‘divide-and-conquer’ :  It is often more convenient and readable to break down a complex protocol into smaller blocks which express the same protocol but with certain aspects factored out into simpler state machines. Accordingly, SPL specifications can define multiple automata, but the external interface hides this abstraction and only exposes a single, flat set of statecalls. Each automaton then executes in parallel, received statecalls are only dispatched to automata which include the statecall in their alphabet. The debugging support is a nice touch (see screenshot below):  This page contains a real-time graphical view of all the automata embedded in the program, along with the set of valid states they can transition to next. Since the granularity of the SPL automata are chosen by the programmer, this is much more useful than the “raw” models obtained through static code analysis which often include a lot of superfluous information. Figure 5 shows a screen capture of the SPL AJAX debugger single-stepping through the global SPL automaton for theMelange SSH server. The mlssh server is blocked waiting for password authentication, having previously attempted to authenticate via null and public-key authentication. In our experience, the debugger was a valuable tool to debug complex protocol bugs in our implementation, as the single-stepping view via this debugger is significantly higher level than the alternative provided by either the native OCaml debugger or gdb.", "pdf_url": "http://anil.recoil.org/papers/2009-icfem-spl.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/combining-static-model-checking-with-dynamic-enforcement-using-the-statecall-policy-language.json"}
{"id": "40883301", "bin": "1000_1100", "summary_sentences": ["What  They suggest some small changes to the GAN training scheme that lead to visually improved results.", "They suggest a new scoring method to compare the results of different GAN models with each other.", "How  Feature Matching  Usually G would be trained to mislead D as often as possible, i.e. to maximize D's output.", "Now they train G to minimize the feature distance between real and fake images.", "I.e. they do:  Pick a layer l from D.  Forward real images through D and extract the features from layer l.  Forward fake images through D and extract the features from layer l.  Compute the squared euclidean distance between the layers and backpropagate.", "Minibatch discrimination  They allow D to look at multiple images in the same minibatch.", "That is, they feed the features (of each image) extracted by an intermediate layer of D through a linear operation, resulting in a matrix per image.", "They then compute the L1-distances between these matrices.", "They then let D make its judgement (fake/real image) based on the features extracted from the image and these distances.", "They add this mechanism so that the diversity of images generated by G increases (which should also prevent collapses).", "Historical averaging  They add a penalty term that punishes weights which are rather far away from their historical average values.", "I.e. the cost is distance(current parameters, average of parameters over the last t batches).", "They argue that this can help the network to find equilibria that normal gradient descent would not find.", "One-sided label smoothing  Usually one would use the labels 0 (image is fake) and 1 (image is real).", "Using smoother labels (0.1 and 0.9) seems to make networks more resistent to adversarial examples.", "* So they smooth the labels of real images (apparently to 0.9?).", "Smoothing the labels of fake images would lead to (mathematical) problems in some cases, so they keep these at 0.", "Virtual Batch Normalization (VBN)  Usually BN normalizes each example with respect to the other examples in the same batch.", "They instead normalize each example with respect to the examples in a reference batch, which was picked once at the start of the training.", "VBN is intended to reduce the dependence of each example on the other examples in the batch.", "VBN is computationally expensive, because it requires forwarding of two minibatches.", "They use VBN for their G.  Inception Scoring  They introduce a new scoring method for GAN results.", "Their method is based on feeding the generated images through another network, here they use Inception.", "For an image x and predicted classes y (softmax-output of Inception):  They argue that they want p(y|x) to have low entropy, i.e. the model should be rather certain of seeing a class (or few classes) in the image.", "They argue that they want p(y) to have high entropy, i.e. the predicted classes (and therefore image contents) should have high diversity.", "(This seems like something that is quite a bit dependend on the used dataset?)", "They combine both measurements to the final score of exp(KL(p(y|x) || p(y))) = exp( <sum over images> p(y|xi) * (log(p(y|xi)) - log(p(y))) ).", "p(y) can be approximated as the mean of the softmax-outputs over many examples.", "Relevant python code that they use (where part seems to be of shape (batch size, number of classes), i.e. the softmax outputs): kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0))); kl = np.mean(np.sum(kl, 1)); scores.append(np.exp(kl));  They average this score over 50,000 generated images.", "Semi-supervised Learning  For a dataset with K classes they extend D by K outputs (leading to K+1 outputs total).", "They then optimize two loss functions jointly:  Unsupervised loss: The classic GAN loss, i.e. D has to predict the fake/real output correctly.", "(The other outputs seem to not influence this loss.)", "Supervised loss: D must correctly predict the image's class label, if it happens to be a real image and if it was annotated with a class.", "They note that training G with feature matching produces the best results for semi-supervised classification.", "They note that training G with minibatch discrimination produces significantly worse results for semi-supervised classification.", "(But visually the samples look better.)", "They note that using semi-supervised learning overall results in higher image quality than not using it.", "They speculate that this has to do with the class labels containing information about image statistics that are important to humans.", "Results  MNIST  They use weight normalization and white noise in D.  Samples of high visual quality when using minibatch discrimination with semi-supervised learning.", "Very good results in semi-supervised learning when using feature matching.", "Using feature matching decreases visual quality of generated images, but improves results of semi-supervised learning.", "CIFAR-10  D: 9-layer CNN with dropout, weight normalization.", "G: 4-layer CNN with batch normalization (so no VBN?).", "Visually very good generated samples when using minibatch discrimination with semi-supervised learning.", "(Probably new record quality.)", "Note: No comparison with nearest neighbours from the dataset.", "When using feature matching the results are visually not as good.", "Again, very good results in semi-supervised learning when using feature matching.", "SVHN  Same setup as in CIFAR-10 and similar results.", "ImageNet  They tried to generate 128x128 images and compared to DCGAN.", "They improved from \"total garbage\" to \"garbage\" (they now hit some textures, but structure is still wildly off).", "Generated CIFAR-10-like images (with minibatch discrimination and semi-supervised learning)."], "summary_text": "What  They suggest some small changes to the GAN training scheme that lead to visually improved results. They suggest a new scoring method to compare the results of different GAN models with each other. How  Feature Matching  Usually G would be trained to mislead D as often as possible, i.e. to maximize D's output. Now they train G to minimize the feature distance between real and fake images. I.e. they do:  Pick a layer l from D.  Forward real images through D and extract the features from layer l.  Forward fake images through D and extract the features from layer l.  Compute the squared euclidean distance between the layers and backpropagate. Minibatch discrimination  They allow D to look at multiple images in the same minibatch. That is, they feed the features (of each image) extracted by an intermediate layer of D through a linear operation, resulting in a matrix per image. They then compute the L1-distances between these matrices. They then let D make its judgement (fake/real image) based on the features extracted from the image and these distances. They add this mechanism so that the diversity of images generated by G increases (which should also prevent collapses). Historical averaging  They add a penalty term that punishes weights which are rather far away from their historical average values. I.e. the cost is distance(current parameters, average of parameters over the last t batches). They argue that this can help the network to find equilibria that normal gradient descent would not find. One-sided label smoothing  Usually one would use the labels 0 (image is fake) and 1 (image is real). Using smoother labels (0.1 and 0.9) seems to make networks more resistent to adversarial examples. * So they smooth the labels of real images (apparently to 0.9?). Smoothing the labels of fake images would lead to (mathematical) problems in some cases, so they keep these at 0. Virtual Batch Normalization (VBN)  Usually BN normalizes each example with respect to the other examples in the same batch. They instead normalize each example with respect to the examples in a reference batch, which was picked once at the start of the training. VBN is intended to reduce the dependence of each example on the other examples in the batch. VBN is computationally expensive, because it requires forwarding of two minibatches. They use VBN for their G.  Inception Scoring  They introduce a new scoring method for GAN results. Their method is based on feeding the generated images through another network, here they use Inception. For an image x and predicted classes y (softmax-output of Inception):  They argue that they want p(y|x) to have low entropy, i.e. the model should be rather certain of seeing a class (or few classes) in the image. They argue that they want p(y) to have high entropy, i.e. the predicted classes (and therefore image contents) should have high diversity. (This seems like something that is quite a bit dependend on the used dataset?) They combine both measurements to the final score of exp(KL(p(y|x) || p(y))) = exp( <sum over images> p(y|xi) * (log(p(y|xi)) - log(p(y))) ). p(y) can be approximated as the mean of the softmax-outputs over many examples. Relevant python code that they use (where part seems to be of shape (batch size, number of classes), i.e. the softmax outputs): kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0))); kl = np.mean(np.sum(kl, 1)); scores.append(np.exp(kl));  They average this score over 50,000 generated images. Semi-supervised Learning  For a dataset with K classes they extend D by K outputs (leading to K+1 outputs total). They then optimize two loss functions jointly:  Unsupervised loss: The classic GAN loss, i.e. D has to predict the fake/real output correctly. (The other outputs seem to not influence this loss.) Supervised loss: D must correctly predict the image's class label, if it happens to be a real image and if it was annotated with a class. They note that training G with feature matching produces the best results for semi-supervised classification. They note that training G with minibatch discrimination produces significantly worse results for semi-supervised classification. (But visually the samples look better.) They note that using semi-supervised learning overall results in higher image quality than not using it. They speculate that this has to do with the class labels containing information about image statistics that are important to humans. Results  MNIST  They use weight normalization and white noise in D.  Samples of high visual quality when using minibatch discrimination with semi-supervised learning. Very good results in semi-supervised learning when using feature matching. Using feature matching decreases visual quality of generated images, but improves results of semi-supervised learning. CIFAR-10  D: 9-layer CNN with dropout, weight normalization. G: 4-layer CNN with batch normalization (so no VBN?). Visually very good generated samples when using minibatch discrimination with semi-supervised learning. (Probably new record quality.) Note: No comparison with nearest neighbours from the dataset. When using feature matching the results are visually not as good. Again, very good results in semi-supervised learning when using feature matching. SVHN  Same setup as in CIFAR-10 and similar results. ImageNet  They tried to generate 128x128 images and compared to DCGAN. They improved from \"total garbage\" to \"garbage\" (they now hit some textures, but structure is still wildly off). Generated CIFAR-10-like images (with minibatch discrimination and semi-supervised learning).", "pdf_url": "https://arxiv.org/pdf/1606.03498", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/improved_techniques_for_training_gans.json"}
{"id": "61580548", "bin": "1000_1100", "summary_sentences": ["Parameter inefficiency, in the context of transfer learning for NLP, arises when an entirely new model needs to be trained for every downstream task and the number of parameters grows too large.", "A recent paper proposes adapter modules which provide parameter efficiency by only adding a few trainable parameters per task, and as new tasks are added previous ones don’t require revisiting.", "The main idea of this paper is to enable transfer learning for NLP on an incoming stream of tasks without training a new model for every new task.", "A standard fine-tuning model copies weights from a pre-trained network and tunes them on a downstream task which requires a new set of weights for each task.", "In other words, the parameters are adjusted together with new layers for each task.", "Fine tuning is advantageous in that it could be more parameter efficient if lower layers of the network are shared between tasks.", "Adapters  The proposed adapter model adds new modules between layers of a pre-trained network called adapters.", "This means that parameters are copied over from pre-training (meaning they remain fixed) and only a few additional task-specific parameters are added for each new task, all without affecting previous ones.", "The innovation here is in the strategy that is used to design the adapter module to achieve parameter efficiency with one single model while not compromising performance.", "In fact, a simple model was compared with a fully fine-tuned BERT model on several text classification tasks.", "The findings show that only 3% of task-specific parameters are needed to almost match the results of the 100% task-specific parameters used by the fully fine-tuned model.", "The traditional way of fine-tuning involves: 1) adding a new layer to fit the targets specified in the downstream task, and 2) co-training the new layer with the original weights.", "In contrast, the adapter tuning strategy injects new layers (randomly initialized) into the original network.", "Parameter sharing between tasks is supported since the original network’s parameters are frozen.", "Keep in mind that only a small number of parameters are introduced in the proposed adapter-based tuning architecture, with the intention of keeping the original network unaffected and the training stable.", "The approach is simply to initialize the adapters to a near-identity function so that it can influence the distribution of activations (which can also be opted out) while training.", "The adapter-based tuning is used with the popular Transformers, which are known to achieve state-of-the-art (SoTA) performance in many NLP tasks such as machine translation and text classification problems.", "The architecture is shown in the figure below:  As you can see in the left of the figure, the standard Transformer is used with an additional adapter layer, added after each sub-layer and before adding the skip connection back.", "The output of the adapter layer is then forwarded to the layer normalization.", "The adapters project the original feature size to a smaller dimension and then projects them to the original size thereafter, ensuring that the number of parameters stays substantially small as compared to the original model (procedure shown on the right of the figure).", "With the reduction of parameters, there is an obvious trade-off between performance and parameter efficiency which is discussed in the experiments below.", "Experiments  The resulting effect of the added adapter layers is that they allow the model to focus on the higher layers of the network, which has generally been found effective in transfer learning.", "The adapter-based model uses a training procedure similar to BERT for fair comparison (see more details in the paper).", "On the GLUE benchmark, adapters achieve a mean GLUE score of 80.0 (with 1.3 times the number of parameters of the pretrained model), compared to 80.4 achieved by a BERT-LARGE (with 9 times the number of parameters of the pretrained model).", "See the detailed results in the table below.", "To further validate the performance of the adapters, other text classification tasks are used to test the effect of parameter efficiency (see the experimental setup in the paper).", "Overall, for all the tasks, adapters perform similar to full fine-tuning, variable fine-tuning (some layers are frozen), and baseline (which uses hyperparameter search), while substantially reduces the number of parameters used.", "See the table below for detailed results:  The figure below shows the parameter/performance trade-off aggregated over all the tasks used in the experiments above.", "For the GLUE benchmark in particular (see left of the figure), the fewer parameters are used in standard fine-tuning the lower the performance.", "Adapters were observed to yield stable performances even when substantially fewer parameters are used.", "See other results in the paper related to the performance/parameter trade-off.", "The adapters were also tested on SQuAD, which involves a question answering task.", "As shown in the figure below, adapters consistently produce comparable performance (on F1 score) to full fine-tuning, while training with substantially fewer parameters.", "An extended ablation study is discussed in the paper in which the authors further investigated the robustness and extensibility of the adapters.", "In summary, adapter-based tuning demonstrates comparable performance to full fine-tuning while at the same time maintaining high parameter-efficiency.", "Parameter-Efficient Transfer Learning for NLP —(Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly)  Paper  Other suggested readings:  Deep Learning for NLP: A Complete Overview  A Light Introduction to Transfer Learning for NLP  XLNet outperforms BERT on several NLP Tasks  A Light Introduction to Transformer-XL"], "summary_text": "Parameter inefficiency, in the context of transfer learning for NLP, arises when an entirely new model needs to be trained for every downstream task and the number of parameters grows too large. A recent paper proposes adapter modules which provide parameter efficiency by only adding a few trainable parameters per task, and as new tasks are added previous ones don’t require revisiting. The main idea of this paper is to enable transfer learning for NLP on an incoming stream of tasks without training a new model for every new task. A standard fine-tuning model copies weights from a pre-trained network and tunes them on a downstream task which requires a new set of weights for each task. In other words, the parameters are adjusted together with new layers for each task. Fine tuning is advantageous in that it could be more parameter efficient if lower layers of the network are shared between tasks. Adapters  The proposed adapter model adds new modules between layers of a pre-trained network called adapters. This means that parameters are copied over from pre-training (meaning they remain fixed) and only a few additional task-specific parameters are added for each new task, all without affecting previous ones. The innovation here is in the strategy that is used to design the adapter module to achieve parameter efficiency with one single model while not compromising performance. In fact, a simple model was compared with a fully fine-tuned BERT model on several text classification tasks. The findings show that only 3% of task-specific parameters are needed to almost match the results of the 100% task-specific parameters used by the fully fine-tuned model. The traditional way of fine-tuning involves: 1) adding a new layer to fit the targets specified in the downstream task, and 2) co-training the new layer with the original weights. In contrast, the adapter tuning strategy injects new layers (randomly initialized) into the original network. Parameter sharing between tasks is supported since the original network’s parameters are frozen. Keep in mind that only a small number of parameters are introduced in the proposed adapter-based tuning architecture, with the intention of keeping the original network unaffected and the training stable. The approach is simply to initialize the adapters to a near-identity function so that it can influence the distribution of activations (which can also be opted out) while training. The adapter-based tuning is used with the popular Transformers, which are known to achieve state-of-the-art (SoTA) performance in many NLP tasks such as machine translation and text classification problems. The architecture is shown in the figure below:  As you can see in the left of the figure, the standard Transformer is used with an additional adapter layer, added after each sub-layer and before adding the skip connection back. The output of the adapter layer is then forwarded to the layer normalization. The adapters project the original feature size to a smaller dimension and then projects them to the original size thereafter, ensuring that the number of parameters stays substantially small as compared to the original model (procedure shown on the right of the figure). With the reduction of parameters, there is an obvious trade-off between performance and parameter efficiency which is discussed in the experiments below. Experiments  The resulting effect of the added adapter layers is that they allow the model to focus on the higher layers of the network, which has generally been found effective in transfer learning. The adapter-based model uses a training procedure similar to BERT for fair comparison (see more details in the paper). On the GLUE benchmark, adapters achieve a mean GLUE score of 80.0 (with 1.3 times the number of parameters of the pretrained model), compared to 80.4 achieved by a BERT-LARGE (with 9 times the number of parameters of the pretrained model). See the detailed results in the table below. To further validate the performance of the adapters, other text classification tasks are used to test the effect of parameter efficiency (see the experimental setup in the paper). Overall, for all the tasks, adapters perform similar to full fine-tuning, variable fine-tuning (some layers are frozen), and baseline (which uses hyperparameter search), while substantially reduces the number of parameters used. See the table below for detailed results:  The figure below shows the parameter/performance trade-off aggregated over all the tasks used in the experiments above. For the GLUE benchmark in particular (see left of the figure), the fewer parameters are used in standard fine-tuning the lower the performance. Adapters were observed to yield stable performances even when substantially fewer parameters are used. See other results in the paper related to the performance/parameter trade-off. The adapters were also tested on SQuAD, which involves a question answering task. As shown in the figure below, adapters consistently produce comparable performance (on F1 score) to full fine-tuning, while training with substantially fewer parameters. An extended ablation study is discussed in the paper in which the authors further investigated the robustness and extensibility of the adapters. In summary, adapter-based tuning demonstrates comparable performance to full fine-tuning while at the same time maintaining high parameter-efficiency. Parameter-Efficient Transfer Learning for NLP —(Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly)  Paper  Other suggested readings:  Deep Learning for NLP: A Complete Overview  A Light Introduction to Transfer Learning for NLP  XLNet outperforms BERT on several NLP Tasks  A Light Introduction to Transformer-XL", "pdf_url": "https://arxiv.org/pdf/1902.00751", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/adapters-a-compact-and-extensible-transfer-learning-method-for-nlp-6d18c2399f62.json"}
{"id": "36200725", "bin": "1000_1100", "summary_sentences": ["What  They propose an object detector that fuses 2D information (from images) and 3D information (from LiDAR, i.e. point cloud) to predict 3D objects in birds eye view (BEV).", "Their method is based on projecting 2D images into point clouds (usually it's the other way round, i.e. point clouds are projected into 2D images).", "They propose a layer to perform the 2D-3D fusion at multiple image scales.", "The result is a fast and fairly accurate object detector.", "How  Basic Architecture  They feed the images through a ResNet18 branch (initialized via ImageNet pretraining).", "They feed the point cloud through a custom residual network.", "Input is the voxelized point cloud in BEV.", "The network is comparable to ResNet18.", "They use a total of 36 convolutions in five blocks, with a growing number of convolutions per block and downscaling at the start of each block.", "This network is initialized via Xavier initialization.", "They features extracted by the image branch are fused at multiple scales into the point cloud BEV branch using Continuous Fusion layers.", "The output of the point cloud BEV branch is comparable to many other object detectors:  Per spatial location classification (object class + background) and regression predictions (x, y, z and width, length, height).", "They use two anchor boxes, one for objects with a rotation of 0 degrees and one for objects with a rotation of 90 degrees.", "As usually, they apply Non-Maximum-Suppression to the resulting bounding boxes.", "Visualization:  Continuous Fusion Layer  The Continuous Fusion Layer fuses 2D (camera) and 3D (LiDAR) features.", "Its basic principle is to project 2D information into 3D.", "(Counterintuitively, it still performs first a projection from 3D to 2D.)", "As their network works in BEV, the fusion merges information from a dense 2D grid (camera) with another dense 2D grid (LiDAR in BEV) instead of a point cloud.", "Assume you have a LIDAR BEV feature map with one channel, i.e. a 2D grid.", "You want to merge information from the camera into one single spatial location (e.g. column 10, row 20) of the grid.", "The following steps are then performed:  For the given spatial location, find the K nearest neighbour points in the BEV point cloud.", "(E.g. find the K neighbours of y=10, x=20.)", "Re-add the z coordinate to each of these K nearest neighbours (projects them from BEV 3D back to full 3D).", "Project these 3D points onto the 2D image plane.", "This indicates which image areas might contain information relevant for the 3D BEV location.", "Use bilinear interpolation to estimate the image's features at the projected locations.", "For each location also estimate the offset vector from the BEV spatial location (e.g. y=10, x=20) to the 3D point (I guess in BEV?).", "Concat the feature vector and offset vector from step (4) to one vector.", "Then apply fully connected layers to that (in their case three layers).", "This generates a vector per projected point.", "Sum these vectors.", "Concat the resulting vector to the point cloud BEV feature map from step (1) at the chosen starting location (e.g. y=10, x=20).", "Visualization of the steps:  Other stuff  As is common, their classification loss is binary cross entropy and their regression loss is smooth L1.", "As is common, they encode the x-, y- and z-coordinates (predicted via the regression branch) is relative offsets to the anchor's center.", "As is common, they encode the widths, lengths and heights logarithmically, e.g. log(width/width_anchor).", "They use Adam without weight decay.", "For training on KITTI, they augment the dataset, e.g. via scaling, translation and rotation of the point cloud and camera image (matched to each other, so that projections from one sensor to the other remain sensible).", "They crop the point cloud to 70m in front of the ego vehicle and 40m to the left and right.", "They voxelize the point cloud to 512x448x32 voxels.", "(Sounds like they apply 2D convs to an input with 32 channels encoding height information.)", "Results  KITTI  Note: They only evaluate on the class \"car\", arguing that -- in the case of their model -- there is not enough training data for the other classes in KITTI.", "They achieve accuracies roughly comparable with other well-performing 3D object detectors.", "(Though not state of the art.)", "Their detector runs at about 15fps or 66ms per scene, beating most other models.", "They are roughly twice as fast as AVOD-FPN, but quite a bit less accurate (~5 percentage points for moderate 3D AP).", "Using only LiDAR (and not images) as input significantly worsens results.", "Using a fusion strategy similar in which only the BEV locations closest to 3D points are projected to the image plane (no K-NN) and without using offset vectors significantly worsens results.", "Removing the offset vector significantly worsens results.", "Picking K>1 for the K-nearest-neighbours search does not produce better results than K=1.", "Limiting the maximum distance during the K-nearest-neighbour search to 10m has only a tiny positive impact on AP.", "Ablation results (\"geometric feature\" is the offset vector):  TOR4D (Uber dataset)  They evaluate here on more classes and increase the forward distance to 100m.", "They decrease the parameters in the network to keep the time per scene roughly comparable (despite increase in max forward distance).", "They evaluate the achieved AP by class and distance:"], "summary_text": "What  They propose an object detector that fuses 2D information (from images) and 3D information (from LiDAR, i.e. point cloud) to predict 3D objects in birds eye view (BEV). Their method is based on projecting 2D images into point clouds (usually it's the other way round, i.e. point clouds are projected into 2D images). They propose a layer to perform the 2D-3D fusion at multiple image scales. The result is a fast and fairly accurate object detector. How  Basic Architecture  They feed the images through a ResNet18 branch (initialized via ImageNet pretraining). They feed the point cloud through a custom residual network. Input is the voxelized point cloud in BEV. The network is comparable to ResNet18. They use a total of 36 convolutions in five blocks, with a growing number of convolutions per block and downscaling at the start of each block. This network is initialized via Xavier initialization. They features extracted by the image branch are fused at multiple scales into the point cloud BEV branch using Continuous Fusion layers. The output of the point cloud BEV branch is comparable to many other object detectors:  Per spatial location classification (object class + background) and regression predictions (x, y, z and width, length, height). They use two anchor boxes, one for objects with a rotation of 0 degrees and one for objects with a rotation of 90 degrees. As usually, they apply Non-Maximum-Suppression to the resulting bounding boxes. Visualization:  Continuous Fusion Layer  The Continuous Fusion Layer fuses 2D (camera) and 3D (LiDAR) features. Its basic principle is to project 2D information into 3D. (Counterintuitively, it still performs first a projection from 3D to 2D.) As their network works in BEV, the fusion merges information from a dense 2D grid (camera) with another dense 2D grid (LiDAR in BEV) instead of a point cloud. Assume you have a LIDAR BEV feature map with one channel, i.e. a 2D grid. You want to merge information from the camera into one single spatial location (e.g. column 10, row 20) of the grid. The following steps are then performed:  For the given spatial location, find the K nearest neighbour points in the BEV point cloud. (E.g. find the K neighbours of y=10, x=20.) Re-add the z coordinate to each of these K nearest neighbours (projects them from BEV 3D back to full 3D). Project these 3D points onto the 2D image plane. This indicates which image areas might contain information relevant for the 3D BEV location. Use bilinear interpolation to estimate the image's features at the projected locations. For each location also estimate the offset vector from the BEV spatial location (e.g. y=10, x=20) to the 3D point (I guess in BEV?). Concat the feature vector and offset vector from step (4) to one vector. Then apply fully connected layers to that (in their case three layers). This generates a vector per projected point. Sum these vectors. Concat the resulting vector to the point cloud BEV feature map from step (1) at the chosen starting location (e.g. y=10, x=20). Visualization of the steps:  Other stuff  As is common, their classification loss is binary cross entropy and their regression loss is smooth L1. As is common, they encode the x-, y- and z-coordinates (predicted via the regression branch) is relative offsets to the anchor's center. As is common, they encode the widths, lengths and heights logarithmically, e.g. log(width/width_anchor). They use Adam without weight decay. For training on KITTI, they augment the dataset, e.g. via scaling, translation and rotation of the point cloud and camera image (matched to each other, so that projections from one sensor to the other remain sensible). They crop the point cloud to 70m in front of the ego vehicle and 40m to the left and right. They voxelize the point cloud to 512x448x32 voxels. (Sounds like they apply 2D convs to an input with 32 channels encoding height information.) Results  KITTI  Note: They only evaluate on the class \"car\", arguing that -- in the case of their model -- there is not enough training data for the other classes in KITTI. They achieve accuracies roughly comparable with other well-performing 3D object detectors. (Though not state of the art.) Their detector runs at about 15fps or 66ms per scene, beating most other models. They are roughly twice as fast as AVOD-FPN, but quite a bit less accurate (~5 percentage points for moderate 3D AP). Using only LiDAR (and not images) as input significantly worsens results. Using a fusion strategy similar in which only the BEV locations closest to 3D points are projected to the image plane (no K-NN) and without using offset vectors significantly worsens results. Removing the offset vector significantly worsens results. Picking K>1 for the K-nearest-neighbours search does not produce better results than K=1. Limiting the maximum distance during the K-nearest-neighbour search to 10m has only a tiny positive impact on AP. Ablation results (\"geometric feature\" is the offset vector):  TOR4D (Uber dataset)  They evaluate here on more classes and increase the forward distance to 100m. They decrease the parameters in the network to keep the time per scene roughly comparable (despite increase in max forward distance). They evaluate the achieved AP by class and distance:", "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/deep_continuous_fusion_for_multi-sensor_3d_object_detection.json"}
{"id": "11354749", "bin": "1000_1100", "summary_sentences": ["How to memorize a random 60-bit string – Ghazvininejad et al. 2105  A bit of fun for today – this paper has been the source of many articles around the net over the last couple of weeks (though not many have dug into the actual algorithms… ).", "Inspired by an XKCD cartoon , the challenge is to convert a randomly generated 60-bit string into a memorable sequence of English words that can deterministically be used to recover the bit string and hence be used as a password.", "Including the XKCD algorithm as a baseline, the authors develop four additional approaches and evaluate them all for ease of human memorisation.", "The XKCD and poetry methods perform the best under this test, so let’s look at those two in more details.", "60-bit XKCD-inspired passwords  XKCD issue #936  Start with a randomly chosen 60-bit password.", "Divide this up into four 15-bit segments, and use each as an index into a 32,768 word dictionary.", "(The XKCD original used a 44-bit password, 11-bit segments, and a 2048 word dictionary).", "This produces passwords such as ‘fees wesley inmate decentralization,’ ‘photos bros nan plain,’ and ’embarass debating gaskell jennie.’  These passwords are nonsensical, but have the advantage of only being four words long.", "Three variations are tried (First-letter mnemonic, All-letter method, and Frequency method) that encode bits or bit-sequences as letters and then generate sentences from them.", "I particularly like the All-letter-method that pairs English words with bit-phrases and then uses the Moses machine translation toolkit to search for the 1-best translation of the 60-bit input string, using this phrase table and a 5-gram English language model.", "Ingenious, and it produces great phrases – for example, “Fox news networks are seeking views from downtown streets.” Users ultimately found these sentences harder to remember though due to their length.", "Poetry-based passwords  My first original poetry contribution so far in The Morning Paper:  If I can make my password rhyme, Then learning it takes much less time.", "Don’t worry, I suspect it will also be my last ;).", "In ancient times, people recorded long, historical epics using poetry, to enhance memorability.", "We follow this idea by turning each system-assigned 60-bit string into a short, distinct English poem.", "Our format is the rhyming iambic tetrameter couplet.", "(Two lines of eight syllables, stress pattern 01010101, and lines ending in a pair of rhyming words.)", "So now all you have to do is find an algorithm that generates memorable rhyming iambic tetrameter couplets that correspond to a unique 60-bit code!", "The authors take this daunting sounding challenge in their stride.", "First they create a Finite State Transducer that translates English words into sequences capturing their essential properties.", "Finite State Transducers (FSTs) are quite common in language processing.", "An FST is a finite-state automaton that produces an output tape as well as reading from its input tape.", "From the wikipedia page :  The two tapes of a transducer are typically viewed as an input tape and an output tape.", "On this view, a transducer is said to transduce (i.e. translate) the contents of its input tape to its output tape, by accepting a string on its input tape and generating another string on its output tape.", "It may do so non-deterministically, and it may produce more than one output for each input string.", "A transducer may also produce no output for a given input string, in which case it is said to reject the input.", "In general, a transducer computes a relation between two formal languages.", "For our purposes though, we can simply think of this stage as mapping each English word to a set of encodings.", "For the sample word ‘create,’ these encodings would be:  0 1         // cre-ATE  0 1 EY-T    // cre-ATE at end of a line, EY-T rhyming pattern 1r 0r       // cre-ATE in the second line (r for reverse) EY-T 1r 0r  // cre-ATE at end of the second line (r for reverse)  A Finite State Acceptor is constructed with a ‘path’ for each legal poem.", "This only accepts sequences of the form:  0 1 0 1 0 1 0 1 X X 1r 0r 1r 0r 1r 0r 1r 0r  (The second line is generated in reverse order so that rhyming can be enforced locally – X stands for any rhyme pattern, e.g. EY-T).", "It remains to map an arbitrary 60-bit string onto a path in the FSA.", "Let k be the integer representation of the 60-bit string.", "If the FSA contains exactly 260 paths, we can easily select the kth path using the following method.", "At each node N of the FSA, we store the total number of paths from N to the final state—this takes linear time if we visit states in reverse topological order.", "We then traverse the FSA deterministically from the start state, using k to guide the path selection.", "The generated FSA actually contains 279 paths, giving more than a million poem choices for each 60-bit string.", "This gives opportunity to use the 5-gram language model again to output the best one:  More precisely, given a 60-bit input string k, we extract not only the kth FSA path, but also the k + i · 260 paths, with i ranging from 1 to 999,999.", "We explicitly list out these paths, reversing the second half of each, and score them with our 5-gram LM.", "We output the poem with the 1-best LM score.", "For example:  Diversity inside replied, retreats or colours justified  To reconstruct the original 60-bit string k, we first find the FSA path corresponding to the user-recalled English string (with second half reversed).", "We use depth-first search to find this path.", "Once we have the path, it is easy to determine which numbered path it is, lexicographically speaking, using the node-labeling scheme above to recover k."], "summary_text": "How to memorize a random 60-bit string – Ghazvininejad et al. 2105  A bit of fun for today – this paper has been the source of many articles around the net over the last couple of weeks (though not many have dug into the actual algorithms… ). Inspired by an XKCD cartoon , the challenge is to convert a randomly generated 60-bit string into a memorable sequence of English words that can deterministically be used to recover the bit string and hence be used as a password. Including the XKCD algorithm as a baseline, the authors develop four additional approaches and evaluate them all for ease of human memorisation. The XKCD and poetry methods perform the best under this test, so let’s look at those two in more details. 60-bit XKCD-inspired passwords  XKCD issue #936  Start with a randomly chosen 60-bit password. Divide this up into four 15-bit segments, and use each as an index into a 32,768 word dictionary. (The XKCD original used a 44-bit password, 11-bit segments, and a 2048 word dictionary). This produces passwords such as ‘fees wesley inmate decentralization,’ ‘photos bros nan plain,’ and ’embarass debating gaskell jennie.’  These passwords are nonsensical, but have the advantage of only being four words long. Three variations are tried (First-letter mnemonic, All-letter method, and Frequency method) that encode bits or bit-sequences as letters and then generate sentences from them. I particularly like the All-letter-method that pairs English words with bit-phrases and then uses the Moses machine translation toolkit to search for the 1-best translation of the 60-bit input string, using this phrase table and a 5-gram English language model. Ingenious, and it produces great phrases – for example, “Fox news networks are seeking views from downtown streets.” Users ultimately found these sentences harder to remember though due to their length. Poetry-based passwords  My first original poetry contribution so far in The Morning Paper:  If I can make my password rhyme, Then learning it takes much less time. Don’t worry, I suspect it will also be my last ;). In ancient times, people recorded long, historical epics using poetry, to enhance memorability. We follow this idea by turning each system-assigned 60-bit string into a short, distinct English poem. Our format is the rhyming iambic tetrameter couplet. (Two lines of eight syllables, stress pattern 01010101, and lines ending in a pair of rhyming words.) So now all you have to do is find an algorithm that generates memorable rhyming iambic tetrameter couplets that correspond to a unique 60-bit code! The authors take this daunting sounding challenge in their stride. First they create a Finite State Transducer that translates English words into sequences capturing their essential properties. Finite State Transducers (FSTs) are quite common in language processing. An FST is a finite-state automaton that produces an output tape as well as reading from its input tape. From the wikipedia page :  The two tapes of a transducer are typically viewed as an input tape and an output tape. On this view, a transducer is said to transduce (i.e. translate) the contents of its input tape to its output tape, by accepting a string on its input tape and generating another string on its output tape. It may do so non-deterministically, and it may produce more than one output for each input string. A transducer may also produce no output for a given input string, in which case it is said to reject the input. In general, a transducer computes a relation between two formal languages. For our purposes though, we can simply think of this stage as mapping each English word to a set of encodings. For the sample word ‘create,’ these encodings would be:  0 1         // cre-ATE  0 1 EY-T    // cre-ATE at end of a line, EY-T rhyming pattern 1r 0r       // cre-ATE in the second line (r for reverse) EY-T 1r 0r  // cre-ATE at end of the second line (r for reverse)  A Finite State Acceptor is constructed with a ‘path’ for each legal poem. This only accepts sequences of the form:  0 1 0 1 0 1 0 1 X X 1r 0r 1r 0r 1r 0r 1r 0r  (The second line is generated in reverse order so that rhyming can be enforced locally – X stands for any rhyme pattern, e.g. EY-T). It remains to map an arbitrary 60-bit string onto a path in the FSA. Let k be the integer representation of the 60-bit string. If the FSA contains exactly 260 paths, we can easily select the kth path using the following method. At each node N of the FSA, we store the total number of paths from N to the final state—this takes linear time if we visit states in reverse topological order. We then traverse the FSA deterministically from the start state, using k to guide the path selection. The generated FSA actually contains 279 paths, giving more than a million poem choices for each 60-bit string. This gives opportunity to use the 5-gram language model again to output the best one:  More precisely, given a 60-bit input string k, we extract not only the kth FSA path, but also the k + i · 260 paths, with i ranging from 1 to 999,999. We explicitly list out these paths, reversing the second half of each, and score them with our 5-gram LM. We output the poem with the 1-best LM score. For example:  Diversity inside replied, retreats or colours justified  To reconstruct the original 60-bit string k, we first find the FSA path corresponding to the user-recalled English string (with second half reversed). We use depth-first search to find this path. Once we have the path, it is easy to determine which numbered path it is, lexicographically speaking, using the node-labeling scheme above to recover k.", "pdf_url": "http://www.isi.edu/natural-language/mt/memorize-random-60.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/how-to-memorize-a-random-60-bit-string.json"}
{"id": "57956236", "bin": "1000_1100", "summary_sentences": ["What  They suggest a single architecture that tries to solve the following tasks:  Face localization (\"Where are faces in the image?\")", "Face landmark localization (\"For a given face, where are its landmarks, e.g. eyes, nose and mouth?\")", "Face landmark visibility estimation (\"For a given face, which of its landmarks are actually visible and which of them are occluded by other objects/people?\")", "Face roll, pitch and yaw estimation (\"For a given face, what is its rotation on the x/y/z-axis?\")", "Face gender estimation (\"For a given face, which gender does the person have?\")", "How  Pretraining the base model  They start with a basic model following the architecture of AlexNet.", "They train that model to classify whether the input images are faces or not faces.", "They then remove the fully connected layers, leaving only the convolutional layers.", "Locating bounding boxes of face candidates  They then use a selective search and segmentation algorithm on images to extract bounding boxes of objects.", "Each bounding box is considered a possible face.", "Each bounding box is rescaled to 227x227.", "Feature extraction per face candidate  They feed each bounding box through the above mentioned pretrained network.", "They extract the activations of the network from the layers max1 (27x27x96), conv3 (13x13x384) and pool5 (6x6x256).", "They apply to the first two extracted tensors (from max1, conv3) convolutions so that their tensor shapes are reduced to 6x6xC.", "They concatenate the three tensors to a 6x6x768 tensor.", "They apply a 1x1 convolution to that tensor to reduce it to 6x6x192.", "They feed the result through a fully connected layer resulting in 3072-dimensional vectors (per face candidate).", "Classification and regression  They feed each 3072-dimensional vector through 5 separate networks:  Detection: Does the bounding box contain a face or no face.", "(2 outputs, i.e. yes/no)  Landmark Localization: What are the coordinates of landmark features (e.g.", "mouth, nose, ...).", "(21 landmarks, each 2 values for x/y = 42 outputs total)  Landmark Visibility: Which landmarks are visible.", "(21 yes/no outputs)  Pose estimation: Roll, pitch, yaw of the face.", "(3 outputs)  Gender estimation: Male/female face.", "(2 outputs)  Each of these network contains a single fully connected layer with 512 nodes, followed by the output layer with the above mentioned number of nodes.", "Architecture Visualization:  Training  The base model is trained once (see above).", "The feature extraction layers and the five classification/regression networks are trained afterwards (jointly).", "The loss functions for the five networks are:  Detection: BCE (binary cross-entropy).", "Detected bounding boxes that have an overlap >=0.5 with an annotated face are considered positive samples, bounding boxes with overlap <0.35 are considered negative samples, everything in between is ignored.", "Landmark localization: Roughly MSE (mean squared error), with some weighting for visibility.", "Only bounding boxes with overlap >0.35 are considered.", "Coordinates are normalized with respect to the bounding boxes center, width and height.", "Landmark visibility: MSE (predicted visibility factor vs. expected visibility factor).", "Only for bounding boxes with overlap >0.35.", "Pose estimation: MSE.", "Gender estimation: BCE.", "Testing  They use two postprocessing methods for detected faces:  Iterative Region Proposals:  They localize landmarks per face region.", "Then they compute a more appropriate face bounding box based on the localized landmarks.", "They feed that new bounding box through the network.", "They compute the face score (face / not face, i.e. number between 0 and 1) for both bounding boxes and choose the one with the higher score.", "This shrinks down bounding boxes that turned out to be too big.", "The method visualized:  Landmarks-based Non-Maximum Suppression:  When multiple detected face bounding boxes overlap, one has to choose which of them to keep.", "A method to do that is to only keep the bounding box with the highest face-score.", "They instead use a median-of-k method.", "Their steps are:  Reduce every box in size so that it is a bounding box around the localized landmarks.", "For every box, find all bounding boxes with a certain amount of overlap.", "Among these bounding boxes, select the k ones with highest face score.", "Based on these boxes, create a new box which's size is derived from the median coordinates of the landmarks.", "Compute the median values for landmark coordinates, landmark visibility, gender, pose and use it as the respective values for the new box.", "Results  Example results:  They test on AFW, AFWL, PASCAL, FDDB, CelebA.", "They achieve the best mean average precision values on PASCAL and AFW (compared to selected competitors).", "AFW results visualized:  Their approach achieve good performance on FDDB.", "It has some problems with small and/or blurry faces.", "If the feature fusion is removed from their approach (i.e. extracting features only from one fully connected layer at the end of the base network instead of merging feature maps from different convolutional layers), the accuracy of the predictions goes down.", "Their architecture ends in 5 shallow networks and shares many layers before them.", "If instead these networks share no or few layers, the accuracy of the predictions goes down.", "The postprocessing of bounding boxes (via Iterative Region Proposals and Landmarks-based Non-Maximum Suppression) has a quite significant influence on the performance.", "Processing time per image is 3s, of which 2s is the selective search algorithm (for the bounding boxes)."], "summary_text": "What  They suggest a single architecture that tries to solve the following tasks:  Face localization (\"Where are faces in the image?\") Face landmark localization (\"For a given face, where are its landmarks, e.g. eyes, nose and mouth?\") Face landmark visibility estimation (\"For a given face, which of its landmarks are actually visible and which of them are occluded by other objects/people?\") Face roll, pitch and yaw estimation (\"For a given face, what is its rotation on the x/y/z-axis?\") Face gender estimation (\"For a given face, which gender does the person have?\") How  Pretraining the base model  They start with a basic model following the architecture of AlexNet. They train that model to classify whether the input images are faces or not faces. They then remove the fully connected layers, leaving only the convolutional layers. Locating bounding boxes of face candidates  They then use a selective search and segmentation algorithm on images to extract bounding boxes of objects. Each bounding box is considered a possible face. Each bounding box is rescaled to 227x227. Feature extraction per face candidate  They feed each bounding box through the above mentioned pretrained network. They extract the activations of the network from the layers max1 (27x27x96), conv3 (13x13x384) and pool5 (6x6x256). They apply to the first two extracted tensors (from max1, conv3) convolutions so that their tensor shapes are reduced to 6x6xC. They concatenate the three tensors to a 6x6x768 tensor. They apply a 1x1 convolution to that tensor to reduce it to 6x6x192. They feed the result through a fully connected layer resulting in 3072-dimensional vectors (per face candidate). Classification and regression  They feed each 3072-dimensional vector through 5 separate networks:  Detection: Does the bounding box contain a face or no face. (2 outputs, i.e. yes/no)  Landmark Localization: What are the coordinates of landmark features (e.g. mouth, nose, ...). (21 landmarks, each 2 values for x/y = 42 outputs total)  Landmark Visibility: Which landmarks are visible. (21 yes/no outputs)  Pose estimation: Roll, pitch, yaw of the face. (3 outputs)  Gender estimation: Male/female face. (2 outputs)  Each of these network contains a single fully connected layer with 512 nodes, followed by the output layer with the above mentioned number of nodes. Architecture Visualization:  Training  The base model is trained once (see above). The feature extraction layers and the five classification/regression networks are trained afterwards (jointly). The loss functions for the five networks are:  Detection: BCE (binary cross-entropy). Detected bounding boxes that have an overlap >=0.5 with an annotated face are considered positive samples, bounding boxes with overlap <0.35 are considered negative samples, everything in between is ignored. Landmark localization: Roughly MSE (mean squared error), with some weighting for visibility. Only bounding boxes with overlap >0.35 are considered. Coordinates are normalized with respect to the bounding boxes center, width and height. Landmark visibility: MSE (predicted visibility factor vs. expected visibility factor). Only for bounding boxes with overlap >0.35. Pose estimation: MSE. Gender estimation: BCE. Testing  They use two postprocessing methods for detected faces:  Iterative Region Proposals:  They localize landmarks per face region. Then they compute a more appropriate face bounding box based on the localized landmarks. They feed that new bounding box through the network. They compute the face score (face / not face, i.e. number between 0 and 1) for both bounding boxes and choose the one with the higher score. This shrinks down bounding boxes that turned out to be too big. The method visualized:  Landmarks-based Non-Maximum Suppression:  When multiple detected face bounding boxes overlap, one has to choose which of them to keep. A method to do that is to only keep the bounding box with the highest face-score. They instead use a median-of-k method. Their steps are:  Reduce every box in size so that it is a bounding box around the localized landmarks. For every box, find all bounding boxes with a certain amount of overlap. Among these bounding boxes, select the k ones with highest face score. Based on these boxes, create a new box which's size is derived from the median coordinates of the landmarks. Compute the median values for landmark coordinates, landmark visibility, gender, pose and use it as the respective values for the new box. Results  Example results:  They test on AFW, AFWL, PASCAL, FDDB, CelebA. They achieve the best mean average precision values on PASCAL and AFW (compared to selected competitors). AFW results visualized:  Their approach achieve good performance on FDDB. It has some problems with small and/or blurry faces. If the feature fusion is removed from their approach (i.e. extracting features only from one fully connected layer at the end of the base network instead of merging feature maps from different convolutional layers), the accuracy of the predictions goes down. Their architecture ends in 5 shallow networks and shares many layers before them. If instead these networks share no or few layers, the accuracy of the predictions goes down. The postprocessing of bounding boxes (via Iterative Region Proposals and Landmarks-based Non-Maximum Suppression) has a quite significant influence on the performance. Processing time per image is 3s, of which 2s is the selective search algorithm (for the bounding boxes).", "pdf_url": "http://arxiv.org/pdf/1603.01249v2", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/hyperface.json"}
{"id": "59277766", "bin": "1000_1100", "summary_sentences": ["A Neural Conversation Model Vinyals & Le, ICML 2015  What happens if you build a bot that is trained on conversational data, and only conversational data: no programmed understanding of the domain at all, just lots and lots of sample conversations…?", "Building on the sequence to sequence technique that we looked at previously, this is exactly what Vinyals & Le set out to find out.", "The main advantage of this (sequence-to-sequence) framework is that it requires little feature engineering and domain specificity whilst matching or surpassing state-of-the-art-results [ in tasks such as machine translation ].", "This advance, in our opinion, allows researchers to work on tasks for which domain knowledge may not be readily available, or for tasks which are simply too hard to design rules manually.", "Conversational modeling can benefit from this formulation because it requires mapping between queries and responses.", "Due to the complexity of this mapping, conversational modeling has previously been designed to be very narrow in domain, with a major undertaking on feature engineering.", "The traditional approach to building bots requires a ‘rather complicated pipeline’ of many stages.", "In contrast, the neural conversational model is a end-to-end approach to the problem which lacks any domain knowledge.", "The surprising thing, is just how well it works.", "We’ll see some example conversations soon.", "Of course the bot has no understanding of what the conversation means.", "Or maybe it does have some kind of understanding (just like a word vector seems to capture some understanding of the meaning of a word), but it’s not an understanding we can tap into.", "Therefore you can’t use this approach to incorporate external knowledge in replies, or to extract intent and take actions etc..", "Even so, a bot trained on an IT helpdesk troubleshooting dataset is able to genuinely help people.", "Assuming you’re familiar with how sequence-to-sequence learning works, the whole neural conversational model can be explained in one short paragraph.", "During training, the conversation so far is used as the input sequence, and the next response is the target output sequence.", "Concretely, suppose we observe a conversation with two turns: the first person utters “ABC”, and the second person replies “WXYZ”.", "We can use a recurrent neural network and train to map “ABC” to “WXYZ” as show in Figure 1 [below].", "The hidden state of the model when it receives the end of sequence symbol  can be viewed as the thought vector because it stores the information of the sentence, or thought, “ABC”.", "When conversing, the conversation so far is fed into the input sequence, and the bot responds with the generated output sequence.", "The approach was tested with an IT helpdesk dataset (30M tokens, with typical interactions being 400 words long and turn-taking clearly signaled), and the Open Subtitles dataset of sentences uttered by characters in movies (62M training sentences, 26M validation sentences).", "Training with the IT helpdesk dataset was done with a single layer LSTM with 1024 memory cells and the 20K most common words.", "Here’s an example conversation with the trained bot:  The OpenSubtitles experiment was performed with a two-layered LSTM with 4096 cells per layer and a 100K word vocabulary.", "“Our simple recurrent model does often produce plausible answers.”  We find it encouraging that the model can remember facts, understand contexts, and perform common sense reasoning without the complexity in traditional pipelines.", "What surprises us is that the model does so without any explicit knowledge representation component except for the parameters in the word vectors.", "Perhaps most practically significant is the fact that the model can generalize to new questions.", "In other words, it does not simply look up for an answer by matching the question with the existing database.", "In fact, most of the questions presented above, except for the first conversation, do not appear in the training set.", "Here’s an example general knowledge Q&A – I don’t think Watson has anything to worry about just yet!", "It’s interesting, but also far from something you’d want to rely on in a real chatbot service at this point in time.", "The model only gives simple and short answers to questions (which may be ‘probable’ answers according to the model, but aren’t always correct from our perspective).", "Perhaps a more problematic drawback is that the model does not capture a consistent personality.", "Indeed, if we ask not identical but semantically similar questions, the answers can sometimes be inconsistent:  So there you have it:  In this paper, we show that a simple language model based on the seq2seq framework can be used to train a conversational engine.", "Our modest results show that it can generate simple and basic conversations, and extract knowledge from a noisy but open-domain dataset.", "Even though the model has obvious limitations, it is surprising to us that a purely data driven approach without any rules can produce rather proper answers to many types of questions.", "However, the model may require substantial modifications to be able to deliver realistic conversations.", "My personal takeaway is that while it’s amazing how much can be done just with conversational data to learn from, any real service is going to need to some more complex logic wrapped around it.", "One possibility the authors hint at is that the neural conversational model could in principle be combined with other systems to re-score a short-list of candidate responses generated by other means."], "summary_text": "A Neural Conversation Model Vinyals & Le, ICML 2015  What happens if you build a bot that is trained on conversational data, and only conversational data: no programmed understanding of the domain at all, just lots and lots of sample conversations…? Building on the sequence to sequence technique that we looked at previously, this is exactly what Vinyals & Le set out to find out. The main advantage of this (sequence-to-sequence) framework is that it requires little feature engineering and domain specificity whilst matching or surpassing state-of-the-art-results [ in tasks such as machine translation ]. This advance, in our opinion, allows researchers to work on tasks for which domain knowledge may not be readily available, or for tasks which are simply too hard to design rules manually. Conversational modeling can benefit from this formulation because it requires mapping between queries and responses. Due to the complexity of this mapping, conversational modeling has previously been designed to be very narrow in domain, with a major undertaking on feature engineering. The traditional approach to building bots requires a ‘rather complicated pipeline’ of many stages. In contrast, the neural conversational model is a end-to-end approach to the problem which lacks any domain knowledge. The surprising thing, is just how well it works. We’ll see some example conversations soon. Of course the bot has no understanding of what the conversation means. Or maybe it does have some kind of understanding (just like a word vector seems to capture some understanding of the meaning of a word), but it’s not an understanding we can tap into. Therefore you can’t use this approach to incorporate external knowledge in replies, or to extract intent and take actions etc.. Even so, a bot trained on an IT helpdesk troubleshooting dataset is able to genuinely help people. Assuming you’re familiar with how sequence-to-sequence learning works, the whole neural conversational model can be explained in one short paragraph. During training, the conversation so far is used as the input sequence, and the next response is the target output sequence. Concretely, suppose we observe a conversation with two turns: the first person utters “ABC”, and the second person replies “WXYZ”. We can use a recurrent neural network and train to map “ABC” to “WXYZ” as show in Figure 1 [below]. The hidden state of the model when it receives the end of sequence symbol  can be viewed as the thought vector because it stores the information of the sentence, or thought, “ABC”. When conversing, the conversation so far is fed into the input sequence, and the bot responds with the generated output sequence. The approach was tested with an IT helpdesk dataset (30M tokens, with typical interactions being 400 words long and turn-taking clearly signaled), and the Open Subtitles dataset of sentences uttered by characters in movies (62M training sentences, 26M validation sentences). Training with the IT helpdesk dataset was done with a single layer LSTM with 1024 memory cells and the 20K most common words. Here’s an example conversation with the trained bot:  The OpenSubtitles experiment was performed with a two-layered LSTM with 4096 cells per layer and a 100K word vocabulary. “Our simple recurrent model does often produce plausible answers.”  We find it encouraging that the model can remember facts, understand contexts, and perform common sense reasoning without the complexity in traditional pipelines. What surprises us is that the model does so without any explicit knowledge representation component except for the parameters in the word vectors. Perhaps most practically significant is the fact that the model can generalize to new questions. In other words, it does not simply look up for an answer by matching the question with the existing database. In fact, most of the questions presented above, except for the first conversation, do not appear in the training set. Here’s an example general knowledge Q&A – I don’t think Watson has anything to worry about just yet! It’s interesting, but also far from something you’d want to rely on in a real chatbot service at this point in time. The model only gives simple and short answers to questions (which may be ‘probable’ answers according to the model, but aren’t always correct from our perspective). Perhaps a more problematic drawback is that the model does not capture a consistent personality. Indeed, if we ask not identical but semantically similar questions, the answers can sometimes be inconsistent:  So there you have it:  In this paper, we show that a simple language model based on the seq2seq framework can be used to train a conversational engine. Our modest results show that it can generate simple and basic conversations, and extract knowledge from a noisy but open-domain dataset. Even though the model has obvious limitations, it is surprising to us that a purely data driven approach without any rules can produce rather proper answers to many types of questions. However, the model may require substantial modifications to be able to deliver realistic conversations. My personal takeaway is that while it’s amazing how much can be done just with conversational data to learn from, any real service is going to need to some more complex logic wrapped around it. One possibility the authors hint at is that the neural conversational model could in principle be combined with other systems to re-score a short-list of candidate responses generated by other means.", "pdf_url": "https://arxiv.org/pdf/1506.05869.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/a-neural-conversation-model.json"}
{"id": "37775721", "bin": "100_200", "summary_sentences": ["Task of translating natural language queries into regular expressions without using domain specific knowledge.", "Proposes a methodology for collecting a large corpus of regular expressions to natural language pairs.", "Reports performance gain of 19.6% over state-of-the-art models.", "Architecture  LSTM based sequence to sequence neural network (with attention)  Six layers  One-word embedding layer  Two encoder layers  Two decoder layers  One dense output layer.", "Attention over encoder layer.", "Dropout with the probability of 0.25.", "20 epochs, minibatch size of 32 and learning rate of 1 (with decay rate of 0.5)  Dataset Generation  Created a public dataset - NL-RX - with 10K pair of (regular expression, natural language)  Two step generate-and-paraphrase approach  Generate step  Use handcrafted grammar to translate regular expressions to natural language.", "Paraphrase step  Crowdsourcing the task of translating the rigid descriptions into more natural expressions.", "Results  Evaluation Metric  Functional equality check (called DFA-Equal) as same regular expression could be written in many ways.", "Proposed architecture outperforms both the baselines - Nearest Neighbor classifier using Bag of Words (BoWNN) and Semantic-Unify"], "summary_text": "Task of translating natural language queries into regular expressions without using domain specific knowledge. Proposes a methodology for collecting a large corpus of regular expressions to natural language pairs. Reports performance gain of 19.6% over state-of-the-art models. Architecture  LSTM based sequence to sequence neural network (with attention)  Six layers  One-word embedding layer  Two encoder layers  Two decoder layers  One dense output layer. Attention over encoder layer. Dropout with the probability of 0.25. 20 epochs, minibatch size of 32 and learning rate of 1 (with decay rate of 0.5)  Dataset Generation  Created a public dataset - NL-RX - with 10K pair of (regular expression, natural language)  Two step generate-and-paraphrase approach  Generate step  Use handcrafted grammar to translate regular expressions to natural language. Paraphrase step  Crowdsourcing the task of translating the rigid descriptions into more natural expressions. Results  Evaluation Metric  Functional equality check (called DFA-Equal) as same regular expression could be written in many ways. Proposed architecture outperforms both the baselines - Nearest Neighbor classifier using Bag of Words (BoWNN) and Semantic-Unify", "pdf_url": "http://arxiv.org/pdf/1608.03000v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/4d803bc021f579d4aa3b24cec5b994.json"}
{"id": "8240954", "bin": "100_200", "summary_sentences": ["\"Using the \"SELU\" activation function, you get better results than any other activation function, and you don't have to do batch normalization.", "The \"SELU\" activation function is:  if x<0, 1.051\\*(1.673\\*e^x-1.673) if x>0, 1.051\\*x\" Source: narfon2, reddit   ``` import numpy as np  def selu(x):     alpha = 1.6732632423543772848170429916717     scale = 1.0507009873554804934193349852946     return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha) ``` Source: CaseOfTuesday, reddit  Discussion here:  [url]"], "summary_text": "\"Using the \"SELU\" activation function, you get better results than any other activation function, and you don't have to do batch normalization. The \"SELU\" activation function is:  if x<0, 1.051\\*(1.673\\*e^x-1.673) if x>0, 1.051\\*x\" Source: narfon2, reddit   ``` import numpy as np  def selu(x):     alpha = 1.6732632423543772848170429916717     scale = 1.0507009873554804934193349852946     return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha) ``` Source: CaseOfTuesday, reddit  Discussion here:  [url]", "pdf_url": "http://arxiv.org/pdf/1706.02515v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/1706.02515.json"}
{"id": "4216862", "bin": "100_200", "summary_sentences": ["What  The authors reevaluate the original residual design of neural networks.", "They compare various architectures of residual units and actually find one that works quite a bit better.", "How  The new variation starts the transformation branch of each residual unit with BN and a ReLU.", "It removes BN and ReLU after the last convolution.", "As a result, the information from previous layers can flow completely unaltered through the shortcut branch of each residual unit.", "The image below shows some variations (of the position of BN and ReLU) that they tested.", "The new and better design is on the right:  They also tried various alternative designs for the shortcut connections.", "However, all of these designs performed worse than the original one.", "Only one (d) came close under certain conditions.", "Therefore, the recommendation is to stick with the old/original design.", "Results  Significantly faster training for very deep residual networks (1001 layers).", "Better regularization due to the placement of BN.", "CIFAR-10 and CIFAR-100 results, old vs. new design:"], "summary_text": "What  The authors reevaluate the original residual design of neural networks. They compare various architectures of residual units and actually find one that works quite a bit better. How  The new variation starts the transformation branch of each residual unit with BN and a ReLU. It removes BN and ReLU after the last convolution. As a result, the information from previous layers can flow completely unaltered through the shortcut branch of each residual unit. The image below shows some variations (of the position of BN and ReLU) that they tested. The new and better design is on the right:  They also tried various alternative designs for the shortcut connections. However, all of these designs performed worse than the original one. Only one (d) came close under certain conditions. Therefore, the recommendation is to stick with the old/original design. Results  Significantly faster training for very deep residual networks (1001 layers). Better regularization due to the placement of BN. CIFAR-10 and CIFAR-100 results, old vs. new design:", "pdf_url": "http://arxiv.org/pdf/1603.05027v2", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/identity_mappings_in_deep_residual_networks.json"}
{"id": "96599676", "bin": "100_200", "summary_sentences": ["So the hypervector is just a big vector created from a network:  `\"We concatenate features from some or all of the feature maps in the network into one long vector for every location which we call the hypercolumn at that location.", "As an example, using pool2 (256 channels), conv4 (384 channels) and fc7 (4096 channels) from the architecture of [28] would lead to a 4736 dimensional vector.", "\"`  So how exactly do we construct the vector?", "!", "[]( [url]"], "summary_text": "So the hypervector is just a big vector created from a network:  `\"We concatenate features from some or all of the feature maps in the network into one long vector for every location which we call the hypercolumn at that location. As an example, using pool2 (256 channels), conv4 (384 channels) and fc7 (4096 channels) from the architecture of [28] would lead to a 4736 dimensional vector. \"`  So how exactly do we construct the vector? ! []( [url]", "pdf_url": "http://arxiv.org/pdf/1411.5752v2", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/1411.5752.json"}
{"id": "20550713", "bin": "100_200", "summary_sentences": ["In this paper, the authors proposed a method for convexifying convolutional neural networks to train them without backpropagation.", "Furthermore, this relaxation to the convex setting allows for theoretical proofs of bounds on the generalization error.", "Succinctly, they propose to use RKHS and the kernel trick to lift the data into a high-dimensional space that is expressive enough to capture certain nonlinear activation functions.", "Hence, on experiments on MNIST and CIFAR-10, they show that they can outperform smaller CNNs by “convexifying” them.", "They note that their method doesn’t work with max pooling or very deep CNNs with lots of bells and whistles.", "This is a thought-provoking paper.", "I like how the authors pursued a theoretically interesting question, even though there isn’t much practical use yet for this.", "I don’t have personal experience writing theory papers, but I imagine that this is a good(?)", "representation of how they often go in ML.", "The research is driven by an interesting theoretical question, not a practical application that needs solving/SOTA results."], "summary_text": "In this paper, the authors proposed a method for convexifying convolutional neural networks to train them without backpropagation. Furthermore, this relaxation to the convex setting allows for theoretical proofs of bounds on the generalization error. Succinctly, they propose to use RKHS and the kernel trick to lift the data into a high-dimensional space that is expressive enough to capture certain nonlinear activation functions. Hence, on experiments on MNIST and CIFAR-10, they show that they can outperform smaller CNNs by “convexifying” them. They note that their method doesn’t work with max pooling or very deep CNNs with lots of bells and whistles. This is a thought-provoking paper. I like how the authors pursued a theoretically interesting question, even though there isn’t much practical use yet for this. I don’t have personal experience writing theory papers, but I imagine that this is a good(?) representation of how they often go in ML. The research is driven by an interesting theoretical question, not a practical application that needs solving/SOTA results.", "pdf_url": "https://arxiv.org/pdf/1609.01000", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/convexified-cnns.json"}
{"id": "34524227", "bin": "100_200", "summary_sentences": ["Diplomat: Using Delegations to Protect Community Repositories – Kuppusamy et al. 2016  Community repositories, such as Docker Hub, Python Package Index (PyPI), RubyGems, and SourceForge provide an easy way for a developer to disseminate software… [they] are immensely popular and collectively serve more than a billion packages per year.", "Unfortunately, the popularity of these repositories also makes them an attractive target to hackers… Major repositories run by Adobe, Apache, Debian, Fedora, FreeBSD, Gentoo, GitHub, GNU Savannah, Linux, Microsoft, npm, Opera, PHP, RedHat, RubyGems, SourceForge, and WordPress have all been compromised at least once.", "This is a topic of immediate importance.", "Diplomat is a practical security system for community repositories that combines immediate project registration (adding new projects happens all the time with popular repositories) and compromise-resilience.", "Diplomat source code and standards documents are freely available at  [url]"], "summary_text": "Diplomat: Using Delegations to Protect Community Repositories – Kuppusamy et al. 2016  Community repositories, such as Docker Hub, Python Package Index (PyPI), RubyGems, and SourceForge provide an easy way for a developer to disseminate software… [they] are immensely popular and collectively serve more than a billion packages per year. Unfortunately, the popularity of these repositories also makes them an attractive target to hackers… Major repositories run by Adobe, Apache, Debian, Fedora, FreeBSD, Gentoo, GitHub, GNU Savannah, Linux, Microsoft, npm, Opera, PHP, RedHat, RubyGems, SourceForge, and WordPress have all been compromised at least once. This is a topic of immediate importance. Diplomat is a practical security system for community repositories that combines immediate project registration (adding new projects happens all the time with popular repositories) and compromise-resilience. Diplomat source code and standards documents are freely available at  [url]", "pdf_url": "https://isis.poly.edu/%7Ejcappos/papers/kuppusamy_nsdi_16.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/diplomat-using-delegations-to-protect-community-repositories.json"}
{"id": "33015723", "bin": "100_200", "summary_sentences": ["They introduce the concept of counting in images by predicting a density map.", "Their training only requires dot annotations on the center of objects.", "Each dot is expanded to a gaussian to form a density.", "A model is trained to predict this density and then the total count is recovered by integrating over the resulting density map.", "They create a function to produce the density based on quantized dense SIFT features  [ref]  from every pixel in the image.", "A simple version of the definition of $F$ is shown below.", "Each pixel becomes an $x_p$ vector which is used to train and model to implement the function $F$.", "$$\\forall p \\in I, \\hspace{10pt } F(p|w) = wx_p $$  The obtained quantized dense SIFT features using the [VLFEAT]( [url]"], "summary_text": "They introduce the concept of counting in images by predicting a density map. Their training only requires dot annotations on the center of objects. Each dot is expanded to a gaussian to form a density. A model is trained to predict this density and then the total count is recovered by integrating over the resulting density map. They create a function to produce the density based on quantized dense SIFT features  [ref]  from every pixel in the image. A simple version of the definition of $F$ is shown below. Each pixel becomes an $x_p$ vector which is used to train and model to implement the function $F$. $$\\forall p \\in I, \\hspace{10pt } F(p|w) = wx_p $$  The obtained quantized dense SIFT features using the [VLFEAT]( [url]", "pdf_url": "http://papers.nips.cc/paper/4043-learning-to-count-objects-in-images.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/lempitskyz10.json"}
{"id": "414696", "bin": "100_200", "summary_sentences": ["DBSherlock: A performance diagnostic tool for transactional databases Yoon et al. SIGMOD ’16  …tens of thousands of concurrent transactions competing for the same resources (e.g.", "CPU, disk I/O, memory) can create highly non-linear and counter-intuitive effects on database performance.", "If you’re a DBA responsible for figuring out what’s going on, this presents quite a challenge.", "You might be awash in stats and graphs (MySQL maintains 260 statistics and variables for example), but still sorely lacking the big picture… “as a consquence, highly-skilled and highly-paid DBAs (a scarce resource themselves) spend many hours diagnosing performance problems through different conjectures and manually inspecting various queries and log files, until the root cause is found.”  DBSherlock (available at  [url]"], "summary_text": "DBSherlock: A performance diagnostic tool for transactional databases Yoon et al. SIGMOD ’16  …tens of thousands of concurrent transactions competing for the same resources (e.g. CPU, disk I/O, memory) can create highly non-linear and counter-intuitive effects on database performance. If you’re a DBA responsible for figuring out what’s going on, this presents quite a challenge. You might be awash in stats and graphs (MySQL maintains 260 statistics and variables for example), but still sorely lacking the big picture… “as a consquence, highly-skilled and highly-paid DBAs (a scarce resource themselves) spend many hours diagnosing performance problems through different conjectures and manually inspecting various queries and log files, until the root cause is found.”  DBSherlock (available at  [url]", "pdf_url": "https://web.eecs.umich.edu/~mozafari/php/data/uploads/sigmod_2016.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/dbsherlock-a-performance-diagnostic-tool-for-transactional-databases.json"}
{"id": "74374635", "bin": "100_200", "summary_sentences": ["The idea in this paper is to develop a version of attention that will incorporate similarity in neighboring bins.", "This aligned with the work  [ref]  which presented a different approach to deal with consistency between classes of predictions.", "In this work the closed form softmax function is replaced by a small optimization problem with this regularizer:  $$ +\\lambda \\sum_{i=1}^{d-1} |y_{i+1}-y_i|$$  Because of this, many of the neighboring probabilities are exactly the same resulting in attention that can be seen as blocks.", "[url]"], "summary_text": "The idea in this paper is to develop a version of attention that will incorporate similarity in neighboring bins. This aligned with the work  [ref]  which presented a different approach to deal with consistency between classes of predictions. In this work the closed form softmax function is replaced by a small optimization problem with this regularizer:  $$ +\\lambda \\sum_{i=1}^{d-1} |y_{i+1}-y_i|$$  Because of this, many of the neighboring probabilities are exactly the same resulting in attention that can be seen as blocks. [url]", "pdf_url": "http://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/niculaeb17.json"}
{"id": "70140653", "bin": "100_200", "summary_sentences": ["The weights at each layer $W$ are initialized based on the number of connections they have.", "Each $w \\in W$  is drawn from a Gaussian distribution with mean $\\mu = 0$ with the variance as follows.", "$$\\text{Var}(W) = \\frac{2}{n_\\text{in}+ n_\\text{out}}$$  Where $n_\\text{in}$ is the number of neurons in the previous layer from the feedforward direction and $n_\\text{out}$ is the number of neurons from the previous layer from the backprop direction.", "Reference: [Andy Jones's Blog]( [url]"], "summary_text": "The weights at each layer $W$ are initialized based on the number of connections they have. Each $w \\in W$  is drawn from a Gaussian distribution with mean $\\mu = 0$ with the variance as follows. $$\\text{Var}(W) = \\frac{2}{n_\\text{in}+ n_\\text{out}}$$  Where $n_\\text{in}$ is the number of neurons in the previous layer from the feedforward direction and $n_\\text{out}$ is the number of neurons from the previous layer from the backprop direction. Reference: [Andy Jones's Blog]( [url]", "pdf_url": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/glorotb10.json"}
{"id": "6469050", "bin": "100_200", "summary_sentences": ["This extends perceptrons  [ref]  but uses what is known as the Hinge loss (aka SVM loss):  $$J_i(w) = max(0,\\gamma -y\\_i f(x\\_i))$$  Where $\\gamma$ is the margin.", "$J_i(w)$ is the error given some weight $w$ parameters.", "$x_i$ and $y_i$ are a training example and correct label.", "$f(x_i)$ is the perceptron function we are trying learn the best weights for."], "summary_text": "This extends perceptrons  [ref]  but uses what is known as the Hinge loss (aka SVM loss):  $$J_i(w) = max(0,\\gamma -y\\_i f(x\\_i))$$  Where $\\gamma$ is the margin. $J_i(w)$ is the error given some weight $w$ parameters. $x_i$ and $y_i$ are a training example and correct label. $f(x_i)$ is the perceptron function we are trying learn the best weights for.", "pdf_url": "http://cseweb.ucsd.edu/~yfreund/papers/LargeMarginsUsingPerceptron.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/freund1999.json"}
{"id": "99295545", "bin": "100_200", "summary_sentences": ["Environment for learning using modalities like vision, audio, semantics, physics and interaction with objects and other agents.", "Motivation  Humans learn by interacting with their surroundings (environment).", "Similarly training an agent in an interactive multi-model environment (virtual embodiment) could be useful for a learning agent.", "Characteristics  Open-source and Open-AI gym compatible  Built on top of 45000 3D house layouts from SUNCG dataset.", "Provides both 3D visual and audio recording.", "Semantic image segmentation and langauge description of objects.", "Components  Rendering Engine  Implemented using Panda 3D game engine.", "Renders RGB+depth scenes based on textures, multi-source lightings and shadows.", "Acoustic Engine  Implemented using EVERT  Supports multiple microphones, sound sources, sound absorption based on material, atmospheric conditions etc.", "Semantics Engine  Provides a short textual description for each object, along with information like color, category, material size, location etc.", "Physics Engine  Implemented using Bullet3 Engine  Supports physical interaction, external forces like gravity and position and velocity information for multiple agents.", "Potential Applications  Visual Question Answering  Conversational Agents  Training an agent to follow instructions  Multi-agent communication"], "summary_text": "Environment for learning using modalities like vision, audio, semantics, physics and interaction with objects and other agents. Motivation  Humans learn by interacting with their surroundings (environment). Similarly training an agent in an interactive multi-model environment (virtual embodiment) could be useful for a learning agent. Characteristics  Open-source and Open-AI gym compatible  Built on top of 45000 3D house layouts from SUNCG dataset. Provides both 3D visual and audio recording. Semantic image segmentation and langauge description of objects. Components  Rendering Engine  Implemented using Panda 3D game engine. Renders RGB+depth scenes based on textures, multi-source lightings and shadows. Acoustic Engine  Implemented using EVERT  Supports multiple microphones, sound sources, sound absorption based on material, atmospheric conditions etc. Semantics Engine  Provides a short textual description for each object, along with information like color, category, material size, location etc. Physics Engine  Implemented using Bullet3 Engine  Supports physical interaction, external forces like gravity and position and velocity information for multiple agents. Potential Applications  Visual Question Answering  Conversational Agents  Training an agent to follow instructions  Multi-agent communication", "pdf_url": "https://arxiv.org/pdf/1711.11017", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/home-a-household-multimodal-environment.json"}
{"id": "5262509", "bin": "100_200", "summary_sentences": ["Musketeer: all for one, one for all in data processing systems – Gog et al. 2015  Musketeer gives you portability of data processing workflows across across data processing systems.", "It can even analyse your workflow and recommend the best system to run it on, as well as combining systems for different parts of the workflow.", "This is important since, as we saw yesterday in part  1 , no one system is universally best across all workload sizes and varieties.", "It  works by introducing a DAG-based intermediate representation (IR) and a translator from existing workflow specifications into this IR (all for one).", "The IR is then analysed and optimised and efficient code is generated for the target back-end system (one for all).", "You’ll find the paper at the link above, and the open source project at  [url]"], "summary_text": "Musketeer: all for one, one for all in data processing systems – Gog et al. 2015  Musketeer gives you portability of data processing workflows across across data processing systems. It can even analyse your workflow and recommend the best system to run it on, as well as combining systems for different parts of the workflow. This is important since, as we saw yesterday in part  1 , no one system is universally best across all workload sizes and varieties. It  works by introducing a DAG-based intermediate representation (IR) and a translator from existing workflow specifications into this IR (all for one). The IR is then analysed and optimised and efficient code is generated for the target back-end system (one for all). You’ll find the paper at the link above, and the open source project at  [url]", "pdf_url": "http://www.cl.cam.ac.uk/research/srg/netos/camsas/pubs/eurosys15-musketeer.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/musketeer-part-ii-one-for-all-and-all-for-one.json"}
{"id": "12625177", "bin": "1100_1200", "summary_sentences": ["Achieving Human Parity in Conversational Speech Recognition Xiong et al. Microsoft Technical Report, 2016  The headline story here is that for the first time a system has been developed that exceeds human performance in one of the most difficult of all human speech recognition tasks: natural conversations held over the telephone.", "This is known as conversational telephone speech, or CTS.", "[CTS] is especially difficult due to the spontaneous (neither read nor planned) nature of the speech, its informality, and the self-corrections, hesitations, and other disfluencies that are pervasive.", "The reference datasets for this task are the Switchboard and Fisher data collections from the 1990s and early 2000s.", "The apocryphal story here is that human performance on the task is about 4% error rate.", "But no-one can quite pin down where that 4% number comes from.", "So the Microsoft team took advantage of an existing professional transcription service used by Microsoft:  To measure human performance, we leveraged an existing pipeline in which Microsoft data is transcribed on a weekly basis.", "This pipeline uses a large commercial vendor to perform a two-pass transcription.", "In the first pass, a transcriber works from scratch to transcribe the data.", "In the second pass, a second listener monitors the data to do error correction.", "Dozens of hours of test data are processed in each batch.", "One week, we added the NIST 2000 CTS evaluation data to the work-list, without further comment…  For the switchboard portion of the dataset, the professional human transcribers achieved a 5.9% error rate, and for the ‘call-home’ portion of the test set 11.3%.", "“The same informality, multiple speakers per channel, and recording conditions that make CallHome hard for computers make it difficult for people as well.”  Notably, the performance of our artificial system aligns almost exactly with the performance of people on both sets.", "And so one can’t help but wonder how much longer those professional transcribers will continue to be needed!", "Did they know they were gathering the evidence that one day might help lead to the elimination of their jobs???", "Here’s a table full of cryptic acronyms that show the performance of the system that Microsoft put together, using various acoustic models.", "The bottom-line (strictly, bottom two lines) is what matters here: parity on the switchboard dataset, and a slight advantage for the ASR (Automatic Speech Recognition) system of the CallHome dataset.", "How did the Microsoft team manage to pull this off?", "Our system’s performance can be attributed to the systematic use of LTSMs for both acoustic and language modeling, as well as CNNs in the acoustic model, and extensive combination of complementary models.", "Training was made feasible (reducing times from month to 1-3 weeks) by using Microsoft’s CNTK Cognitive Toolkit to parallelize SGD training, coupled with the use of the 1-bit SGD parallelization techique from prior work:  In [65], we showed that gradient values can be quantized to just a single bit, if one carries over the quantization error from one minibatch to the next.", "Each time a sub-gradient is quantized, the quantization error is computed and remembered, and then added to the next minibatch’s sub-gradient.", "This reduces the required bandwidth 32-fold with minimal loss in accuracy.", "How it works under the covers  The model details are concisely explained, targeting an audience of speech recognition experts (i.e. not me!).", "It is still possible for a lay-reader to gain some appreciation of what’s involved though.", "It’s also another reminder that we’re rapidly assembling a powerful collection of building blocks that through good systems engineering can be combined into very effective systems.", "Expect to see an explosion of applied AI/ML/whatever-your-preferred-phrase-is applications as this trend continues.", "Our progress is a result of the careful engineering and optimization of convolutional and recurrent neural networks.", "While the basic structures have been well known for a long period, it is only recently that they have emerged as the best models for speech recognition.", "Surprisingly, this is the case for both acoustic modeling and language modeling.", "The CNN and RNN based acoustic models can model a large amount of acoustic context with temporal invariance, and in the case of CNNs, with frequency invariance as well.", "In language modeling, RNNs improve on classical N-gram models through the use of an unbounded word history and the generalization ability of continuous word representations .", "The paper describes a whole family of systems that were explored in order to find the best performing combination.", "The best acoustic model was formed by combining independently trained ResNet and VGG models using a score fusion weight.", "‘VGG’ stands for the University of Oxford Visual Geometry Group and the architecture they developed in “ Very deep convolutional networks for large-scale visual recognition ”.", "Their networks use 16-19 layers with small 3×3 filters in all convolutional layers.", "The ResNet architecture is also borrowed from the field of image recognition.", "Speaker adaptive modeling is then applied by conditioning the network on an i-vector characterization of each speaker using 100-dimensional i-vectors.", "The i-vectors are added to the activation of each CNN layer via a learnable weight matrix.", "After initial training, model parameters are optimized using a maximum mutual information (MMI) objective function:  where w is a word sequence, and a is an acoustic realization of a word sequence.", "The performance improvements obtained from this lattice-free MMI (LFMMI) training phase, as well as i-vectors, can be seen in the following table:  An initial decoding of acoustic model outputs is done with a WFST ( Weighted Finite State Transducer ) decoder.", "We use an N-gram language model trained and pruned with the SRILM toolkit.", "The first-pass LM has approximately 15.9 million bigrams, trigrams, and 4-grams, and a vocabulary of 30500 words, and give a perplexity of 54 on RT-03 speech transcripts.", "The N-best performing hypotheses from the WFST decoding are then rescored using a combination of a large N-gram language model and neural net language models.", "The best performing language model used LTSMs with three hidden layers, and 1000 hidden units in each layer.", "“For the final system, we interpolated two LSTM-LMs with an N-gram LM for the forward direction LM, and similarly for the backward direction LM.”"], "summary_text": "Achieving Human Parity in Conversational Speech Recognition Xiong et al. Microsoft Technical Report, 2016  The headline story here is that for the first time a system has been developed that exceeds human performance in one of the most difficult of all human speech recognition tasks: natural conversations held over the telephone. This is known as conversational telephone speech, or CTS. [CTS] is especially difficult due to the spontaneous (neither read nor planned) nature of the speech, its informality, and the self-corrections, hesitations, and other disfluencies that are pervasive. The reference datasets for this task are the Switchboard and Fisher data collections from the 1990s and early 2000s. The apocryphal story here is that human performance on the task is about 4% error rate. But no-one can quite pin down where that 4% number comes from. So the Microsoft team took advantage of an existing professional transcription service used by Microsoft:  To measure human performance, we leveraged an existing pipeline in which Microsoft data is transcribed on a weekly basis. This pipeline uses a large commercial vendor to perform a two-pass transcription. In the first pass, a transcriber works from scratch to transcribe the data. In the second pass, a second listener monitors the data to do error correction. Dozens of hours of test data are processed in each batch. One week, we added the NIST 2000 CTS evaluation data to the work-list, without further comment…  For the switchboard portion of the dataset, the professional human transcribers achieved a 5.9% error rate, and for the ‘call-home’ portion of the test set 11.3%. “The same informality, multiple speakers per channel, and recording conditions that make CallHome hard for computers make it difficult for people as well.”  Notably, the performance of our artificial system aligns almost exactly with the performance of people on both sets. And so one can’t help but wonder how much longer those professional transcribers will continue to be needed! Did they know they were gathering the evidence that one day might help lead to the elimination of their jobs??? Here’s a table full of cryptic acronyms that show the performance of the system that Microsoft put together, using various acoustic models. The bottom-line (strictly, bottom two lines) is what matters here: parity on the switchboard dataset, and a slight advantage for the ASR (Automatic Speech Recognition) system of the CallHome dataset. How did the Microsoft team manage to pull this off? Our system’s performance can be attributed to the systematic use of LTSMs for both acoustic and language modeling, as well as CNNs in the acoustic model, and extensive combination of complementary models. Training was made feasible (reducing times from month to 1-3 weeks) by using Microsoft’s CNTK Cognitive Toolkit to parallelize SGD training, coupled with the use of the 1-bit SGD parallelization techique from prior work:  In [65], we showed that gradient values can be quantized to just a single bit, if one carries over the quantization error from one minibatch to the next. Each time a sub-gradient is quantized, the quantization error is computed and remembered, and then added to the next minibatch’s sub-gradient. This reduces the required bandwidth 32-fold with minimal loss in accuracy. How it works under the covers  The model details are concisely explained, targeting an audience of speech recognition experts (i.e. not me!). It is still possible for a lay-reader to gain some appreciation of what’s involved though. It’s also another reminder that we’re rapidly assembling a powerful collection of building blocks that through good systems engineering can be combined into very effective systems. Expect to see an explosion of applied AI/ML/whatever-your-preferred-phrase-is applications as this trend continues. Our progress is a result of the careful engineering and optimization of convolutional and recurrent neural networks. While the basic structures have been well known for a long period, it is only recently that they have emerged as the best models for speech recognition. Surprisingly, this is the case for both acoustic modeling and language modeling. The CNN and RNN based acoustic models can model a large amount of acoustic context with temporal invariance, and in the case of CNNs, with frequency invariance as well. In language modeling, RNNs improve on classical N-gram models through the use of an unbounded word history and the generalization ability of continuous word representations . The paper describes a whole family of systems that were explored in order to find the best performing combination. The best acoustic model was formed by combining independently trained ResNet and VGG models using a score fusion weight. ‘VGG’ stands for the University of Oxford Visual Geometry Group and the architecture they developed in “ Very deep convolutional networks for large-scale visual recognition ”. Their networks use 16-19 layers with small 3×3 filters in all convolutional layers. The ResNet architecture is also borrowed from the field of image recognition. Speaker adaptive modeling is then applied by conditioning the network on an i-vector characterization of each speaker using 100-dimensional i-vectors. The i-vectors are added to the activation of each CNN layer via a learnable weight matrix. After initial training, model parameters are optimized using a maximum mutual information (MMI) objective function:  where w is a word sequence, and a is an acoustic realization of a word sequence. The performance improvements obtained from this lattice-free MMI (LFMMI) training phase, as well as i-vectors, can be seen in the following table:  An initial decoding of acoustic model outputs is done with a WFST ( Weighted Finite State Transducer ) decoder. We use an N-gram language model trained and pruned with the SRILM toolkit. The first-pass LM has approximately 15.9 million bigrams, trigrams, and 4-grams, and a vocabulary of 30500 words, and give a perplexity of 54 on RT-03 speech transcripts. The N-best performing hypotheses from the WFST decoding are then rescored using a combination of a large N-gram language model and neural net language models. The best performing language model used LTSMs with three hidden layers, and 1000 hidden units in each layer. “For the final system, we interpolated two LSTM-LMs with an N-gram LM for the forward direction LM, and similarly for the backward direction LM.”", "pdf_url": "https://arxiv.org/pdf/1610.05256", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/achieving-human-parity-in-conversational-speech-recognition.json"}
{"id": "51116334", "bin": "1100_1200", "summary_sentences": ["What  The paper describes a method to transfer the style (e.g. choice of colors, structure of brush strokes) of an image to a whole video.", "The method is designed so that the transfered style is consistent over many frames.", "Examples for such consistency:  No flickering of style between frames.", "So the next frame has always roughly the same style in the same locations.", "No artefacts at the boundaries of objects, even if they are moving.", "If an area gets occluded and then unoccluded a few frames later, the style of that area is still the same as before the occlusion.", "How  Assume that we have a frame to stylize x and an image from which to extract the style a.", "The basic process is the same as in the original Artistic Style Transfer paper , they just add a bit on top of that.", "They start with a gaussian noise image x' and change it gradually so that a loss function gets minimized.", "The loss function has the following components:  Content loss (old, same as in the Artistic Style Transfer paper)  This loss makes sure that the content in the generated/stylized image still matches the content of the original image.", "x and x' are fed forward through a pretrained network (VGG in their case).", "Then the generated representations of the intermediate layers of the network are extracted/read.", "One or more layers are picked and the difference between those layers for x and x' is measured via a MSE.", "E.g. if we used only the representations of the layer conv5 then we would get something like (conv5(x) - conv5(x'))^2 per example.", "(Where conv5() also executes all previous layers.)", "Style loss (old)  This loss makes sure that the style of the generated/stylized image matches the style source a.  x' and a are fed forward through a pretrained network (VGG in their case).", "Then the generated representations of the intermediate layers of the network are extracted/read.", "One or more layers are picked and the Gram Matrices of those layers are calculated.", "Then the difference between those matrices is measured via a MSE.", "Temporal loss (new)  This loss enforces consistency in style between a pair of frames.", "The main sources of inconsistency are boundaries of moving objects and areas that get unonccluded.", "They use the optical flow to detect motion.", "Applying an optical flow method to two frames (i, i+1) returns per pixel the movement of that pixel, i.e. if the pixel at (x=1, y=2) moved to (x=2, y=4) the optical flow at that pixel would be (u=1, v=2).", "The optical flow can be split into the forward flow (here fw) and the backward flow (here bw).", "The forward flow is the flow from frame i to i+1 (as described in the previous point).", "The backward flow is the flow from frame i+1 to i (reverse direction in time).", "Boundaries  At boundaries of objects the derivative of the flow is high, i.e. the flow \"suddenly\" changes significantly from one pixel to the other.", "So to detect boundaries they use (per pixel) roughly the equation gradient(u)^2 + gradient(v)^2 > length((u,v)).", "Occlusions and disocclusions  If a pixel does not get occluded/disoccluded between frames, the optical flow method should be able to correctly estimate the motion of that pixel between the frames.", "The forward and backward flows then should be roughly equal, just in opposing directions.", "If a pixel does get occluded/disoccluded between frames, it will not be visible in one the two frames and therefore the optical flow method cannot reliably estimate the motion for that pixel.", "It is then expected that the forward and backward flow are unequal.", "To measure that effect they roughly use (per pixel) a formula matching length(fw + bw)^2 > length(fw)^2 + length(bw)^2.", "Mask c  They create a mask c with the size of the frame.", "For every pixel they estimate whether the boundary-equation or the disocclusion-equation is true.", "If either of them is true, they add a 0 to the mask, otherwise a 1.", "So the mask is 1 wherever there is no disocclusion or motion boundary.", "Combination  The final temporal loss is the mean (over all pixels) of c*(x-w)^2.", "x is the frame to stylize.", "w is the previous stylized frame (frame i-1), warped according to the optical flow between frame i-1 and i.  c is the mask value at the pixel.", "By using the difference x-w they ensure that the difference in styles between two frames is low.", "By adding c they ensure the style-consistency only at pixels that probably should have a consistent style.", "Long-term loss (new)  This loss enforces consistency in style between pairs of frames that are longer apart from each other.", "It is a simple extension of the temporal (short-term) loss.", "The temporal loss was computed for frames (i-1, i).", "The long-term loss is the sum of the temporal losses for the frame pairs {(i-4,i), (i-2,i), (i-1,i)}.", "The c mask is recomputed for every pair and 1 if there are no boundaries/disocclusions detected, but only if there is not a 1 for the same pixel in a later mask.", "The additional condition is intended to associate pixels with their closest neighbours in time to minimize possible errors.", "Note that the long-term loss can completely replace the temporal loss as the latter one is contained in the former one.", "Multi-pass approach (new)  They had problems with contrast around the boundaries of the frames.", "To combat that, they use a multi-pass method in which they seem to calculate the optical flow in multiple forward and backward passes?", "(Not very clear here what they do and why it would help.)", "Initialization with previous frame (new)  Instead of starting at a gaussian noise image every time, they instead use the previous stylized frame.", "That immediately leads to more similarity between the frames.", "Results  Video"], "summary_text": "What  The paper describes a method to transfer the style (e.g. choice of colors, structure of brush strokes) of an image to a whole video. The method is designed so that the transfered style is consistent over many frames. Examples for such consistency:  No flickering of style between frames. So the next frame has always roughly the same style in the same locations. No artefacts at the boundaries of objects, even if they are moving. If an area gets occluded and then unoccluded a few frames later, the style of that area is still the same as before the occlusion. How  Assume that we have a frame to stylize x and an image from which to extract the style a. The basic process is the same as in the original Artistic Style Transfer paper , they just add a bit on top of that. They start with a gaussian noise image x' and change it gradually so that a loss function gets minimized. The loss function has the following components:  Content loss (old, same as in the Artistic Style Transfer paper)  This loss makes sure that the content in the generated/stylized image still matches the content of the original image. x and x' are fed forward through a pretrained network (VGG in their case). Then the generated representations of the intermediate layers of the network are extracted/read. One or more layers are picked and the difference between those layers for x and x' is measured via a MSE. E.g. if we used only the representations of the layer conv5 then we would get something like (conv5(x) - conv5(x'))^2 per example. (Where conv5() also executes all previous layers.) Style loss (old)  This loss makes sure that the style of the generated/stylized image matches the style source a.  x' and a are fed forward through a pretrained network (VGG in their case). Then the generated representations of the intermediate layers of the network are extracted/read. One or more layers are picked and the Gram Matrices of those layers are calculated. Then the difference between those matrices is measured via a MSE. Temporal loss (new)  This loss enforces consistency in style between a pair of frames. The main sources of inconsistency are boundaries of moving objects and areas that get unonccluded. They use the optical flow to detect motion. Applying an optical flow method to two frames (i, i+1) returns per pixel the movement of that pixel, i.e. if the pixel at (x=1, y=2) moved to (x=2, y=4) the optical flow at that pixel would be (u=1, v=2). The optical flow can be split into the forward flow (here fw) and the backward flow (here bw). The forward flow is the flow from frame i to i+1 (as described in the previous point). The backward flow is the flow from frame i+1 to i (reverse direction in time). Boundaries  At boundaries of objects the derivative of the flow is high, i.e. the flow \"suddenly\" changes significantly from one pixel to the other. So to detect boundaries they use (per pixel) roughly the equation gradient(u)^2 + gradient(v)^2 > length((u,v)). Occlusions and disocclusions  If a pixel does not get occluded/disoccluded between frames, the optical flow method should be able to correctly estimate the motion of that pixel between the frames. The forward and backward flows then should be roughly equal, just in opposing directions. If a pixel does get occluded/disoccluded between frames, it will not be visible in one the two frames and therefore the optical flow method cannot reliably estimate the motion for that pixel. It is then expected that the forward and backward flow are unequal. To measure that effect they roughly use (per pixel) a formula matching length(fw + bw)^2 > length(fw)^2 + length(bw)^2. Mask c  They create a mask c with the size of the frame. For every pixel they estimate whether the boundary-equation or the disocclusion-equation is true. If either of them is true, they add a 0 to the mask, otherwise a 1. So the mask is 1 wherever there is no disocclusion or motion boundary. Combination  The final temporal loss is the mean (over all pixels) of c*(x-w)^2. x is the frame to stylize. w is the previous stylized frame (frame i-1), warped according to the optical flow between frame i-1 and i.  c is the mask value at the pixel. By using the difference x-w they ensure that the difference in styles between two frames is low. By adding c they ensure the style-consistency only at pixels that probably should have a consistent style. Long-term loss (new)  This loss enforces consistency in style between pairs of frames that are longer apart from each other. It is a simple extension of the temporal (short-term) loss. The temporal loss was computed for frames (i-1, i). The long-term loss is the sum of the temporal losses for the frame pairs {(i-4,i), (i-2,i), (i-1,i)}. The c mask is recomputed for every pair and 1 if there are no boundaries/disocclusions detected, but only if there is not a 1 for the same pixel in a later mask. The additional condition is intended to associate pixels with their closest neighbours in time to minimize possible errors. Note that the long-term loss can completely replace the temporal loss as the latter one is contained in the former one. Multi-pass approach (new)  They had problems with contrast around the boundaries of the frames. To combat that, they use a multi-pass method in which they seem to calculate the optical flow in multiple forward and backward passes? (Not very clear here what they do and why it would help.) Initialization with previous frame (new)  Instead of starting at a gaussian noise image every time, they instead use the previous stylized frame. That immediately leads to more similarity between the frames. Results  Video", "pdf_url": "http://arxiv.org/pdf/1604.08610", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/artistic_style_transfer_for_videos.json"}
{"id": "13187019", "bin": "1100_1200", "summary_sentences": ["Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing Matejka & Fitzmaurice et al., CHI’17  Today’s paper choice is inspired by the keynote that Prof. Miriah Meyer gave at the recent Velocity conference in London, ‘ Why an interactive picture is worth a thousand numbers .’ She made a wonderful and thought-provoking case for the power of visualisations, and especially visualisations you can interact with, playing a central role in our process of understanding.", "Better ways of seeing and exploring data lead to better insights.", "Meyer opened her talk by showing us Cairo’s Datasaurus (‘ Never trust summary statistics alone; always visualize your data ’).", "You can calculate all the statistic summaries you like over the datasaurus dataset, run regressions, perform clustering, and so on.", "But until you look at it with the right visualisation, you’re never going to reach the same understanding as you get from looking at the data this way:  Since the early 70’s, Anscome’s Quartet has been frequently used to illustrate the importance of graphical representations when exploring data:  The effectiveness of Anscombe’s Quartet is not due to simply having four different data sets which generate the same statistical properties, it is that four clearly different and identifiably distinct datasets are producing the same statistical properties.", "In ‘Same Stats, Different Graphs,’ Matjeka & Fitzmaurice show a method for purposefully creating datasets which are identical over a range of statistical properties (of your choosing), yet produce dissimilar graphics.", "In my mind there’s a connection here to the idea of adversarial inputs to deep neural nets, which we might similarly express on some level as ‘Same Stats, Different Classes.’ Another thing I get from this paper is a very visual reminder of ‘Same Outcome (in terms of stats), Different Causes.’ There are lots of different hypotheses you could come up with that may produce the effect you’re seeing.", "Their method doesn’t just produce datasets that retain the same statistical properties while looking different though, it also allows you to guide the way in which the visualisation looks different (a bit like crafting an adversarial input to produce a specific mis-classification).", "For example, in the figure below we have an initial data set (top left) and a set of target shapes for data set generation.", "Here are the results produced by the technique when using these target shapes – every one of these has the same summary statistics as the initial dataset!", "These examples are all in 2D, but there’s nothing about the technique that limits it to two dimensions.", "Here are some 1-D examples:  How it works  The key insight behind our approach is that while generating a dataset from scratch to have particular statistical properties is relatively difficult, it is relatively easy to take an existing dataset, modify it slightly, and maintain (nearly) the same statistical properties.", "With repetition, this process creates a dataset with a different visual appearance from the original, while maintaining the same statistical properties.", "Further, if the modifications to the dataset are biased to move the points towards a particular goal, the resulting graph can be directed towards a particular visual appearance.", "In pseudo-code, it looks like this:  Where:  Initial_ds is the seed dataset defining the statistical properties which should be maintained  Perturb modifies the current version of the dataset by moving one or more points by a small amount in a random direction.", "The small amount is chosen from a normal distribution and calibrated so that 95% or more of movements should result in the overall statistical properties remaining unchanged (to two decimal places).", "The temp parameter is a temperature used for simulated annealing.", "The Fit function checks if perturbing the points has improved the overall fitness, and accepts it if so.", "When coercing the dataset into a target shape, it uses the average distance of all points to the nearest point on the target shape.", "To avoid getting stuck in a locally-optimal solution, a perturbation may also be accepted if the current temperature is greater than a random number between 0 and 1.", "Temperature starts out at 0.4, and is gradually reduced to 0.01 using a quadratically-smoothed monotonic cooling schedule.", "The newly returned perturbation is then tested (isErrorOk) to ensure that overall stats have not changed (within 2 decimal places), and becomes the current dataset if so.", "Here’s an example of dataset evolution across 200,000 iterations:  And of course you don’t have to start with a random cloud, you can evolve from any seed dataset.", "Here are some transformations from our friend the datasaurus:  Simpson’s paradox  Here’s a fun example where the target shape is used to produce a dataset exhibiting Simpson’s paradox.", "Start out with a strongly positively correlated dataset, for example:  Then give the algorithm a target shape that directs the dataset towards a series of negatively sloping lines:  A few iterations later, and we have a dataset with the same overall strongly positive correlation that we started with, but each subset of the data has an individually negative correlation.", "I find that a very satisfying way of demonstrating the effect!", "Cloning datasets for anonymity  As discussed by Govindaraju and Haslett another use for datasets with the same statistical properties is the creation of “cloned” datasets to anonymise sensitive data.", "In this case, it is important that individual data points are changed while the overall structure of the data remains similar.", "This can be accomplished by performing a Kolmogorov-Smirnov test within the isErrorOk function…  This is clearly similar to the idea of adding noise to a dataset to enhance privacy.", "I guess I need to read Govindaraju and Haslett’s paper though, as it seems to me at first glance that if all you have maintained are the overall statistical properties you might as well provide those alone.", "Anything else inferred from the generated data must just be an artificial artefact?", "It must depend on how far you move from the original dataset…  The code and datasets presented in this work are available from www.autodeskresearch.com/publications/samestats .", "If the thought of finding better insights through better visualisations inspires you, you might want to check out Miriah Meyer’s forthcoming book: ‘ Making data visual: a practical guide to using visualisation for insight.", "’"], "summary_text": "Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing Matejka & Fitzmaurice et al., CHI’17  Today’s paper choice is inspired by the keynote that Prof. Miriah Meyer gave at the recent Velocity conference in London, ‘ Why an interactive picture is worth a thousand numbers .’ She made a wonderful and thought-provoking case for the power of visualisations, and especially visualisations you can interact with, playing a central role in our process of understanding. Better ways of seeing and exploring data lead to better insights. Meyer opened her talk by showing us Cairo’s Datasaurus (‘ Never trust summary statistics alone; always visualize your data ’). You can calculate all the statistic summaries you like over the datasaurus dataset, run regressions, perform clustering, and so on. But until you look at it with the right visualisation, you’re never going to reach the same understanding as you get from looking at the data this way:  Since the early 70’s, Anscome’s Quartet has been frequently used to illustrate the importance of graphical representations when exploring data:  The effectiveness of Anscombe’s Quartet is not due to simply having four different data sets which generate the same statistical properties, it is that four clearly different and identifiably distinct datasets are producing the same statistical properties. In ‘Same Stats, Different Graphs,’ Matjeka & Fitzmaurice show a method for purposefully creating datasets which are identical over a range of statistical properties (of your choosing), yet produce dissimilar graphics. In my mind there’s a connection here to the idea of adversarial inputs to deep neural nets, which we might similarly express on some level as ‘Same Stats, Different Classes.’ Another thing I get from this paper is a very visual reminder of ‘Same Outcome (in terms of stats), Different Causes.’ There are lots of different hypotheses you could come up with that may produce the effect you’re seeing. Their method doesn’t just produce datasets that retain the same statistical properties while looking different though, it also allows you to guide the way in which the visualisation looks different (a bit like crafting an adversarial input to produce a specific mis-classification). For example, in the figure below we have an initial data set (top left) and a set of target shapes for data set generation. Here are the results produced by the technique when using these target shapes – every one of these has the same summary statistics as the initial dataset! These examples are all in 2D, but there’s nothing about the technique that limits it to two dimensions. Here are some 1-D examples:  How it works  The key insight behind our approach is that while generating a dataset from scratch to have particular statistical properties is relatively difficult, it is relatively easy to take an existing dataset, modify it slightly, and maintain (nearly) the same statistical properties. With repetition, this process creates a dataset with a different visual appearance from the original, while maintaining the same statistical properties. Further, if the modifications to the dataset are biased to move the points towards a particular goal, the resulting graph can be directed towards a particular visual appearance. In pseudo-code, it looks like this:  Where:  Initial_ds is the seed dataset defining the statistical properties which should be maintained  Perturb modifies the current version of the dataset by moving one or more points by a small amount in a random direction. The small amount is chosen from a normal distribution and calibrated so that 95% or more of movements should result in the overall statistical properties remaining unchanged (to two decimal places). The temp parameter is a temperature used for simulated annealing. The Fit function checks if perturbing the points has improved the overall fitness, and accepts it if so. When coercing the dataset into a target shape, it uses the average distance of all points to the nearest point on the target shape. To avoid getting stuck in a locally-optimal solution, a perturbation may also be accepted if the current temperature is greater than a random number between 0 and 1. Temperature starts out at 0.4, and is gradually reduced to 0.01 using a quadratically-smoothed monotonic cooling schedule. The newly returned perturbation is then tested (isErrorOk) to ensure that overall stats have not changed (within 2 decimal places), and becomes the current dataset if so. Here’s an example of dataset evolution across 200,000 iterations:  And of course you don’t have to start with a random cloud, you can evolve from any seed dataset. Here are some transformations from our friend the datasaurus:  Simpson’s paradox  Here’s a fun example where the target shape is used to produce a dataset exhibiting Simpson’s paradox. Start out with a strongly positively correlated dataset, for example:  Then give the algorithm a target shape that directs the dataset towards a series of negatively sloping lines:  A few iterations later, and we have a dataset with the same overall strongly positive correlation that we started with, but each subset of the data has an individually negative correlation. I find that a very satisfying way of demonstrating the effect! Cloning datasets for anonymity  As discussed by Govindaraju and Haslett another use for datasets with the same statistical properties is the creation of “cloned” datasets to anonymise sensitive data. In this case, it is important that individual data points are changed while the overall structure of the data remains similar. This can be accomplished by performing a Kolmogorov-Smirnov test within the isErrorOk function…  This is clearly similar to the idea of adding noise to a dataset to enhance privacy. I guess I need to read Govindaraju and Haslett’s paper though, as it seems to me at first glance that if all you have maintained are the overall statistical properties you might as well provide those alone. Anything else inferred from the generated data must just be an artificial artefact? It must depend on how far you move from the original dataset…  The code and datasets presented in this work are available from www.autodeskresearch.com/publications/samestats . If the thought of finding better insights through better visualisations inspires you, you might want to check out Miriah Meyer’s forthcoming book: ‘ Making data visual: a practical guide to using visualisation for insight. ’", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3025453.3025912?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/same-stats-different-graphs-generating-datasets-with-varied-appearance-and-identical-statistics-through-simulated-annealing.json"}
{"id": "31459000", "bin": "1100_1200", "summary_sentences": ["Towards deploying decommissioned mobile devices as cheap energy-efficient compute nodes Shahrad & Wentzlaff, HotCloud’17  I have one simple rule when it comes to selecting papers for The Morning Paper: I only cover papers that I like and find interesting.", "There are some papers though, that manage to generate in me a genuine feeling of excitement, as in “this is so cool, I can’t wait to share it.” This is one of those papers!", "You may remember back in 2010 when the US Air Force had the breakthrough idea to create a supercomputer out of 1,760 Sony Playstations .", "Well, Shahrad & Wentzlaff want us to pack our data centers with racks of decommissioned mobile phones!", "The case they put forward is both fascinating and compelling.", "Firstly, there are a lot of old mobile phones, and the mobile system on chips (SoCs) inside them have been gaining in power while having a low TCO.", "Secondly, it’s possible to pack a number of them in a 2U unit.", "Thirdly, there are a number of use cases for which such a collection of wimpy nodes seem well suited.", "And finally of course, it’s a wonderfully green way of recycling older devices that may e.g., have cracked screens, sluggish software etc..", "Deploying decommissioned mobile devices can be a major move towards green computing.", "This is mostly due to the fact that most of the carbon footprint of those devices comes from their production.", "Such deployment extends effective lifetime of mobile devices and decreases their average global warming potential (GWP), benefiting the environment.", "Are mobile phones really powerful enough to be useful in a data center?", "Both industry and academia already have their eye on mobile SoCs as the next most cost-effective platform in HPC – the gap between mobile SoCs and commodity server processors is shrinking and their TCO is much lower.", "If you look at mobile SoC performance for the last five years, something very interesting shows up:  (What a lovely s-curve example btw.).", "Moore’s law is kicking in, and the performance gap between a new and 3-year old device will shrink  The relative performance gap between high-end and low-end SoCs is shrinking, leading to similar performance on a cheaper device.", "Mobile CPU single core thermal design point has saturated at around 1.5W, so the performance power budget should stay steady as devices scale.", "Meanwhile, newer devices actually have slightly lower energy efficiency as they push for the last reserves of power.", "So decommissioned devices will actually have better overall energy efficiency.", "What applications could you run on a bunch of old phones?", "Due to their energy efficiency and improved performance, ARM-based architectures have recently gained substantial attention for HPC and cloud infrastructure deployment.", "ARM multicores deliver good energy proportionality for server workloads.", "Here are some promising use cases:  I/O intensive applications that are unable to saturate their CPU.", "Modern mobile SoCs support high bandwidth I/O and ample RAM size so I/O intensive applications can run on them with less I/O-CPU mismatch.", "Will your next VM be running on a decommissioned mobile phone?", "Low-end VMs on Amazon EC2 burstable t2.nano and t2.micro have 0.5GB and 1GB of memory respectively.", "Common hypervisors (KVM, Xen, …) support virtualizing ARM and an average mobile device has more than 2GB of memory – so a cloud provider could assign multiple such instances to each device!", "Applications requiring low-end GPU acceleration for platforms such as OpenCL.", "A SoC’s GPU can be shared between multiple tenants.", "Increasing the heterogeneity of cloud infrastructure to diversify reliability.", "How do you efficiently install mobile phone arrays inside a data center?", "The authors’ proposed design shows that decommissioned mobile devices can be housed in standard server racks.", "With three rows of fans, a network router, and a power supply, there is room for 84 cages (smartphones) of a size that fits more than 75% of models (notably excluding tablets!).", "With an average of 5.6 CPU cores per device, that adds up to about 470 cores in a 2U server box.", "Networking can be achieved either with a USB tree and shared master node, or USB on-the-go to each device.", "The latter will give much higher network performance, but requires more network switches.", "The phones come with another advantage that we get for free – batteries!", "Researchers have proposed using distributed UPSs or batteries to shave peak power in data centers.", "This allows installing more servers using the same power infrastructure and decreases the TCO… Distributed batteries effectively dampen temporal power demand variations; shaving the peak power under high utilization, while storing energy under low utilization.", "The high energy storage density enables more aggressive power capping of servers that are filled with used mobile devices.", "Even assuming 15% battery degradation per year, the capacity will be 4-8x denser than purpose designed distributed UPS solutions.", "Is it cost effective?", "We’ve seen that in theory racks of decommissioned mobile devices can be done, and we’ve seen that there are some potential use cases for such systems.", "But does it make financial sense??", "The authors choose the Samsung Galaxy Note 4 as a representative three-year old device, and match it against a Lenovo Flex System x880 X6 which has similar performance as 84 Note 4s.", "CAPEX and OPEX work out as follows (the authors assumed that the monitoring engineering, and installation cost of the mobile array is twice that of a standard server):  A TCO analysis shows that the mobile array beats the traditional server on TCO by some margin.", "(In the figures below ‘A’ is the traditional server, ‘B’ is the mobile array, and &delta; is the depreciation rate).", "The right sub-figures in Figure 5 (above) compare TCO when those two servers have different lifetimes.", "This analysis is essential for a fair comparison because we anticipate our proposed server to have a shorter lifetime compared to a new high-end server.", "It can be seen that with much shorter lifetimes, our proposed server can deliver better TCO values.", "It also shows how the equal-TCO margin (the line between light and dark areas) varies for different depreciation rates."], "summary_text": "Towards deploying decommissioned mobile devices as cheap energy-efficient compute nodes Shahrad & Wentzlaff, HotCloud’17  I have one simple rule when it comes to selecting papers for The Morning Paper: I only cover papers that I like and find interesting. There are some papers though, that manage to generate in me a genuine feeling of excitement, as in “this is so cool, I can’t wait to share it.” This is one of those papers! You may remember back in 2010 when the US Air Force had the breakthrough idea to create a supercomputer out of 1,760 Sony Playstations . Well, Shahrad & Wentzlaff want us to pack our data centers with racks of decommissioned mobile phones! The case they put forward is both fascinating and compelling. Firstly, there are a lot of old mobile phones, and the mobile system on chips (SoCs) inside them have been gaining in power while having a low TCO. Secondly, it’s possible to pack a number of them in a 2U unit. Thirdly, there are a number of use cases for which such a collection of wimpy nodes seem well suited. And finally of course, it’s a wonderfully green way of recycling older devices that may e.g., have cracked screens, sluggish software etc.. Deploying decommissioned mobile devices can be a major move towards green computing. This is mostly due to the fact that most of the carbon footprint of those devices comes from their production. Such deployment extends effective lifetime of mobile devices and decreases their average global warming potential (GWP), benefiting the environment. Are mobile phones really powerful enough to be useful in a data center? Both industry and academia already have their eye on mobile SoCs as the next most cost-effective platform in HPC – the gap between mobile SoCs and commodity server processors is shrinking and their TCO is much lower. If you look at mobile SoC performance for the last five years, something very interesting shows up:  (What a lovely s-curve example btw.). Moore’s law is kicking in, and the performance gap between a new and 3-year old device will shrink  The relative performance gap between high-end and low-end SoCs is shrinking, leading to similar performance on a cheaper device. Mobile CPU single core thermal design point has saturated at around 1.5W, so the performance power budget should stay steady as devices scale. Meanwhile, newer devices actually have slightly lower energy efficiency as they push for the last reserves of power. So decommissioned devices will actually have better overall energy efficiency. What applications could you run on a bunch of old phones? Due to their energy efficiency and improved performance, ARM-based architectures have recently gained substantial attention for HPC and cloud infrastructure deployment. ARM multicores deliver good energy proportionality for server workloads. Here are some promising use cases:  I/O intensive applications that are unable to saturate their CPU. Modern mobile SoCs support high bandwidth I/O and ample RAM size so I/O intensive applications can run on them with less I/O-CPU mismatch. Will your next VM be running on a decommissioned mobile phone? Low-end VMs on Amazon EC2 burstable t2.nano and t2.micro have 0.5GB and 1GB of memory respectively. Common hypervisors (KVM, Xen, …) support virtualizing ARM and an average mobile device has more than 2GB of memory – so a cloud provider could assign multiple such instances to each device! Applications requiring low-end GPU acceleration for platforms such as OpenCL. A SoC’s GPU can be shared between multiple tenants. Increasing the heterogeneity of cloud infrastructure to diversify reliability. How do you efficiently install mobile phone arrays inside a data center? The authors’ proposed design shows that decommissioned mobile devices can be housed in standard server racks. With three rows of fans, a network router, and a power supply, there is room for 84 cages (smartphones) of a size that fits more than 75% of models (notably excluding tablets!). With an average of 5.6 CPU cores per device, that adds up to about 470 cores in a 2U server box. Networking can be achieved either with a USB tree and shared master node, or USB on-the-go to each device. The latter will give much higher network performance, but requires more network switches. The phones come with another advantage that we get for free – batteries! Researchers have proposed using distributed UPSs or batteries to shave peak power in data centers. This allows installing more servers using the same power infrastructure and decreases the TCO… Distributed batteries effectively dampen temporal power demand variations; shaving the peak power under high utilization, while storing energy under low utilization. The high energy storage density enables more aggressive power capping of servers that are filled with used mobile devices. Even assuming 15% battery degradation per year, the capacity will be 4-8x denser than purpose designed distributed UPS solutions. Is it cost effective? We’ve seen that in theory racks of decommissioned mobile devices can be done, and we’ve seen that there are some potential use cases for such systems. But does it make financial sense?? The authors choose the Samsung Galaxy Note 4 as a representative three-year old device, and match it against a Lenovo Flex System x880 X6 which has similar performance as 84 Note 4s. CAPEX and OPEX work out as follows (the authors assumed that the monitoring engineering, and installation cost of the mobile array is twice that of a standard server):  A TCO analysis shows that the mobile array beats the traditional server on TCO by some margin. (In the figures below ‘A’ is the traditional server, ‘B’ is the mobile array, and &delta; is the depreciation rate). The right sub-figures in Figure 5 (above) compare TCO when those two servers have different lifetimes. This analysis is essential for a fair comparison because we anticipate our proposed server to have a shorter lifetime compared to a new high-end server. It can be seen that with much shorter lifetimes, our proposed server can deliver better TCO values. It also shows how the equal-TCO margin (the line between light and dark areas) varies for different depreciation rates.", "pdf_url": "https://www.usenix.org/system/files/conference/hotcloud17/hotcloud17-paper-shahrad.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/towards-deploying-decommissioned-mobile-devices-as-cheap-energy-efficient-compute-nodes.json"}
{"id": "8151447", "bin": "1100_1200", "summary_sentences": ["Congestion Avoidance and Control – Jacobson & Karels, 1988  (** corrected spelling of Jacobs_o_n **)  It’s October 1986 and there’s trouble on the internet.", "A congestion collapse has reduced the bandwidth between LBL and UC Berkeley by a factor of a thousand.", "These two sites happened to be 400 yds apart.", "And that drop in capacity?", "From 32 Kbps to 40  bps!", "This was just the first in a series of such collapses.", "Van Jacobsen and Michael Karels set out to investigate.", "Beyond just TCP/IP, the lessons they learned have relevance for any situation where traffic management & flow control are important.", "For example, they can be applied when managing back-offs and retries with circuit breakers and dimmer switches , and when deciding how quickly to let traffic through again on restoring flow (with a little imagination to consider a window of outstanding requests).", "To avoid congestion collapse, the authors propose the principle of the conservation of packets.", "By ‘conservation of packets’ we mean that for a connection ‘in equilibrium’,  i.e., running stably with a full window of data in transit, the packet flow is what a physicist would call ‘conservative’: A new packet isn’ t put into the network until an old packet leaves.", "The physics of flow predicts that systems with this property should be robust in the face of congestion.", "Note that QJump which we looked at last week uses exactly this principle.", "If this principle were obeyed, congestion collapse would become the exception rather than the rule.", "Thus congestion control involves finding places that violate conservation and fixing them.", "There are only three ways that packets can fail to be conserved:  The connection doesn’t get to equilibrium (typically, the sender floods the receiver on start or restart)  A sender injects a new packet before an old packet has exited  The equilibrium can’t be reached because of resource limits along the path  These can be addressed by, respectively: starting slowly to bring a connection up to equilibrium; conserving equilibrium through management of RTT variability to avoid too many retransmissions; and adapting the path by throttling back sources under in-network congestion.", "Slow Start  Once a connection is up and running acks can regulate a ‘clock’ that stops packets being sent too quickly for the receiver, since acks can’t be generated any faster than packets can get through the network.", "But how do you get the data flowing in the first place?", "To start the `clock’, we developed a slow-start algorithm to gradually increase the amount of data in-transit.", "Although we flatter ourselves that the design of this algorithm is rather subtle, the implementation is trivial—one new state variable and three lines of code in the sender:  Add a congestion window, cwnd, to the per-connection state.", "When starting or restarting after a loss, set cwnd to one packet.", "On each ack for newdata, increase cwnd by one packet.", "When sending, send the minimum of receiver’s advertised window and cwnd.", "Actually, the slow-start window increase isn’ t that slow: it takes time R log2W where R is the round-trip-time and W is the window size in packets.", "This means the window opens quickly enough to have a negligible effect on performance, even on links with a large bandwidth–delay product.", "Retransmissions (retries)  A good round trip time estimator, the core of the retransmit timer, is the single most important feature of any protocol implementation that expects to survive heavy load.", "And it is frequently botched…One mistake is not estimating the variation of the round trip time, R .", "From queuing theory we know that R and the variation in R increase quickly with load.", "See the details in appendix A of the paper for a cheap to compute variation estimator (prior versions of the TCP specification recommended a constant value to use), which can then be used to update the retransmit timeout operator for the next packet sent.", "Another timer mistake is in the backoff after a retransmit: If a packet has to be retransmitted more than once, how should the retransmits be spaced?", "For a transport endpoint embedded in a network of unknown topology and with an unknown, unknowable and constantly changing population of competing conversations, only one scheme has any hope of working—exponential backoff—but a proof of this is beyond the scope of this paper.", "It’s all to do with linear system theory apparently:  Linear system theory says that if a system is stable, the stability is exponential.", "This suggests that an unstable system (a network subject to random load shocks and prone to congestive collapse can be stabilized by adding some exponential damping (exponential timer backoff) to its primary excitation (senders, traffic sources).", "Congestion management  If timeouts are set appropriately as above, then timeouts will be due to lost packets.", "Packets could be damaged in transit (comparatively rare), or dropped due to congestion in the network.", "Therefore we can assume that timeouts are a good proxy sign for network congestion.", "The network must be able to signal the transport endpoints that congestion is occurring (or about to occur).", "And the endpoints must have a policy that decreases utilization if this signal is received and increases utilization if the signal isn’ t received.", "We can use timeouts as the signal, but what shoud the policy be?", "When the network is congested… the queue lengths will start increasing exponentially.", "The system will stabilize only if the traffic sources throttle back at least as quickly as the queues are growing.", "Load is controlled by adjusting the size of the window.", "So when congestion is signalled the window  is resized to d.W, where d is a constant   The first thought is to use a symmetric, multiplicative increase, possibly with a longer time constant,  Wi = bWi-1 where 1 < b < 1/d .", "This is a mistake.", "The result will oscillate wildly and, on the average, deliver poor throughput.", "The best increase policy is to make small, constant changes to the window size: Wi = Wi-1 + u.", "We end up with the following simple implementation:  On any timeout, set cwnd to half the current window size (this is the multiplicative decrease).", "On each ack for new data, increase cwnd by 1/cwnd (this is the additive increase).", "When sending, send the minimum of the receiver’ s advertised window and cwnd."], "summary_text": "Congestion Avoidance and Control – Jacobson & Karels, 1988  (** corrected spelling of Jacobs_o_n **)  It’s October 1986 and there’s trouble on the internet. A congestion collapse has reduced the bandwidth between LBL and UC Berkeley by a factor of a thousand. These two sites happened to be 400 yds apart. And that drop in capacity? From 32 Kbps to 40  bps! This was just the first in a series of such collapses. Van Jacobsen and Michael Karels set out to investigate. Beyond just TCP/IP, the lessons they learned have relevance for any situation where traffic management & flow control are important. For example, they can be applied when managing back-offs and retries with circuit breakers and dimmer switches , and when deciding how quickly to let traffic through again on restoring flow (with a little imagination to consider a window of outstanding requests). To avoid congestion collapse, the authors propose the principle of the conservation of packets. By ‘conservation of packets’ we mean that for a connection ‘in equilibrium’,  i.e., running stably with a full window of data in transit, the packet flow is what a physicist would call ‘conservative’: A new packet isn’ t put into the network until an old packet leaves. The physics of flow predicts that systems with this property should be robust in the face of congestion. Note that QJump which we looked at last week uses exactly this principle. If this principle were obeyed, congestion collapse would become the exception rather than the rule. Thus congestion control involves finding places that violate conservation and fixing them. There are only three ways that packets can fail to be conserved:  The connection doesn’t get to equilibrium (typically, the sender floods the receiver on start or restart)  A sender injects a new packet before an old packet has exited  The equilibrium can’t be reached because of resource limits along the path  These can be addressed by, respectively: starting slowly to bring a connection up to equilibrium; conserving equilibrium through management of RTT variability to avoid too many retransmissions; and adapting the path by throttling back sources under in-network congestion. Slow Start  Once a connection is up and running acks can regulate a ‘clock’ that stops packets being sent too quickly for the receiver, since acks can’t be generated any faster than packets can get through the network. But how do you get the data flowing in the first place? To start the `clock’, we developed a slow-start algorithm to gradually increase the amount of data in-transit. Although we flatter ourselves that the design of this algorithm is rather subtle, the implementation is trivial—one new state variable and three lines of code in the sender:  Add a congestion window, cwnd, to the per-connection state. When starting or restarting after a loss, set cwnd to one packet. On each ack for newdata, increase cwnd by one packet. When sending, send the minimum of receiver’s advertised window and cwnd. Actually, the slow-start window increase isn’ t that slow: it takes time R log2W where R is the round-trip-time and W is the window size in packets. This means the window opens quickly enough to have a negligible effect on performance, even on links with a large bandwidth–delay product. Retransmissions (retries)  A good round trip time estimator, the core of the retransmit timer, is the single most important feature of any protocol implementation that expects to survive heavy load. And it is frequently botched…One mistake is not estimating the variation of the round trip time, R . From queuing theory we know that R and the variation in R increase quickly with load. See the details in appendix A of the paper for a cheap to compute variation estimator (prior versions of the TCP specification recommended a constant value to use), which can then be used to update the retransmit timeout operator for the next packet sent. Another timer mistake is in the backoff after a retransmit: If a packet has to be retransmitted more than once, how should the retransmits be spaced? For a transport endpoint embedded in a network of unknown topology and with an unknown, unknowable and constantly changing population of competing conversations, only one scheme has any hope of working—exponential backoff—but a proof of this is beyond the scope of this paper. It’s all to do with linear system theory apparently:  Linear system theory says that if a system is stable, the stability is exponential. This suggests that an unstable system (a network subject to random load shocks and prone to congestive collapse can be stabilized by adding some exponential damping (exponential timer backoff) to its primary excitation (senders, traffic sources). Congestion management  If timeouts are set appropriately as above, then timeouts will be due to lost packets. Packets could be damaged in transit (comparatively rare), or dropped due to congestion in the network. Therefore we can assume that timeouts are a good proxy sign for network congestion. The network must be able to signal the transport endpoints that congestion is occurring (or about to occur). And the endpoints must have a policy that decreases utilization if this signal is received and increases utilization if the signal isn’ t received. We can use timeouts as the signal, but what shoud the policy be? When the network is congested… the queue lengths will start increasing exponentially. The system will stabilize only if the traffic sources throttle back at least as quickly as the queues are growing. Load is controlled by adjusting the size of the window. So when congestion is signalled the window  is resized to d.W, where d is a constant   The first thought is to use a symmetric, multiplicative increase, possibly with a longer time constant,  Wi = bWi-1 where 1 < b < 1/d . This is a mistake. The result will oscillate wildly and, on the average, deliver poor throughput. The best increase policy is to make small, constant changes to the window size: Wi = Wi-1 + u. We end up with the following simple implementation:  On any timeout, set cwnd to half the current window size (this is the multiplicative decrease). On each ack for new data, increase cwnd by 1/cwnd (this is the additive increase). When sending, send the minimum of the receiver’ s advertised window and cwnd.", "pdf_url": "http://ee.lbl.gov/papers/congavoid.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/congestion-avoidance-and-control.json"}
{"id": "13135318", "bin": "1100_1200", "summary_sentences": ["Derflow: Distributed Deterministic Dataflow programming for Erlang – Bravo et al. 2014  Today’s choice is part of the work of the SyncFree European research project on large-scale computation without synchronisation.", "Non-determinism makes it very difficult to reason about distributed applications.", "So Bravo et al. figured life might be easier if we could just make them deterministic instead.", "How do you do that?", "Just go with Derflow…  Concurrent programs in non-deterministic languages are notoriously hard to prove correct and have led to well-known disasters….", "we believe that limiting the ability to write non-deterministic code provides a reasonable alternative to exhaustively checking our applications for correctness.", "Derflow is based on the idea of a distributed single-assignment data store, and is built on top of riak core.", "Given the same input values, a program written in deterministic dataflow style will always return the same output values, or never return.", "These input values can be data streams as well, which is a natural generalization of functional programming to the concurrent setting.", "Our proposed solution provides a distributed deterministic dataflow solution which operates transparently over distributed Erlang, providing the ability to have highly-available, fault-tolerant, deterministic computations.", "The basic Derflow model is easy to understand.", "Think of a single assignment store where the keys are variable identifiers, and the values are either as-yet-unassigned, a reference to another variable, or an Erlang term.", "Once you’ve assigned a value to a variable, you can never change it.", "There’s also a whole bunch of hidden metadata associated with each variable that the system keeps track of:  what variables are bound to this one  what processes are waiting for the variable to be bound to a value  if a variable is part of a stream, what the ‘next’ value in the stream is (see below)  any processes waiting on a reader before setting the value (to support lazy production of values)  any monitors tracking the availability/reachability of the variable (to handle partitions etc.)", "The basic programming model consists of declare() (creates a new variable and returns its id), bind(x,v) (bind a variable x to value v) , and read(x).", "A process reading an unbound variable will block until a value is bound.", "Derflow also makes use of Erlang’s spawn to support concurrency.", "Streaming data is supported by treating the stream as a list of dataflow variables:  Streams are a useful technique which allow threads, or processes, to communicate and synchronize in concurrent programming.", "A stream is represented here as a list of dataflow variables, with an unbound dataflow variable as the final element of the list.", "The operation produce extends the tail of the stream by binding a new value to the last element, and creating a new unbound last variable (‘next’) which it returns.", "consume reads an element from the stream and also returns the id of the next variable in the stream.", "Failures need to be carefully handled if they are not to introduce non-determinism.", "determinism and dataflow variables provide a very useful property for failure handling: redundant computation will not affect the correctness of a deterministic dataflow program.", "We propose a failure handling model where failed processes or temporarily unreachable processes, can be restarted while still providing the guarantees of the deterministic programming model.", "If a computing process fails, it is just re-executed.", "In the Derflow model, duplicate processing cannot alter the outcome.", "(Though of course this relies on programs being side-effect free too).", "If a process is blocked because a variable it is waiting to read never becomes available, then restarts have to cascade more deeply:  The re-execution of blocked process will result in the process immediately blocking again.", "Therefore we must provide a way to identify dependencies between processes and dataflow variables in order to provide a deterministic restart strategy which guarantees progress.", "A common strategy to ensure progress in this situation is to restart the process that declared the failed dataflow variable.", "In addition, all the processes depending on the restarted process should also be restarted.", "The implementation of Derflow builds on riak core.", "mnesia was considered but rejected, in part because of its behaviour under network partitions:  Problems arise in the presence of network partitions where the mnesia nodes on either side of the network partition are able to make progress independently.", "Currently, no mechanisms exist for reconciling the changes made to the database when nodes reconnect, nor reasoning about concurrent or causally influenced operations.", "Several examples of Derflow programs are given, which show that the outcome of the program is independent of the degree of concurrency used in computing it.", "In Derflow, any function that uses dataflow variables can be run in a different process while keeping the final result same.", "Thus, programmers can transparently add concurrency to their programs (either parallelism or distribution) in a secure way without thinking about data races and possible bugs.", "However, this concurrency is not managed automatically via the runtime – programmers must explicitly specify it (contrast this to e.g. an execution planner for datalog).", "The semantics of Derflow programs are very clearly explained in the paper,  and the deterministic model of concurrent execution certainly looks interesting.", "Yet I found it hard to assess just from this one paper whether Derflow would be an interesting way of writing real systems, or whether the restrictions would feel limiting.", "Perhaps the answers to this question are found in the literature on Kahn Process Networks on which Derflow is based.", "Deterministic dataflow was first proposed by Gilles Kahn in 1974, in a programming model that is now known as Kahn networks.", "In 1977, a lazy version of this same model was proposed by Kahn and David MacQueen.", "However, up until recently this model has never become part of mainstream concurrent programming.", "This may be due to either the model’s inability to express non-determinism or the simultaneous invention of two other models for handling concurrent programming: the actor model (message passing) and monitors (shared state).", "(Note that Derflow also introduces a mechanism to support explicit introduction of small amounts of non-determinism where you really need them)."], "summary_text": "Derflow: Distributed Deterministic Dataflow programming for Erlang – Bravo et al. 2014  Today’s choice is part of the work of the SyncFree European research project on large-scale computation without synchronisation. Non-determinism makes it very difficult to reason about distributed applications. So Bravo et al. figured life might be easier if we could just make them deterministic instead. How do you do that? Just go with Derflow…  Concurrent programs in non-deterministic languages are notoriously hard to prove correct and have led to well-known disasters…. we believe that limiting the ability to write non-deterministic code provides a reasonable alternative to exhaustively checking our applications for correctness. Derflow is based on the idea of a distributed single-assignment data store, and is built on top of riak core. Given the same input values, a program written in deterministic dataflow style will always return the same output values, or never return. These input values can be data streams as well, which is a natural generalization of functional programming to the concurrent setting. Our proposed solution provides a distributed deterministic dataflow solution which operates transparently over distributed Erlang, providing the ability to have highly-available, fault-tolerant, deterministic computations. The basic Derflow model is easy to understand. Think of a single assignment store where the keys are variable identifiers, and the values are either as-yet-unassigned, a reference to another variable, or an Erlang term. Once you’ve assigned a value to a variable, you can never change it. There’s also a whole bunch of hidden metadata associated with each variable that the system keeps track of:  what variables are bound to this one  what processes are waiting for the variable to be bound to a value  if a variable is part of a stream, what the ‘next’ value in the stream is (see below)  any processes waiting on a reader before setting the value (to support lazy production of values)  any monitors tracking the availability/reachability of the variable (to handle partitions etc.) The basic programming model consists of declare() (creates a new variable and returns its id), bind(x,v) (bind a variable x to value v) , and read(x). A process reading an unbound variable will block until a value is bound. Derflow also makes use of Erlang’s spawn to support concurrency. Streaming data is supported by treating the stream as a list of dataflow variables:  Streams are a useful technique which allow threads, or processes, to communicate and synchronize in concurrent programming. A stream is represented here as a list of dataflow variables, with an unbound dataflow variable as the final element of the list. The operation produce extends the tail of the stream by binding a new value to the last element, and creating a new unbound last variable (‘next’) which it returns. consume reads an element from the stream and also returns the id of the next variable in the stream. Failures need to be carefully handled if they are not to introduce non-determinism. determinism and dataflow variables provide a very useful property for failure handling: redundant computation will not affect the correctness of a deterministic dataflow program. We propose a failure handling model where failed processes or temporarily unreachable processes, can be restarted while still providing the guarantees of the deterministic programming model. If a computing process fails, it is just re-executed. In the Derflow model, duplicate processing cannot alter the outcome. (Though of course this relies on programs being side-effect free too). If a process is blocked because a variable it is waiting to read never becomes available, then restarts have to cascade more deeply:  The re-execution of blocked process will result in the process immediately blocking again. Therefore we must provide a way to identify dependencies between processes and dataflow variables in order to provide a deterministic restart strategy which guarantees progress. A common strategy to ensure progress in this situation is to restart the process that declared the failed dataflow variable. In addition, all the processes depending on the restarted process should also be restarted. The implementation of Derflow builds on riak core. mnesia was considered but rejected, in part because of its behaviour under network partitions:  Problems arise in the presence of network partitions where the mnesia nodes on either side of the network partition are able to make progress independently. Currently, no mechanisms exist for reconciling the changes made to the database when nodes reconnect, nor reasoning about concurrent or causally influenced operations. Several examples of Derflow programs are given, which show that the outcome of the program is independent of the degree of concurrency used in computing it. In Derflow, any function that uses dataflow variables can be run in a different process while keeping the final result same. Thus, programmers can transparently add concurrency to their programs (either parallelism or distribution) in a secure way without thinking about data races and possible bugs. However, this concurrency is not managed automatically via the runtime – programmers must explicitly specify it (contrast this to e.g. an execution planner for datalog). The semantics of Derflow programs are very clearly explained in the paper,  and the deterministic model of concurrent execution certainly looks interesting. Yet I found it hard to assess just from this one paper whether Derflow would be an interesting way of writing real systems, or whether the restrictions would feel limiting. Perhaps the answers to this question are found in the literature on Kahn Process Networks on which Derflow is based. Deterministic dataflow was first proposed by Gilles Kahn in 1974, in a programming model that is now known as Kahn networks. In 1977, a lazy version of this same model was proposed by Kahn and David MacQueen. However, up until recently this model has never become part of mainstream concurrent programming. This may be due to either the model’s inability to express non-determinism or the simultaneous invention of two other models for handling concurrent programming: the actor model (message passing) and monitors (shared state). (Note that Derflow also introduces a mechanism to support explicit introduction of small amounts of non-determinism where you really need them).", "pdf_url": "http://www.info.ucl.ac.be/~pvr/erlang14cameraready.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/derflow-distributed-deterministic-dataflow-programming-for-erlang.json"}
{"id": "76635514", "bin": "1100_1200", "summary_sentences": ["IncludeOS: A minimal, resource efficient unikernel for cloud systems – Bratterud et al. 2015  There has been lots of excitement around unikernels over the last year, and especially with the recent acquisition of the Unikernel Systems team by Docker ( MirageOS , Mergeable Persistent Data Structures , Jitsu: Just-in time summoning of Unikernels ).", "Whereas MirageOS is built around an OCaml stack, in today’s paper choice we get a look at IncludeOS, which is built on a C++ stack.", "In true unikernel style, you just include the parts of the operating system you need, directly linked with your application.", "What makes me smile every time is that in IncludeOS this is literally achieved via ‘#include <os>’ !", "In this paper we present IncludeOS, a single-tasking operating system designed for virtualizedenvironments.", "IncludeOS provides a novel way for developers to build their C++-based code directlyinto a virtual machine at compile-time…  A fully virtualized “Hello World” service in IncludeOS (which of course includes the necessary components of the OS) uses only 8.45MB of memory.", "A Ubuntu 14.04 OS image (the default guest OS for OpenStack) is around 300MB by comparison.", "Even running a regular Java Hello World program (ignoring the OS), just the Java process itself takes about 28MB.", "If you’re spinning up lots of instances in your cluster, this reduction in memory overhead can result in significant savings – memory being one of the most expensive resources.", "A minimal IncludeOS VM can also boot in about 0.3s.", "A DNS service built with IncludeOS results in a 158K disk image (for comparison, the MirageOS DNS server image came in at 200K).", "Finally, IncludeOS is designed to be very efficient at runtime, when idle it uses no CPU at all.", "The designers of IncludeOS were guided by the ‘Zero Overhead Principle‘ :  IncludeOS aims for true minimality in the sense that nothing should be included by default that the service does not explicitly need.", "This corresponds to the zero overhead principle of e.g. C++; ”what you don’t use you don’t pay for.” … While many other projects are related, IncludeOS is different: where systems such as Mirage and OSv aims to provide a platform for a high-level language-runtimes, which impose significant resource penalties in themselves, IncludeOS aims to represent absolute minimality.", "Including only what is needed in an OS image is a job that can be delegated to the GCC tool chain:  The mechanism used for extracting only what is needed from the operating system, is the one provided by default by modern linkers.", "Each part of the OS is compiled into an object-file, such as ip4.o, udp.o, pci_device.o etc., which are then combined using ar to form a static library os.a.", "When a program links with this library, only what’s necessary will automatically be extracted by the linker and end up in the final binary.", "To facilitate this build process a custom GCC-toolchain has been created.", "For the C standard library, IncludeOS uses RedHat’s standard library implementation due to its small size, reliance on only a handful of system calls, and ability to be compiled into a statically linked library.", "“The C++ standard library is larger and trickier…” Currently IncludeOS uses Electronic Art’s EASTL exception-free implementation, future work will include a port of a full-featured implementation.", "IncludeOS currently has only one device driver, namely a VirtioNet Device driver.", "The key benefit of virtio is that the hypervisor does not need to emulate a certain physical device, but instead can insert data directly into a queue in memory shared by the guest.", "While Virtio 1.0 has recently emerged as an OASIS standard, none of the hypervisors used during development supported any of the new features.", "Therefore the driver currently only implements Virtio Legacy functionality, but development has been done with future support for Virtio 1.0 in mind.", "The network stack was a more complex challenge, since existing network stacks are often entangled with the operating system and not designed with the zero overhead principle in mind.", "The IncludeOS project is working on a completely modularized networking stack – the current implementation is sufficiently advanced to support e.g. the DNS server implementation previously mentioned, and work is underway to complete a full TCP and IPv6 stack that will also be running standalone in Linux user space.", "Currently, all IRQ handlers in IncludeOS will simply (atomically) update a counter, and defer further handling to the main event-loop, whenever there is time.", "This eliminates the need for a context switch, while also eliminating concurrency-related issues such as race conditions.", "The CPU is kept busy by having all I/O be asynchronous, so that no blocking occurs.", "This encourages a callback-based programming model, such as is common in modern Javascript applications.", "This is one of a number of factors that contribute to IncludeOS’s excellent runtime performance:  There is no system call overhead as the OS and the service are one binary, eliminating the need for memory protection barriers.", "There is no unnecessary overhead from timer interrupts.", "There is no I/O waiting, since IncludeOS uses an asynchronous event-based I/O model.", "There is no overhead from emulating the Programmable Interrupt Timer (i.e. no periodic timer interrupts, and no pre-emptive scheduling).", "The number of protected instructions has been kept very low reduce VM exits.", "That being so, IncludeOS in its current form is not fit for every task.", "In particular, deferring all IRQ’s will cause the VM to seem unresponsive (i.e. not answer ping) under workloads requiring a lot of CPU activity per request (this is not the case for DNS)… For services requiring several seconds of CPU-processing for each request, ICMP-packets would simply be queued until the virtio-queue was full, at which point they would be dropped, giving the impression of an unresponsive service.", "A really interesting future development for IncludeOS is the design of a Node.js style framework for supporting high performance web-applications:  A near-future use case for IncludeOS will be running high-performance web-applications, written in an asynchronous programming style similar to Node.js, but in a maximally efficient and minimal-overhead C++ language framework.", "These services will have no host kernel dependencies, running directly on top of virtual hardware, in any IaaS cloud."], "summary_text": "IncludeOS: A minimal, resource efficient unikernel for cloud systems – Bratterud et al. 2015  There has been lots of excitement around unikernels over the last year, and especially with the recent acquisition of the Unikernel Systems team by Docker ( MirageOS , Mergeable Persistent Data Structures , Jitsu: Just-in time summoning of Unikernels ). Whereas MirageOS is built around an OCaml stack, in today’s paper choice we get a look at IncludeOS, which is built on a C++ stack. In true unikernel style, you just include the parts of the operating system you need, directly linked with your application. What makes me smile every time is that in IncludeOS this is literally achieved via ‘#include <os>’ ! In this paper we present IncludeOS, a single-tasking operating system designed for virtualizedenvironments. IncludeOS provides a novel way for developers to build their C++-based code directlyinto a virtual machine at compile-time…  A fully virtualized “Hello World” service in IncludeOS (which of course includes the necessary components of the OS) uses only 8.45MB of memory. A Ubuntu 14.04 OS image (the default guest OS for OpenStack) is around 300MB by comparison. Even running a regular Java Hello World program (ignoring the OS), just the Java process itself takes about 28MB. If you’re spinning up lots of instances in your cluster, this reduction in memory overhead can result in significant savings – memory being one of the most expensive resources. A minimal IncludeOS VM can also boot in about 0.3s. A DNS service built with IncludeOS results in a 158K disk image (for comparison, the MirageOS DNS server image came in at 200K). Finally, IncludeOS is designed to be very efficient at runtime, when idle it uses no CPU at all. The designers of IncludeOS were guided by the ‘Zero Overhead Principle‘ :  IncludeOS aims for true minimality in the sense that nothing should be included by default that the service does not explicitly need. This corresponds to the zero overhead principle of e.g. C++; ”what you don’t use you don’t pay for.” … While many other projects are related, IncludeOS is different: where systems such as Mirage and OSv aims to provide a platform for a high-level language-runtimes, which impose significant resource penalties in themselves, IncludeOS aims to represent absolute minimality. Including only what is needed in an OS image is a job that can be delegated to the GCC tool chain:  The mechanism used for extracting only what is needed from the operating system, is the one provided by default by modern linkers. Each part of the OS is compiled into an object-file, such as ip4.o, udp.o, pci_device.o etc., which are then combined using ar to form a static library os.a. When a program links with this library, only what’s necessary will automatically be extracted by the linker and end up in the final binary. To facilitate this build process a custom GCC-toolchain has been created. For the C standard library, IncludeOS uses RedHat’s standard library implementation due to its small size, reliance on only a handful of system calls, and ability to be compiled into a statically linked library. “The C++ standard library is larger and trickier…” Currently IncludeOS uses Electronic Art’s EASTL exception-free implementation, future work will include a port of a full-featured implementation. IncludeOS currently has only one device driver, namely a VirtioNet Device driver. The key benefit of virtio is that the hypervisor does not need to emulate a certain physical device, but instead can insert data directly into a queue in memory shared by the guest. While Virtio 1.0 has recently emerged as an OASIS standard, none of the hypervisors used during development supported any of the new features. Therefore the driver currently only implements Virtio Legacy functionality, but development has been done with future support for Virtio 1.0 in mind. The network stack was a more complex challenge, since existing network stacks are often entangled with the operating system and not designed with the zero overhead principle in mind. The IncludeOS project is working on a completely modularized networking stack – the current implementation is sufficiently advanced to support e.g. the DNS server implementation previously mentioned, and work is underway to complete a full TCP and IPv6 stack that will also be running standalone in Linux user space. Currently, all IRQ handlers in IncludeOS will simply (atomically) update a counter, and defer further handling to the main event-loop, whenever there is time. This eliminates the need for a context switch, while also eliminating concurrency-related issues such as race conditions. The CPU is kept busy by having all I/O be asynchronous, so that no blocking occurs. This encourages a callback-based programming model, such as is common in modern Javascript applications. This is one of a number of factors that contribute to IncludeOS’s excellent runtime performance:  There is no system call overhead as the OS and the service are one binary, eliminating the need for memory protection barriers. There is no unnecessary overhead from timer interrupts. There is no I/O waiting, since IncludeOS uses an asynchronous event-based I/O model. There is no overhead from emulating the Programmable Interrupt Timer (i.e. no periodic timer interrupts, and no pre-emptive scheduling). The number of protected instructions has been kept very low reduce VM exits. That being so, IncludeOS in its current form is not fit for every task. In particular, deferring all IRQ’s will cause the VM to seem unresponsive (i.e. not answer ping) under workloads requiring a lot of CPU activity per request (this is not the case for DNS)… For services requiring several seconds of CPU-processing for each request, ICMP-packets would simply be queued until the virtio-queue was full, at which point they would be dropped, giving the impression of an unresponsive service. A really interesting future development for IncludeOS is the design of a Node.js style framework for supporting high performance web-applications:  A near-future use case for IncludeOS will be running high-performance web-applications, written in an asynchronous programming style similar to Node.js, but in a maximally efficient and minimal-overhead C++ language framework. These services will have no host kernel dependencies, running directly on top of virtual hardware, in any IaaS cloud.", "pdf_url": "https://github.com/hioa-cs/IncludeOS/blob/master/doc/papers/IncludeOS_IEEE_CloudCom2015_PREPRINT.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/includeos.json"}
{"id": "32533290", "bin": "1100_1200", "summary_sentences": ["What and why  Traditionally neural nets use max pooling with 2x2 grids (2MP).", "2MP reduces the image dimensions by a factor of 2.", "An alternative would be to use pooling schemes that reduce by factors other than two, e.g. 1 < factor < 2.", "Pooling by a factor of sqrt(2) would allow twice as many pooling layers as 2MP, resulting in \"softer\" image size reduction throughout the network.", "Fractional Max Pooling (FMP) is such a method to perform max pooling by factors other than 2.", "How  In 2MP you move a 2x2 grid always by 2 pixels.", "Imagine that these step sizes follow a sequence, i.e. for 2MP: 2222222...", "If you mix in just a single 1 you get a pooling factor of <2.", "By chosing the right amount of 1s vs. 2s you can pool by any factor between 1 and 2.", "The sequences of 1s and 2s can be generated in fully random order or in pseudorandom order, where pseudorandom basically means \"predictable sub patterns\" (e.g. 211211211211211...).", "FMP can happen disjoint or overlapping.", "Disjoint means 2x2 grids, overlapping means 3x3.", "Results  FMP seems to perform generally better than 2MP.", "Better results on various tests, including CIFAR-10 and CIFAR-100 (often quite significant improvement).", "Best configuration seems to be random sequences with overlapping regions.", "Results are especially better if each test is repeated multiple times per image (as the random sequence generation creates randomness, similar to dropout).", "First 5-10 repetitions seem to be most valuable, but even 100+ give some improvement.", "An FMP-factor of sqrt(2) was usually used.", "Random FMP with a factor of sqrt(2) applied five times to the same input image (results upscaled back to original size).", "Rough chapter-wise notes  (1) Convolutional neural networks  Advantages of 2x2 max pooling (2MP): fast; a bit invariant to translations and distortions; quick reduction of image sizes  Disadvantages: \"disjoint nature of pooling regions\" can limit generalization (i.e. that they don't overlap?", "); reduction of image sizes can be too quick  Alternatives to 2MP: 3x3 pooling with stride 2, stochastic 2x2 pooling  All suggested alternatives to 2MP also reduce sizes by a factor of 2  Author wants to have reduction by sqrt(2) as that would enable to use twice as many pooling layers  Fractional Max Pooling = Pooling that reduces image sizes by a factor of 1 < alpha < 2  FMP introduces randomness into pooling (by the choice of pooling regions)  Settings of FMP:  Pooling Factor alpha in range [1, 2] (1 = no change in image sizes, 2 = image sizes get halfed)  Choice of Pooling-Regions: Random or pseudorandom.", "Random is stronger (?).", "Random+Dropout can result in underfitting.", "Disjoint or overlapping pooling regions.", "Results for overlapping are better.", "(2) Fractional max-pooling  For traditional 2MP, every grid's top left coordinate is at (2i-1, 2j-1) and it's bottom right coordinate at (2i, 2j) (i=col, j=row).", "It will reduce the original size N to 1/2N, i.e. 2N_in = N_out.", "Paper analyzes 1 < alpha < 2, but alpha > 2 is also possible.", "Grid top left positions can be described by sequences of integers, e.g. (only column): 1, 3, 5, ...  Disjoint 2x2 pooling might be 1, 3, 5, ... while overlapping would have the same sequence with a larger 3x3 grid.", "The increment of the sequences can be random or pseudorandom for alphas < 2.", "For 2x2 FMP you can represent any alpha with a \"good\" sequence of increments that all have values 1 or 2, e.g. 2111121122111121...", "In the case of random FMP, the optimal fraction of 1s and 2s is calculated.", "Then a random permutation of a sequence of 1s and 2s is generated.", "In the case of pseudorandom FMP, the 1s and 2s follow a pattern that leads to the correct alpha, e.g. 112112121121211212...  Random FMP creates varying distortions of the input image.", "Pseudorandom FMP is a faithful downscaling.", "(3) Implementation  In their tests they use a convnet starting with 10 convolutions, then 20, then 30, ...", "They add FMP with an alpha of sqrt(2) after every conv layer.", "They calculate the desired output size, then go backwards through their network to the input.", "They multiply the size of the image by sqrt(2) with every FMP layer and add a flat 1 for every conv layer.", "The result is the required image size.", "They pad the images to that size.", "They use dropout, with increasing strength from 0% to 50% towards the output.", "They use LeakyReLUs.", "Every time they apply an FMP layer, they generate a new sequence of 1s and 2s.", "That indirectly makes the network an ensemble of similar networks.", "The output of the network can be averaged over several forward passes (for the same image).", "The result then becomes more accurate (especially up to >=6 forward passes).", "(4) Results  Tested on MNIST and CIFAR-100  Architectures (somehow different from (3)?", "):  MNIST: 36x36 img -> 6 times (32 conv (3x3?)", "-> FMP alpha=sqrt(2)) -> ?", "-> ?", "-> output  CIFAR-100: 94x94 img -> 12 times (64 conv (3x3?)", "-> FMP alpha=2^(1/3)) -> ?", "-> ?", "-> output  Overlapping pooling regions seemed to perform better than disjoint regions.", "Random FMP seemed to perform better than pseudorandom FMP.", "Other tests:  \"The Online Handwritten Assamese Characters Dataset\": FMP performed better than 2MP (though their network architecture seemed to have significantly more parameters  \"CASIA-OLHWDB1.1 database\": FMP performed better than 2MP (again, seemed to have more parameters)  CIFAR-10: FMP performed better than current best network (especially with many tests per image)"], "summary_text": "What and why  Traditionally neural nets use max pooling with 2x2 grids (2MP). 2MP reduces the image dimensions by a factor of 2. An alternative would be to use pooling schemes that reduce by factors other than two, e.g. 1 < factor < 2. Pooling by a factor of sqrt(2) would allow twice as many pooling layers as 2MP, resulting in \"softer\" image size reduction throughout the network. Fractional Max Pooling (FMP) is such a method to perform max pooling by factors other than 2. How  In 2MP you move a 2x2 grid always by 2 pixels. Imagine that these step sizes follow a sequence, i.e. for 2MP: 2222222... If you mix in just a single 1 you get a pooling factor of <2. By chosing the right amount of 1s vs. 2s you can pool by any factor between 1 and 2. The sequences of 1s and 2s can be generated in fully random order or in pseudorandom order, where pseudorandom basically means \"predictable sub patterns\" (e.g. 211211211211211...). FMP can happen disjoint or overlapping. Disjoint means 2x2 grids, overlapping means 3x3. Results  FMP seems to perform generally better than 2MP. Better results on various tests, including CIFAR-10 and CIFAR-100 (often quite significant improvement). Best configuration seems to be random sequences with overlapping regions. Results are especially better if each test is repeated multiple times per image (as the random sequence generation creates randomness, similar to dropout). First 5-10 repetitions seem to be most valuable, but even 100+ give some improvement. An FMP-factor of sqrt(2) was usually used. Random FMP with a factor of sqrt(2) applied five times to the same input image (results upscaled back to original size). Rough chapter-wise notes  (1) Convolutional neural networks  Advantages of 2x2 max pooling (2MP): fast; a bit invariant to translations and distortions; quick reduction of image sizes  Disadvantages: \"disjoint nature of pooling regions\" can limit generalization (i.e. that they don't overlap? ); reduction of image sizes can be too quick  Alternatives to 2MP: 3x3 pooling with stride 2, stochastic 2x2 pooling  All suggested alternatives to 2MP also reduce sizes by a factor of 2  Author wants to have reduction by sqrt(2) as that would enable to use twice as many pooling layers  Fractional Max Pooling = Pooling that reduces image sizes by a factor of 1 < alpha < 2  FMP introduces randomness into pooling (by the choice of pooling regions)  Settings of FMP:  Pooling Factor alpha in range [1, 2] (1 = no change in image sizes, 2 = image sizes get halfed)  Choice of Pooling-Regions: Random or pseudorandom. Random is stronger (?). Random+Dropout can result in underfitting. Disjoint or overlapping pooling regions. Results for overlapping are better. (2) Fractional max-pooling  For traditional 2MP, every grid's top left coordinate is at (2i-1, 2j-1) and it's bottom right coordinate at (2i, 2j) (i=col, j=row). It will reduce the original size N to 1/2N, i.e. 2N_in = N_out. Paper analyzes 1 < alpha < 2, but alpha > 2 is also possible. Grid top left positions can be described by sequences of integers, e.g. (only column): 1, 3, 5, ...  Disjoint 2x2 pooling might be 1, 3, 5, ... while overlapping would have the same sequence with a larger 3x3 grid. The increment of the sequences can be random or pseudorandom for alphas < 2. For 2x2 FMP you can represent any alpha with a \"good\" sequence of increments that all have values 1 or 2, e.g. 2111121122111121... In the case of random FMP, the optimal fraction of 1s and 2s is calculated. Then a random permutation of a sequence of 1s and 2s is generated. In the case of pseudorandom FMP, the 1s and 2s follow a pattern that leads to the correct alpha, e.g. 112112121121211212...  Random FMP creates varying distortions of the input image. Pseudorandom FMP is a faithful downscaling. (3) Implementation  In their tests they use a convnet starting with 10 convolutions, then 20, then 30, ... They add FMP with an alpha of sqrt(2) after every conv layer. They calculate the desired output size, then go backwards through their network to the input. They multiply the size of the image by sqrt(2) with every FMP layer and add a flat 1 for every conv layer. The result is the required image size. They pad the images to that size. They use dropout, with increasing strength from 0% to 50% towards the output. They use LeakyReLUs. Every time they apply an FMP layer, they generate a new sequence of 1s and 2s. That indirectly makes the network an ensemble of similar networks. The output of the network can be averaged over several forward passes (for the same image). The result then becomes more accurate (especially up to >=6 forward passes). (4) Results  Tested on MNIST and CIFAR-100  Architectures (somehow different from (3)? ):  MNIST: 36x36 img -> 6 times (32 conv (3x3?) -> FMP alpha=sqrt(2)) -> ? -> ? -> output  CIFAR-100: 94x94 img -> 12 times (64 conv (3x3?) -> FMP alpha=2^(1/3)) -> ? -> ? -> output  Overlapping pooling regions seemed to perform better than disjoint regions. Random FMP seemed to perform better than pseudorandom FMP. Other tests:  \"The Online Handwritten Assamese Characters Dataset\": FMP performed better than 2MP (though their network architecture seemed to have significantly more parameters  \"CASIA-OLHWDB1.1 database\": FMP performed better than 2MP (again, seemed to have more parameters)  CIFAR-10: FMP performed better than current best network (especially with many tests per image)", "pdf_url": "http://arxiv.org/pdf/1412.6071", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/fractional_max_pooling.json"}
{"id": "69013017", "bin": "1100_1200", "summary_sentences": ["What  They describe a regularization method similar to dropout and stochastic depth.", "The method could be viewed as a merge of the two techniques (dropout, stochastic depth).", "The method seems to regularize better than any of the two alone.", "How  Let x be the input to a layer.", "That layer produces an output.", "The output can be:  Feed forward (\"classic\") network: F(x).", "Residual network: x + F(x).", "The standard dropout-like methods do the following:  Dropout in feed forward networks: Sometimes 0, sometimes F(x).", "Decided per unit.", "Dropout in residual networks (rarely used): Sometimes 0, sometimes x + F(x).", "Decided per unit.", "Stochastic depth (only in residual networks): Sometimes x, sometimes x + F(x).", "Decided per layer.", "Skip forward (only in residual networks): Sometimes x, sometimes x + F(x).", "Decided per unit.", "Swapout (any network): Sometimes 0, sometimes F(x), sometimes x, sometimes x + F(x).", "Decided per unit.", "Swapout can be represented using the formula y = theta_1 * x + theta_2 * F(x).", "* is the element-wise product.", "theta_1 and theta_2 are tensors following bernoulli distributions, i.e. their values are all exactly 0 or exactly 1.", "Setting the values of theta_1 and theta_2 per unit in the right way leads to the values 0 (both 0), x (1, 0), F(x) (0, 1) or x + F(x) (1, 1).", "Deterministic and Stochastic Inference  Ideally, when using a dropout-like technique you would like to get rid of its stochastic effects during prediction, so that you can predict values with exactly one forward pass through the network (instead of having to average over many passes).", "For Swapout it can be mathematically shown that you can't calculate a deterministic version of it that performs equally to the stochastic one (averaging over many forward passes).", "This is even more the case when using Batch Normalization in a network.", "(Actually also when not using Swapout, but instead Dropout + BN.)", "So for best results you should use the stochastic method (averaging over many forward passes).", "Results  They compare various dropout-like methods, including Swapout, applied to residual networks.", "(On CIFAR-10 and CIFAR-100.)", "General performance:  Results with Swapout are better than with the other methods.", "According to their results, the ranking of methods is roughly: Swapout > Dropout > Stochastic Depth > Skip Forward > None.", "Stochastic vs deterministic method:  The stochastic method of swapout (average over N forward passes) performs significantly better than the deterministic one.", "Using about 15-30 forward passes seems to yield good results.", "Optimal parameter choice:  Previously the Swapout-formula y = theta_1 * x + theta_2 * F(x) was mentioned.", "theta_1 and theta_2 are generated via Bernoulli distributions which have parameters p_1 and p_2.", "If using fixed values for p_1 and p_2 throughout the network, it seems to be best to either set both of them to 0.5 or to set p_1 to >0.5 and p_2 to <0.5 (preference towards y = x).", "It's best however to start both at 1.0 (always y = x + F(x)) and to then linearly decay them to both 0.5 towards the end of the network, i.e. to apply less noise to the early layers.", "(This is similar to the results in the Stochastic Depth paper.)", "Thin vs. wide residual networks:  The standard residual networks that they compared to used a (16, 32, 64) pattern for their layers, i.e. they started with layers of each having 16 convolutional filters, followed by some layers with each having 32 filters, followed by some layers with 64 filters.", "They tried instead a (32, 64, 128) pattern, i.e. they doubled the amount of filters.", "Then they reduced the number of layers from 100 down to 20.", "Their wider residual network performed significantly better than the deep and thin counterpart.", "However, their parameter count also increased by about 4 times.", "Increasing the pattern again to (64, 128, 256) and increasing the number of layers from 20 to 32 leads to another performance improvement, beating a 1000-layer network of pattern (16, 32, 64).", "(Parameter count is then 27 times the original value.)", "Comments  Stochastic depth works layer-wise, while Swapout works unit-wise.", "When a layer in Stochastic Depth is dropped, its whole forward- and backward-pass don't have to be calculated.", "That saves time.", "Swapout is not going to save time.", "They argue that dropout+BN would also profit from using stochastic inference instead of deterministic inference, just like Swapout does.", "However, they don't mention using it for dropout in their comparison, only for Swapout.", "They show that linear decay for their parameters (less dropping on early layers, more on later ones) significantly improves the results of Swapout.", "However, they don't mention testing the same thing for dropout.", "Maybe dropout would also profit from it?", "For the above two points: Dropout's test error is at 5.87, Swapout's test error is at 5.68.", "So the difference is already quite small, making any disadvantage for dropout significant.", "Visualization of how Swapout works.", "From left to right: An input x; a standard layer is applied to the input F(x); a residual layer is applied to the input x + F(x); Skip Forward is applied to the layer; Swapout is applied to the layer.", "Stochastic Depth would be all units being orange (x) or blue (x + F(x))."], "summary_text": "What  They describe a regularization method similar to dropout and stochastic depth. The method could be viewed as a merge of the two techniques (dropout, stochastic depth). The method seems to regularize better than any of the two alone. How  Let x be the input to a layer. That layer produces an output. The output can be:  Feed forward (\"classic\") network: F(x). Residual network: x + F(x). The standard dropout-like methods do the following:  Dropout in feed forward networks: Sometimes 0, sometimes F(x). Decided per unit. Dropout in residual networks (rarely used): Sometimes 0, sometimes x + F(x). Decided per unit. Stochastic depth (only in residual networks): Sometimes x, sometimes x + F(x). Decided per layer. Skip forward (only in residual networks): Sometimes x, sometimes x + F(x). Decided per unit. Swapout (any network): Sometimes 0, sometimes F(x), sometimes x, sometimes x + F(x). Decided per unit. Swapout can be represented using the formula y = theta_1 * x + theta_2 * F(x). * is the element-wise product. theta_1 and theta_2 are tensors following bernoulli distributions, i.e. their values are all exactly 0 or exactly 1. Setting the values of theta_1 and theta_2 per unit in the right way leads to the values 0 (both 0), x (1, 0), F(x) (0, 1) or x + F(x) (1, 1). Deterministic and Stochastic Inference  Ideally, when using a dropout-like technique you would like to get rid of its stochastic effects during prediction, so that you can predict values with exactly one forward pass through the network (instead of having to average over many passes). For Swapout it can be mathematically shown that you can't calculate a deterministic version of it that performs equally to the stochastic one (averaging over many forward passes). This is even more the case when using Batch Normalization in a network. (Actually also when not using Swapout, but instead Dropout + BN.) So for best results you should use the stochastic method (averaging over many forward passes). Results  They compare various dropout-like methods, including Swapout, applied to residual networks. (On CIFAR-10 and CIFAR-100.) General performance:  Results with Swapout are better than with the other methods. According to their results, the ranking of methods is roughly: Swapout > Dropout > Stochastic Depth > Skip Forward > None. Stochastic vs deterministic method:  The stochastic method of swapout (average over N forward passes) performs significantly better than the deterministic one. Using about 15-30 forward passes seems to yield good results. Optimal parameter choice:  Previously the Swapout-formula y = theta_1 * x + theta_2 * F(x) was mentioned. theta_1 and theta_2 are generated via Bernoulli distributions which have parameters p_1 and p_2. If using fixed values for p_1 and p_2 throughout the network, it seems to be best to either set both of them to 0.5 or to set p_1 to >0.5 and p_2 to <0.5 (preference towards y = x). It's best however to start both at 1.0 (always y = x + F(x)) and to then linearly decay them to both 0.5 towards the end of the network, i.e. to apply less noise to the early layers. (This is similar to the results in the Stochastic Depth paper.) Thin vs. wide residual networks:  The standard residual networks that they compared to used a (16, 32, 64) pattern for their layers, i.e. they started with layers of each having 16 convolutional filters, followed by some layers with each having 32 filters, followed by some layers with 64 filters. They tried instead a (32, 64, 128) pattern, i.e. they doubled the amount of filters. Then they reduced the number of layers from 100 down to 20. Their wider residual network performed significantly better than the deep and thin counterpart. However, their parameter count also increased by about 4 times. Increasing the pattern again to (64, 128, 256) and increasing the number of layers from 20 to 32 leads to another performance improvement, beating a 1000-layer network of pattern (16, 32, 64). (Parameter count is then 27 times the original value.) Comments  Stochastic depth works layer-wise, while Swapout works unit-wise. When a layer in Stochastic Depth is dropped, its whole forward- and backward-pass don't have to be calculated. That saves time. Swapout is not going to save time. They argue that dropout+BN would also profit from using stochastic inference instead of deterministic inference, just like Swapout does. However, they don't mention using it for dropout in their comparison, only for Swapout. They show that linear decay for their parameters (less dropping on early layers, more on later ones) significantly improves the results of Swapout. However, they don't mention testing the same thing for dropout. Maybe dropout would also profit from it? For the above two points: Dropout's test error is at 5.87, Swapout's test error is at 5.68. So the difference is already quite small, making any disadvantage for dropout significant. Visualization of how Swapout works. From left to right: An input x; a standard layer is applied to the input F(x); a residual layer is applied to the input x + F(x); Skip Forward is applied to the layer; Swapout is applied to the layer. Stochastic Depth would be all units being orange (x) or blue (x + F(x)).", "pdf_url": "http://arxiv.org/pdf/1605.06465v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/swapout.json"}
{"id": "74547298", "bin": "1100_1200", "summary_sentences": ["Recursive Programming – Dijkstra 1960  * Updated link to one that is not behind a paywall – thanks to Graham Markall for the catch *  This paper deals with something we take so much for granted that it’s hard to imagine a time when it had yet to be introduced to the world.", "That time was 1960, the concept is the language runtime stack, and the author of the paper is none other than Dijkstra.", "In fact, we’re so familiar with the stack, that it takes a while to get your head around what went before: each subroutine in a program had its own private fixed working space.", "That is, suppose you write a program that contains 30 subroutines – then there will be 30 reserved areas in memory, one for each subroutine to use.", "Dijkstra points out a couple of difficulties with this arrangement:  In the first place, the storage allocations for all the subroutines together will, in general, occupy much more memory space than they ever need simultaneously, and the available memory space is therefore used rather uneconomically.", "Furthermore – and this is a more serious objection – it is then impossible to call a subroutine while one or more of the previous activations of the same subroutine have not yet come to an end, without losing the possibility of finishing them off properly later on.", "In other words, if each subroutine has its own fixed storage area, then recursive programming is not possible (you’ll overwrite the state in the one fixed storage area for the subroutine).", "Since this rather limits the design space of programs, Dijkstra was interested in finding a technique that could eliminate the restriction:  The basic concept of the method is the so-called stack.", "Dijkstra describes how to build a stack with a block of memory and a stack pointer – I’m going to assume you’re all familiar with the idea!", "The following insight is key to the use of the stack in this context:  If we mark off, on a time axis, the moments when a unit is added to or removed from the stack, by using an opening bracket for the addition of a unit, and a closing bracket for its removal, then we obtain a correctly nested bracket structure, in which the opening and closing brackets form pairs in the same way as they do in a normal algebraic expression involving brackets.", "This is closely related to the circumstance that we can use a stack for storing the intermediate results formed in the evaluation of an arbitrary algebraic expression by means of elementary algebraic operations.", "In this case, the interest is always restricted to the most recent element in the stack.", "As the intermediate results are used only once, use of an element implies its removal from the stack.", "Take an expression  A + (B - C) * (D|E + F)  We know that we can record this in reverse polish notation as :  A, B, C, -, D, E, |, F, +, *, +  The above is well-known, and so elegant that we could not refrain from trying to extend this technique by consistent application of its principles.", "The example above assumes that A, B, C etc are numerical values that can be found in memory.", "But Dijkstra points out that C could equally have been an expression (for example C = (P/(Q-R + S*T )), and by the time we have done evaluating C, the net result would be the same as if we had the value of C accessible directly.", "In other words, it is immaterial to the “surroundings” in which the value C is used, whether the value C can be found ready-made in memory, or whether it is necessary to make temporary use of a number of the next stack locations for its evaluation.", "Now suppose it wasn’t an expression that appears for the value of C, but a function (to be evaluated by a subroutine): “this provides a strong argument for arranging the subroutine in such a way that it operates in the first free places of the stack, in just the same way as a compound term written out in full.”  The stack can be used by a subroutine for its parameters, its local variables, and even anonymous intermediate results created during execution of the subroutine.", "Inside the subroutine we store the most anonymous intermediate results in the “top” of the stack in just the same way.", "Every reference to a local quantiity, however, implies one is interested in a place that is situated deeper within the stack, and here one is interested in random access to the stack places, in other words we must be able to give the places deeper in the stack some kind of address.", "The value of that reference point is, ‘to be derived from the value of the stack pointer at the moment of the call.’ With a final flourish, Dijkstra goes on to show that ‘link’ information must be preserved  in the stack when calling subroutines, so that regardless of complexity we can pick up exactly where we left off (in the ALU)- this is to include a return address.", "We can now follow what happens when subroutine A calls subroutine B, and observe that:  In this process, nothings forbids A from being identical with B.", "The subroutine only has to appear in the memory once, but it may have more than one simultaneous “incarnation” from a dynamic point of view: the “innermost” activation causes the same piece of text to work in a higher part of the stack.", "Thus the subroutine has developed into a defining element that can be used completely recursively.", "Tada!", "Note: 1960 is also the year of “Recursive Functions of Symbolic Expressions and their Computation by Machine,” McCarthy’s famous paper which “describes a formalism for defining functions recursively,” and then shows how it can be implemented in the LISP programming system for the IBM 704.", "\"Recursive Functions of Symbolic Expressions…\" McCarthy, 1960 (LISP)  [url]"], "summary_text": "Recursive Programming – Dijkstra 1960  * Updated link to one that is not behind a paywall – thanks to Graham Markall for the catch *  This paper deals with something we take so much for granted that it’s hard to imagine a time when it had yet to be introduced to the world. That time was 1960, the concept is the language runtime stack, and the author of the paper is none other than Dijkstra. In fact, we’re so familiar with the stack, that it takes a while to get your head around what went before: each subroutine in a program had its own private fixed working space. That is, suppose you write a program that contains 30 subroutines – then there will be 30 reserved areas in memory, one for each subroutine to use. Dijkstra points out a couple of difficulties with this arrangement:  In the first place, the storage allocations for all the subroutines together will, in general, occupy much more memory space than they ever need simultaneously, and the available memory space is therefore used rather uneconomically. Furthermore – and this is a more serious objection – it is then impossible to call a subroutine while one or more of the previous activations of the same subroutine have not yet come to an end, without losing the possibility of finishing them off properly later on. In other words, if each subroutine has its own fixed storage area, then recursive programming is not possible (you’ll overwrite the state in the one fixed storage area for the subroutine). Since this rather limits the design space of programs, Dijkstra was interested in finding a technique that could eliminate the restriction:  The basic concept of the method is the so-called stack. Dijkstra describes how to build a stack with a block of memory and a stack pointer – I’m going to assume you’re all familiar with the idea! The following insight is key to the use of the stack in this context:  If we mark off, on a time axis, the moments when a unit is added to or removed from the stack, by using an opening bracket for the addition of a unit, and a closing bracket for its removal, then we obtain a correctly nested bracket structure, in which the opening and closing brackets form pairs in the same way as they do in a normal algebraic expression involving brackets. This is closely related to the circumstance that we can use a stack for storing the intermediate results formed in the evaluation of an arbitrary algebraic expression by means of elementary algebraic operations. In this case, the interest is always restricted to the most recent element in the stack. As the intermediate results are used only once, use of an element implies its removal from the stack. Take an expression  A + (B - C) * (D|E + F)  We know that we can record this in reverse polish notation as :  A, B, C, -, D, E, |, F, +, *, +  The above is well-known, and so elegant that we could not refrain from trying to extend this technique by consistent application of its principles. The example above assumes that A, B, C etc are numerical values that can be found in memory. But Dijkstra points out that C could equally have been an expression (for example C = (P/(Q-R + S*T )), and by the time we have done evaluating C, the net result would be the same as if we had the value of C accessible directly. In other words, it is immaterial to the “surroundings” in which the value C is used, whether the value C can be found ready-made in memory, or whether it is necessary to make temporary use of a number of the next stack locations for its evaluation. Now suppose it wasn’t an expression that appears for the value of C, but a function (to be evaluated by a subroutine): “this provides a strong argument for arranging the subroutine in such a way that it operates in the first free places of the stack, in just the same way as a compound term written out in full.”  The stack can be used by a subroutine for its parameters, its local variables, and even anonymous intermediate results created during execution of the subroutine. Inside the subroutine we store the most anonymous intermediate results in the “top” of the stack in just the same way. Every reference to a local quantiity, however, implies one is interested in a place that is situated deeper within the stack, and here one is interested in random access to the stack places, in other words we must be able to give the places deeper in the stack some kind of address. The value of that reference point is, ‘to be derived from the value of the stack pointer at the moment of the call.’ With a final flourish, Dijkstra goes on to show that ‘link’ information must be preserved  in the stack when calling subroutines, so that regardless of complexity we can pick up exactly where we left off (in the ALU)- this is to include a return address. We can now follow what happens when subroutine A calls subroutine B, and observe that:  In this process, nothings forbids A from being identical with B. The subroutine only has to appear in the memory once, but it may have more than one simultaneous “incarnation” from a dynamic point of view: the “innermost” activation causes the same piece of text to work in a higher part of the stack. Thus the subroutine has developed into a defining element that can be used completely recursively. Tada! Note: 1960 is also the year of “Recursive Functions of Symbolic Expressions and their Computation by Machine,” McCarthy’s famous paper which “describes a formalism for defining functions recursively,” and then shows how it can be implemented in the LISP programming system for the IBM 704. \"Recursive Functions of Symbolic Expressions…\" McCarthy, 1960 (LISP)  [url]", "pdf_url": "http://oai.cwi.nl/oai/asset/9253/9253A.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/recursive-programming.json"}
{"id": "30485653", "bin": "1100_1200", "summary_sentences": ["What  They describe a model for human pose estimation, i.e. one that finds the joints (\"skeleton\") of a person in an image.", "They argue that part of their model resembles a Markov Random Field (but in reality its implemented as just one big neural network).", "How  They have two components in their network:  Part-Detector:  Finds candidate locations for human joints in an image.", "Pretty standard ConvNet.", "A few convolutional layers with pooling and ReLUs.", "They use two branches: A fine and a coarse one.", "Both branches have practically the same architecture (convolutions, pooling etc.).", "The coarse one however receives the image downscaled by a factor of 2 (half width/height) and upscales it by a factor of 2 at the end of the branch.", "At the end they merge the results of both branches with more convolutions.", "The output of this model are 4 heatmaps (one per joint?", "unclear), each having lower resolution than the original image.", "Spatial-Model:  Takes the results of the part detector and tries to remove all detections that were false positives.", "They derive their architecture from a fully connected Markov Random Field which would be solved with one step of belief propagation.", "They use large convolutions (128x128) to resemble the \"fully connected\" part.", "They initialize the weights of the convolutions with joint positions gathered from the training set.", "The convolutions are followed by log(), element-wise additions and exp() to resemble an energy function.", "The end result are the input heatmaps, but cleaned up.", "Results  Beats all previous models (with and without spatial model).", "Accuracy seems to be around 90% (with enough (16px) tolerance in pixel distance from ground truth).", "Adding the spatial model adds a few percentage points of accuracy.", "Using two branches instead of one (in the part detector) adds a bit of accuracy.", "Adding a third branch adds a tiny bit more.", "Example results.", "Part Detector network.", "Spatial Model (apparently only for two input heatmaps).", "Rough chapter-wise notes  (1) Introduction  Human Pose Estimation (HPE) from RGB images is difficult due to the high dimensionality of the input.", "Approaches:  Deformable-part models: Traditionally based on hand-crafted features.", "Deep-learning based disciminative models: Recently outperformed other models.", "However, it is hard to incorporate priors (e.g. possible joint- inter-connectivity) into the model.", "They combine:  A part-detector (ConvNet, utilizes multi-resolution feature representation with overlapping receptive fields)  Part-based Spatial-Model (approximates loopy belief propagation)  They backpropagate through the spatial model and then the part-detector.", "(3) Model  (3.1) Convolutional Network Part-Detector  This model locates possible positions of human key joints in the image (\"part detector\").", "Input: RGB image.", "Output: 4 heatmaps, one per key joint (per pixel: likelihood).", "They use a fully convolutional network.", "They argue that applying convolutions to every pixel is similar to moving a sliding window over the image.", "They use two receptive field sizes for their \"sliding window\": A large but coarse/blurry one, a small but fine one.", "To implement that, they use two branches.", "Both branches are mostly identical (convolutions, poolings, ReLU).", "They simply feed a downscaled (half width/height) version of the input image into the coarser branch.", "At the end they upscale the coarser branch once and then merge both branches.", "After the merge they apply 9x9 convolutions and then 1x1 convolutions to get it down to 4xHxW (H=60, W=90 where expected input was H=320, W=240).", "(3.2) Higher-level Spatial-Model  This model takes the detected joint positions (heatmaps) and tries to remove those that are probably false positives.", "It is a ConvNet, which tries to emulate (1) a Markov Random Field and (2) solving that MRF approximately via one step of belief propagation.", "The raw MRF formula would be something like <likelihood of joint A per px> = normalize( <product over joint v from joints V> <probability of joint A per px given a> * <probability of joint v at px?> + someBiasTerm).", "They treat the probabilities as energies and remove from the formula the partition function (normalize) for various reasons (e.g. because they are only interested in the maximum value anyways).", "They use exp() in combination with log() to replace the product with a sum.", "They apply SoftPlus and ReLU so that the energies are always positive (and therefore play well with log).", "Apparently <probability of joint v at px?> are the input heatmaps of the part detector.", "Apparently <probability of joint A per px given a> is implemented as the weights of a convolution.", "Apparently someBiasTerm is implemented as the bias of a convolution.", "The convolutions that they use are large (128x128) to emulate a fully connected graph.", "They initialize the convolution weights based on histograms gathered from the dataset (empirical distribution of joint displacements).", "(3.3) Unified Models  They combine the part-based model and the spatial model to a single one.", "They first train only the part-based model, then only the spatial model, then both.", "(4) Results  Used datasets: FLIC (4k training images, 1k test, mostly front-facing and standing poses), FLIC-plus (17k, 1k ?", "), extended-LSP (10k, 1k).", "FLIC contains images showing multiple persons with only one being annotated.", "So for FLIC they add a heatmap of the annotated body torso to the input (i.e. the part-detector does not have to search for the person any more).", "The evaluation metric roughly measures, how often predicted joint positions are within a certain radius of the true joint positions.", "Their model performs significantly better than competing models (on both FLIC and LSP).", "Accuracy seems to be at around 80%-95% per joint (when choosing high enough evaluation tolerance, i.e. 10px+).", "Adding the spatial model to the part detector increases the accuracy by around 10-15 percentage points.", "Training the part detector and the spatial model jointly adds ~3 percentage points accuracy over training them separately.", "Adding the second filter bank (coarser branch in the part detector) adds around 5 percentage points accuracy.", "Adding a third filter bank adds a tiny bit more accuracy."], "summary_text": "What  They describe a model for human pose estimation, i.e. one that finds the joints (\"skeleton\") of a person in an image. They argue that part of their model resembles a Markov Random Field (but in reality its implemented as just one big neural network). How  They have two components in their network:  Part-Detector:  Finds candidate locations for human joints in an image. Pretty standard ConvNet. A few convolutional layers with pooling and ReLUs. They use two branches: A fine and a coarse one. Both branches have practically the same architecture (convolutions, pooling etc.). The coarse one however receives the image downscaled by a factor of 2 (half width/height) and upscales it by a factor of 2 at the end of the branch. At the end they merge the results of both branches with more convolutions. The output of this model are 4 heatmaps (one per joint? unclear), each having lower resolution than the original image. Spatial-Model:  Takes the results of the part detector and tries to remove all detections that were false positives. They derive their architecture from a fully connected Markov Random Field which would be solved with one step of belief propagation. They use large convolutions (128x128) to resemble the \"fully connected\" part. They initialize the weights of the convolutions with joint positions gathered from the training set. The convolutions are followed by log(), element-wise additions and exp() to resemble an energy function. The end result are the input heatmaps, but cleaned up. Results  Beats all previous models (with and without spatial model). Accuracy seems to be around 90% (with enough (16px) tolerance in pixel distance from ground truth). Adding the spatial model adds a few percentage points of accuracy. Using two branches instead of one (in the part detector) adds a bit of accuracy. Adding a third branch adds a tiny bit more. Example results. Part Detector network. Spatial Model (apparently only for two input heatmaps). Rough chapter-wise notes  (1) Introduction  Human Pose Estimation (HPE) from RGB images is difficult due to the high dimensionality of the input. Approaches:  Deformable-part models: Traditionally based on hand-crafted features. Deep-learning based disciminative models: Recently outperformed other models. However, it is hard to incorporate priors (e.g. possible joint- inter-connectivity) into the model. They combine:  A part-detector (ConvNet, utilizes multi-resolution feature representation with overlapping receptive fields)  Part-based Spatial-Model (approximates loopy belief propagation)  They backpropagate through the spatial model and then the part-detector. (3) Model  (3.1) Convolutional Network Part-Detector  This model locates possible positions of human key joints in the image (\"part detector\"). Input: RGB image. Output: 4 heatmaps, one per key joint (per pixel: likelihood). They use a fully convolutional network. They argue that applying convolutions to every pixel is similar to moving a sliding window over the image. They use two receptive field sizes for their \"sliding window\": A large but coarse/blurry one, a small but fine one. To implement that, they use two branches. Both branches are mostly identical (convolutions, poolings, ReLU). They simply feed a downscaled (half width/height) version of the input image into the coarser branch. At the end they upscale the coarser branch once and then merge both branches. After the merge they apply 9x9 convolutions and then 1x1 convolutions to get it down to 4xHxW (H=60, W=90 where expected input was H=320, W=240). (3.2) Higher-level Spatial-Model  This model takes the detected joint positions (heatmaps) and tries to remove those that are probably false positives. It is a ConvNet, which tries to emulate (1) a Markov Random Field and (2) solving that MRF approximately via one step of belief propagation. The raw MRF formula would be something like <likelihood of joint A per px> = normalize( <product over joint v from joints V> <probability of joint A per px given a> * <probability of joint v at px?> + someBiasTerm). They treat the probabilities as energies and remove from the formula the partition function (normalize) for various reasons (e.g. because they are only interested in the maximum value anyways). They use exp() in combination with log() to replace the product with a sum. They apply SoftPlus and ReLU so that the energies are always positive (and therefore play well with log). Apparently <probability of joint v at px?> are the input heatmaps of the part detector. Apparently <probability of joint A per px given a> is implemented as the weights of a convolution. Apparently someBiasTerm is implemented as the bias of a convolution. The convolutions that they use are large (128x128) to emulate a fully connected graph. They initialize the convolution weights based on histograms gathered from the dataset (empirical distribution of joint displacements). (3.3) Unified Models  They combine the part-based model and the spatial model to a single one. They first train only the part-based model, then only the spatial model, then both. (4) Results  Used datasets: FLIC (4k training images, 1k test, mostly front-facing and standing poses), FLIC-plus (17k, 1k ? ), extended-LSP (10k, 1k). FLIC contains images showing multiple persons with only one being annotated. So for FLIC they add a heatmap of the annotated body torso to the input (i.e. the part-detector does not have to search for the person any more). The evaluation metric roughly measures, how often predicted joint positions are within a certain radius of the true joint positions. Their model performs significantly better than competing models (on both FLIC and LSP). Accuracy seems to be at around 80%-95% per joint (when choosing high enough evaluation tolerance, i.e. 10px+). Adding the spatial model to the part detector increases the accuracy by around 10-15 percentage points. Training the part detector and the spatial model jointly adds ~3 percentage points accuracy over training them separately. Adding the second filter bank (coarser branch in the part detector) adds around 5 percentage points accuracy. Adding a third filter bank adds a tiny bit more accuracy.", "pdf_url": "http://arxiv.org/pdf/1406.2984", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/joint_training_of_a_convnet_and_a_pgm_for_hpe.json"}
{"id": "37263635", "bin": "1100_1200", "summary_sentences": ["Church’s Thesis and Functional Programming – Turner 2006  One of a collection of papers celebrating the 70th anniversary of Church’s thesis in 2006, as recently recommended by Erik Meijer on twitter.", "Both the thesis and the lambda calculus have been of seminal influence on the development of Computing Science.", "There were three independently developed definitions for the computable functions of the natural numbers in the early -mid 1930s.", "These were proved to be equivalent, and then…  A few months later Turing (1936) introduced his concept of a logical computing machine – a finite automation with an unbounded tape divided into squares….", "In reviewing the Turing paper in 1937, Church wrote:  there is involved here the equivalence of three different notions: computability by a Turing machine, general recursiveness .", ".", ".", "and lambda-definability .", ".", ".The first has the advantage of making the identification with effectiveness in the ordinary sense evident immediately … The second and third have the advantage of suitability for embodiment in a system of symbolic logic  The Turing machine ultimately led to the development of the Turing/von-Neumann computer.", "The second and third notions led to the development of functional programming.", "Of the various convergent notions of computability Church’s lambda calculus is distinguished by its combination of simplicity with remarkable expressive power.", "The lambda calculus has three productions: a variable, function application, and function abstraction (a lambda expression, for example λx.x ).", "The essential rule of substitution, called beta reduction, tells us how to supply a value for a function parameter and substitute all occurences of that parameter in the expression body for the provided value.", "In other words, what it means to provide an argument to a function.", "Also note that it is from the lamda calculus we get the term ‘combinator’.", "A combinator is simply a lambda expression with no unbound variables.", "(called a ‘closed term’).", "Famous combinators are named by single letters.", "For example, the identity combinator is called ‘I’:  I = λx.x  And the constant combinator is called ‘K’:  K = λx.λy.x  (Constant because K z returns a function that always returns z, whatever argument you give it…).", "Combinators can also be used to support recursive functions:  The master-stroke, which shows every recursive function to be λ-definable is to find a universal fixpoint operator, that is a term Y with the property that for any term F, Y F ⇔ F(Y F) There are many such terms, of which the simplest is due to H.B.Curry.", "Y = λf.", "(λx.f(xx))(λx.f(xx))  By convention, this combinator is represented by the letter Y.", "Yes HN, that’s the ‘y-combinator.’  Church showed how to represent numbers as function iterators (a, f a, f f a, f f f a , …).", "The scheme also works for any data type.", "Moreover we are not limited to arithmetic.", "The idea behind the Church numerals is very general and allows any data type — pairs, lists, trees and so on — to be represented in a purely functional way.", "Each datum is encoded as a function that captures its elimination operation, that is the way in which information is extracted from it during computation.", "It is also possible to represent codata, such as infinite lists, infinitary trees and so on.", "There follows a wonderful section on functional programming, contrasted with imperative programming:  Imperative programming languages, from the earliest such as FORTRAN and COBOL which emerged in the 1950’s to current ”object-oriented” ones such as C++ and Java have certain features in common.", "Their basic action is the assignment command, which changes the content of a location in memory and they have an explicit flow of control by which these state changes are ordered.", "This reflects more or less directly the structure of the Turing/von Neumann computer, as a central processing unit operating on a passive store.", "Backus (1978) calls them ”von Neumann languages”.", "Functional programming languages offer a radical alternative — they are descriptive rather than imperative, have no assignment command and no explicit flow of control — sub-computations are ordered only partially, by data dependency.", "(We’ve encountered this idea earlier in the morning paper series too, at #themorningpaper no.", "20 ).", "Now comes an important point about what happens when you try to add functional support to imperative languages, echoed by Erik Meijer recently The Curse of the Excluded Middle , #themorningpaper no.", "41. :  The disadvantages of functional programming within a language that includes imperative features are two.", "First, you are not forced to explore the limits of the functional style, since you can escape at will into an imperative idiom.", "Second, the presence of side effects, exceptions etc., even if they are rarely used, invalidate important theorems on which the benefits of the style rest.", "There are many other good passages within the paper which we do not have space here to enumerate.", "I leave you with an extended quotation from the conclusions:  Church’s Thesis played a founding role in computing theory by providing a single notion of effective computability.", "Without this foundation we might have been stuck with a plethora of notions of computability depending on computer architecture, programming language etc.", ": we might have Motorola-computable versus Intel-computable, Java-computable versus C-computable and so on.", "The λ-calculus, which Church developed during the period of convergence from which the Thesis emerged, has influenced almost every aspect of the development of programming and programming languages.", "It is the basis of functional programming, which after a long infancy is entering adulthood as a practical alternative to traditional ad-hoc imperative programming languages.", "Many important ideas in mainstream programming languages—recursion, procedures as parameters, linked lists and trees, garbage collectors — came by cross fertilization from functional programming.", "Moreover the main schools of both operational and denotational semantics are λ-calculus based and amount to using functional programming to explain other programming systems."], "summary_text": "Church’s Thesis and Functional Programming – Turner 2006  One of a collection of papers celebrating the 70th anniversary of Church’s thesis in 2006, as recently recommended by Erik Meijer on twitter. Both the thesis and the lambda calculus have been of seminal influence on the development of Computing Science. There were three independently developed definitions for the computable functions of the natural numbers in the early -mid 1930s. These were proved to be equivalent, and then…  A few months later Turing (1936) introduced his concept of a logical computing machine – a finite automation with an unbounded tape divided into squares…. In reviewing the Turing paper in 1937, Church wrote:  there is involved here the equivalence of three different notions: computability by a Turing machine, general recursiveness . . . and lambda-definability . . .The first has the advantage of making the identification with effectiveness in the ordinary sense evident immediately … The second and third have the advantage of suitability for embodiment in a system of symbolic logic  The Turing machine ultimately led to the development of the Turing/von-Neumann computer. The second and third notions led to the development of functional programming. Of the various convergent notions of computability Church’s lambda calculus is distinguished by its combination of simplicity with remarkable expressive power. The lambda calculus has three productions: a variable, function application, and function abstraction (a lambda expression, for example λx.x ). The essential rule of substitution, called beta reduction, tells us how to supply a value for a function parameter and substitute all occurences of that parameter in the expression body for the provided value. In other words, what it means to provide an argument to a function. Also note that it is from the lamda calculus we get the term ‘combinator’. A combinator is simply a lambda expression with no unbound variables. (called a ‘closed term’). Famous combinators are named by single letters. For example, the identity combinator is called ‘I’:  I = λx.x  And the constant combinator is called ‘K’:  K = λx.λy.x  (Constant because K z returns a function that always returns z, whatever argument you give it…). Combinators can also be used to support recursive functions:  The master-stroke, which shows every recursive function to be λ-definable is to find a universal fixpoint operator, that is a term Y with the property that for any term F, Y F ⇔ F(Y F) There are many such terms, of which the simplest is due to H.B.Curry. Y = λf. (λx.f(xx))(λx.f(xx))  By convention, this combinator is represented by the letter Y. Yes HN, that’s the ‘y-combinator.’  Church showed how to represent numbers as function iterators (a, f a, f f a, f f f a , …). The scheme also works for any data type. Moreover we are not limited to arithmetic. The idea behind the Church numerals is very general and allows any data type — pairs, lists, trees and so on — to be represented in a purely functional way. Each datum is encoded as a function that captures its elimination operation, that is the way in which information is extracted from it during computation. It is also possible to represent codata, such as infinite lists, infinitary trees and so on. There follows a wonderful section on functional programming, contrasted with imperative programming:  Imperative programming languages, from the earliest such as FORTRAN and COBOL which emerged in the 1950’s to current ”object-oriented” ones such as C++ and Java have certain features in common. Their basic action is the assignment command, which changes the content of a location in memory and they have an explicit flow of control by which these state changes are ordered. This reflects more or less directly the structure of the Turing/von Neumann computer, as a central processing unit operating on a passive store. Backus (1978) calls them ”von Neumann languages”. Functional programming languages offer a radical alternative — they are descriptive rather than imperative, have no assignment command and no explicit flow of control — sub-computations are ordered only partially, by data dependency. (We’ve encountered this idea earlier in the morning paper series too, at #themorningpaper no. 20 ). Now comes an important point about what happens when you try to add functional support to imperative languages, echoed by Erik Meijer recently The Curse of the Excluded Middle , #themorningpaper no. 41. :  The disadvantages of functional programming within a language that includes imperative features are two. First, you are not forced to explore the limits of the functional style, since you can escape at will into an imperative idiom. Second, the presence of side effects, exceptions etc., even if they are rarely used, invalidate important theorems on which the benefits of the style rest. There are many other good passages within the paper which we do not have space here to enumerate. I leave you with an extended quotation from the conclusions:  Church’s Thesis played a founding role in computing theory by providing a single notion of effective computability. Without this foundation we might have been stuck with a plethora of notions of computability depending on computer architecture, programming language etc. : we might have Motorola-computable versus Intel-computable, Java-computable versus C-computable and so on. The λ-calculus, which Church developed during the period of convergence from which the Thesis emerged, has influenced almost every aspect of the development of programming and programming languages. It is the basis of functional programming, which after a long infancy is entering adulthood as a practical alternative to traditional ad-hoc imperative programming languages. Many important ideas in mainstream programming languages—recursion, procedures as parameters, linked lists and trees, garbage collectors — came by cross fertilization from functional programming. Moreover the main schools of both operational and denotational semantics are λ-calculus based and amount to using functional programming to explain other programming systems.", "pdf_url": "https://www.cs.kent.ac.uk/people/staff/dat/miranda/ctfp.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/churchs-thesis-and-functional-programming.json"}
{"id": "72744034", "bin": "1100_1200", "summary_sentences": ["When and Why Your Code Starts to Smell Bad – Tufano et al. 2015  Yesterday we saw that maintaining project quality is a key issue for integrators (maintainers).", "So it seems appropriate that my third choice from the recent ICSE ’15 conference papers examines the question of when quality starts to slip at the code level, and what causes it.", "Bad code smells (shortly “code smells” or “smells”), i.e., symptoms of poor design and implementation choices, represent one important factor contributing to technical debt, and possibly affecting the maintainability of a software system… to the best of our knowledge, there is no comprehensive empirical investigation into when and why code smells are introduced in software projects.", "Common wisdom suggests that urgent maintenance activities and pressure to deliver features while prioritizing time-to-market over code quality are often the causes of such smells.", "Generally speaking, software evolution has always been considered as one of the reasons behind “software ageing” or “increasing complexity.”  Tufano et al. studied 200 open source projects from the Apache, Eclipse, and Android ecosystems to understand when and why smells were introduced.", "Over half a million commits were analysed.", "The study was based on a mix of smells related to large and complex components as well as smells related to lack of adoption of good practices, so as to be representative of the different categories of smells.", "We focus our study on the following types of smells: 1) Blob Class: a large class with different responsibilities that monopolizes most of the system’s processing;  2) Class Data Should be Private: a class exposing its attributes, violating the information hiding principle;  3) Complex Class: a class having a high cyclomatic complexity;  4) Functional Decomposition: a class where inheritance and polymorphism are poorly used, declaring many private fields and implementing few methods; 5) Spaghetti Code: a class without structure that declares long methods without parameters.", "I’m going to skip over the methodology and jump straight to the findings – see the full paper for more details on how the analysis was conducted.", "When are code smells introduced?", "The surprising finding in the light of the software ageing theory is that most of the smell instances are introduced when a code entity is first added to the versioning system.", "When a smell does appear at a later point, “its symptoms (metric value increases) occur very fast, and not gradually.” In the case of Blobs for example:  For the overall dataset, the slope for classes that will become Blobs is 849.90 as compared to the 0.25 of clean classes.", "Thus, while the cohesion of classes generally decreases over time, classes destined to become Blobs exhibit cohesion metric loss orders of magnitude faster than clean classes.", "In general, the results in Table V show strong differences in the metrics’ slope between clean and smelly files, indicating that it could be possible to create recommenders warning developers when the changes performed on a specific code component show a dangerous trend that could lead to the introduction of a bad smell.", "Why are code smells introduced?", "Among the three different ecosystems analyzed, results show that smell instances are mainly introduced when developers perform enhancement operations on the system.", "When considering the three ecosystems altogether, for all the considered types of smells the percentage of smell-introducing commits tagged as enhancement ranges between 60% and 66%.", "Note that by enhancement we mean changes applied by developers on existing features aimed at improving them.", "If you consider both enhancements and new features the percentage rises to over 80%.", "Another endorsement for the theory that most smells are introduced on day one.", "Bug-fixes do of course introduce smells as well (between 6-16% of smells are introduced during bug fixing).", "Finally, refactoring – which is supposed to clean up the code and reduce smells, is also a source of  smell introduction!", "While refactoring is the principal treatment to remove smells, we found 394 cases in which developers introduced new smells when performing refactoring operations.", "What I can’t easily see in the figures is a comparison to baseline activity.", "For example, 6-16% of smells may be introduced during bug fixing, but does that make bug fixing more or less smelly than other activities?", "We don’t know unless we also know what overall % of activity is devoted to bug fixing…  More smells are also introduced in the run-up to a release (but again, we don’t know if more work overall is also done in the run-up to a release – often yes in my experience):  As expected, most of the smells are introduced the last month before issuing a release.", "Indeed, the percentage of smells introduced more than one month prior to issuing a release is really low (ranging between 0% and 11%).", "This consideration holds for all the ecosystems and for all the bad smells analyzed, thus confirming the common wisdom that the deadline pressure on developers can be one of the main causes for smell introduction.", "And those smells are often introduced by the files’ owners, not by newcomers:  We can also observe that generally the developers who introduce a smell are not newcomers while often they are owners of the files.", "At the first glance, this could look like an unexpected result.", "The owner of the file—one of the most experienced developers of the file—is the one that has the higher likelihood of introducing a smell.", "However, as also discussed by Zeller in his book Why programs fail, more experienced developers tend to perform more complex and critical tasks.", "Thus, it is likely that their commits are more prone to introducing design problems.", "(It’s also the case that the owner of the file is just more likely to do more work in the file, and hence have more chance of introducing smells.", "Plus, we’ve already been told that many smells are introduced when an entity is first created, and by definition that is by the owner of the file)."], "summary_text": "When and Why Your Code Starts to Smell Bad – Tufano et al. 2015  Yesterday we saw that maintaining project quality is a key issue for integrators (maintainers). So it seems appropriate that my third choice from the recent ICSE ’15 conference papers examines the question of when quality starts to slip at the code level, and what causes it. Bad code smells (shortly “code smells” or “smells”), i.e., symptoms of poor design and implementation choices, represent one important factor contributing to technical debt, and possibly affecting the maintainability of a software system… to the best of our knowledge, there is no comprehensive empirical investigation into when and why code smells are introduced in software projects. Common wisdom suggests that urgent maintenance activities and pressure to deliver features while prioritizing time-to-market over code quality are often the causes of such smells. Generally speaking, software evolution has always been considered as one of the reasons behind “software ageing” or “increasing complexity.”  Tufano et al. studied 200 open source projects from the Apache, Eclipse, and Android ecosystems to understand when and why smells were introduced. Over half a million commits were analysed. The study was based on a mix of smells related to large and complex components as well as smells related to lack of adoption of good practices, so as to be representative of the different categories of smells. We focus our study on the following types of smells: 1) Blob Class: a large class with different responsibilities that monopolizes most of the system’s processing;  2) Class Data Should be Private: a class exposing its attributes, violating the information hiding principle;  3) Complex Class: a class having a high cyclomatic complexity;  4) Functional Decomposition: a class where inheritance and polymorphism are poorly used, declaring many private fields and implementing few methods; 5) Spaghetti Code: a class without structure that declares long methods without parameters. I’m going to skip over the methodology and jump straight to the findings – see the full paper for more details on how the analysis was conducted. When are code smells introduced? The surprising finding in the light of the software ageing theory is that most of the smell instances are introduced when a code entity is first added to the versioning system. When a smell does appear at a later point, “its symptoms (metric value increases) occur very fast, and not gradually.” In the case of Blobs for example:  For the overall dataset, the slope for classes that will become Blobs is 849.90 as compared to the 0.25 of clean classes. Thus, while the cohesion of classes generally decreases over time, classes destined to become Blobs exhibit cohesion metric loss orders of magnitude faster than clean classes. In general, the results in Table V show strong differences in the metrics’ slope between clean and smelly files, indicating that it could be possible to create recommenders warning developers when the changes performed on a specific code component show a dangerous trend that could lead to the introduction of a bad smell. Why are code smells introduced? Among the three different ecosystems analyzed, results show that smell instances are mainly introduced when developers perform enhancement operations on the system. When considering the three ecosystems altogether, for all the considered types of smells the percentage of smell-introducing commits tagged as enhancement ranges between 60% and 66%. Note that by enhancement we mean changes applied by developers on existing features aimed at improving them. If you consider both enhancements and new features the percentage rises to over 80%. Another endorsement for the theory that most smells are introduced on day one. Bug-fixes do of course introduce smells as well (between 6-16% of smells are introduced during bug fixing). Finally, refactoring – which is supposed to clean up the code and reduce smells, is also a source of  smell introduction! While refactoring is the principal treatment to remove smells, we found 394 cases in which developers introduced new smells when performing refactoring operations. What I can’t easily see in the figures is a comparison to baseline activity. For example, 6-16% of smells may be introduced during bug fixing, but does that make bug fixing more or less smelly than other activities? We don’t know unless we also know what overall % of activity is devoted to bug fixing…  More smells are also introduced in the run-up to a release (but again, we don’t know if more work overall is also done in the run-up to a release – often yes in my experience):  As expected, most of the smells are introduced the last month before issuing a release. Indeed, the percentage of smells introduced more than one month prior to issuing a release is really low (ranging between 0% and 11%). This consideration holds for all the ecosystems and for all the bad smells analyzed, thus confirming the common wisdom that the deadline pressure on developers can be one of the main causes for smell introduction. And those smells are often introduced by the files’ owners, not by newcomers:  We can also observe that generally the developers who introduce a smell are not newcomers while often they are owners of the files. At the first glance, this could look like an unexpected result. The owner of the file—one of the most experienced developers of the file—is the one that has the higher likelihood of introducing a smell. However, as also discussed by Zeller in his book Why programs fail, more experienced developers tend to perform more complex and critical tasks. Thus, it is likely that their commits are more prone to introducing design problems. (It’s also the case that the owner of the file is just more likely to do more work in the file, and hence have more chance of introducing smells. Plus, we’ve already been told that many smells are introduced when an entity is first created, and by definition that is by the owner of the file).", "pdf_url": "https://dibt.unimol.it/fpalomba/documents/C4.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/when-and-why-your-code-starts-to-smell-bad.json"}
{"id": "75779006", "bin": "1200_1300", "summary_sentences": ["Symmetry Reduction Enables Model Checking of More Complex Emerging Behaviours of Swarm Navigation Algorithms – Antuñya et al. 2015  Don’t let the title put you off – this paper is all about robot swarms!", "Previously we looked at some nature-inspired optimisation algorithms, including Particle Swarm Optimisation which draws inspiration from the behaviour of flocks of birds.", "Today’s paper comes from the field of robotics, and it’s all about emulating some of nature’s behaviours by creating swarms of robots.", "Each individual robot follows simple rules, but taken together those simple rules, when applied in a swarm of robots, produce interesting emergent behaviours.", "It would be nice to know the robots we let loose will always behave as we wish…  There are some fun parallels to distributed systems too.", "Robotic swarms consist of a set of robots with simple individual behaviour rules, working together in cooperation to achieve a more complex or emergent final behaviour.", "Appealing characteristics of swarms are the low cost incurred in producing the robots, which have a simple hardware design, scalability, and fault tolerance.", "Examples of their application to real-life tasks include nanorobotics, disaster rescue missions, and mining or agricultural foraging tasks.", "Just as with concurrent and distributed systems, we have to worry about safety and liveness in this context.", "And as we’re releasing real machines out into the real world, we might want to verify those properties hold.", "The emergent behaviours of a swarm of robots need to be verified, with respect to safety and liveness requirements, and validated to determine whether it is fit for purpose in the target environment.", "Safety requirements are the allowed behaviours of the system, and liveness requirements specify the dynamic behaviours expected to happen during the execution of the system.", "Also in common with formal methods in distributed systems, state space explosion puts practical bounds on the complexity of systems that can be addressed.", "Letting the robots move around in continuous space (in  modelling terms – they obviously do IRL) leads to an infinite state space.", "Instead we can chop space up into cells in a grid…  The discretization of the continuous space into cells of fixed size —i.e., a grid— is a solution that has been applied in swarms, to enable model checking.", "Even with the discretization of the environment into a “small” grid (e.g., 4×4 cells), the state-space explosion problem can occur due to the presence of other variables, which results in too many possible configurations of the robots in the grid.", "Symmetry reduction techniques have been used to reduce state spaces in model checking.", "The authors  show a way of encoding a swarm environment that eliminates symmetrically equivalent states from the state space – thus enabling formal verification to be applied to larger scale problems.", "We implemented a relative encoding of a swarm environment model that eliminates symmetrically equivalent states from the state space.", "The swarm is assumed to be homogeneous; i.e., all the robots are considered identical in capabilities and rank.", "In the relative encoding, one robot is set as the “reference”, with a fixed location and direction of motion.", "The other robots’ locations and directions are defined based on this reference.", "In an absolute encoding, if all the robots in a grid are simultaneously rotated in the same direction and shifted horizontally or vertically by the same distance, the robot’s new configurations change in location and direction.", "But with the relative encoding the locations and directions remain the same.", "This is the key to the state-space reduction.", "In a grid of size m x m, and for robots that can travel in d different directions, the state space is reduced by a factor of dm2:  If a model with r robots in a m × m size grid (locations), with d possible directions, p other robots’ variables of domain sizes vi,  i = 1, …, p, and q global variables of domain sizes sj ,  j = 1, …, q, is globally encoded, the size of the state space to be explored is (d × m2 × v1 × v2 × … × vp)r × (s1 × s2 × … × sq).", "In a relative encoding, the reference robot will have fixed location and direction, and the resulting state space will be of size (v1 × v2 × … × vp) x (d × m2 × v1 × v2 × … × vp)r-1 × (s1 × s2 × … × sq).", "This corresponds to a reduction of the state space of d×m2.", "In practice this bound changes according to the variables used in the relative encoding of an algorithm.", "This also means that finding a counterexample in the relative model is equivalent to a class of counterexamples in the global model.", "This approach is validated on the Alpha navigation algorithm, which rather disappointingly is known not to produce the desired behaviour!", "The Alpha algorithm has been used as a case study to demonstrate how to verify emergent behaviours in swarms through model checking tools.", "In the Alpha algorithm, the robots in the swarm navigate the environment trying to maintain connectivity, defined as a wireless range.", "This is achieved by the  following rules: (a) the default movement of a robot is forward, maintaining its current direction.", "(b) When a robot loses connection with another robot, if the remaining number of connected robots is smaller than a value α, the robot makes a 180 degree turn.", "(c) Every time a robot regains connectivity with another, it performs a random turn.", "The desired behaviour for a swarm is this case is that all robots shall eventually be connected.", "In prior work, a formal expression of the algorithm in Linear Temporal Logic showed this is false.", "A global model and a relative model are both encoded for the NuSMV model checker.", "Both were able to show that the desired condition does not hold, but the relative model gives a much smaller state space (by about 2 orders of magnitude):  Thus, verification through model checking can be performed over larger grid sizes and higher numbers of robots, and also for more detailed abstractions that model navigation algorithms using more variables.", "Although the state space reduction is more significant in terms of the grid size, and not as expressive if considering realistic swarm sizes, analysing small robot groups can help to understand larger swarms within the lower limit bounds of swarm size, which demands more from the navigation algorithm.", "Here’s an example of a counter-example found using the global encoding:  And this a more compact example found using the relative encoding:"], "summary_text": "Symmetry Reduction Enables Model Checking of More Complex Emerging Behaviours of Swarm Navigation Algorithms – Antuñya et al. 2015  Don’t let the title put you off – this paper is all about robot swarms! Previously we looked at some nature-inspired optimisation algorithms, including Particle Swarm Optimisation which draws inspiration from the behaviour of flocks of birds. Today’s paper comes from the field of robotics, and it’s all about emulating some of nature’s behaviours by creating swarms of robots. Each individual robot follows simple rules, but taken together those simple rules, when applied in a swarm of robots, produce interesting emergent behaviours. It would be nice to know the robots we let loose will always behave as we wish…  There are some fun parallels to distributed systems too. Robotic swarms consist of a set of robots with simple individual behaviour rules, working together in cooperation to achieve a more complex or emergent final behaviour. Appealing characteristics of swarms are the low cost incurred in producing the robots, which have a simple hardware design, scalability, and fault tolerance. Examples of their application to real-life tasks include nanorobotics, disaster rescue missions, and mining or agricultural foraging tasks. Just as with concurrent and distributed systems, we have to worry about safety and liveness in this context. And as we’re releasing real machines out into the real world, we might want to verify those properties hold. The emergent behaviours of a swarm of robots need to be verified, with respect to safety and liveness requirements, and validated to determine whether it is fit for purpose in the target environment. Safety requirements are the allowed behaviours of the system, and liveness requirements specify the dynamic behaviours expected to happen during the execution of the system. Also in common with formal methods in distributed systems, state space explosion puts practical bounds on the complexity of systems that can be addressed. Letting the robots move around in continuous space (in  modelling terms – they obviously do IRL) leads to an infinite state space. Instead we can chop space up into cells in a grid…  The discretization of the continuous space into cells of fixed size —i.e., a grid— is a solution that has been applied in swarms, to enable model checking. Even with the discretization of the environment into a “small” grid (e.g., 4×4 cells), the state-space explosion problem can occur due to the presence of other variables, which results in too many possible configurations of the robots in the grid. Symmetry reduction techniques have been used to reduce state spaces in model checking. The authors  show a way of encoding a swarm environment that eliminates symmetrically equivalent states from the state space – thus enabling formal verification to be applied to larger scale problems. We implemented a relative encoding of a swarm environment model that eliminates symmetrically equivalent states from the state space. The swarm is assumed to be homogeneous; i.e., all the robots are considered identical in capabilities and rank. In the relative encoding, one robot is set as the “reference”, with a fixed location and direction of motion. The other robots’ locations and directions are defined based on this reference. In an absolute encoding, if all the robots in a grid are simultaneously rotated in the same direction and shifted horizontally or vertically by the same distance, the robot’s new configurations change in location and direction. But with the relative encoding the locations and directions remain the same. This is the key to the state-space reduction. In a grid of size m x m, and for robots that can travel in d different directions, the state space is reduced by a factor of dm2:  If a model with r robots in a m × m size grid (locations), with d possible directions, p other robots’ variables of domain sizes vi,  i = 1, …, p, and q global variables of domain sizes sj ,  j = 1, …, q, is globally encoded, the size of the state space to be explored is (d × m2 × v1 × v2 × … × vp)r × (s1 × s2 × … × sq). In a relative encoding, the reference robot will have fixed location and direction, and the resulting state space will be of size (v1 × v2 × … × vp) x (d × m2 × v1 × v2 × … × vp)r-1 × (s1 × s2 × … × sq). This corresponds to a reduction of the state space of d×m2. In practice this bound changes according to the variables used in the relative encoding of an algorithm. This also means that finding a counterexample in the relative model is equivalent to a class of counterexamples in the global model. This approach is validated on the Alpha navigation algorithm, which rather disappointingly is known not to produce the desired behaviour! The Alpha algorithm has been used as a case study to demonstrate how to verify emergent behaviours in swarms through model checking tools. In the Alpha algorithm, the robots in the swarm navigate the environment trying to maintain connectivity, defined as a wireless range. This is achieved by the  following rules: (a) the default movement of a robot is forward, maintaining its current direction. (b) When a robot loses connection with another robot, if the remaining number of connected robots is smaller than a value α, the robot makes a 180 degree turn. (c) Every time a robot regains connectivity with another, it performs a random turn. The desired behaviour for a swarm is this case is that all robots shall eventually be connected. In prior work, a formal expression of the algorithm in Linear Temporal Logic showed this is false. A global model and a relative model are both encoded for the NuSMV model checker. Both were able to show that the desired condition does not hold, but the relative model gives a much smaller state space (by about 2 orders of magnitude):  Thus, verification through model checking can be performed over larger grid sizes and higher numbers of robots, and also for more detailed abstractions that model navigation algorithms using more variables. Although the state space reduction is more significant in terms of the grid size, and not as expressive if considering realistic swarm sizes, analysing small robot groups can help to understand larger swarms within the lower limit bounds of swarm size, which demands more from the navigation algorithm. Here’s an example of a counter-example found using the global encoding:  And this a more compact example found using the relative encoding:", "pdf_url": "http://lib-arxiv-008.serverfarm.cornell.edu/pdf/1505.05695v2.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/symmetry-reduction-enables-model-checking-of-more-complex-emerging-behaviours-of-swarm-navigation-algorithms.json"}
{"id": "31194274", "bin": "1200_1300", "summary_sentences": ["European Union regulations on algorithmic decision-making and a “right to explanation” Goodman & Flaxman, 2016  In just over a year, the General Data Protection Regulation (GDPR) becomes law in European member states.", "This paper focuses on just one particular aspect of the new law, article 22, as it relates to profiling, non-discrimination, and the right to an explanation.", "Article 22: Automated individual decision-making, including profiling, potentially prohibits a wide swath of algorithms currently in use in, e.g., recommendation systems, credit and insurance risk assessments, computational advertising, and social networks.", "This raises important issues that are of particular concern to the machine learning community.", "In its current form, the GDPR’s requirements could require a complete overhaul of standard and widely used algorithmic techniques.", "Profiling has a very inclusive definition, being anything “aimed at analysing or predicting aspects concerning that natural person’s performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location, or movements.” Underlying this is the right to non-discrimination.", "Non-discrimination  The use of algorithmic profiling for the allocation of resources is, in a certain sense, inherently discriminatory: profiling takes place when data subjects are grouped in categories according to various variables, and decisions are made on the basis of subjects falling within so-defined groups.", "It is thus not surprising that concerns over discrimination have begun to take root in discussion over the ethics of big data.", "Personal data is any information relating to an identified or identifiable natural person.", "Sensitive personal data includes “personal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, or trade-union membership, and the processing of genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health or data concerning a natural person’s sex life or sexual orientation…” Profiling using personal data is permitted when explicit consent is obtained, it is deemed necessary for the contract between a subject and a data controller, and suitable measures are in place to safeguard the data subject’s rights and freedoms.", "However, such profiling is not permitted if it involves sensitive data.", "Goodman and Flaxman discuss two possible interpretations of the prohibition on the use of sensitive data in profiling.", "The minimal interpretation says that it refers only to cases where an algorithm makes explicit direct use of sensitive data.", "However, it is widely acknowledged that simply removing certain variables from a model does not ensure predictions that are, in effect, uncorrelated to those variables.", "For example, if a certain geographic region has a high number of low income or minority residents, an algorithm that employs geographic data to determine loan eligibility is likely to produce results that are, in effect, informed by race and income.", "(On this point, I recently read and enjoyed “ Weapons of Math Destruction ” by Cathy O’Neil – with thanks to Daniel Bryant for the recommendation).", "A second maximal interpretation is therefore possible in which decisions based on sensitive data extend to those using variables correlated with sensitive data.", "The difficulty here is that correlations can be very difficult to detect.", "The link between geography and income may be obvious, but less obvious correlations – say between IP address and race – are likely to exist within large enough datasets and could lead to discriminatory effects… With sufficiently large data sets, the task of exhaustively identifying and excluding data features correlated with “sensitive categories” a priori may be impossible.", "Companies may also be reluctant to exclude certain covariates – web-browsing patterns are a very good predictor for various recommendation systems, but they are also correlated with sensitive categories.", "In another example of ‘bias in, bias out’, Goodman and Flaxman provide a thought-provoking example whereby purging variables from the dataset still leaves open a door for unintentional discrimination.", "They call this uncertainty bias, and it arises under two conditions:  One group is underrepresented in the sample, so there is more uncertainty associated with predictions about that group.", "The algorithm is risk averse, so it will ceteris paribus prefer to make decisions about which it is more confident (i.e., those with smaller confidence intervals.", "Here’s a concrete example showing how biased decisions can emerge under such conditions:  A classifier that genuinely had ‘white’ and ‘non-white’ categories would definitely fall under all interpretations of sensitive data.", "However in practice a classifier will most likely use complicated combinations of multiple categories (occupation, location, consumption patterns, etc.", "), and any rare combinations will have very few observations.", "The complexity and multifaceted nature of algorithmic discrimination suggests that appropriate solutions will require an understanding of how it arises in practice.", "This highlights the need for human-intelligible explanations of algorithmic decision making.", "The right to an explanation  When profiling takes place, a data subject has the right to “meaningful information about the logic involved.” In “ How the machine thinks: understanding opacity in machine learning algorithms ” Burrell outlines three barriers to transparency:  Intentional concealment on the part of corporations or other institutions, where decision making procedures are kept from public scrutiny  Gaps in technical literacy which mean that, for most people, simply having access to underlying code is insufficient  A “mismatch between the mathematical optimization in high-dimensionality characteristic of machine learning and the demands of human-scale reasoning and styles of interpretation.”  The first barrier is addressed by the requirement for information to be made available to the data subject.", "For the second barrier, the GDPR requires that communication with data subjects is in a “concise, intelligible, and easily accessible form” (emphasis mine).", "The third barrier is mostly a function of algorithmic selection and design (though see e.g. “ Why should I trust you?", "Explaining the predictions of any classifier ”  for a system that attempts to explain classifier results ex post facto).", "Putting aside any barriers arising from technical fluency, and also ignoring the importance of training the model, it stands to reason that an algorithm can only be explained if the trained model can be articulated and understood by a human.", "It is reasonable to suppose that any adequate explanation would, at a minimum, provide an account of how input features relate to predictions, allowing one to answer questions such as: Is the model more or less likely to recommend a loan if the applicant is a minority Which features play the largest role in prediction?", "A description of your network architecture and the values of all of the parameters is unlikely to cut it.", "The last word  Above all else, the GDPR is a vital acknowledgement that, when algorithms are deployed in society, few if any decisions are purely “technical”.", "Rather, the ethical design of algorithms requires coordination between technical and philosophical resources of the highest caliber.", "A start has been made, but there is far to go."], "summary_text": "European Union regulations on algorithmic decision-making and a “right to explanation” Goodman & Flaxman, 2016  In just over a year, the General Data Protection Regulation (GDPR) becomes law in European member states. This paper focuses on just one particular aspect of the new law, article 22, as it relates to profiling, non-discrimination, and the right to an explanation. Article 22: Automated individual decision-making, including profiling, potentially prohibits a wide swath of algorithms currently in use in, e.g., recommendation systems, credit and insurance risk assessments, computational advertising, and social networks. This raises important issues that are of particular concern to the machine learning community. In its current form, the GDPR’s requirements could require a complete overhaul of standard and widely used algorithmic techniques. Profiling has a very inclusive definition, being anything “aimed at analysing or predicting aspects concerning that natural person’s performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location, or movements.” Underlying this is the right to non-discrimination. Non-discrimination  The use of algorithmic profiling for the allocation of resources is, in a certain sense, inherently discriminatory: profiling takes place when data subjects are grouped in categories according to various variables, and decisions are made on the basis of subjects falling within so-defined groups. It is thus not surprising that concerns over discrimination have begun to take root in discussion over the ethics of big data. Personal data is any information relating to an identified or identifiable natural person. Sensitive personal data includes “personal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, or trade-union membership, and the processing of genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health or data concerning a natural person’s sex life or sexual orientation…” Profiling using personal data is permitted when explicit consent is obtained, it is deemed necessary for the contract between a subject and a data controller, and suitable measures are in place to safeguard the data subject’s rights and freedoms. However, such profiling is not permitted if it involves sensitive data. Goodman and Flaxman discuss two possible interpretations of the prohibition on the use of sensitive data in profiling. The minimal interpretation says that it refers only to cases where an algorithm makes explicit direct use of sensitive data. However, it is widely acknowledged that simply removing certain variables from a model does not ensure predictions that are, in effect, uncorrelated to those variables. For example, if a certain geographic region has a high number of low income or minority residents, an algorithm that employs geographic data to determine loan eligibility is likely to produce results that are, in effect, informed by race and income. (On this point, I recently read and enjoyed “ Weapons of Math Destruction ” by Cathy O’Neil – with thanks to Daniel Bryant for the recommendation). A second maximal interpretation is therefore possible in which decisions based on sensitive data extend to those using variables correlated with sensitive data. The difficulty here is that correlations can be very difficult to detect. The link between geography and income may be obvious, but less obvious correlations – say between IP address and race – are likely to exist within large enough datasets and could lead to discriminatory effects… With sufficiently large data sets, the task of exhaustively identifying and excluding data features correlated with “sensitive categories” a priori may be impossible. Companies may also be reluctant to exclude certain covariates – web-browsing patterns are a very good predictor for various recommendation systems, but they are also correlated with sensitive categories. In another example of ‘bias in, bias out’, Goodman and Flaxman provide a thought-provoking example whereby purging variables from the dataset still leaves open a door for unintentional discrimination. They call this uncertainty bias, and it arises under two conditions:  One group is underrepresented in the sample, so there is more uncertainty associated with predictions about that group. The algorithm is risk averse, so it will ceteris paribus prefer to make decisions about which it is more confident (i.e., those with smaller confidence intervals. Here’s a concrete example showing how biased decisions can emerge under such conditions:  A classifier that genuinely had ‘white’ and ‘non-white’ categories would definitely fall under all interpretations of sensitive data. However in practice a classifier will most likely use complicated combinations of multiple categories (occupation, location, consumption patterns, etc. ), and any rare combinations will have very few observations. The complexity and multifaceted nature of algorithmic discrimination suggests that appropriate solutions will require an understanding of how it arises in practice. This highlights the need for human-intelligible explanations of algorithmic decision making. The right to an explanation  When profiling takes place, a data subject has the right to “meaningful information about the logic involved.” In “ How the machine thinks: understanding opacity in machine learning algorithms ” Burrell outlines three barriers to transparency:  Intentional concealment on the part of corporations or other institutions, where decision making procedures are kept from public scrutiny  Gaps in technical literacy which mean that, for most people, simply having access to underlying code is insufficient  A “mismatch between the mathematical optimization in high-dimensionality characteristic of machine learning and the demands of human-scale reasoning and styles of interpretation.”  The first barrier is addressed by the requirement for information to be made available to the data subject. For the second barrier, the GDPR requires that communication with data subjects is in a “concise, intelligible, and easily accessible form” (emphasis mine). The third barrier is mostly a function of algorithmic selection and design (though see e.g. “ Why should I trust you? Explaining the predictions of any classifier ”  for a system that attempts to explain classifier results ex post facto). Putting aside any barriers arising from technical fluency, and also ignoring the importance of training the model, it stands to reason that an algorithm can only be explained if the trained model can be articulated and understood by a human. It is reasonable to suppose that any adequate explanation would, at a minimum, provide an account of how input features relate to predictions, allowing one to answer questions such as: Is the model more or less likely to recommend a loan if the applicant is a minority Which features play the largest role in prediction? A description of your network architecture and the values of all of the parameters is unlikely to cut it. The last word  Above all else, the GDPR is a vital acknowledgement that, when algorithms are deployed in society, few if any decisions are purely “technical”. Rather, the ethical design of algorithms requires coordination between technical and philosophical resources of the highest caliber. A start has been made, but there is far to go.", "pdf_url": "https://arxiv.org/pdf/1606.08813v3.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/european-union-regulations-on-algorithmic-decision-making-and-a-right-to-explanation.json"}
{"id": "69467563", "bin": "1200_1300", "summary_sentences": ["Dynamic word embeddings for evolving semantic discovery Yao et al., WSDM’18  One of the most popular posts on this blog is my introduction to word embeddings with word2vec (‘ The amazing power of word vectors ’).", "In today’s paper choice Yao et al. introduce a lovely extension that enables you to track how the meaning of words changes over time.", "It would be a superb tool for analysing brands for example.", "Human language is an evolving construct, with word semantic associations changing over time.", "For example, apple which was traditionally only associated with fruits, is now also associated with a technology company.", "Similarly the association of names of famous personalities (e.g., trump) changes with a change in their roles.", "For this reason, understanding and tracking word evolution is useful for time-aware knowledge extraction tasks….", "Let’s go straight to some examples of what we’re talking about here:  ( Enlarge )  Consider the trajectory of ‘apple’: in 1994 it’s most closely associated with fruits, and by 2000 changing dietary associations can be seen, and apple is associated with the less healthy ‘cake,’ ‘tart,’ and ‘cream.’ From 2005 through 2016 though, the word is strongly associated with Apple the company, and moreover you can see the changing associations with Apple over time, from ‘iTunes’ to Google, Microsoft, Samsung et al..", "Likewise ‘amazon’ moves from a river to the company Amazon, and ‘Obama’ moves from his pre-presidential roles to president, as does ‘Trump.’  These embeddings are learned from articles in The New York Times between 1990 and 2016.", "The results are really interesting (we’ll see more fun things you can do with them shortly), but you might be wondering why this is hard to do.", "Why not simply divide up the articles in the corpus (e.g., by year), learn word embeddings for each partition (which we know how to do), and then compare them?", "What makes this complicated is that when you learn an embedding for a word in one time window (e.g., ‘bank’), there’s no guarantee that the embedding will match that in another time window, even if there is no semantic change in the meaning of the word across the two.", "So the meaning of ‘bank’ in 1990 and 1995 could be substantially the same, and yet the learned embeddings might not be.", "This is known as the alignment problem.", "A key practical issue of learning different word embeddings for different time periods is alignment.", "Specifically, most cost functions for training are invariant to rotations, as a byproduct, the learned embeddings across time may not be placed in the same latent space.", "Prior approaches to solving this problem first use independent learning as per our straw man, and then post process the embeddings in an alignment phase to try and match them up.", "But Yao et al. have found a way to learn temporal embeddings in all time slices concurrently, doing away with the need for a separate alignment phase.", "The experimental results suggests that this yields better outcomes that the prior two-step methods, and the approach is also robust against data sparsity (it will tolerate time slices where some words are rarely present or even missing).", "Temporal word embeddings  Recall that underpinning word embeddings is the idea of a co-occurrence matrix (see ‘ GloVe: Global vectors for word representation ’) capturing the pointwise mutual information (PMI) between any two words in the vocabulary.", "Given a corpus  , we can compute a PMI matrix using windows of size L (around the word in question), where the entry at (w,c) for words w and_c_ is given by:  Where  counts the number of times that words w and c co-occur within a window of size L in corpus  and  and  count the number of occurences of w and c in the corpus respectively.", "The learned embedding vectors for w and c,  and  , are such that  .", "Adding a temporal dimension to this, for each time slice t the positive PMI matrix  is defined as :  And the temporal word embeddings  must satisfy  .", "This still doesn’t solve the alignment problem though.", "To encourage alignment, the authors cast finding temporal word embeddings as the solution to the following joint optimisation problem:  where  and  are configurable parameters greater than zero.", "The penalty term  enforces low-rank data fidelity as has been widely adopted in previous work  The smoothing term  encourages the word embeddings to align.", "The parameter  controls how fast the embeddings can change over time.", "This is decomposed to solve the objective function across time for each  , using block coordinate descent (BCD) which minimises with respect to a single block ($U(t)$) at a time.", "In theory BCD lacks convergence guarantees, but in practice it seems to work well.", "You could always swap BCD for e.g. SGD if you wanted to (but it would make slower progress).", "Finding equivalent terms over time  Let’s return to the fun things you can do with the resulting embeddings.", "Here’s an example of finding conceptually equivalent items or people over time.", "For example, the closest equivalent to the ’iPhone’ as of 2012 was ‘pc’ in 2003, and by 2013-16 it was ‘smartphone.’  Likewise back in 1990-94 the equivalents of twitter were ‘broadcast’, ‘cnn’, ‘radio’ etc..", "In the ‘mp3’ column you can clearly see associations with the dominant form of music consumption in the given time periods.", "We can do a similar thing asking who played a certain political role in a given year:  Even more impressive, is this search for equivalence in sport by looking for the ATP No.", "1 ranked male tennis player in a given year.", "Here we’re asking, who played the same role that Nadal did in the year 2010?", "Tracking popularity over time  The learned word vector norms across times grow with word frequency, and can be viewed as a time series for detecting trending concepts with more robustness than word frequency.", "Generally, comparing to frequencies which are more sporadic and noisy, we note that the norm of our embeddings encourages smoothness and normalization while being indicative of the periods when the corresponding words were making news rounds.", "Here’s a comparison of norms (top chart) and word frequency (bottom chart) for the names of US presidents over time.", "The last word  Our proposed method simultaneously learns the embeddings and aligns them across time, and has several benefits: higher interpretability for embeddings, better quality with less data, and more reliable alignment for across-time querying."], "summary_text": "Dynamic word embeddings for evolving semantic discovery Yao et al., WSDM’18  One of the most popular posts on this blog is my introduction to word embeddings with word2vec (‘ The amazing power of word vectors ’). In today’s paper choice Yao et al. introduce a lovely extension that enables you to track how the meaning of words changes over time. It would be a superb tool for analysing brands for example. Human language is an evolving construct, with word semantic associations changing over time. For example, apple which was traditionally only associated with fruits, is now also associated with a technology company. Similarly the association of names of famous personalities (e.g., trump) changes with a change in their roles. For this reason, understanding and tracking word evolution is useful for time-aware knowledge extraction tasks…. Let’s go straight to some examples of what we’re talking about here:  ( Enlarge )  Consider the trajectory of ‘apple’: in 1994 it’s most closely associated with fruits, and by 2000 changing dietary associations can be seen, and apple is associated with the less healthy ‘cake,’ ‘tart,’ and ‘cream.’ From 2005 through 2016 though, the word is strongly associated with Apple the company, and moreover you can see the changing associations with Apple over time, from ‘iTunes’ to Google, Microsoft, Samsung et al.. Likewise ‘amazon’ moves from a river to the company Amazon, and ‘Obama’ moves from his pre-presidential roles to president, as does ‘Trump.’  These embeddings are learned from articles in The New York Times between 1990 and 2016. The results are really interesting (we’ll see more fun things you can do with them shortly), but you might be wondering why this is hard to do. Why not simply divide up the articles in the corpus (e.g., by year), learn word embeddings for each partition (which we know how to do), and then compare them? What makes this complicated is that when you learn an embedding for a word in one time window (e.g., ‘bank’), there’s no guarantee that the embedding will match that in another time window, even if there is no semantic change in the meaning of the word across the two. So the meaning of ‘bank’ in 1990 and 1995 could be substantially the same, and yet the learned embeddings might not be. This is known as the alignment problem. A key practical issue of learning different word embeddings for different time periods is alignment. Specifically, most cost functions for training are invariant to rotations, as a byproduct, the learned embeddings across time may not be placed in the same latent space. Prior approaches to solving this problem first use independent learning as per our straw man, and then post process the embeddings in an alignment phase to try and match them up. But Yao et al. have found a way to learn temporal embeddings in all time slices concurrently, doing away with the need for a separate alignment phase. The experimental results suggests that this yields better outcomes that the prior two-step methods, and the approach is also robust against data sparsity (it will tolerate time slices where some words are rarely present or even missing). Temporal word embeddings  Recall that underpinning word embeddings is the idea of a co-occurrence matrix (see ‘ GloVe: Global vectors for word representation ’) capturing the pointwise mutual information (PMI) between any two words in the vocabulary. Given a corpus  , we can compute a PMI matrix using windows of size L (around the word in question), where the entry at (w,c) for words w and_c_ is given by:  Where  counts the number of times that words w and c co-occur within a window of size L in corpus  and  and  count the number of occurences of w and c in the corpus respectively. The learned embedding vectors for w and c,  and  , are such that  . Adding a temporal dimension to this, for each time slice t the positive PMI matrix  is defined as :  And the temporal word embeddings  must satisfy  . This still doesn’t solve the alignment problem though. To encourage alignment, the authors cast finding temporal word embeddings as the solution to the following joint optimisation problem:  where  and  are configurable parameters greater than zero. The penalty term  enforces low-rank data fidelity as has been widely adopted in previous work  The smoothing term  encourages the word embeddings to align. The parameter  controls how fast the embeddings can change over time. This is decomposed to solve the objective function across time for each  , using block coordinate descent (BCD) which minimises with respect to a single block ($U(t)$) at a time. In theory BCD lacks convergence guarantees, but in practice it seems to work well. You could always swap BCD for e.g. SGD if you wanted to (but it would make slower progress). Finding equivalent terms over time  Let’s return to the fun things you can do with the resulting embeddings. Here’s an example of finding conceptually equivalent items or people over time. For example, the closest equivalent to the ’iPhone’ as of 2012 was ‘pc’ in 2003, and by 2013-16 it was ‘smartphone.’  Likewise back in 1990-94 the equivalents of twitter were ‘broadcast’, ‘cnn’, ‘radio’ etc.. In the ‘mp3’ column you can clearly see associations with the dominant form of music consumption in the given time periods. We can do a similar thing asking who played a certain political role in a given year:  Even more impressive, is this search for equivalence in sport by looking for the ATP No. 1 ranked male tennis player in a given year. Here we’re asking, who played the same role that Nadal did in the year 2010? Tracking popularity over time  The learned word vector norms across times grow with word frequency, and can be viewed as a time series for detecting trending concepts with more robustness than word frequency. Generally, comparing to frequencies which are more sporadic and noisy, we note that the norm of our embeddings encourages smoothness and normalization while being indicative of the periods when the corresponding words were making news rounds. Here’s a comparison of norms (top chart) and word frequency (bottom chart) for the names of US presidents over time. The last word  Our proposed method simultaneously learns the embeddings and aligns them across time, and has several benefits: higher interpretability for embeddings, better quality with less data, and more reliable alignment for across-time querying.", "pdf_url": "https://arxiv.org/pdf/1703.00607", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/dynamic-word-embeddings-for-evolving-semantic-discovery.json"}
{"id": "3913778", "bin": "1200_1300", "summary_sentences": ["Software engineering for machine learning: a case study Amershi et al., ICSE’19  Previously on The Morning Paper we’ve looked at the spread of machine learning through Facebook and Google and some of the lessons learned together with processes and tools to address the challenges arising.", "Today it’s the turn of Microsoft.", "More specifically, we’ll be looking at the results of an internal study with over 500 participants designed to figure out how product development and software engineering is changing at Microsoft with the rise of AI and ML.", "… integration of machine learning components is happening all over the company, not just on teams historically known for it.", "A list of application areas includes search, advertising, machine translation, predicting customer purchases, voice recognition, image recognition, identifying customer leads, providing design advice for presentations and word processing documents, creating unique drawing features, healthcare, improving gameplay, sales forecasting, decision optimisation, incident reporting, bug analysis, fraud detection, and security monitoring.", "As you might imagine, these are underpinned by a wide variety of different ML models.", "The teams doing the work are also varied in their make-up, some containing data scientists with many years of experience, and others just starting out.", "In a manner that’s very reminiscent of the online experimentation evolution model at Microsoft we looked at previously, data science moves from a bolt-on specialized skill to a deeply integrated capability over time:  Some software teams employ polymath data scientists, who “do it all,” but as data science needs to scale up, their roles specialize into domain experts who deeply understand the business problems, modelers who develop predictive models, and platform builders who create the cloud-based infrastructure.", "To help spread these skills through the company a variety of tactics are used: a twice-yearly internal conference on machine learning and data science dedicates at least one day to the basics of technologies, algorithms, and best practices; internal talks are given year round on engineering details behind projects, and cutting-edge advances from academic conferences; several teams host weekly open forums on ML and deep learning; and there are mailing lists and online forums with thousands of participants.", "A survey informed by conversations with 14 experienced ML leaders within Microsoft was sent to 4,195 members of those internal mailing lists, garnering 551 replies.", "Respondents were well spread across data and applied science (42%), software engineering (32%), program management (17%), research (7%) and other (1%).", "21% of respondents were managers and the rest were individual contributors.", "A general process  The generic machine learning process looks like this:  ( Enlarge )  That diagram is hopefully pretty self-explanatory so I won’t spell out all of the individual stages.", "For simplicity the view in Figure 1 is linear, however, machine learning workflows are highly non-linear and contain several feedback loops.", "For example, if engineers notice that there is a large distribution shift between the training data and the data in the real world, they might want to go back and collect more representative data and rerun the workflow… This workflow can become even more complex if the system is integrative, containing multiple ML components which interact together in complex and unexpected ways.", "Learnings and emerging best practices  Having a seamless development experience covering (possibly) all the different stages in the process outlined above is important to automation.", "But getting there is far from easy!", "It is important to develop a “rock solid data pipeline, capable of continuously loading and massaging data, enabling engineers to try out many permutations of AI algorithms with different hyper-parameters without hassle.”  IDEs with visual tools are useful when starting out with machine learning, but teams tend to grow out of them with experience.", "The success of ML-centric projects depends heavily on data availability, quality, and management.", "In addition to availability, our respondents focus most heavily on supporting the following data attributes: “accessibility, accuracy, authoritativeness, freshness, latency, structuredness, ontological typing, connectedness, and semantic joinability.”  Microsoft teams found a need to blend traditional data management tools with their ML frameworks and pipelines.", "Data sources are continuously changing and rigorous data versioning and sharing techniques are required.", "Models have a provenance tag explaining which data it has been trained on and which version of the model was used.", "Datasets are tagged with information about where they came from and the version of the code used to extract it.", "ML-centric software also sees frequent revisions initiated by model changes, parameter tuning, and data updates, the combination of which can have a significant impact on system performance.", "To address this, rigorous rollout processes are required.", "… [teams] developed systematic processes by adopting combo-flighting techniques (i.e., flighting a combination of changes and updates), including multiple metrics in their experiment score cards, and performing human-driven evaluation for more sensitive data categories.", "Model building should be integrated with the rest of the software development process, including common code repositories and tightly coupled sprints and stand-ups.", "The support a team requires changes according to their level of experience with ML, but regardless of experience levels, data availability, collection, cleaning, and management remains the number one concern!", "( Enlarge )  The big three  We identified three aspects of the AI domain that make it fundamentally different than prior application domains.", "Their impact will require significant research efforts to address in the future.", "Discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering.", "“While there are very well-designed technologies to version code, the same is not true for data…“  Model customisation and model reuse require very different skills than those typically found in software teams (“you can’t simply change parameters with a text editor” !!).", "AI components are more difficult to handle as distinct modules than traditional software components — models may be “entangled” in complex ways and experience non-monotonic error behaviour.", "While the first two points are hopefully pretty self-explanatory, the third warrants a little more unpacking.", "Maintaining strict module boundaries between machine learned models is difficult for two reasons.", "First, models are not easily extensible.", "For example, one cannot (yet) take an NLP model of English and add a separate NLP model for ordering pizza and expect them to work properly together… Second, models interact in non-obvious ways.", "In large scale systems with more than a single model, each model’s results will affect one another’s training and tuning processes.", "Under these conditions, even with separated code, one model’s effectiveness can change as a result of changes in another model.", "This phenomenon is sometimes known as component entanglement and can lead to non-monotonic error propagation: improvements in one part of the system may actually decreases the overall system quality."], "summary_text": "Software engineering for machine learning: a case study Amershi et al., ICSE’19  Previously on The Morning Paper we’ve looked at the spread of machine learning through Facebook and Google and some of the lessons learned together with processes and tools to address the challenges arising. Today it’s the turn of Microsoft. More specifically, we’ll be looking at the results of an internal study with over 500 participants designed to figure out how product development and software engineering is changing at Microsoft with the rise of AI and ML. … integration of machine learning components is happening all over the company, not just on teams historically known for it. A list of application areas includes search, advertising, machine translation, predicting customer purchases, voice recognition, image recognition, identifying customer leads, providing design advice for presentations and word processing documents, creating unique drawing features, healthcare, improving gameplay, sales forecasting, decision optimisation, incident reporting, bug analysis, fraud detection, and security monitoring. As you might imagine, these are underpinned by a wide variety of different ML models. The teams doing the work are also varied in their make-up, some containing data scientists with many years of experience, and others just starting out. In a manner that’s very reminiscent of the online experimentation evolution model at Microsoft we looked at previously, data science moves from a bolt-on specialized skill to a deeply integrated capability over time:  Some software teams employ polymath data scientists, who “do it all,” but as data science needs to scale up, their roles specialize into domain experts who deeply understand the business problems, modelers who develop predictive models, and platform builders who create the cloud-based infrastructure. To help spread these skills through the company a variety of tactics are used: a twice-yearly internal conference on machine learning and data science dedicates at least one day to the basics of technologies, algorithms, and best practices; internal talks are given year round on engineering details behind projects, and cutting-edge advances from academic conferences; several teams host weekly open forums on ML and deep learning; and there are mailing lists and online forums with thousands of participants. A survey informed by conversations with 14 experienced ML leaders within Microsoft was sent to 4,195 members of those internal mailing lists, garnering 551 replies. Respondents were well spread across data and applied science (42%), software engineering (32%), program management (17%), research (7%) and other (1%). 21% of respondents were managers and the rest were individual contributors. A general process  The generic machine learning process looks like this:  ( Enlarge )  That diagram is hopefully pretty self-explanatory so I won’t spell out all of the individual stages. For simplicity the view in Figure 1 is linear, however, machine learning workflows are highly non-linear and contain several feedback loops. For example, if engineers notice that there is a large distribution shift between the training data and the data in the real world, they might want to go back and collect more representative data and rerun the workflow… This workflow can become even more complex if the system is integrative, containing multiple ML components which interact together in complex and unexpected ways. Learnings and emerging best practices  Having a seamless development experience covering (possibly) all the different stages in the process outlined above is important to automation. But getting there is far from easy! It is important to develop a “rock solid data pipeline, capable of continuously loading and massaging data, enabling engineers to try out many permutations of AI algorithms with different hyper-parameters without hassle.”  IDEs with visual tools are useful when starting out with machine learning, but teams tend to grow out of them with experience. The success of ML-centric projects depends heavily on data availability, quality, and management. In addition to availability, our respondents focus most heavily on supporting the following data attributes: “accessibility, accuracy, authoritativeness, freshness, latency, structuredness, ontological typing, connectedness, and semantic joinability.”  Microsoft teams found a need to blend traditional data management tools with their ML frameworks and pipelines. Data sources are continuously changing and rigorous data versioning and sharing techniques are required. Models have a provenance tag explaining which data it has been trained on and which version of the model was used. Datasets are tagged with information about where they came from and the version of the code used to extract it. ML-centric software also sees frequent revisions initiated by model changes, parameter tuning, and data updates, the combination of which can have a significant impact on system performance. To address this, rigorous rollout processes are required. … [teams] developed systematic processes by adopting combo-flighting techniques (i.e., flighting a combination of changes and updates), including multiple metrics in their experiment score cards, and performing human-driven evaluation for more sensitive data categories. Model building should be integrated with the rest of the software development process, including common code repositories and tightly coupled sprints and stand-ups. The support a team requires changes according to their level of experience with ML, but regardless of experience levels, data availability, collection, cleaning, and management remains the number one concern! ( Enlarge )  The big three  We identified three aspects of the AI domain that make it fundamentally different than prior application domains. Their impact will require significant research efforts to address in the future. Discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering. “While there are very well-designed technologies to version code, the same is not true for data…“  Model customisation and model reuse require very different skills than those typically found in software teams (“you can’t simply change parameters with a text editor” !!). AI components are more difficult to handle as distinct modules than traditional software components — models may be “entangled” in complex ways and experience non-monotonic error behaviour. While the first two points are hopefully pretty self-explanatory, the third warrants a little more unpacking. Maintaining strict module boundaries between machine learned models is difficult for two reasons. First, models are not easily extensible. For example, one cannot (yet) take an NLP model of English and add a separate NLP model for ordering pizza and expect them to work properly together… Second, models interact in non-obvious ways. In large scale systems with more than a single model, each model’s results will affect one another’s training and tuning processes. Under these conditions, even with separated code, one model’s effectiveness can change as a result of changes in another model. This phenomenon is sometimes known as component entanglement and can lead to non-monotonic error propagation: improvements in one part of the system may actually decreases the overall system quality.", "pdf_url": "https://dl.acm.org/doi/pdf/10.1109/ICSE-SEIP.2019.00042?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/software-engineering-for-machine-learning.json"}
{"id": "50897768", "bin": "1200_1300", "summary_sentences": ["Texture Networks: Feed-forward synthesis of textures and stylized images Ulyanov et al., arXiv, March 2016  During the summer break I mostly stayed away from news feeds and twitter, which induces terrible FOMO (Fear Of Missing Out) to start with.", "What great research was published / discussed that I missed?", "Was there a major industry announcement I’m completely ignorant of?", "One thing I’m glad I didn’t miss was the Prisma app that produces quite beautiful stylized versions of photos from your smartphone.", "It’s a great example of deep technology behind a simple interface, and also of the rapid packaging and exploitation of research results – today’s choice is the paper describing the technology breakthrough that makes Prisma possible, and it was released to arXiv in March 2016.", "The source code and models described in the paper can also be found on GitHub.", "Gatys et al. recently (2015) showed that deep networks can generate beautiful textures and stylized images from a single texture example.", "If you want to style a lot of images though (to provide styling-as-a-service for example), you’ll find that their technique is slow and uses a lot of memory.", "To generate images of equivalent quality, an implementation of Gatys et al. required about 10 seconds and 1.1GB of memory, whereas the approach described by Ulyanov et al.", "in this paper requires about 20ms and only 170MB of memory.", "Significantly faster and cheaper therefore, and although the algorithm doesn’t quite match the results of Gatys et al. for all images, it’s still very good.", "Just in case you haven’t seen it, here are some examples.", "First, generating textures in the style of sample image:  And combining a style image with a content image:  If you download the app, you can play with examples using your own photos.", "One of the possibilities I’m personally excited about is the opportunities the image creation speed opens up for applying the technique to movies.", "I like the images, but when I saw the movies created by Ruder et al. using an extension of the Gatys technique I was really blown away [ paper , explanation and video ].", "Update: I just learned about the Artisto app that does this for you!", "High-level approach  In general, one may look at the process of generating an image x as the problem of drawing a sample from a certain distribution p(x).", "In texture synthesis, the distribution is induced by an example texture instance x0 such that we can write x ~ p(x|x0).", "In style transfer, the distributed is induced by an image x0 representative of the visual style (e.g. an impressionist painting) and a second image x1 representative of the visual content (e.g.", "a boat), such that x ~ p(x|x0,x1).", "Gatys et al. cast this as an optimisation problem looking to minimise the difference between certain image statistics of the generated image, and the statistics of the example image(s).", "They use an iterative optimisation procedure with back propagation to gradually change the values of the pixels in the generated image until the desired statistics are achieved.", "In contrast, in the texture networks approach a feed-forward generation network produces the image, which requires only a single evaluation of the network and does not incur in the cost of backpropagation.", "A separate generator network is trained for each texture or style and, once trained, it can synthesize an arbitrary number of images of arbitrary size in an efficient feed-forward manner.", "The loss function used in training the generator network is derived from Gatys et al. and compares image statistics extracted from a fixed pre-trained descriptor CNN.", "This is used to measure the mismatch between the prototype texture and the generated image.", "The texture loss function compares feature activations across all spatial locations.", "A similar content loss function compares feature activations at corresponding spatial locations, and therefore preserves spatial information.", "Analogously to Gatys et al. we use the texture loss alone when training a generator network for texture synthesis, and we use a weighted combination of the texture loss and the content loss when training a generator network for stylization.", "Textures  A texture generator network is trained to transform a noise vector sampled from a certain distribution into texture samples that match, according to the texture loss function, a certain prototype texture x0, a three colour channel tensor.", "We experimented with several architectures for the generator network g… we found that multi-scale architectures result in images with small texture loss and better perceptual quality while using fewer parameters and training faster.", "[…] Each random noise tensor is first processed by a sequence of convolutional and non-linear activation layers, then upsampled by a factor of two, and finally concatenated as additional feature channels to the partially processed tensor from the scale below.", "(Click on image for larger view).", "Each convolutional block contains three convolutional layers containing respectively 3×3, 3×3, and 1×1 filters applied using circular convolution to remove boundary effects.", "Each convolutional layer is followed by a ReLU activation layer.", "When learning using stochastic gradient descent each iteration draws a mini-batch of noise vectors, performs forward evaluation of the generator network to obtain the corresponding images, and computes the loss vs x0.", "… After that, the gradient of the texture loss with respect to the generator network parameters θ is computed using backpropagation, and the gradient is used to update the parameters.", "Styling  For stylized image generator networks the network is modified to take as input in addition to the noise vector z , the image y to which the noise should be applied.", "The generator network is then trained to output an image x that is close in content to y and in texture/style to a reference texture x0.", "The architecture is the same as that used for texture synthesis, _with the important difference that the noise tensors at the K scales are concatenated (as additional feature channels) with downsampled versions of the input image y.", "The learning objective is to minimize the combination of the content and texture loss.", "In practice, we found that learning is surprisingly resilient to overfitting and that it suffices to approximate the distribution on natural images with a very small pool of images (e.g 16).", "Broader applicability  The success of this approach highlights the suitability of feed-forward networks for complex data generation and for solving complex tasks in general.", "The key to this success is the use of complex loss functions that involve different feed-forward architectures serving as “experts” assessing the performance of the feed-forward generator."], "summary_text": "Texture Networks: Feed-forward synthesis of textures and stylized images Ulyanov et al., arXiv, March 2016  During the summer break I mostly stayed away from news feeds and twitter, which induces terrible FOMO (Fear Of Missing Out) to start with. What great research was published / discussed that I missed? Was there a major industry announcement I’m completely ignorant of? One thing I’m glad I didn’t miss was the Prisma app that produces quite beautiful stylized versions of photos from your smartphone. It’s a great example of deep technology behind a simple interface, and also of the rapid packaging and exploitation of research results – today’s choice is the paper describing the technology breakthrough that makes Prisma possible, and it was released to arXiv in March 2016. The source code and models described in the paper can also be found on GitHub. Gatys et al. recently (2015) showed that deep networks can generate beautiful textures and stylized images from a single texture example. If you want to style a lot of images though (to provide styling-as-a-service for example), you’ll find that their technique is slow and uses a lot of memory. To generate images of equivalent quality, an implementation of Gatys et al. required about 10 seconds and 1.1GB of memory, whereas the approach described by Ulyanov et al. in this paper requires about 20ms and only 170MB of memory. Significantly faster and cheaper therefore, and although the algorithm doesn’t quite match the results of Gatys et al. for all images, it’s still very good. Just in case you haven’t seen it, here are some examples. First, generating textures in the style of sample image:  And combining a style image with a content image:  If you download the app, you can play with examples using your own photos. One of the possibilities I’m personally excited about is the opportunities the image creation speed opens up for applying the technique to movies. I like the images, but when I saw the movies created by Ruder et al. using an extension of the Gatys technique I was really blown away [ paper , explanation and video ]. Update: I just learned about the Artisto app that does this for you! High-level approach  In general, one may look at the process of generating an image x as the problem of drawing a sample from a certain distribution p(x). In texture synthesis, the distribution is induced by an example texture instance x0 such that we can write x ~ p(x|x0). In style transfer, the distributed is induced by an image x0 representative of the visual style (e.g. an impressionist painting) and a second image x1 representative of the visual content (e.g. a boat), such that x ~ p(x|x0,x1). Gatys et al. cast this as an optimisation problem looking to minimise the difference between certain image statistics of the generated image, and the statistics of the example image(s). They use an iterative optimisation procedure with back propagation to gradually change the values of the pixels in the generated image until the desired statistics are achieved. In contrast, in the texture networks approach a feed-forward generation network produces the image, which requires only a single evaluation of the network and does not incur in the cost of backpropagation. A separate generator network is trained for each texture or style and, once trained, it can synthesize an arbitrary number of images of arbitrary size in an efficient feed-forward manner. The loss function used in training the generator network is derived from Gatys et al. and compares image statistics extracted from a fixed pre-trained descriptor CNN. This is used to measure the mismatch between the prototype texture and the generated image. The texture loss function compares feature activations across all spatial locations. A similar content loss function compares feature activations at corresponding spatial locations, and therefore preserves spatial information. Analogously to Gatys et al. we use the texture loss alone when training a generator network for texture synthesis, and we use a weighted combination of the texture loss and the content loss when training a generator network for stylization. Textures  A texture generator network is trained to transform a noise vector sampled from a certain distribution into texture samples that match, according to the texture loss function, a certain prototype texture x0, a three colour channel tensor. We experimented with several architectures for the generator network g… we found that multi-scale architectures result in images with small texture loss and better perceptual quality while using fewer parameters and training faster. […] Each random noise tensor is first processed by a sequence of convolutional and non-linear activation layers, then upsampled by a factor of two, and finally concatenated as additional feature channels to the partially processed tensor from the scale below. (Click on image for larger view). Each convolutional block contains three convolutional layers containing respectively 3×3, 3×3, and 1×1 filters applied using circular convolution to remove boundary effects. Each convolutional layer is followed by a ReLU activation layer. When learning using stochastic gradient descent each iteration draws a mini-batch of noise vectors, performs forward evaluation of the generator network to obtain the corresponding images, and computes the loss vs x0. … After that, the gradient of the texture loss with respect to the generator network parameters θ is computed using backpropagation, and the gradient is used to update the parameters. Styling  For stylized image generator networks the network is modified to take as input in addition to the noise vector z , the image y to which the noise should be applied. The generator network is then trained to output an image x that is close in content to y and in texture/style to a reference texture x0. The architecture is the same as that used for texture synthesis, _with the important difference that the noise tensors at the K scales are concatenated (as additional feature channels) with downsampled versions of the input image y. The learning objective is to minimize the combination of the content and texture loss. In practice, we found that learning is surprisingly resilient to overfitting and that it suffices to approximate the distribution on natural images with a very small pool of images (e.g 16). Broader applicability  The success of this approach highlights the suitability of feed-forward networks for complex data generation and for solving complex tasks in general. The key to this success is the use of complex loss functions that involve different feed-forward architectures serving as “experts” assessing the performance of the feed-forward generator.", "pdf_url": "https://arxiv.org/pdf/1603.03417.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images.json"}
{"id": "7305163", "bin": "1200_1300", "summary_sentences": ["Fine-grained, secure and efficient data provenance on blockchain systems Ruan et al., VLDB’19  We haven’t covered a blockchain paper on The Morning Paper  for a while, and today’s choice won the best paper award at VLDB’19.", "The goal here is to enable smart contracts to be written in which the contract logic depends on the history, or provenance of its inputs.", "For example, a contract that sends a reward amount of tokens to a user based on that user’s average balance per day over some period.", "That’s hard to do in today’s blockchain systems for two reasons:  Provenance can only be determined by querying and replaying all on-chain transactions, which is inefficient and an offline activity.", "As a consequence the computation of provenance and issuing a subsequent transaction are decoupled and hence there are no serializability guarantees.", "“In blockchains with native currencies, serializabiliy violations can be exploited for Transaction-Ordering attacks that cause substantial financial loss to the users.”  In other words, smart contracts cannot access historical blockchain states in a tamper-evident manner.", "In designing a blockchain-friendly provenance mechanism, three aspects unique to the blockchain environment differentiate the problem from that of traditional database provenance:  There are no higher-order data operators (e.g. join) whose semantics capture provenance in the form of input-output dependencies.", "Instead we have to work directly in terms of smart contract read and write sets.", "Blockchains assume an adversarial environment, so the captured provenance must be made tamper-evident  Provenance queries must be fast to avoid imposing a large cost on miners during verification (see the Verifier’s Dilemma ).", "Let’s look at each of these in turn.", "Capturing provenance  In LineageChain, every contract method can be made provenance-friendly via a helper method (prov_helper).", "LineageChain invokes prov_helper immediately after every successful contract execution.", "If the smart chain developer doesn’t provide an implementation, then the default behaviour is to make all identifiers in the read set of the transaction be dependencies of every write in the write set.", "My biggest question here is what guarantees we can have on provenance when the developer does provide their own implementation.", "Since we’re assuming an adversarial environment, what stops the prov_helper method from making up whatever it likes?", "The paper is silent on this question.", "Presumably we can always check that a write dependency is not declared on a read we didn’t make (because we know the read set), and likewise we can check that we’re not claiming any writes we didn’t make.", "But outside of that, the whole point of the helper method seems to be to give the developer control over the provenance.", "For example, suppose the read set is  and the write set is  .", "In the true calculation the value of D depends on all of A, B, and C. But in the helper method the developer could return a provenance that says D only depends on B.", "Or on nothing at all.", "What does this mean for the trust we can place in the provenance chain??", "All the fancy provenance tracking mechanisms that follow mean nothing if the provenance inputs themselves can’t be trusted.", "Maybe subsetting the provenance history like this is always safe?", "I’d like to see that argument made in the paper if so, rather than being left to reason this out for myself.", "Inspecting the public code of all smart contracts to make sure developers aren’t cheating doesn’t seem very satisfactory.", "Anyway, putting that question aside, once we’ve got provenance data, smart contracts can access it via three additional smart contract APIs that are exposed: Hist for finding the value of a stateID at the start of a given block, Backward for tracing provenance backwards in the chain, and Forward for tracing provenance forwards in the chain.", "Here’s an example that marks an address as blacklisted if one of its last 5 transactions is with a blacklisted address.", "Securely storing provenance information  To securely store provenance information, LineageChain turns the original Merkle tree backing a blockchain into a Merkle DAG.", "In the above figure,  is a unique identifier of a smart contract account whose state is to be tracked,  is a version number (block number), and  is the state of the contract at version  .", "You can see in the figure above that the Merkle tree has additional branches feeding into the root,  one for each tracked smart contract, with the smart contract state hashes aggregating along transaction dependency edges.", "Our new Merkle DAG can be easily integrated to existing blockchain index structures….", "Since the [state entry hash] is protected by the Merkle index for tamper evidence, so is the state history.", "In other words, we add integrity protection for provenance without any extra cost to the index structure.", "Forward dependencies for a version  are added when the state is next updated to version  .", "At this point we know it is safe to do so as no other forward dependencies can now emerge from the old version.", "Making provenance queries fast  At this point we have the structures and APIs needed to make version queries, but now we need to make them fast.", "This is especially important since version queries will have to be executed by miners during verification, and we don’t want an adversary to deliberately submit expensive version queries (e.g. starting from a very early block id).", "The solution here is to build a skip-list based index on top of the Merkle DAG.", "Compared to a normal skip list we have two additional properties: (i) we know that the blockchain is append-only, and (ii) the index structure is uniquely determined by the values of the appended items.", "This gives us a Deterministic Append-Only Skip List (DASL).", "DASL queries are executed in just the same way as for a regular skip list,  appends are made using the algorithm below:  The node structure for the DASL nodes is stored in the Merkle DAG state entries.", "For permissioned blockchains we’re all set.", "For permissionless blockchains we also need to figure out who pays for the extra storage overheads…  As DASL consumes resources, its costs must be explicitly accounted for in permissionless blockchains.", "More specifically, during deployment, the contract owner specifies which states require DASL support.", "Alternatively, DASL support can be automatically inferred from the contract’s source code.", "The deployment fee should reflect this extra storage cost for DASL…  Implementation and evaluation  The implementation is part of the Hyperledger++ project, and the evaluation uses Blockbench .", "Full details can be found in sections 6 and 7 of the paper, but the short version is this:  We implemented LineageChain on top of Hyperledger and benchmarked it against several baselines.", "The results show the benefits of LineageChain in supporting rich, provenance-dependent applications.", "They demonstrate that provenance queries are efficient, and the system incurs small storage overhead."], "summary_text": "Fine-grained, secure and efficient data provenance on blockchain systems Ruan et al., VLDB’19  We haven’t covered a blockchain paper on The Morning Paper  for a while, and today’s choice won the best paper award at VLDB’19. The goal here is to enable smart contracts to be written in which the contract logic depends on the history, or provenance of its inputs. For example, a contract that sends a reward amount of tokens to a user based on that user’s average balance per day over some period. That’s hard to do in today’s blockchain systems for two reasons:  Provenance can only be determined by querying and replaying all on-chain transactions, which is inefficient and an offline activity. As a consequence the computation of provenance and issuing a subsequent transaction are decoupled and hence there are no serializability guarantees. “In blockchains with native currencies, serializabiliy violations can be exploited for Transaction-Ordering attacks that cause substantial financial loss to the users.”  In other words, smart contracts cannot access historical blockchain states in a tamper-evident manner. In designing a blockchain-friendly provenance mechanism, three aspects unique to the blockchain environment differentiate the problem from that of traditional database provenance:  There are no higher-order data operators (e.g. join) whose semantics capture provenance in the form of input-output dependencies. Instead we have to work directly in terms of smart contract read and write sets. Blockchains assume an adversarial environment, so the captured provenance must be made tamper-evident  Provenance queries must be fast to avoid imposing a large cost on miners during verification (see the Verifier’s Dilemma ). Let’s look at each of these in turn. Capturing provenance  In LineageChain, every contract method can be made provenance-friendly via a helper method (prov_helper). LineageChain invokes prov_helper immediately after every successful contract execution. If the smart chain developer doesn’t provide an implementation, then the default behaviour is to make all identifiers in the read set of the transaction be dependencies of every write in the write set. My biggest question here is what guarantees we can have on provenance when the developer does provide their own implementation. Since we’re assuming an adversarial environment, what stops the prov_helper method from making up whatever it likes? The paper is silent on this question. Presumably we can always check that a write dependency is not declared on a read we didn’t make (because we know the read set), and likewise we can check that we’re not claiming any writes we didn’t make. But outside of that, the whole point of the helper method seems to be to give the developer control over the provenance. For example, suppose the read set is  and the write set is  . In the true calculation the value of D depends on all of A, B, and C. But in the helper method the developer could return a provenance that says D only depends on B. Or on nothing at all. What does this mean for the trust we can place in the provenance chain?? All the fancy provenance tracking mechanisms that follow mean nothing if the provenance inputs themselves can’t be trusted. Maybe subsetting the provenance history like this is always safe? I’d like to see that argument made in the paper if so, rather than being left to reason this out for myself. Inspecting the public code of all smart contracts to make sure developers aren’t cheating doesn’t seem very satisfactory. Anyway, putting that question aside, once we’ve got provenance data, smart contracts can access it via three additional smart contract APIs that are exposed: Hist for finding the value of a stateID at the start of a given block, Backward for tracing provenance backwards in the chain, and Forward for tracing provenance forwards in the chain. Here’s an example that marks an address as blacklisted if one of its last 5 transactions is with a blacklisted address. Securely storing provenance information  To securely store provenance information, LineageChain turns the original Merkle tree backing a blockchain into a Merkle DAG. In the above figure,  is a unique identifier of a smart contract account whose state is to be tracked,  is a version number (block number), and  is the state of the contract at version  . You can see in the figure above that the Merkle tree has additional branches feeding into the root,  one for each tracked smart contract, with the smart contract state hashes aggregating along transaction dependency edges. Our new Merkle DAG can be easily integrated to existing blockchain index structures…. Since the [state entry hash] is protected by the Merkle index for tamper evidence, so is the state history. In other words, we add integrity protection for provenance without any extra cost to the index structure. Forward dependencies for a version  are added when the state is next updated to version  . At this point we know it is safe to do so as no other forward dependencies can now emerge from the old version. Making provenance queries fast  At this point we have the structures and APIs needed to make version queries, but now we need to make them fast. This is especially important since version queries will have to be executed by miners during verification, and we don’t want an adversary to deliberately submit expensive version queries (e.g. starting from a very early block id). The solution here is to build a skip-list based index on top of the Merkle DAG. Compared to a normal skip list we have two additional properties: (i) we know that the blockchain is append-only, and (ii) the index structure is uniquely determined by the values of the appended items. This gives us a Deterministic Append-Only Skip List (DASL). DASL queries are executed in just the same way as for a regular skip list,  appends are made using the algorithm below:  The node structure for the DASL nodes is stored in the Merkle DAG state entries. For permissioned blockchains we’re all set. For permissionless blockchains we also need to figure out who pays for the extra storage overheads…  As DASL consumes resources, its costs must be explicitly accounted for in permissionless blockchains. More specifically, during deployment, the contract owner specifies which states require DASL support. Alternatively, DASL support can be automatically inferred from the contract’s source code. The deployment fee should reflect this extra storage cost for DASL…  Implementation and evaluation  The implementation is part of the Hyperledger++ project, and the evaluation uses Blockbench . Full details can be found in sections 6 and 7 of the paper, but the short version is this:  We implemented LineageChain on top of Hyperledger and benchmarked it against several baselines. The results show the benefits of LineageChain in supporting rich, provenance-dependent applications. They demonstrate that provenance queries are efficient, and the system incurs small storage overhead.", "pdf_url": "http://www.vldb.org/pvldb/vol12/p975-ruan.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/blockchain-provenance.json"}
{"id": "96308075", "bin": "1200_1300", "summary_sentences": ["Early detection of configuration errors to reduce failure damage Xu et al, OSDI ’16  Here’s one of those wonderful papers that you can read in the morning, and be taking advantage of the results the same afternoon!", "Remember the ‘ Simple testing can prevent most critical failures ‘ paper from OSDI’14 that we looked at last month?", "In that paper we learned that trivial mistakes in error handling, which are easy to test for, accounted for a vast majority of catastrophic production incidents.", "Well, as soon as you’ve got your error / exception handlers sorted out, you might want to read today’s paper to discover another class of easy-to-test for bugs that are also disproportionately responsible for nasty production failures.", "Facebook’s ‘ Holistic configuration management ‘ paper stresses the importance of version control and testing for configuration, as configuration errors are a major source of site errors.", "Xu et al. study configuration parameters in the wild, focusing especially on those associated with reliability, availability, and serviceability (RAS) features.", "What they find is that very often configuration values are not tested as part of system initialization.", "The program runs along happily until reaching a point (say for example, it needs to failover) where it needs to read some configuration for the first time, and then it blows up – typically when you most need it.", "So here’s the short takeaway test all of your configuration settings as part of system initialization and fail-fast if there’s a problem.", "Do that, and you’ll cut out another big source of major production errors.", "The paper itself is in two parts: the first part (where I’ll focus most of my attention in this short write-up) is an analysis of these latent configuration errors in real code bases; the second part introduces a tool called PCheck, which if your program is written in C or Java can even find latent configuration usage and automatically write tests for you!", "Latent configuration errors can result in severe failures, as they are often associated with configurations used to control critical situations such as fail-over, error handling, backup, load balancing, mirroring, etc… Their detection or exposure is often too late to limit the failure damage.", "In a study of real-world configuration issues in the the products of COMP-A, “major storage company in the US,” with footnote “we are required to keep the company and its products anonymous,” it turns out that 75% of all high severity  configuration-related errors are caused by latent configuration errors.", "It may well be that the authors are required to keep COMP-A anonymous, but I couldn’t help noticing the author affiliations printed in big type on the front page.", "A more than fair chance that company is NetApp I would say!", "The authors also studied a number of real-world open-source systems (see table below), and inspected usage of all of their RAS-related configuration parameters.", "They looked at how many of those parameters were explicity checked vs simply being used when first required, yielding the results below:  Many of the studied RAS parameters do not have any special code for checking the correctness of their settings.", "Instead, the correctness is verified (implicitly) when the parameters’ values are actually used in operations such as a file open call.", "Here’s an example of a real-world latent configuration error in MapReduce:  And here are some other bugs found during the study, in the most recent versions of the software under inspection in (a) HDFS:  and (b), Apache httpd:  So we know that many configuration parameters aren’t checked before usage.", "It’s also the case that many of these parameters aren’t used during system startup (and so are not verified even implicitily):  Many (12-38.6%) of the studied RAS configuration parameters are not used at all during the system’s initialization phase.", "Put these two finding together, and what you have is a collection of ticking time bombs!", "Remember that since these are configuration settings they may be changed on deployment – i.e. these are not bugs that unit testing can catch.", "4.7-38.6% of the studied RAS parameters do not have any early checks and and thereby subject to latent configuration errors which can cause severe impact on the system’s dependability.", "Here’s the summary of how many of these ticking time bombs can exist in the systems studied:  The threats are prevalent: Latent configuration errors can reside in 10+% of the RAS parameters in five out of six systems.", "As all theses latent configuration erros are discovered in the latest versions, any of them could appear in a real deployment and would impair the system’s dependability in a latent fashion.", "The authors wrote a tool called PCheck which uses static code analysis to find instructions that load configuration parameters into program variables, looks for all instructions that use the parameter value, figures out the execution context of those instructions and composes checkers that can be run at initialization to verify the configurarion is well-formed in the target environment.", "I feel bad skipping over all of the details of the authors hard work here, but I’m going to refer you to the paper for full details if you’re interested.", "With the PCheck tests in place, the authors harvested 830 configuration files for the studied systems (from mailing lists and technical forums) and validated them.", "With the checks in place, 70+% of latent configuration errors were detected.", "PCheck reports 282 true configuration errors (from 830 configs!)", "and three false alarms.", "Many (37.5-87.8%) of the reported configuration erros can only be detected by considering the system’s native execution environment.", "These configuration settings are valid in terms of format and syntax (in fact, they are likely to be correct in the original hosts).", "However, they are erroneous when used on the current system because the values violate environment constraints such as undefined environment variables, non-existing file-paths, unreachable IP addresses etc..  (Which leaves me a little unsure as to what the authors count as a ‘true configuration error’  – a configuration file which may have been perfectly valid on the system from which it was cut-and-pasted onto a mailing list may of course give problems in another context, but this doesn’t imply it was truly a configuration error on the original system).", "Nevertheless, I think the overall message of this paper is clear: Ladies and Gentlemen, please eagerly check all of your configuration variables on system startup.", "This paper advocates early detection of configuration errors to minimize failure davage, especially in cloud and data-center systems.", "Despite all the efforts of validation, review, and testing, configuration errros (even those obvious errors) still cause many high-impact incidents of today’s Internet and cloud systems."], "summary_text": "Early detection of configuration errors to reduce failure damage Xu et al, OSDI ’16  Here’s one of those wonderful papers that you can read in the morning, and be taking advantage of the results the same afternoon! Remember the ‘ Simple testing can prevent most critical failures ‘ paper from OSDI’14 that we looked at last month? In that paper we learned that trivial mistakes in error handling, which are easy to test for, accounted for a vast majority of catastrophic production incidents. Well, as soon as you’ve got your error / exception handlers sorted out, you might want to read today’s paper to discover another class of easy-to-test for bugs that are also disproportionately responsible for nasty production failures. Facebook’s ‘ Holistic configuration management ‘ paper stresses the importance of version control and testing for configuration, as configuration errors are a major source of site errors. Xu et al. study configuration parameters in the wild, focusing especially on those associated with reliability, availability, and serviceability (RAS) features. What they find is that very often configuration values are not tested as part of system initialization. The program runs along happily until reaching a point (say for example, it needs to failover) where it needs to read some configuration for the first time, and then it blows up – typically when you most need it. So here’s the short takeaway test all of your configuration settings as part of system initialization and fail-fast if there’s a problem. Do that, and you’ll cut out another big source of major production errors. The paper itself is in two parts: the first part (where I’ll focus most of my attention in this short write-up) is an analysis of these latent configuration errors in real code bases; the second part introduces a tool called PCheck, which if your program is written in C or Java can even find latent configuration usage and automatically write tests for you! Latent configuration errors can result in severe failures, as they are often associated with configurations used to control critical situations such as fail-over, error handling, backup, load balancing, mirroring, etc… Their detection or exposure is often too late to limit the failure damage. In a study of real-world configuration issues in the the products of COMP-A, “major storage company in the US,” with footnote “we are required to keep the company and its products anonymous,” it turns out that 75% of all high severity  configuration-related errors are caused by latent configuration errors. It may well be that the authors are required to keep COMP-A anonymous, but I couldn’t help noticing the author affiliations printed in big type on the front page. A more than fair chance that company is NetApp I would say! The authors also studied a number of real-world open-source systems (see table below), and inspected usage of all of their RAS-related configuration parameters. They looked at how many of those parameters were explicity checked vs simply being used when first required, yielding the results below:  Many of the studied RAS parameters do not have any special code for checking the correctness of their settings. Instead, the correctness is verified (implicitly) when the parameters’ values are actually used in operations such as a file open call. Here’s an example of a real-world latent configuration error in MapReduce:  And here are some other bugs found during the study, in the most recent versions of the software under inspection in (a) HDFS:  and (b), Apache httpd:  So we know that many configuration parameters aren’t checked before usage. It’s also the case that many of these parameters aren’t used during system startup (and so are not verified even implicitily):  Many (12-38.6%) of the studied RAS configuration parameters are not used at all during the system’s initialization phase. Put these two finding together, and what you have is a collection of ticking time bombs! Remember that since these are configuration settings they may be changed on deployment – i.e. these are not bugs that unit testing can catch. 4.7-38.6% of the studied RAS parameters do not have any early checks and and thereby subject to latent configuration errors which can cause severe impact on the system’s dependability. Here’s the summary of how many of these ticking time bombs can exist in the systems studied:  The threats are prevalent: Latent configuration errors can reside in 10+% of the RAS parameters in five out of six systems. As all theses latent configuration erros are discovered in the latest versions, any of them could appear in a real deployment and would impair the system’s dependability in a latent fashion. The authors wrote a tool called PCheck which uses static code analysis to find instructions that load configuration parameters into program variables, looks for all instructions that use the parameter value, figures out the execution context of those instructions and composes checkers that can be run at initialization to verify the configurarion is well-formed in the target environment. I feel bad skipping over all of the details of the authors hard work here, but I’m going to refer you to the paper for full details if you’re interested. With the PCheck tests in place, the authors harvested 830 configuration files for the studied systems (from mailing lists and technical forums) and validated them. With the checks in place, 70+% of latent configuration errors were detected. PCheck reports 282 true configuration errors (from 830 configs!) and three false alarms. Many (37.5-87.8%) of the reported configuration erros can only be detected by considering the system’s native execution environment. These configuration settings are valid in terms of format and syntax (in fact, they are likely to be correct in the original hosts). However, they are erroneous when used on the current system because the values violate environment constraints such as undefined environment variables, non-existing file-paths, unreachable IP addresses etc..  (Which leaves me a little unsure as to what the authors count as a ‘true configuration error’  – a configuration file which may have been perfectly valid on the system from which it was cut-and-pasted onto a mailing list may of course give problems in another context, but this doesn’t imply it was truly a configuration error on the original system). Nevertheless, I think the overall message of this paper is clear: Ladies and Gentlemen, please eagerly check all of your configuration variables on system startup. This paper advocates early detection of configuration errors to minimize failure davage, especially in cloud and data-center systems. Despite all the efforts of validation, review, and testing, configuration errros (even those obvious errors) still cause many high-impact incidents of today’s Internet and cloud systems.", "pdf_url": "https://www.usenix.org/system/files/conference/osdi16/osdi16-xu.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/early-detection-of-configuration-errors-to-reduce-failure-damage.json"}
{"id": "68399070", "bin": "1200_1300", "summary_sentences": ["What  The original R-CNN had three major disadvantages:  Two-staged training pipeline: Instead of only training a CNN, one had to train first a CNN and then multiple SVMs.", "Expensive training: Training was slow and required lots of disk space (feature vectors needed to be written to disk for all region proposals (2000 per image) before training the SVMs).", "Slow test: Each region proposal had to be handled independently.", "Fast R-CNN ist an improved version of R-CNN and tackles the mentioned problems.", "It no longer uses SVMs, only CNNs (single-stage).", "It does one single feature extraction per image instead of per region, making it much faster (9x faster at training, 213x faster at test).", "It is more accurate than R-CNN.", "How  The basic architecture, training and testing methods are mostly copied from R-CNN.", "For each image at test time they do:  They generate region proposals via selective search.", "They feed the image once through the convolutional layers of a pre-trained network, usually VGG16.", "For each region proposal they extract the respective region from the features generated by the network.", "The regions can have different sizes, but the following steps need fixed size vectors.", "So each region is downscaled via max-pooling so that it has a size of 7x7 (so apparently they ignore regions of sizes below 7x7...?).", "This is called Region of Interest Pooling (RoI-Pooling).", "During the backwards pass, partial derivatives can be transferred to the maximum value (as usually in max pooling).", "That derivative values are summed up over different regions (in the same image).", "They reshape the 7x7 regions to vectors of length F*7*7, where F was the number of filters in the last convolutional layer.", "They feed these vectors through another network which predicts:  The class of the region (including background class).", "Top left x-coordinate, top left y-coordinate, log height and log width of the bounding box (i.e. it fine-tunes the region proposal's bounding box).", "These values are predicted once for every class (so K*4 values).", "Architecture as image:  Sampling for training  Efficiency  If batch size is B it is inefficient to sample regions proposals from B images as each image will require a full forward pass through the base network (e.g. VGG16).", "It is much more efficient to use few images to share most of the computation between region proposals.", "They use two images per batch (each 64 region proposals) during training.", "This technique introduces correlations between examples in batches, but they did not observe any problems from that.", "They call this technique \"hierarchical sampling\" (first images, then region proposals).", "IoUs  Positive examples for specific classes during training are region proposals that have an IoU with ground truth bounding boxes of >=0.5.", "Examples for background region proposals during training have IoUs with any ground truth box in the interval (0.1, 0.5].", "Not picking IoUs below 0.1 is similar to hard negative mining.", "They use 25% positive examples, 75% negative/background examples per batch.", "They apply horizontal flipping as data augmentation, nothing else.", "Outputs  For their class predictions the use a simple softmax with negative log likelihood.", "For their bounding box regression they use a smooth L1 loss (similar to mean absolute error, but switches to mean squared error for very low values).", "Smooth L1 loss is less sensitive to outliers and less likely to suffer from exploding gradients.", "The smooth L1 loss is only active for positive examples (not background examples).", "(Not active means that it is zero.)", "Training schedule  The use SGD.", "They train 30k batches with learning rate 0.001, then 0.0001 for another 10k batches.", "(On Pascal VOC, they use more batches on larger datasets.)", "They use twice the learning rate for the biases.", "They use momentum of 0.9.", "They use parameter decay of 0.0005.", "Truncated SVD  The final network for class prediction and bounding box regression has to be applied to every region proposal.", "It contains one large fully connected hidden layer and one fully connected output layer (K+1 classes plus K*4 regression values).", "For 2000 proposals that becomes slow.", "So they compress the layers after training to less weights via truncated SVD.", "A weights matrix is approximated via  U (u x t) are the first t left-singular vectors of W.  Sigma is a t x t diagonal matrix of the top t singular values.", "V (v x t) are the first t right-singular vectors of W.  W is then replaced by two layers: One contains Sigma V^T as weights (no biases), the other contains U as weights (with original biases).", "Parameter count goes down to t(u+v) from uv.", "Results  They try three base models:  AlexNet (Small, S)  VGG-CNN-M-1024 (Medium, M)  VGG16 (Large, L)  On VGG16 and Pascal VOC 2007, compared to original R-CNN:  Training time down to 9.5h from 84h (8.8x faster).", "Test rate with SVD (1024 singular values) improves from 47 seconds per image to 0.22 seconds per image (213x faster).", "Test rate without SVD improves similarly to 0.32 seconds per image.", "mAP improves from 66.0% to 66.6% (66.9% without SVD).", "Per class accuracy results:  Fast_R-CNN__pvoc2012.jpg  Fixing the weights of VGG16's convolutional layers and only fine-tuning the fully connected layers (those are applied to each region proposal), decreases the accuracy to 61.4%.", "This decrease in accuracy is most significant for the later convolutional layers, but marginal for the first layers.", "Therefor they only train the convolutional layers starting with conv3_1 (9 out of 13 layers), which speeds up training.", "Multi-task training  Training models on classification and bounding box regression instead of only on classification improves the mAP (from 62.6% to 66.9%).", "Doing this in one hierarchy instead of two seperate models (one for classification, one for bounding box regression) increases mAP by roughly 2-3 percentage points.", "They did not find a significant benefit of training the model on multiple scales (e.g. same image sometimes at 400x400, sometimes at 600x600, sometimes at 800x800 etc.).", "Note that their raw CNN (everything before RoI-Pooling) is fully convolutional, so they can feed the images at any scale through the network.", "Increasing the amount of training data seemed to improve mAP a bit, but not as much as one might hope for.", "Using a softmax loss instead of an SVM seemed to marginally increase mAP (0-1 percentage points).", "Using more region proposals from selective search does not simply increase mAP.", "Instead it can lead to higher recall, but lower precision.", "Using densely sampled region proposals (as in sliding window) significantly reduces mAP (from 59.2% to 52.9%).", "If SVMs instead of softmaxes are used, the results are even worse (49.3%)."], "summary_text": "What  The original R-CNN had three major disadvantages:  Two-staged training pipeline: Instead of only training a CNN, one had to train first a CNN and then multiple SVMs. Expensive training: Training was slow and required lots of disk space (feature vectors needed to be written to disk for all region proposals (2000 per image) before training the SVMs). Slow test: Each region proposal had to be handled independently. Fast R-CNN ist an improved version of R-CNN and tackles the mentioned problems. It no longer uses SVMs, only CNNs (single-stage). It does one single feature extraction per image instead of per region, making it much faster (9x faster at training, 213x faster at test). It is more accurate than R-CNN. How  The basic architecture, training and testing methods are mostly copied from R-CNN. For each image at test time they do:  They generate region proposals via selective search. They feed the image once through the convolutional layers of a pre-trained network, usually VGG16. For each region proposal they extract the respective region from the features generated by the network. The regions can have different sizes, but the following steps need fixed size vectors. So each region is downscaled via max-pooling so that it has a size of 7x7 (so apparently they ignore regions of sizes below 7x7...?). This is called Region of Interest Pooling (RoI-Pooling). During the backwards pass, partial derivatives can be transferred to the maximum value (as usually in max pooling). That derivative values are summed up over different regions (in the same image). They reshape the 7x7 regions to vectors of length F*7*7, where F was the number of filters in the last convolutional layer. They feed these vectors through another network which predicts:  The class of the region (including background class). Top left x-coordinate, top left y-coordinate, log height and log width of the bounding box (i.e. it fine-tunes the region proposal's bounding box). These values are predicted once for every class (so K*4 values). Architecture as image:  Sampling for training  Efficiency  If batch size is B it is inefficient to sample regions proposals from B images as each image will require a full forward pass through the base network (e.g. VGG16). It is much more efficient to use few images to share most of the computation between region proposals. They use two images per batch (each 64 region proposals) during training. This technique introduces correlations between examples in batches, but they did not observe any problems from that. They call this technique \"hierarchical sampling\" (first images, then region proposals). IoUs  Positive examples for specific classes during training are region proposals that have an IoU with ground truth bounding boxes of >=0.5. Examples for background region proposals during training have IoUs with any ground truth box in the interval (0.1, 0.5]. Not picking IoUs below 0.1 is similar to hard negative mining. They use 25% positive examples, 75% negative/background examples per batch. They apply horizontal flipping as data augmentation, nothing else. Outputs  For their class predictions the use a simple softmax with negative log likelihood. For their bounding box regression they use a smooth L1 loss (similar to mean absolute error, but switches to mean squared error for very low values). Smooth L1 loss is less sensitive to outliers and less likely to suffer from exploding gradients. The smooth L1 loss is only active for positive examples (not background examples). (Not active means that it is zero.) Training schedule  The use SGD. They train 30k batches with learning rate 0.001, then 0.0001 for another 10k batches. (On Pascal VOC, they use more batches on larger datasets.) They use twice the learning rate for the biases. They use momentum of 0.9. They use parameter decay of 0.0005. Truncated SVD  The final network for class prediction and bounding box regression has to be applied to every region proposal. It contains one large fully connected hidden layer and one fully connected output layer (K+1 classes plus K*4 regression values). For 2000 proposals that becomes slow. So they compress the layers after training to less weights via truncated SVD. A weights matrix is approximated via  U (u x t) are the first t left-singular vectors of W.  Sigma is a t x t diagonal matrix of the top t singular values. V (v x t) are the first t right-singular vectors of W.  W is then replaced by two layers: One contains Sigma V^T as weights (no biases), the other contains U as weights (with original biases). Parameter count goes down to t(u+v) from uv. Results  They try three base models:  AlexNet (Small, S)  VGG-CNN-M-1024 (Medium, M)  VGG16 (Large, L)  On VGG16 and Pascal VOC 2007, compared to original R-CNN:  Training time down to 9.5h from 84h (8.8x faster). Test rate with SVD (1024 singular values) improves from 47 seconds per image to 0.22 seconds per image (213x faster). Test rate without SVD improves similarly to 0.32 seconds per image. mAP improves from 66.0% to 66.6% (66.9% without SVD). Per class accuracy results:  Fast_R-CNN__pvoc2012.jpg  Fixing the weights of VGG16's convolutional layers and only fine-tuning the fully connected layers (those are applied to each region proposal), decreases the accuracy to 61.4%. This decrease in accuracy is most significant for the later convolutional layers, but marginal for the first layers. Therefor they only train the convolutional layers starting with conv3_1 (9 out of 13 layers), which speeds up training. Multi-task training  Training models on classification and bounding box regression instead of only on classification improves the mAP (from 62.6% to 66.9%). Doing this in one hierarchy instead of two seperate models (one for classification, one for bounding box regression) increases mAP by roughly 2-3 percentage points. They did not find a significant benefit of training the model on multiple scales (e.g. same image sometimes at 400x400, sometimes at 600x600, sometimes at 800x800 etc.). Note that their raw CNN (everything before RoI-Pooling) is fully convolutional, so they can feed the images at any scale through the network. Increasing the amount of training data seemed to improve mAP a bit, but not as much as one might hope for. Using a softmax loss instead of an SVM seemed to marginally increase mAP (0-1 percentage points). Using more region proposals from selective search does not simply increase mAP. Instead it can lead to higher recall, but lower precision. Using densely sampled region proposals (as in sliding window) significantly reduces mAP (from 59.2% to 52.9%). If SVMs instead of softmaxes are used, the results are even worse (49.3%).", "pdf_url": "https://arxiv.org/pdf/1504.08083", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/fast_r-cnn.json"}
{"id": "30995172", "bin": "1200_1300", "summary_sentences": ["FAWN: A Fast Array of Wimpy Nodes – Andersen et al. 2009  A few days ago we looked at FaRM (Fast Remote Memory), which used RDMA to match network speed with the speed of CPUs and got some very impressive results in terms of queries & transactions per second.", "But maybe there’s another way of addressing the imbalance – use less powerful CPUs!", "Which perhaps sounds a little odd at first, but starts to make sense when you consider a less-often quoted metric: queries / transactions per Joule.", "High performance DRAM-based clusters, storing terabytes or petabytes of data, are both expensive and consume a surprising amount of power—two 2 GB DIMMs consume as much energy as a 1 TB disk.", "The power draw of these clusters is becoming an increasing fraction of their cost—up to 50% of the three-year total cost of owning a computer.", "The density of the datacenters that house them is in turn limited by their ability to supply and cool 10–20 kW of power per rack and up to 10–20 MW per datacenter.", "Future datacenters may require as much as 200 MW, and datacenters are being constructed today with dedicated electrical substations to feed them.", "So queries per Joule (qpj) is a significant determinant of the overall cost of your solution.", "These challenges necessitate the question: Can we build a cost-effective cluster for data-intensive workloads that uses less than a tenth of the power required by a conventional architecture, but that still meets the same capacity, availability, throughput, and latency requirements?", "Andersen et al. set out to do just this with FAWN by building a fast array of “wimpy” (low-power AMD) nodes with SSDs, and a key value store on top designed to work well on this hardware platform.", "A  21-node FAWN cluster built with 500MHz CPUs (that were old even in 2009 when this paper was written) achieved 364 qpj, two orders of magnitude better than traditional disk-based clusters.", "Each node could support up to 1300 256 byte queries per second, enough to exploit nearly all the raw capacity of its attached SSDs.", "Total power consumption under 5W per node, compared to an Intel processor consuming about 83-90W under load.", "The workload under study is a key-value store.", "For other big data workloads, it turns out that the network and disk may not be the bottleneck, it could still be the CPU !", "This suggests yet another way of addressing the imbalance – do lots of expensive serialization and deserialization!", "That wouldn’t be my top recommendation ;).", "In their analysis, the authors consider dataset size and desired query rate, and provide recommendations as to the most cost-effective hardware+software platform to meet the targets.", "The result is shown in Figure 16 in the paper (reproduced below) – which is striking for showing that there is only a narrow band in which traditional approaches actually make sense for the small random access workloads (e.g. K-V store) studied!", "Let’s take a look at FAWN’s hardware choices and rationale, and then dive into a few details of the K-V store built on top…  FAWN Hardware  FAWN couples low-power, efficient embedded CPUs with flash storage to provide efficient, fast, and cost-effective access to large, random-access data.", "Flash is significantly faster than disk, much cheaper than the equivalent amount of DRAM, and consumes less power than either.", "CPU power consumption grows super-linearly with speed, therefore you can get a better price-performance point by moving back down the curve:  A FAWN cluster’s slower CPUs dedicate more transistors to basic operations.", "These CPUs execute significantly more instructions per Joule than their faster counterparts: multi-GHz superscalar quad-core processors can execute approximately 100 million instructions per Joule, assuming all cores are active and avoid stalls or mispredictions.", "Lower-frequency in-order CPUs, in contrast, can provide over 1 billion instructions per Joule—an order of magnitude more efficient while still running at 1/3rd the frequency.", "Flash devices support fast random reads and efficient I/O,  but with slower random writes.", "Flash devices consume less than one Watt even under heavy load, whereas mechanical disks can consume over 10 W at load.", "Flash is over two orders of magnitude more efficient than mechanical disks in terms of queries/Joule.", "The evaluation hardware consisted of single-core 500 MHz AMD Geode LX processors, with 256 MB DDR SDRAM operating at 400 MHz, and 100 Mbit/s Ethernet.", "Each node contained one 4 GB Sandisk Extreme IV CompactFlash device.", "FAWN K-V Store  The FAWN data store is a log-structured key-value store.", "FAWN-DS is designed specifically to perform well on flash storage and to operate within the constrained DRAM available on wimpy nodes: all writes to the datastore are sequential, and reads require a single random access.", "To provide this property, FAWN-DS maintains an in-DRAM hash table (Hash Index) that maps keys to an offset in the append-only Data Log on flash….", "The key design choice in FAWN-KV is the use of a log- structured per-node datastore called FAWN-DS that provides high performance reads and writes using flash memory.", "This append-only data log provides the basis for replication and strong consistency using chain replication between nodes.", "FAWN backends divide up the key space using consistent hashing, with each physical node responsible for multiple key ranges.", "Chain replication is used between nodes.", "Individual nodes use an in-memory hash index to map keys to values stored in the data log.", "To save space only a fragment of the key (the index bits) is kept in the index.", "This allows for a small probability that the key retrieved is not actually the one being sought (collision).", "In this situation hash-chaining is used continue searching the hash table.", "With the 15-bit key fragment, only 1 in 32,768 retrievals from the flash will be incorrect and require fetching an additional record.", "A smaller tier (about 1:80) of front-end nodes sits in front of the data storing back ends.", "Each front-end node manages the VID membership list and queries for a large contiguous chunk of the key space (in other words, the circular key space is divided into pie-wedges, each owned by a front-end).", "A front-end receiving queries for keys outside of its range forwards the queries to the appropriate front-end node.", "This design either requires clients to be roughly aware of the front-end mapping, or doubles the traffic that front-ends must handle, but it permits front ends to cache values without a cache consistency protocol.", "The front-end and back-end nodes implement a two-level caching hierarchy.", "Front-end nodes maintain a small high-speed query cache that reduces latency and helps managed hot spots.", "Back-ends implicitly cache recently accessed data in their file system buffer cache.", "About 1300 queries per second can be served from flash, but 85,000 queries per second from the buffer cache."], "summary_text": "FAWN: A Fast Array of Wimpy Nodes – Andersen et al. 2009  A few days ago we looked at FaRM (Fast Remote Memory), which used RDMA to match network speed with the speed of CPUs and got some very impressive results in terms of queries & transactions per second. But maybe there’s another way of addressing the imbalance – use less powerful CPUs! Which perhaps sounds a little odd at first, but starts to make sense when you consider a less-often quoted metric: queries / transactions per Joule. High performance DRAM-based clusters, storing terabytes or petabytes of data, are both expensive and consume a surprising amount of power—two 2 GB DIMMs consume as much energy as a 1 TB disk. The power draw of these clusters is becoming an increasing fraction of their cost—up to 50% of the three-year total cost of owning a computer. The density of the datacenters that house them is in turn limited by their ability to supply and cool 10–20 kW of power per rack and up to 10–20 MW per datacenter. Future datacenters may require as much as 200 MW, and datacenters are being constructed today with dedicated electrical substations to feed them. So queries per Joule (qpj) is a significant determinant of the overall cost of your solution. These challenges necessitate the question: Can we build a cost-effective cluster for data-intensive workloads that uses less than a tenth of the power required by a conventional architecture, but that still meets the same capacity, availability, throughput, and latency requirements? Andersen et al. set out to do just this with FAWN by building a fast array of “wimpy” (low-power AMD) nodes with SSDs, and a key value store on top designed to work well on this hardware platform. A  21-node FAWN cluster built with 500MHz CPUs (that were old even in 2009 when this paper was written) achieved 364 qpj, two orders of magnitude better than traditional disk-based clusters. Each node could support up to 1300 256 byte queries per second, enough to exploit nearly all the raw capacity of its attached SSDs. Total power consumption under 5W per node, compared to an Intel processor consuming about 83-90W under load. The workload under study is a key-value store. For other big data workloads, it turns out that the network and disk may not be the bottleneck, it could still be the CPU ! This suggests yet another way of addressing the imbalance – do lots of expensive serialization and deserialization! That wouldn’t be my top recommendation ;). In their analysis, the authors consider dataset size and desired query rate, and provide recommendations as to the most cost-effective hardware+software platform to meet the targets. The result is shown in Figure 16 in the paper (reproduced below) – which is striking for showing that there is only a narrow band in which traditional approaches actually make sense for the small random access workloads (e.g. K-V store) studied! Let’s take a look at FAWN’s hardware choices and rationale, and then dive into a few details of the K-V store built on top…  FAWN Hardware  FAWN couples low-power, efficient embedded CPUs with flash storage to provide efficient, fast, and cost-effective access to large, random-access data. Flash is significantly faster than disk, much cheaper than the equivalent amount of DRAM, and consumes less power than either. CPU power consumption grows super-linearly with speed, therefore you can get a better price-performance point by moving back down the curve:  A FAWN cluster’s slower CPUs dedicate more transistors to basic operations. These CPUs execute significantly more instructions per Joule than their faster counterparts: multi-GHz superscalar quad-core processors can execute approximately 100 million instructions per Joule, assuming all cores are active and avoid stalls or mispredictions. Lower-frequency in-order CPUs, in contrast, can provide over 1 billion instructions per Joule—an order of magnitude more efficient while still running at 1/3rd the frequency. Flash devices support fast random reads and efficient I/O,  but with slower random writes. Flash devices consume less than one Watt even under heavy load, whereas mechanical disks can consume over 10 W at load. Flash is over two orders of magnitude more efficient than mechanical disks in terms of queries/Joule. The evaluation hardware consisted of single-core 500 MHz AMD Geode LX processors, with 256 MB DDR SDRAM operating at 400 MHz, and 100 Mbit/s Ethernet. Each node contained one 4 GB Sandisk Extreme IV CompactFlash device. FAWN K-V Store  The FAWN data store is a log-structured key-value store. FAWN-DS is designed specifically to perform well on flash storage and to operate within the constrained DRAM available on wimpy nodes: all writes to the datastore are sequential, and reads require a single random access. To provide this property, FAWN-DS maintains an in-DRAM hash table (Hash Index) that maps keys to an offset in the append-only Data Log on flash…. The key design choice in FAWN-KV is the use of a log- structured per-node datastore called FAWN-DS that provides high performance reads and writes using flash memory. This append-only data log provides the basis for replication and strong consistency using chain replication between nodes. FAWN backends divide up the key space using consistent hashing, with each physical node responsible for multiple key ranges. Chain replication is used between nodes. Individual nodes use an in-memory hash index to map keys to values stored in the data log. To save space only a fragment of the key (the index bits) is kept in the index. This allows for a small probability that the key retrieved is not actually the one being sought (collision). In this situation hash-chaining is used continue searching the hash table. With the 15-bit key fragment, only 1 in 32,768 retrievals from the flash will be incorrect and require fetching an additional record. A smaller tier (about 1:80) of front-end nodes sits in front of the data storing back ends. Each front-end node manages the VID membership list and queries for a large contiguous chunk of the key space (in other words, the circular key space is divided into pie-wedges, each owned by a front-end). A front-end receiving queries for keys outside of its range forwards the queries to the appropriate front-end node. This design either requires clients to be roughly aware of the front-end mapping, or doubles the traffic that front-ends must handle, but it permits front ends to cache values without a cache consistency protocol. The front-end and back-end nodes implement a two-level caching hierarchy. Front-end nodes maintain a small high-speed query cache that reduces latency and helps managed hot spots. Back-ends implicitly cache recently accessed data in their file system buffer cache. About 1300 queries per second can be served from flash, but 85,000 queries per second from the buffer cache.", "pdf_url": "http://www.sigops.org/sosp/sosp09/papers/andersen-sosp09.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/fawn-a-fast-array-of-wimpy-nodes.json"}
{"id": "11779349", "bin": "1200_1300", "summary_sentences": ["This paper presents a novel RL exploration bonus based on an adaptation of count-based exploration for high-dimensional spaces.", "The main contribution is the derivation of the relationships between prediction gain (PG), a quantity called the pseudo-count, and the well-known information gain from the intrinsic RL literature.", "The overall presentation is clear and precise, especially when it comes to defining notation and explaining the core concepts.", "The authors use results from prior work on applying count-based exploration in complex domains, so reading this paper without having that background is a bit challenging (see the bibliography for recent papers by Bellemare and Veness on this topic).", "These results will help make previous count-based exploration methods, such as those devised around the idea of optimism in the face of uncertainty, viable for high-dimensional problems.", "Quick Definitions  This paper presents the following definitions and notation, along with theoretical results about their properties, in a rich manner.", "I will simply present definitions necessary for understanding the main contribution here; please see the paper for further details.", "$ \\mathcal{X} := $ finite or countable alphabet, with sub-sequences of length $n$ denoted by $x_{1:n} \\in \\mathcal{X}^n $  A sequential density model over $\\mathcal{X}$ is a mapping from finite sub-sequences to probalitiy distributions over $\\mathcal{X}$.", "Denote these distributions by $\\rho_{n}(x) := \\rho(x ; x_{1:n})$  The recoding probability of a symbol $x$ is denoted by $\\rho_{n}^{\\prime} := \\rho(x ; x_{1:n}x)$.", "That is, the probability assigned to $x$ by our sequential density model after observing a new occurence of $x$  $ \\hat{N}(x) := $ the pseudo-count function  $ \\hat{n} := $ the pseudo-count total  Main Result  The following equations summarize the main results presented in this paper.", "The authors relate the pseudo-count function and the pseudo-count total as follows.", "From (1), we can write:  By choosing an appropriate sequential density model, such as a graphical model.", "See the appendix of the paper for the description of the CTS model used by the authors in the experimentation; this model treats a 42 x 42 processed image as a factorizable observation, computing the probability of the image $x$ as the product of the probabilities of each (i, j) pixel.", "These pixel probabilities are computed via the neighbors of that pixel.", "See section 5.2 as well for a discussion on using directed graphical models as sequential density models.", "The prediction gain is defined as  For the information gain (IG) defined as the change in posterior w.r.t.", "the KL-divergence within a mixture model $\\xi$ defined over a class $\\mathcal{M}$ of sequential density models, we have:  The reward bonus proposed in this paper is as follows:  with $\\beta$ = 0.05 selected from a short parameter sweep and the small added constant for numerical stability.", "The authors compared different forms of this bonus, using $\\hat{N}(x)^{-1}$ and $PG_{n}(x)$.", "Notes  They implemented their exploration as a reward bonus for Double-DQN and A3C , and were able to show significant improvements on many Atari tasks.", "Most notably, they achieved the highest score to date on Montezuma’s Revenge.", "I am very intrigued by this result; without using some form of memory such as an LSTM to “remember” long-term behaviors, their agent was able to explore efficiently enough to learn how to achieve the hierarchical sub-goals required to visit most of the rooms and achieve high scores.", "One of the main benefits of prediction gain and pseudo-counts is that the agent is able to recognize and adjust its behaviors efficiently to salient events, which clearly plays a major role in solving games like Montezuma’s Revenge.", "A convolutional neural net used to represent the value function for an Atari game must have a very large learning capacity, which is normally under-utilized when it comes to “hard” games due to the inefficiency of $\\epsilon$-greedy exploration.", "For example, Montezuma’s Revenge has 23 total rooms the agent is able to visit; the authors showed that the agent tends to only visit about 2 of these rooms without the reward bonus.", "After 100 million frames of training on Montezuma’s Revenge with the suggested bonus, the agent had visited about 15 of these rooms!", "The CNN that uses a reward bonus must be learning representations from within all or most of the rooms, and encoding them in its hidden layers.", "Even though Montezuma’s Revenge is partially observable, the agent is able to “remember” how to do the sub-tasks just by eventually stumbling upon the sparse rewards and cleverly updating its Q-values by means of the bonus.", "I would like to see what representations were encoded by the hidden layers of the network after training.", "Perhaps using a recurrent network and/or memory with attention + the reward bonus would help improve the learning speed, so that the sequential nature of the sub-tasks within the game can be encoded more readily.", "It would be nice to see this approach compared with VIME .", "VIME computes the amount of information gained about the dynamics model due to the agent taking an action and seeing a certain following state.", "The authors show that the results should be similar, as maximizing the information gain also maximizes a lower bound on the inverse of the pseudo count.", "The authors mention that other sequential density models with more sophistication could be used, as opposed to the simple CTS model.", "There clearly is a trade-off, however, such that more complex sequential density models would increase the time and space complexity significantly.", "For example, Oh, et al., 2015 designed a recurrent convolutional neural network architecture to predict future frames from a video sequence of an Atari game.", "They estimated the visitation frequency of a predicted frame by an empirical distribution over the contents of the replay memory; the count was computed using a gaussian kernel that provided a distance metric between frames.", "This was used as an exploration bonus.", "The authors presented the appropriate metrics and figures to convince the reader of the effectiveness of their solution.", "They tested it widely on many (~60) Atari games to prove its widespread impact.", "(On a related note, it makes the difficulty for researchers that do not have access to the computational resources that DeepMind does to carry out such extensive experimentation all the more apparent!", ")."], "summary_text": "This paper presents a novel RL exploration bonus based on an adaptation of count-based exploration for high-dimensional spaces. The main contribution is the derivation of the relationships between prediction gain (PG), a quantity called the pseudo-count, and the well-known information gain from the intrinsic RL literature. The overall presentation is clear and precise, especially when it comes to defining notation and explaining the core concepts. The authors use results from prior work on applying count-based exploration in complex domains, so reading this paper without having that background is a bit challenging (see the bibliography for recent papers by Bellemare and Veness on this topic). These results will help make previous count-based exploration methods, such as those devised around the idea of optimism in the face of uncertainty, viable for high-dimensional problems. Quick Definitions  This paper presents the following definitions and notation, along with theoretical results about their properties, in a rich manner. I will simply present definitions necessary for understanding the main contribution here; please see the paper for further details. $ \\mathcal{X} := $ finite or countable alphabet, with sub-sequences of length $n$ denoted by $x_{1:n} \\in \\mathcal{X}^n $  A sequential density model over $\\mathcal{X}$ is a mapping from finite sub-sequences to probalitiy distributions over $\\mathcal{X}$. Denote these distributions by $\\rho_{n}(x) := \\rho(x ; x_{1:n})$  The recoding probability of a symbol $x$ is denoted by $\\rho_{n}^{\\prime} := \\rho(x ; x_{1:n}x)$. That is, the probability assigned to $x$ by our sequential density model after observing a new occurence of $x$  $ \\hat{N}(x) := $ the pseudo-count function  $ \\hat{n} := $ the pseudo-count total  Main Result  The following equations summarize the main results presented in this paper. The authors relate the pseudo-count function and the pseudo-count total as follows. From (1), we can write:  By choosing an appropriate sequential density model, such as a graphical model. See the appendix of the paper for the description of the CTS model used by the authors in the experimentation; this model treats a 42 x 42 processed image as a factorizable observation, computing the probability of the image $x$ as the product of the probabilities of each (i, j) pixel. These pixel probabilities are computed via the neighbors of that pixel. See section 5.2 as well for a discussion on using directed graphical models as sequential density models. The prediction gain is defined as  For the information gain (IG) defined as the change in posterior w.r.t. the KL-divergence within a mixture model $\\xi$ defined over a class $\\mathcal{M}$ of sequential density models, we have:  The reward bonus proposed in this paper is as follows:  with $\\beta$ = 0.05 selected from a short parameter sweep and the small added constant for numerical stability. The authors compared different forms of this bonus, using $\\hat{N}(x)^{-1}$ and $PG_{n}(x)$. Notes  They implemented their exploration as a reward bonus for Double-DQN and A3C , and were able to show significant improvements on many Atari tasks. Most notably, they achieved the highest score to date on Montezuma’s Revenge. I am very intrigued by this result; without using some form of memory such as an LSTM to “remember” long-term behaviors, their agent was able to explore efficiently enough to learn how to achieve the hierarchical sub-goals required to visit most of the rooms and achieve high scores. One of the main benefits of prediction gain and pseudo-counts is that the agent is able to recognize and adjust its behaviors efficiently to salient events, which clearly plays a major role in solving games like Montezuma’s Revenge. A convolutional neural net used to represent the value function for an Atari game must have a very large learning capacity, which is normally under-utilized when it comes to “hard” games due to the inefficiency of $\\epsilon$-greedy exploration. For example, Montezuma’s Revenge has 23 total rooms the agent is able to visit; the authors showed that the agent tends to only visit about 2 of these rooms without the reward bonus. After 100 million frames of training on Montezuma’s Revenge with the suggested bonus, the agent had visited about 15 of these rooms! The CNN that uses a reward bonus must be learning representations from within all or most of the rooms, and encoding them in its hidden layers. Even though Montezuma’s Revenge is partially observable, the agent is able to “remember” how to do the sub-tasks just by eventually stumbling upon the sparse rewards and cleverly updating its Q-values by means of the bonus. I would like to see what representations were encoded by the hidden layers of the network after training. Perhaps using a recurrent network and/or memory with attention + the reward bonus would help improve the learning speed, so that the sequential nature of the sub-tasks within the game can be encoded more readily. It would be nice to see this approach compared with VIME . VIME computes the amount of information gained about the dynamics model due to the agent taking an action and seeing a certain following state. The authors show that the results should be similar, as maximizing the information gain also maximizes a lower bound on the inverse of the pseudo count. The authors mention that other sequential density models with more sophistication could be used, as opposed to the simple CTS model. There clearly is a trade-off, however, such that more complex sequential density models would increase the time and space complexity significantly. For example, Oh, et al., 2015 designed a recurrent convolutional neural network architecture to predict future frames from a video sequence of an Atari game. They estimated the visitation frequency of a predicted frame by an empirical distribution over the contents of the replay memory; the count was computed using a gaussian kernel that provided a distance metric between frames. This was used as an exploration bonus. The authors presented the appropriate metrics and figures to convince the reader of the effectiveness of their solution. They tested it widely on many (~60) Atari games to prove its widespread impact. (On a related note, it makes the difficulty for researchers that do not have access to the computational resources that DeepMind does to carry out such extensive experimentation all the more apparent! ).", "pdf_url": "https://arxiv.org/pdf/1606.01868", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/unifying-count-based-exploration-and-intrinsic-motivation.json"}
{"id": "81001972", "bin": "1200_1300", "summary_sentences": ["The QUIC transport protocol: design and Internet-scale deployment Langley et al., SIGCOMM’17  QUIC is a transport protocol designed from the ground up by Google improve the performance of HTTPS traffic.", "The chances are you’ve already used it – QUIC is deployed in Chrome, in the YouTube mobile app, and in the Google Search app on Android.", "It’s also used at Google’s front-end servers, handling billions of requests a day.", "All told, about 7% of global Internet traffic is now flowing over QUIC.", "There are three interesting aspects to the paper, that we can essentially divide into the Why, the What, and the How.", "How QUIC was built and evolved over time is an integral part of the story.", "Why build a new protocol  Two forces combined to create the pressures that eventually led to QUIC.", "On the one hand, the use of the web as a platform for applications, and the latency sensitivity of those applications (and their key metrics), create a strong pressure to reduce web latency.", "On the other hand, the rapid shift towards securing web traffic adds delays.", "TLS adds two round trips to connection establishment.", "Both of these forces must contend with the speed of light that sets a lower bound on round-trip times.", "Meanwhile, HTTP/2 allows multiplexing of object streams, but is still fundamentally limited by TCP’s bytestream abstraction that forces all application frames to pay a latency tax when wait for retransmission of lost TCP segments from any stream.", "Furthermore, rolling out changes to a typical TCP based stack takes a long time as it is embedded in the OS kernel (including on end user controlled devices).", "And if you could roll out a change, it’s likely that it will break due to entrenched middleboxes:  Middleboxes have accidentally become key control points in the Internet’s architecture: firewalls tend to block anything unfamiliar for security reasons and Network Address Translators (NATs) rewrite the transport header, making both incapable of allowing traffic from new transports without adding explicit support for them.", "What is QUIC?", "An overview  QUIC was designed to be easily deployable and secure, and to reduce handshake and head-of-line blocking delays.", "Security and deployability are both helped by one of the key QUIC decisions – to base on top of UDP.", "QUIC encrypts transport headers and builds transport functions atop UDP, avoiding dependence on vendors and network operators and moving control of transport deployment to the applications that directly benefit from them.", "As well as being more secure, encrypting everything also stops those pesky middleboxes from ossifying the protocol by baking in dependencies on specific formats etc..  Establishing a connection  QUIC combines the cryptographic and transport handshake into one round trip when setting up a secure transport connection.", "If a connection is successfully established the client can cache information about the origin it connected to.", "When reconnecting to the same origin this cached information can be used to establish an encrypted connection with no additional round trips.", "Data can be sent immediately after sending the client hello, without even needing to wait for the the server’s reply (0-RTT).", "If the client’s cached information becomes out of date, then this 0-RTT handshake falls back into the 1-RTT variant.", "Stream multiplexing  To avoid head-of-line blocking due to TCP’s sequential delivery, QUIC supports multiple streams within a connection, ensuring that a lost UDP packet only impacts those streams whose data was carried in the packet.", "Streams are lightweight enough that a new stream can reasonably be used for each of a series of small messages.", "Loss recovery  In TCP, retransmitted segments carry the same sequence numbers as the original packet, which complicates ACKs.", "In QUIC, every packet carries a new sequence number, including those carrying retransmitted data.", "Stream offsets in stream frames are used for delivery ordering, separating the two functions [ordering and acknowledgement] that TCP conflates.", "The packet number represents an explicit time-ordering, which enables simpler and more accurate loss detection than in TCP.", "The separation also allows more accurate network RTT estimation helping delay-sensing congestion controllers such as BBR and PCC.", "Authentication and encryption  With the exception of the early handshake packets and reset packets, QUIC packets are fully authenticated and mostly encrypted, as shown below:  QUIC’s cryptography provides two levels of secrecy: initial client data is encrypted using initial keys, and subsequent client data and all server data are encrypted using forward-secure keys.", "All of the information sent in the clear is also included in the derivation of the final connection keys – so any in network tampering will be detected and cause the connection to fail.", "Flow control  QUIC use stream-level flow control, in conjunction with an overall connection flow control mechanism.", "As in HTTP/2, the flow control mechanism is credit based:  A QUIC receiver advertises the absolute byte offset within each stream up to which the receiver is willing to receive data.", "As data is sent, received, and delivered on a particular stream, the receiver periodically sends window update frames that increase the advertised offset limit for that stream, allowing the peer to send more data on that stream.", "Congestion control  Congestion control in QUIC is pluggable.", "The initial implementation uses Cubic .", "How Google built and evolved QUIC  Our development of the QUIC protocol relies heavily on continual Internet-scale experimentation to examine the value of various features and to tune parameters… We drove QUIC experimentation by implementing it in Chrome, which has a strong experimentation and analysis framework that allows new features to be A/B tested and evaluated before full launch.", "QUIC was also able to directly link experiments into the analytics of the application services using QUIC connections.", "This enabled the team to quantify the end-to-end effect on user and application centric performance metrics.", "QUIC support in the YouTube and the Android Google Search apps was also able to take advantage of their respective experimentation frameworks.", "Through small but repeatable improvements and rapid iteration, the QUIC project has been able to establish and sustain an appreciable and steady trajectory of cumulative performance gains.", "Is QUIC actually quick?", "The table below shows the reductions in search and video latency achieved when using the QUIC protocol.", "The improvement mostly comes from reducing handshake latency.", "Users of QUIC also saw reduced playback interruptions due to rebuffering.", "This is due to QUIC’s reduced loss-recovery latency.", "And here’s how QUIC stands up against the TCP/TLS alternative:  The downside is that QUIC is currently twice as expensive as TCP/TLS in terms of CPU."], "summary_text": "The QUIC transport protocol: design and Internet-scale deployment Langley et al., SIGCOMM’17  QUIC is a transport protocol designed from the ground up by Google improve the performance of HTTPS traffic. The chances are you’ve already used it – QUIC is deployed in Chrome, in the YouTube mobile app, and in the Google Search app on Android. It’s also used at Google’s front-end servers, handling billions of requests a day. All told, about 7% of global Internet traffic is now flowing over QUIC. There are three interesting aspects to the paper, that we can essentially divide into the Why, the What, and the How. How QUIC was built and evolved over time is an integral part of the story. Why build a new protocol  Two forces combined to create the pressures that eventually led to QUIC. On the one hand, the use of the web as a platform for applications, and the latency sensitivity of those applications (and their key metrics), create a strong pressure to reduce web latency. On the other hand, the rapid shift towards securing web traffic adds delays. TLS adds two round trips to connection establishment. Both of these forces must contend with the speed of light that sets a lower bound on round-trip times. Meanwhile, HTTP/2 allows multiplexing of object streams, but is still fundamentally limited by TCP’s bytestream abstraction that forces all application frames to pay a latency tax when wait for retransmission of lost TCP segments from any stream. Furthermore, rolling out changes to a typical TCP based stack takes a long time as it is embedded in the OS kernel (including on end user controlled devices). And if you could roll out a change, it’s likely that it will break due to entrenched middleboxes:  Middleboxes have accidentally become key control points in the Internet’s architecture: firewalls tend to block anything unfamiliar for security reasons and Network Address Translators (NATs) rewrite the transport header, making both incapable of allowing traffic from new transports without adding explicit support for them. What is QUIC? An overview  QUIC was designed to be easily deployable and secure, and to reduce handshake and head-of-line blocking delays. Security and deployability are both helped by one of the key QUIC decisions – to base on top of UDP. QUIC encrypts transport headers and builds transport functions atop UDP, avoiding dependence on vendors and network operators and moving control of transport deployment to the applications that directly benefit from them. As well as being more secure, encrypting everything also stops those pesky middleboxes from ossifying the protocol by baking in dependencies on specific formats etc..  Establishing a connection  QUIC combines the cryptographic and transport handshake into one round trip when setting up a secure transport connection. If a connection is successfully established the client can cache information about the origin it connected to. When reconnecting to the same origin this cached information can be used to establish an encrypted connection with no additional round trips. Data can be sent immediately after sending the client hello, without even needing to wait for the the server’s reply (0-RTT). If the client’s cached information becomes out of date, then this 0-RTT handshake falls back into the 1-RTT variant. Stream multiplexing  To avoid head-of-line blocking due to TCP’s sequential delivery, QUIC supports multiple streams within a connection, ensuring that a lost UDP packet only impacts those streams whose data was carried in the packet. Streams are lightweight enough that a new stream can reasonably be used for each of a series of small messages. Loss recovery  In TCP, retransmitted segments carry the same sequence numbers as the original packet, which complicates ACKs. In QUIC, every packet carries a new sequence number, including those carrying retransmitted data. Stream offsets in stream frames are used for delivery ordering, separating the two functions [ordering and acknowledgement] that TCP conflates. The packet number represents an explicit time-ordering, which enables simpler and more accurate loss detection than in TCP. The separation also allows more accurate network RTT estimation helping delay-sensing congestion controllers such as BBR and PCC. Authentication and encryption  With the exception of the early handshake packets and reset packets, QUIC packets are fully authenticated and mostly encrypted, as shown below:  QUIC’s cryptography provides two levels of secrecy: initial client data is encrypted using initial keys, and subsequent client data and all server data are encrypted using forward-secure keys. All of the information sent in the clear is also included in the derivation of the final connection keys – so any in network tampering will be detected and cause the connection to fail. Flow control  QUIC use stream-level flow control, in conjunction with an overall connection flow control mechanism. As in HTTP/2, the flow control mechanism is credit based:  A QUIC receiver advertises the absolute byte offset within each stream up to which the receiver is willing to receive data. As data is sent, received, and delivered on a particular stream, the receiver periodically sends window update frames that increase the advertised offset limit for that stream, allowing the peer to send more data on that stream. Congestion control  Congestion control in QUIC is pluggable. The initial implementation uses Cubic . How Google built and evolved QUIC  Our development of the QUIC protocol relies heavily on continual Internet-scale experimentation to examine the value of various features and to tune parameters… We drove QUIC experimentation by implementing it in Chrome, which has a strong experimentation and analysis framework that allows new features to be A/B tested and evaluated before full launch. QUIC was also able to directly link experiments into the analytics of the application services using QUIC connections. This enabled the team to quantify the end-to-end effect on user and application centric performance metrics. QUIC support in the YouTube and the Android Google Search apps was also able to take advantage of their respective experimentation frameworks. Through small but repeatable improvements and rapid iteration, the QUIC project has been able to establish and sustain an appreciable and steady trajectory of cumulative performance gains. Is QUIC actually quick? The table below shows the reductions in search and video latency achieved when using the QUIC protocol. The improvement mostly comes from reducing handshake latency. Users of QUIC also saw reduced playback interruptions due to rebuffering. This is due to QUIC’s reduced loss-recovery latency. And here’s how QUIC stands up against the TCP/TLS alternative:  The downside is that QUIC is currently twice as expensive as TCP/TLS in terms of CPU.", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3098822.3098842?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/the-quic-transport-protocol-design-and-internet-scale-deployment.json"}
{"id": "37135417", "bin": "1200_1300", "summary_sentences": ["ScootR: scaling R dataframes on dataflow systems Kunft et al., SoCC’18  The language of big data is Java ( / Scala).", "The languages of data science are Python and R. So what do you do when you want to run your data science analysis over large amounts of data?", "…programming languages with rich support for data manipulation and statistics, such as R and Python, have become increasingly popular… [but]… they are typically designed for single machine and in-memory usage….", "In contrast, parallel dataflow systems, such as Apache Flink and Apache Spark, are able to handle large amounts of data.", "However, data scientists are often unfamiliar with the systems’ native language and programming abstraction, which is crucial to achieve good performance.", "A tempting solution is to embed Python / R support within the dataflow engine.", "There are two basic approaches to this today:  Keep the guest language components in a separate process and use IPC (inter-process communication) to exchange input and output data between the dataflow engine and the guest language process.", "This approach can support the full power of the guest language, but pays a heavy price in IPC and serialisation costs.", "Use source-to-source (STS) translation to translate guest language code into the dataflows native API.", "The translated code can achieve near native performance, but comprehensive source-to-source translation is difficult and so tends to be restricted to a subset of the guest language functions and libraries.", "SparkR is of interest here because it supports both STS translation and IPC.", "It uses STS where possible, and falls back to IPC outside of this.", "Executing a simple user-defined function via IPC is about 100x slower than native execution after STS translation:  Clearly what we want is the performance of native execution (STS), but with the flexibility to use the full scope of the guest language and libraries.", "In theory we could just invest in building better and better STS translators (e.g., R to Java in this case), but this entails a huge effort and results in a hard-to-maintain point solution.", "When faced with an M:N style integration problem, it often pays to look for a common intermediate representation (IR).", "If only there was some IR which both Java and R (and Python, Ruby, JavaScript,… ) could compile into, then we could build the inter-operation at the IR level.", "The JVM has byte code, LLVM has bitcode, and a little bit closer to home, Weld has an IR based on linear types.", "In this research, Graal and Truffle provide the common ground (see One VM to rule them all ).", "Graal, Truffle, and FastR  Truffle is a language implementation framework that supports development of high performance language runtimes through self-optimising AST interpreters.", "The ASTs collect profiling information at runtime and specialise their structure accordingly.", "Languages built on top of Truffle can efficiently exchange data and access functions.", "Graal is a dynamic compiler that produces highly-optimised machine code as soon as a Truffle AST reaches a stable state.", "De-optimisations and speculation failures are handled automatically by falling back to the AST.", "The GraalVM is a multi-language execution runtime capable of running multiple languages in the same virtual machine instance, with full interoperability between all its supported languages.", "GraalVM can execute Java applications on top of the HotSpot Java VM, and can execute other Truffle-based language runtimes such as JavaScript, Ruby, Python, and LLVM.", "One of the default languages of the GraalVM is fastR, a high-performance GNU-R compatible R language runtime implemented using Truffle and relying on the Graal dynamic compiler.", "fastR supports the C API from GNU-R, and so can support many of the R packages that depend on underlying C implementations.", "Some R packages rely on GNU-R internals which make fastR integration harder, but fastR is continually being enhanced to support these too.", "Introducing ScootR  ScootR builds on the capabilities of the GraalVM to expose Flink’s internal data structures to the fastR engine.", "First ScootR creates an execution plan based on the R source code, and then this plan can be deployed and executed on a Flink cluster.", "R user-defined functions (UDFs) are executed in parallel by each worker node, and automatically optimised by the Graal JIT compiler.", "Here’s an example R application making use of the ScootR dataframe API:  R dataframes are mapped to Flink TupleN dataset types, and invocations to ScootR’s API are mapped to Flink operators via new Truffle AST nodes (_RBuiltinNode_s).", "For R functions that involve user-defined code, ScootR needs to infer the input and output types.", "Input types are specified by the user when reading files (e.g., line 7 in the listing above).", "For output types ScootR just instantiates a temporary tuple, invokes the function, and inspects the output.", "For efficient access of Java data types within R and vice-versa, ScootR makes use of Truffle’s language interoperability features.", "R functions are also rewritten to create and return Flink tuples directly in the R function.", "Here’s an example of the execution plan generated for the sample R program above.", "Only the apply function includes user-defined code, all other functions are replaced with the corresponding Flink operators during the plan generation phase.", "Evaluation  The evaluation is based on two datasets:  The Airline On-Time Performance Dataset with arrival data for US flights, converted into CSV format.", "The resulting file size is 9.5GB.", "The Reddit Comments Dataset (four consecutive months’ worth, at about 14GB per month in CSV format).", "ScootR is compared against native GNU-R, fastR, and SparkR, as well as against natively coded pipelines in Spark and Flink.", "Here are the single node and cluster comparisons for an ETL pipeline on the airline data as follows:  Key findings from the evaluation are as follows:  For non-UDF functions, both ScootR and SparkR provide reliable mapping of R functions to native API calls with overhead below 1.2x.", "With UDFs, ScootRs performance is competitive with SparkR’s STS approach when SparkR is able to use STS, and an order of magnitude faster when SparkR has to fallback to IPC.", "Total overheads in operator pipelines (vs fully native) are up to 1.2x for SparkR with STS, and up to 1.4x for ScootR.", "Both SparkR and ScootR outperform GNU-R and fastR, even for single-threaded execution on a single node.", "One possible direction for future work is to integrate other dynamic languages with Truffle support, such as JavasScript or Python."], "summary_text": "ScootR: scaling R dataframes on dataflow systems Kunft et al., SoCC’18  The language of big data is Java ( / Scala). The languages of data science are Python and R. So what do you do when you want to run your data science analysis over large amounts of data? …programming languages with rich support for data manipulation and statistics, such as R and Python, have become increasingly popular… [but]… they are typically designed for single machine and in-memory usage…. In contrast, parallel dataflow systems, such as Apache Flink and Apache Spark, are able to handle large amounts of data. However, data scientists are often unfamiliar with the systems’ native language and programming abstraction, which is crucial to achieve good performance. A tempting solution is to embed Python / R support within the dataflow engine. There are two basic approaches to this today:  Keep the guest language components in a separate process and use IPC (inter-process communication) to exchange input and output data between the dataflow engine and the guest language process. This approach can support the full power of the guest language, but pays a heavy price in IPC and serialisation costs. Use source-to-source (STS) translation to translate guest language code into the dataflows native API. The translated code can achieve near native performance, but comprehensive source-to-source translation is difficult and so tends to be restricted to a subset of the guest language functions and libraries. SparkR is of interest here because it supports both STS translation and IPC. It uses STS where possible, and falls back to IPC outside of this. Executing a simple user-defined function via IPC is about 100x slower than native execution after STS translation:  Clearly what we want is the performance of native execution (STS), but with the flexibility to use the full scope of the guest language and libraries. In theory we could just invest in building better and better STS translators (e.g., R to Java in this case), but this entails a huge effort and results in a hard-to-maintain point solution. When faced with an M:N style integration problem, it often pays to look for a common intermediate representation (IR). If only there was some IR which both Java and R (and Python, Ruby, JavaScript,… ) could compile into, then we could build the inter-operation at the IR level. The JVM has byte code, LLVM has bitcode, and a little bit closer to home, Weld has an IR based on linear types. In this research, Graal and Truffle provide the common ground (see One VM to rule them all ). Graal, Truffle, and FastR  Truffle is a language implementation framework that supports development of high performance language runtimes through self-optimising AST interpreters. The ASTs collect profiling information at runtime and specialise their structure accordingly. Languages built on top of Truffle can efficiently exchange data and access functions. Graal is a dynamic compiler that produces highly-optimised machine code as soon as a Truffle AST reaches a stable state. De-optimisations and speculation failures are handled automatically by falling back to the AST. The GraalVM is a multi-language execution runtime capable of running multiple languages in the same virtual machine instance, with full interoperability between all its supported languages. GraalVM can execute Java applications on top of the HotSpot Java VM, and can execute other Truffle-based language runtimes such as JavaScript, Ruby, Python, and LLVM. One of the default languages of the GraalVM is fastR, a high-performance GNU-R compatible R language runtime implemented using Truffle and relying on the Graal dynamic compiler. fastR supports the C API from GNU-R, and so can support many of the R packages that depend on underlying C implementations. Some R packages rely on GNU-R internals which make fastR integration harder, but fastR is continually being enhanced to support these too. Introducing ScootR  ScootR builds on the capabilities of the GraalVM to expose Flink’s internal data structures to the fastR engine. First ScootR creates an execution plan based on the R source code, and then this plan can be deployed and executed on a Flink cluster. R user-defined functions (UDFs) are executed in parallel by each worker node, and automatically optimised by the Graal JIT compiler. Here’s an example R application making use of the ScootR dataframe API:  R dataframes are mapped to Flink TupleN dataset types, and invocations to ScootR’s API are mapped to Flink operators via new Truffle AST nodes (_RBuiltinNode_s). For R functions that involve user-defined code, ScootR needs to infer the input and output types. Input types are specified by the user when reading files (e.g., line 7 in the listing above). For output types ScootR just instantiates a temporary tuple, invokes the function, and inspects the output. For efficient access of Java data types within R and vice-versa, ScootR makes use of Truffle’s language interoperability features. R functions are also rewritten to create and return Flink tuples directly in the R function. Here’s an example of the execution plan generated for the sample R program above. Only the apply function includes user-defined code, all other functions are replaced with the corresponding Flink operators during the plan generation phase. Evaluation  The evaluation is based on two datasets:  The Airline On-Time Performance Dataset with arrival data for US flights, converted into CSV format. The resulting file size is 9.5GB. The Reddit Comments Dataset (four consecutive months’ worth, at about 14GB per month in CSV format). ScootR is compared against native GNU-R, fastR, and SparkR, as well as against natively coded pipelines in Spark and Flink. Here are the single node and cluster comparisons for an ETL pipeline on the airline data as follows:  Key findings from the evaluation are as follows:  For non-UDF functions, both ScootR and SparkR provide reliable mapping of R functions to native API calls with overhead below 1.2x. With UDFs, ScootRs performance is competitive with SparkR’s STS approach when SparkR is able to use STS, and an order of magnitude faster when SparkR has to fallback to IPC. Total overheads in operator pipelines (vs fully native) are up to 1.2x for SparkR with STS, and up to 1.4x for ScootR. Both SparkR and ScootR outperform GNU-R and fastR, even for single-threaded execution on a single node. One possible direction for future work is to integrate other dynamic languages with Truffle support, such as JavasScript or Python.", "pdf_url": "http://www.user.tu-berlin.de/akunft/paper/socc18-paper35.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/scootr-scaling-r-dataframes-on-dataflow-systems.json"}
{"id": "2852538", "bin": "1200_1300", "summary_sentences": ["Sanh et al. (2018) propose a new method based on multi-task learning, trained in a hierarchical fashion, to achieve state of the art results on various NLP tasks such as Named Entity Recognition (NER), Entity Mention Detection (EMD) and Relation Extraction (RE).", "(See brief definitions of these tasks at the end of this article if you are not familiar with them.)", "The main question this paper aims to address is whether linguistic hierarchies and multi-task learning can be leveraged to improve results on the above-mentioned semantic-related tasks.", "Previously, multi-task frameworks have not been trained to leverage the strengths of inductive transfer to achieve more generalized capabilities.", "Complimentary aspects of a sentence (e.g., syntax and word order) can be combined to produce generalized sentence embeddings.", "This paper proposes a unified model that trains and combines four semantic NLP tasks on the basis that they share inter-dependencies with each other.", "For instance, in the example provided in the table below (highlighted in blue) you can observe that by resolving that “Dell” and “the company” refer to the same real-world entity, it is also likely to represent an organization as opposed to a person.", "This knowledge could be beneficial and transferred to other tasks such as EMD and NER.", "Image source  The proposed model (shown in the figure below) consists of a hierarchy between the tasks in the lower levels while encouraging complex interactions at deeper layers.", "This implies that simple supervised tasks will be placed at lower layers and more complex tasks at the higher layers.", "This is done in an end-to-end setting and without using hand-engineered features.", "A new sampling strategy for multi-task learning, called proportional sampling, is also proposed (more on this later).", "HMTL framework— Image source  Hierarchical Model  The input of the model consists of the concatenations of three type of word embeddings: fine-tuned GloVe embeddings, ELMo embeddings, and character-level embeddings.", "The first group of layers in the model are supervised by NER labels where the inputs are the concatenated embeddings and the output represents the hidden states produced by the biLSTMs.", "A CRF tagging layer represents the last layer in this group as seen in the model figure above.", "The second group of layers is supervised by EMD labels where the input is the concatenation of the output of lower layers and input embeddings, and the output represents sequence embeddings.", "Similar to NER, CRF is used to make tagging decisions.", "Note that the input contains information from the lower layers, establishing the hierarchical architecture.", "The highest level of the model is supervised by a Coreference resolution (CR) task where the input is the concatenated embeddings combined with the output of the lower layers, and the outputs are fed to the mention pair scorer.", "In this same level of the architecture, the model is also supervised by the RE task.", "The RE task involves identifying mentions and classifying their relations, thus it also tries to link mentions similar to the CR task (refer to the paper for more details).", "Experiments  Overall, two datasets are used for the experiments.", "For NER, the English portion of OntoNotes 5.0 ( Pradhan et al. 2013 ) is used.", "For CR, EMD, and RE, the ACE05 corpus ( Doddington et al. 2004 ) is used.", "Refer to the paper for more details on these datasets and how they were used.", "Data statistics can be found in the table below:  Image source  To avoid catastrophic forgetting (an issue common when training multi-task models), a simple yet effective training method is employed.", "Specifically, after each parameter update, a task from the pipeline is randomly selected and batches linked to this task are also randomly sampled.", "The sampling of a task is achieved using proportional sampling which is a function of the relative size of a dataset compared to the cumulative size of all datasets.", "Results  In summary, the proposed hierarchical and multi-task learning framework, coined HMTL, achieved state-of-the-art (SOTA) results on three tasks, namely NER ( +0.52 ), EMD( +3.8 ), and RE ( +6.8 ).", "The results are summarized in the table below:  Image source  The full model (A-GM) model (highlighted in blue) produces SOTA results for EMD and RE.", "These results suggest that having different type of information on different sentences produces valuable richer information.", "B, C, D, E, and E-GM are all single-task setups (highlighted in pink) which are outperformed by the full model (A) with exception of the EMD task.", "However, A-GM outperforms the single-task setup of EMD, keeping in mind that this model makes use of gold mentions.", "For the rest of the setups (e.g., F, G, etc.", "), varying combinations of tasks are used during training (highlighted in green).", "These results show how much much one task or tasks can contribute to the other/s.", "Note that the authors also experimented with the order of the tasks such (e.g., F vs. K) and how this decision influenced results.", "The table below shows the ablation study conducted on the input embeddings:  Image source  You can observe the strength of the contextualized ELMo embeddings by the differences shown in the metrics.", "In addition, the authors also discuss what the encoders and embeddings in the multi-task, hierarchical architecture are learning through various probing tasks (see full details in the paper).", "In the table below, you can also observe the differences in training time (defined by parameter updates) between a multi-task framework and single-task framework.", "The lower the values in the time column and the higher the performance the better the results.", "Image source  References  Paper: A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks — (Victor Sanh, Thomas Wolf, Sebastian Ruder)  Code: GitHub repo  Online Demo: HMTL for NLP  Detailed post (includes code snippets): HuggingFace Blog  Task Definitions  Named Entity Recognition (NER) aims to identify mentions of named entities in a sequence and classify them into predefined categories.", "Entity Mention Detection (EMD) is similar to NER but more general as it aims to identify all the mentions related to a real-life entity, whereas NER only focuses on the named entities.", "Coreference resolution (CR) aims to identify mentions that are referring to the same real-life entity and cluster them together.", "Relation Extraction (RE) aims to identify semantic relational structure between entity mentions in unstructured text."], "summary_text": "Sanh et al. (2018) propose a new method based on multi-task learning, trained in a hierarchical fashion, to achieve state of the art results on various NLP tasks such as Named Entity Recognition (NER), Entity Mention Detection (EMD) and Relation Extraction (RE). (See brief definitions of these tasks at the end of this article if you are not familiar with them.) The main question this paper aims to address is whether linguistic hierarchies and multi-task learning can be leveraged to improve results on the above-mentioned semantic-related tasks. Previously, multi-task frameworks have not been trained to leverage the strengths of inductive transfer to achieve more generalized capabilities. Complimentary aspects of a sentence (e.g., syntax and word order) can be combined to produce generalized sentence embeddings. This paper proposes a unified model that trains and combines four semantic NLP tasks on the basis that they share inter-dependencies with each other. For instance, in the example provided in the table below (highlighted in blue) you can observe that by resolving that “Dell” and “the company” refer to the same real-world entity, it is also likely to represent an organization as opposed to a person. This knowledge could be beneficial and transferred to other tasks such as EMD and NER. Image source  The proposed model (shown in the figure below) consists of a hierarchy between the tasks in the lower levels while encouraging complex interactions at deeper layers. This implies that simple supervised tasks will be placed at lower layers and more complex tasks at the higher layers. This is done in an end-to-end setting and without using hand-engineered features. A new sampling strategy for multi-task learning, called proportional sampling, is also proposed (more on this later). HMTL framework— Image source  Hierarchical Model  The input of the model consists of the concatenations of three type of word embeddings: fine-tuned GloVe embeddings, ELMo embeddings, and character-level embeddings. The first group of layers in the model are supervised by NER labels where the inputs are the concatenated embeddings and the output represents the hidden states produced by the biLSTMs. A CRF tagging layer represents the last layer in this group as seen in the model figure above. The second group of layers is supervised by EMD labels where the input is the concatenation of the output of lower layers and input embeddings, and the output represents sequence embeddings. Similar to NER, CRF is used to make tagging decisions. Note that the input contains information from the lower layers, establishing the hierarchical architecture. The highest level of the model is supervised by a Coreference resolution (CR) task where the input is the concatenated embeddings combined with the output of the lower layers, and the outputs are fed to the mention pair scorer. In this same level of the architecture, the model is also supervised by the RE task. The RE task involves identifying mentions and classifying their relations, thus it also tries to link mentions similar to the CR task (refer to the paper for more details). Experiments  Overall, two datasets are used for the experiments. For NER, the English portion of OntoNotes 5.0 ( Pradhan et al. 2013 ) is used. For CR, EMD, and RE, the ACE05 corpus ( Doddington et al. 2004 ) is used. Refer to the paper for more details on these datasets and how they were used. Data statistics can be found in the table below:  Image source  To avoid catastrophic forgetting (an issue common when training multi-task models), a simple yet effective training method is employed. Specifically, after each parameter update, a task from the pipeline is randomly selected and batches linked to this task are also randomly sampled. The sampling of a task is achieved using proportional sampling which is a function of the relative size of a dataset compared to the cumulative size of all datasets. Results  In summary, the proposed hierarchical and multi-task learning framework, coined HMTL, achieved state-of-the-art (SOTA) results on three tasks, namely NER ( +0.52 ), EMD( +3.8 ), and RE ( +6.8 ). The results are summarized in the table below:  Image source  The full model (A-GM) model (highlighted in blue) produces SOTA results for EMD and RE. These results suggest that having different type of information on different sentences produces valuable richer information. B, C, D, E, and E-GM are all single-task setups (highlighted in pink) which are outperformed by the full model (A) with exception of the EMD task. However, A-GM outperforms the single-task setup of EMD, keeping in mind that this model makes use of gold mentions. For the rest of the setups (e.g., F, G, etc. ), varying combinations of tasks are used during training (highlighted in green). These results show how much much one task or tasks can contribute to the other/s. Note that the authors also experimented with the order of the tasks such (e.g., F vs. K) and how this decision influenced results. The table below shows the ablation study conducted on the input embeddings:  Image source  You can observe the strength of the contextualized ELMo embeddings by the differences shown in the metrics. In addition, the authors also discuss what the encoders and embeddings in the multi-task, hierarchical architecture are learning through various probing tasks (see full details in the paper). In the table below, you can also observe the differences in training time (defined by parameter updates) between a multi-task framework and single-task framework. The lower the values in the time column and the higher the performance the better the results. Image source  References  Paper: A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks — (Victor Sanh, Thomas Wolf, Sebastian Ruder)  Code: GitHub repo  Online Demo: HMTL for NLP  Detailed post (includes code snippets): HuggingFace Blog  Task Definitions  Named Entity Recognition (NER) aims to identify mentions of named entities in a sequence and classify them into predefined categories. Entity Mention Detection (EMD) is similar to NER but more general as it aims to identify all the mentions related to a real-life entity, whereas NER only focuses on the named entities. Coreference resolution (CR) aims to identify mentions that are referring to the same real-life entity and cluster them together. Relation Extraction (RE) aims to identify semantic relational structure between entity mentions in unstructured text.", "pdf_url": "https://arxiv.org/pdf/1811.06031", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/hmtl-multi-task-learning-for-state-of-the-art-nlp-245572bbb601.json"}
{"id": "80421460", "bin": "1200_1300", "summary_sentences": ["Deep learning scaling is predictable, empirically Hestness et al., arXiv, Dec.2017  With thanks to Nathan Benaich for highlighting this paper in his excellent summary of the AI world in 1Q18  This is a really wonderful study with far-reaching implications that could even impact company strategies in some cases.", "It starts with a simple question: “how can we improve the state of the art in deep learning?” We have three main lines of attack:  We can search for improved model architectures.", "We can scale computation.", "We can create larger training data sets.", "As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art.", "Finding better model architectures often depends on ‘unreliable epiphany,’ and as the results show, has limited impact compared to increasing the amount of data available.", "We’ve known this for some time of course, including from the 2009 Google paper, ‘The unreasonable effectiveness of data .’  The results from today’s paper help us to quantify the data advantage across a range of deep learning applications.", "The key to understanding is captured in the following equation:  Which says that the generalisation error  as a function of the amount of training data  , follows a power-law with exponent  .", "Empirically,  usually seems to be in the range -0.07 and -0.35.", "With error of course, lower is better, hence the negative exponents.", "We normally plot power-laws on log-log graphs, where they result in straight lines with the gradient indicating the exponent.", "Making better models can move the y-intercept down (until we reach the irreducible error level), but doesn’t seem to impact the power law coefficient.", "On the other hand, more data puts us on an power law path of improvement.", "The three learning zones  The learning curves for real applications can be broken down into three regions:  The small data region is where models struggle to learn from insufficient data and models can only perform as well as ‘best’ or ‘random’ guessing.", "The middle region is the power-law region, where the power-law exponent defines the steepness of the curve (slope on a log-log scale).", "The exponent is an indicator of the difficulty for models to represent the data generating function.", "“Results in this paper indicate that the power-law exponent is unlikely to be easily predicted with prior theory and probably dependent on aspects of the problem domain or data distribution.”  The irreducible error region is the non-zero lower-bound error past which models will be unable to improve.", "With sufficiently large training sets, models saturate in this region.", "On model size  We expect the number of model parameters to fit a data set should follow  where  is the required model size to fin a training set of size  , and  .", "Best-fit models grow sublinearly in training shard size.", "Higher values of  indicate models that make less effective use of extra parameters on larger data sets.", "Despite model size scaling differences though, “for a given model architecture, we can accurately predict the model size that will best fit increasingly larger data sets.”  Implications of the power-law  Predictable learning curves and model size scaling indicate some significant implications on how DL could proceed.", "For machine learning practitioners and researchers, predictable scaling can aid model and optimization debugging and iteration time, and offer a way to estimate the most impactful next steps to improve model accuracy.", "Operationally, predictable curves can guid decision making about whether or how to grow data sets or computation.", "One interesting consequence is that model exploration can be done on smaller data sets (and hence faster / cheaper).", "The data set needs to be large enough to show accuracy in the power-law region of the curve.", "Then the most promising models can be scaled to larger data sets to ensure proportional accuracy gains.", "This works because growing training sets and models is likely to result in the same relative gains across models.", "When building a company we look for product-market fit before scaling the business.", "In deep learning it seems the analogy is to look for model-problem fit before scaling, a search then scale strategy:  If you need to make a business decision about the return on investment, or likely accuracy improvement, from investing in the collection of more data, the power-law can help you predict returns:  Conversely, if generalisation error within the power-law region drifts from the power-law predictions, it’s a clue that increasing model size might help, or a more extensive hyperparameter search (i.e., more compute), might help:  …predictable learning and model size curves may offer a way to project the compute requirements to reach a particular accuracy level.", "Can you beat the power law?", "Model architecture improvements (e.g., increasing model depth) seem only to shift learning curves down, but not improve the power-law exponent.", "We have yet to find factors that affect the power-law exponent.", "To beat the power-law as we increase data set size, models would need to learn more concepts with successively less data.", "In other words, models must successively extract more marginal information from each additional training sample.", "If we can find ways to improve the power-law exponent though, then the potential accuracy improvements in some problem domains are ‘immense.’  The empirical data  The empirical data to back all this up was collected by testing various training data sizes (in powers of two) with state-of-the-art deep learning models in a number of different domains.", "Here are the neural machine translation learning curves.", "On the right you can see results for the best-fit model at each training set size.", "As training set sizes grow, the empirical error tends away from the power-law trend, and a more exhaustive hyperparameter search would be needed to bring it back in line.", "The next domain is word language models.", "A variety of model architectures are used, and although they differ appreciably, they all show the same learning curve profile as characterised by the power-law exponent.", "And here are the results for character language models:  With image classification, we see the ‘small data region’ appear on the plots when there is insufficient data to learn a good classifier:  Finally, here’s speech recognition:  We empirically validate that DL model accuracy improves as a power-law as we grow training sets for state-of-the-art (SOTA) model architectures in four machine learning domains: machine translation, language modeling, image processing, and speech recognition.", "These power-law learning curves exist across all tested domains, model architectures, optimizers, and loss functions."], "summary_text": "Deep learning scaling is predictable, empirically Hestness et al., arXiv, Dec.2017  With thanks to Nathan Benaich for highlighting this paper in his excellent summary of the AI world in 1Q18  This is a really wonderful study with far-reaching implications that could even impact company strategies in some cases. It starts with a simple question: “how can we improve the state of the art in deep learning?” We have three main lines of attack:  We can search for improved model architectures. We can scale computation. We can create larger training data sets. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. Finding better model architectures often depends on ‘unreliable epiphany,’ and as the results show, has limited impact compared to increasing the amount of data available. We’ve known this for some time of course, including from the 2009 Google paper, ‘The unreasonable effectiveness of data .’  The results from today’s paper help us to quantify the data advantage across a range of deep learning applications. The key to understanding is captured in the following equation:  Which says that the generalisation error  as a function of the amount of training data  , follows a power-law with exponent  . Empirically,  usually seems to be in the range -0.07 and -0.35. With error of course, lower is better, hence the negative exponents. We normally plot power-laws on log-log graphs, where they result in straight lines with the gradient indicating the exponent. Making better models can move the y-intercept down (until we reach the irreducible error level), but doesn’t seem to impact the power law coefficient. On the other hand, more data puts us on an power law path of improvement. The three learning zones  The learning curves for real applications can be broken down into three regions:  The small data region is where models struggle to learn from insufficient data and models can only perform as well as ‘best’ or ‘random’ guessing. The middle region is the power-law region, where the power-law exponent defines the steepness of the curve (slope on a log-log scale). The exponent is an indicator of the difficulty for models to represent the data generating function. “Results in this paper indicate that the power-law exponent is unlikely to be easily predicted with prior theory and probably dependent on aspects of the problem domain or data distribution.”  The irreducible error region is the non-zero lower-bound error past which models will be unable to improve. With sufficiently large training sets, models saturate in this region. On model size  We expect the number of model parameters to fit a data set should follow  where  is the required model size to fin a training set of size  , and  . Best-fit models grow sublinearly in training shard size. Higher values of  indicate models that make less effective use of extra parameters on larger data sets. Despite model size scaling differences though, “for a given model architecture, we can accurately predict the model size that will best fit increasingly larger data sets.”  Implications of the power-law  Predictable learning curves and model size scaling indicate some significant implications on how DL could proceed. For machine learning practitioners and researchers, predictable scaling can aid model and optimization debugging and iteration time, and offer a way to estimate the most impactful next steps to improve model accuracy. Operationally, predictable curves can guid decision making about whether or how to grow data sets or computation. One interesting consequence is that model exploration can be done on smaller data sets (and hence faster / cheaper). The data set needs to be large enough to show accuracy in the power-law region of the curve. Then the most promising models can be scaled to larger data sets to ensure proportional accuracy gains. This works because growing training sets and models is likely to result in the same relative gains across models. When building a company we look for product-market fit before scaling the business. In deep learning it seems the analogy is to look for model-problem fit before scaling, a search then scale strategy:  If you need to make a business decision about the return on investment, or likely accuracy improvement, from investing in the collection of more data, the power-law can help you predict returns:  Conversely, if generalisation error within the power-law region drifts from the power-law predictions, it’s a clue that increasing model size might help, or a more extensive hyperparameter search (i.e., more compute), might help:  …predictable learning and model size curves may offer a way to project the compute requirements to reach a particular accuracy level. Can you beat the power law? Model architecture improvements (e.g., increasing model depth) seem only to shift learning curves down, but not improve the power-law exponent. We have yet to find factors that affect the power-law exponent. To beat the power-law as we increase data set size, models would need to learn more concepts with successively less data. In other words, models must successively extract more marginal information from each additional training sample. If we can find ways to improve the power-law exponent though, then the potential accuracy improvements in some problem domains are ‘immense.’  The empirical data  The empirical data to back all this up was collected by testing various training data sizes (in powers of two) with state-of-the-art deep learning models in a number of different domains. Here are the neural machine translation learning curves. On the right you can see results for the best-fit model at each training set size. As training set sizes grow, the empirical error tends away from the power-law trend, and a more exhaustive hyperparameter search would be needed to bring it back in line. The next domain is word language models. A variety of model architectures are used, and although they differ appreciably, they all show the same learning curve profile as characterised by the power-law exponent. And here are the results for character language models:  With image classification, we see the ‘small data region’ appear on the plots when there is insufficient data to learn a good classifier:  Finally, here’s speech recognition:  We empirically validate that DL model accuracy improves as a power-law as we grow training sets for state-of-the-art (SOTA) model architectures in four machine learning domains: machine translation, language modeling, image processing, and speech recognition. These power-law learning curves exist across all tested domains, model architectures, optimizers, and loss functions.", "pdf_url": "https://arxiv.org/pdf/1712.00409", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/deep-learning-scaling-is-predictable-empirically.json"}
{"id": "13040160", "bin": "1200_1300", "summary_sentences": ["Machine learning systems are stuck in a rut Barham & Isard, HotOS’19  In this paper we argue that systems for numerical computing are stuck in a local basin of performance and programmability.", "Systems researchers are doing an excellent job improving the performance of 5-year old benchmarks, but gradually making it harder to explore innovative machine learning research ideas.", "The thrust of the argument is that there’s a chain of inter-linked assumptions / dependencies from the hardware all the way to the programming model, and any time you step outside of the mainstream it’s sufficiently hard to get acceptable performance that researchers are discouraged from doing so.", "Take a simple example: it would be really nice if we could have named dimensions instead of always having to work with indices.", "Named dimensions improve readability by making it easier to determine how dimensions in the code correspond to the semantic dimensions described in, .e.g., a research paper.", "We believe their impact could be even greater in improving code modularity, as named dimensions would enable a language to move away from fixing an order on the dimensions of a given tensor, which in turn would make function lifting more convenient…  For the readability point I feel I ought to mention you could always declare a ‘constant’ , e.g FEATURE_NAME = 1, and use that to make your code more readable (constant is in quotes there because Python doesn’t really have constants, but it still has variable names!).", "But that won’t solve the ordering issue of course.", "It would be interesting to explore unordered sets of named dimensions in the programming model and see what benefits that could bring, however,…  It is hard to experiment with front-end features like named dimensions, because it is painful to match them to back ends that expect calls to monolithic kernels with fixed layout.", "On the other hand, there is little incentive to build high quality back ends that support other features, because all the front ends currently work in terms of monolithic operators.", "Named dimensions are an easy to understand example, but the challenges go much deeper than that, and were keenly felt by the authors during research on Capsule networks .", "Challenges compiling non-standard kernels  Convolutional Capsule primitives can be implemented reasonably efficiently on CPU but problems arise on accelerators (e.g. GPU and TPU).", "Performance on accelerators matters because almost all current machine learning research, and most training of production models, uses them.", "Scheduling instructions for good performance in accelerators is a complex business.", "It’s “very challenging” just for standard convolutions, and convolutional capsules add several dimensions of complexity.", "Because it’s so tricky, high-performance back-ends for accelerators tend to spend a lot of effort optimising a small set of computational kernels.", "New primitives that don’t fit into these existing kernels can be compiled into custom kernels using e.g. Tensor Comprehensions or PlaidML , but the current state-of-the-art only really supports small code fragments and frequently doesn’t get close to peak performance (e.g.", "a factor of 8x slower after a one hour search, for a conventional 2D convolution the authors used as an experiment).", "Our interpretation of these results is that current frameworks excel at workloads where it makes sense to manually tune the small set of computations used by a particular model or family of models.", "Unfortunately, frameworks become poorly suited to research, because there is a performance cliff when experimenting with computations that haven’t previously been identified as important.", "That said, after around 17 minutes Tensor Comprehensions does find a solution that outperforms a hand-tuned CUDA solution.", "Which doesn’t seem so bad, until you remember this is just one kernel out of what may be a large overall computation.", "The easiest and best performing solution for convolutional Capsules in both TensorFlow and PyTorch turns out to be to target high-level operations already supported by those frameworks.", "This comes at a cost though; copying, rearranging, and materialising to memory two orders of magnitude more data than is strictly necessary.", "Challenges optimising whole programs  It might be hard to performance tune a single non-standard kernel, but full programs must typically evaluate a large graph of kernels.", "In order to make use of pre-optimised kernels, it’s necessary to use one of a small number of parameter layouts that have been chosen ahead of time to be optimal in isolation.", "In practice there are so few choices of layout available that frameworks like XLA and TVM do not attempt a global layout assignment, and instead choose fixed layouts for expensive operators like convolution, then propagate those layouts locally through the operator graph inserting transposes where necessary.", "Similar considerations make it hard to experiment with different choices for quantised and  low-precision types.", "Whole program optimisations such as common sub-expression elimination are attractive for machine learning, but hard to exploit to the fullest extent with the current state-of-the-art: “it seems likely that it will be necessary to architect machine learning frameworks with automatic optimizers in mind before it will be possible to make the best use of whole-program optimization.“  Challenges evolving programming languages  Recall that back ends are structured around calls to large monolithic kernels.", "In this section we argue that this back-end design approach is slowing progress in the maintainability, debuggability, and expressiveness of programming models.", "Worse, the resulting brake on innovation in languages is in itself reducing the incentive for back-end developers to improve on the current situation.", "We saw one such example at the top of this piece: support for named dimensions.", "Another consequence is the choice of kernels or ‘operators’ as the dominant abstraction, with user programs in Python calling into operators written in terms of specific back-end languages and libraries.", "This tends to fix both the set of operators and also their interfaces.", "Breaking out of the rut  Our main concern is that the inflexibility of languages and back ends is a real brake on innovative research, that risks slowing progress in this very active field…  How might we break out of the rut?", "Embrace language design including automatic differentiation, using purely named dimensions and kernels expressed within the language syntax  Support a back-end IR defining a graph of layout-agnostic general purpose loop nests  Use transformation passes over the IR to lower it to a concrete common sub-expression elimination strategy, with layouts for each materialised intermediate  Compilation passes that generate accelerator code given the lowered IR, with adequate code produced quickly and close to peak performance achievable after searching  At a high level, this reminds me a little of the approach taken by Musketeer ."], "summary_text": "Machine learning systems are stuck in a rut Barham & Isard, HotOS’19  In this paper we argue that systems for numerical computing are stuck in a local basin of performance and programmability. Systems researchers are doing an excellent job improving the performance of 5-year old benchmarks, but gradually making it harder to explore innovative machine learning research ideas. The thrust of the argument is that there’s a chain of inter-linked assumptions / dependencies from the hardware all the way to the programming model, and any time you step outside of the mainstream it’s sufficiently hard to get acceptable performance that researchers are discouraged from doing so. Take a simple example: it would be really nice if we could have named dimensions instead of always having to work with indices. Named dimensions improve readability by making it easier to determine how dimensions in the code correspond to the semantic dimensions described in, .e.g., a research paper. We believe their impact could be even greater in improving code modularity, as named dimensions would enable a language to move away from fixing an order on the dimensions of a given tensor, which in turn would make function lifting more convenient…  For the readability point I feel I ought to mention you could always declare a ‘constant’ , e.g FEATURE_NAME = 1, and use that to make your code more readable (constant is in quotes there because Python doesn’t really have constants, but it still has variable names!). But that won’t solve the ordering issue of course. It would be interesting to explore unordered sets of named dimensions in the programming model and see what benefits that could bring, however,…  It is hard to experiment with front-end features like named dimensions, because it is painful to match them to back ends that expect calls to monolithic kernels with fixed layout. On the other hand, there is little incentive to build high quality back ends that support other features, because all the front ends currently work in terms of monolithic operators. Named dimensions are an easy to understand example, but the challenges go much deeper than that, and were keenly felt by the authors during research on Capsule networks . Challenges compiling non-standard kernels  Convolutional Capsule primitives can be implemented reasonably efficiently on CPU but problems arise on accelerators (e.g. GPU and TPU). Performance on accelerators matters because almost all current machine learning research, and most training of production models, uses them. Scheduling instructions for good performance in accelerators is a complex business. It’s “very challenging” just for standard convolutions, and convolutional capsules add several dimensions of complexity. Because it’s so tricky, high-performance back-ends for accelerators tend to spend a lot of effort optimising a small set of computational kernels. New primitives that don’t fit into these existing kernels can be compiled into custom kernels using e.g. Tensor Comprehensions or PlaidML , but the current state-of-the-art only really supports small code fragments and frequently doesn’t get close to peak performance (e.g. a factor of 8x slower after a one hour search, for a conventional 2D convolution the authors used as an experiment). Our interpretation of these results is that current frameworks excel at workloads where it makes sense to manually tune the small set of computations used by a particular model or family of models. Unfortunately, frameworks become poorly suited to research, because there is a performance cliff when experimenting with computations that haven’t previously been identified as important. That said, after around 17 minutes Tensor Comprehensions does find a solution that outperforms a hand-tuned CUDA solution. Which doesn’t seem so bad, until you remember this is just one kernel out of what may be a large overall computation. The easiest and best performing solution for convolutional Capsules in both TensorFlow and PyTorch turns out to be to target high-level operations already supported by those frameworks. This comes at a cost though; copying, rearranging, and materialising to memory two orders of magnitude more data than is strictly necessary. Challenges optimising whole programs  It might be hard to performance tune a single non-standard kernel, but full programs must typically evaluate a large graph of kernels. In order to make use of pre-optimised kernels, it’s necessary to use one of a small number of parameter layouts that have been chosen ahead of time to be optimal in isolation. In practice there are so few choices of layout available that frameworks like XLA and TVM do not attempt a global layout assignment, and instead choose fixed layouts for expensive operators like convolution, then propagate those layouts locally through the operator graph inserting transposes where necessary. Similar considerations make it hard to experiment with different choices for quantised and  low-precision types. Whole program optimisations such as common sub-expression elimination are attractive for machine learning, but hard to exploit to the fullest extent with the current state-of-the-art: “it seems likely that it will be necessary to architect machine learning frameworks with automatic optimizers in mind before it will be possible to make the best use of whole-program optimization.“  Challenges evolving programming languages  Recall that back ends are structured around calls to large monolithic kernels. In this section we argue that this back-end design approach is slowing progress in the maintainability, debuggability, and expressiveness of programming models. Worse, the resulting brake on innovation in languages is in itself reducing the incentive for back-end developers to improve on the current situation. We saw one such example at the top of this piece: support for named dimensions. Another consequence is the choice of kernels or ‘operators’ as the dominant abstraction, with user programs in Python calling into operators written in terms of specific back-end languages and libraries. This tends to fix both the set of operators and also their interfaces. Breaking out of the rut  Our main concern is that the inflexibility of languages and back ends is a real brake on innovative research, that risks slowing progress in this very active field…  How might we break out of the rut? Embrace language design including automatic differentiation, using purely named dimensions and kernels expressed within the language syntax  Support a back-end IR defining a graph of layout-agnostic general purpose loop nests  Use transformation passes over the IR to lower it to a concrete common sub-expression elimination strategy, with layouts for each materialised intermediate  Compilation passes that generate accelerator code given the lowered IR, with adequate code produced quickly and close to peak performance achievable after searching  At a high level, this reminds me a little of the approach taken by Musketeer .", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3317550.3321441?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/machine-learning-systems-are-stuck-in-a-rut.json"}
{"id": "79305631", "bin": "1300_1400", "summary_sentences": ["Towards multiverse databases Marzoev et al., HotOS’19  A typical backing store for a web application contains data for many users.", "The application makes queries on behalf of an authenticated user, but it is up to the application itself to make sure that the user only sees data they are entitled to see.", "Any frontend can access the whole store, regardless of the application user consuming the results.", "Therefore, frontend code is responsible for permission checks and privacy-preserving transformations that protect user’s data.", "This is dangerous and error-prone, and has caused many real-world bugs… the trusted computing base (TCB) effectively includes the entire application.", "The central idea behind multiverse databases is to push the data access and privacy rules into the database itself.", "The database takes on responsibility for authorization and transformation, and the application retains responsibility only for authentication and correct delegation of the authenticated principal on a database call.", "Such a design rules out an entire class of application errors, protecting private data from accidentally leaking.", "It would be safer and easier to specify and transparently enforce access policies once, at the shared backend store interface.", "Although state-of-the-are databases have security features designed for exactly this purpose, such as row-level access policies and grants of views, these features are too limiting for many web applications.", "In particular, data-dependent privacy policies may not fit neatly into row- or column-level access controls, and it may be permissible to expose aggregate or transformed information that traditional access control would prevent.", "With multiverse databases, each user sees a consistent “parallel universe” database containing only the data that user is allowed to see.", "Thus an application can issue any query, and we can rest safe in the knowledge that it will only see permitted data.", "The challenging thing of course, is efficiently maintaining all of these parallel universes.", "We’ll get to that, but first let’s look at some examples of privacy policies and how they can be expressed.", "Expressing privacy policies  In the prototype implementation, policies are expressed in a language similar to Google Cloud Firestore security rules.", "A policy just needs to be a deterministic function of a given update’s record data and the database contents.", "Today the following are supported:  Row suppression policies (e.g. exclude rows matching this pattern)  Column rewrite policies (e.g.", "translate / mask values)  Group policies, supporting role-based (i.e., data-dependent access controls)  Aggregation policies, which restrict a universe to see certain tables or columns only in aggregated or differentially private form.", "Consider a class discussion forum application (e.g. Piazza) in which students can post questions that are anonymous to other students, but not anonymous to instructors.", "We can express this policy with a combination of row suppression and column rewriting:  Maybe we want to allow teaching assistants (TAs) to see anonymous posts in the classes they teach.", "We can define a group via a membership condition and then attach policies to that group:  Write policies (not supported in the current implementation) permit specification of allowed updates.", "For example:  An aggregation policy could be used to rewrite any matching aggregation into a differentially-private version.", "The basis for this could be e.g. Chan et al.’s ‘ Private and continual release of statistics ’.", "Composing such policies with other policies remains an open research question.", "Managing universes  A multiverse database consists of a base universe, which represents the database without any read-side privacy policies applied, and many user universes, which are transformed copies of the database.", "For good query performance we’d like to pre-compute these per-user universes.", "If we do that naively though, we’re going to end up with a lot of universes to store and maintain and the storage requirements alone will be prohibitive.", "A space- and compute-efficient multiverse database clearly cannot materialize all user universes in their entirety, and must support high-performance incremental updates to the user universes.", "It therefore requires partially-materialized views that support high-performance updates.", "Recent research has provided this missing key primitive.", "Specifically, scalable, parallel streaming dataflow computing systems now support partially-stateful and dynamically-changing dataflows.", "These ideas make an efficient multiverse database possible.", "So, we make the database tables in the base universe be the root vertices of a dataflow, and as the base universe is updated records move through the flow into user universes.", "Where an edge in the dataflow graph crosses a universe boundary, any necessary dataflow operators to enforce the required privacy policies are inserted.", "All applicable policies are applied on every edge that transitions into a given user universe, so whichever path data takes to get there we know the policies will have been enforced.", "We can build the dataflow graph up dynamically, extending the flow’s for a user’s universe the first time a query is executed.", "The amount of computation required on a base update can be reduced by sharing computation and cached data between universes.", "Implementing this as a joint partially-stateful dataflow is the key to doing this safely.", "By reasoning about all users’ queries as a joint dataflow, the system can detect such sharing: when identical dataflow paths exist, they can be merged.", "Logically distinct, but functionally equivalent dataflow vertices can also share a common backing store.", "Any record reaching such a vertex in a given universe implies that universe has access to it, so the system can safely expose the shared copy.", "Just as user universes can be created on demand, so inactive universes can be destroyed on demand as well.", "Under the covers, these are all manipulations of the dataflow graph, which partially-stateful dataflow can support without downtime.", "Prototype evaluation  The authors have built a prototype implementation of these ideas based on the Noria dataflow engine.", "It runs to about 2,000 lines of Rust.", "A Piazza-style class forum discussion application with 1M posts, 1,000 classes, and a privacy policy allowing TAs to see anonymous posts is used as the basis for benchmarking.", "The team compare the prototype with 5,000 active user universes, a MySQL implementation with inlined privacy policies (‘with AP’) and a MySQL implementation that does not enforce the privacy policy (‘without AP’):  Since the prototype is serving reads from a pre-computed universe stored in memory cached results are fast and make for a very favourable comparison against MySQL.", "Writes are significantly slower though (about 2x) – much of this overhead is in the implementation rather than essential.", "Memory footprint is 0.5GB with one universe, and 1.1GB with 5,000 universes, introduces a shared record store for identical queries reduces their space footprint by 94%.", "These results are encouraging, but a realistic multiverse database must further reduce memory overhead and efficiently run millions of user universes across machines.", "Neither Noria nor any other current dataflow system support execution of the huge dataflows that such a deployment requires.", "In particular, changes to the dataflow must avoid full traversals of the dataflow graph for faster universe creation.", "Support for write authorization policies (with some tricky consistency considerations for data-dependent policies) is future work, as is the development of a policy-checker (perhaps similar to Amazon’s SMT-based policy checker for AWS ) to help ensure policies themselves are consistent and complete.", "Our initial results indicate that a large, dynamic, and partially-stateful dataflow can support practical multiverse databases that are easy to use and achieve good performance and acceptable overheads.", "We are excited to further explore the multiverse database paradigm and associated research directions."], "summary_text": "Towards multiverse databases Marzoev et al., HotOS’19  A typical backing store for a web application contains data for many users. The application makes queries on behalf of an authenticated user, but it is up to the application itself to make sure that the user only sees data they are entitled to see. Any frontend can access the whole store, regardless of the application user consuming the results. Therefore, frontend code is responsible for permission checks and privacy-preserving transformations that protect user’s data. This is dangerous and error-prone, and has caused many real-world bugs… the trusted computing base (TCB) effectively includes the entire application. The central idea behind multiverse databases is to push the data access and privacy rules into the database itself. The database takes on responsibility for authorization and transformation, and the application retains responsibility only for authentication and correct delegation of the authenticated principal on a database call. Such a design rules out an entire class of application errors, protecting private data from accidentally leaking. It would be safer and easier to specify and transparently enforce access policies once, at the shared backend store interface. Although state-of-the-are databases have security features designed for exactly this purpose, such as row-level access policies and grants of views, these features are too limiting for many web applications. In particular, data-dependent privacy policies may not fit neatly into row- or column-level access controls, and it may be permissible to expose aggregate or transformed information that traditional access control would prevent. With multiverse databases, each user sees a consistent “parallel universe” database containing only the data that user is allowed to see. Thus an application can issue any query, and we can rest safe in the knowledge that it will only see permitted data. The challenging thing of course, is efficiently maintaining all of these parallel universes. We’ll get to that, but first let’s look at some examples of privacy policies and how they can be expressed. Expressing privacy policies  In the prototype implementation, policies are expressed in a language similar to Google Cloud Firestore security rules. A policy just needs to be a deterministic function of a given update’s record data and the database contents. Today the following are supported:  Row suppression policies (e.g. exclude rows matching this pattern)  Column rewrite policies (e.g. translate / mask values)  Group policies, supporting role-based (i.e., data-dependent access controls)  Aggregation policies, which restrict a universe to see certain tables or columns only in aggregated or differentially private form. Consider a class discussion forum application (e.g. Piazza) in which students can post questions that are anonymous to other students, but not anonymous to instructors. We can express this policy with a combination of row suppression and column rewriting:  Maybe we want to allow teaching assistants (TAs) to see anonymous posts in the classes they teach. We can define a group via a membership condition and then attach policies to that group:  Write policies (not supported in the current implementation) permit specification of allowed updates. For example:  An aggregation policy could be used to rewrite any matching aggregation into a differentially-private version. The basis for this could be e.g. Chan et al.’s ‘ Private and continual release of statistics ’. Composing such policies with other policies remains an open research question. Managing universes  A multiverse database consists of a base universe, which represents the database without any read-side privacy policies applied, and many user universes, which are transformed copies of the database. For good query performance we’d like to pre-compute these per-user universes. If we do that naively though, we’re going to end up with a lot of universes to store and maintain and the storage requirements alone will be prohibitive. A space- and compute-efficient multiverse database clearly cannot materialize all user universes in their entirety, and must support high-performance incremental updates to the user universes. It therefore requires partially-materialized views that support high-performance updates. Recent research has provided this missing key primitive. Specifically, scalable, parallel streaming dataflow computing systems now support partially-stateful and dynamically-changing dataflows. These ideas make an efficient multiverse database possible. So, we make the database tables in the base universe be the root vertices of a dataflow, and as the base universe is updated records move through the flow into user universes. Where an edge in the dataflow graph crosses a universe boundary, any necessary dataflow operators to enforce the required privacy policies are inserted. All applicable policies are applied on every edge that transitions into a given user universe, so whichever path data takes to get there we know the policies will have been enforced. We can build the dataflow graph up dynamically, extending the flow’s for a user’s universe the first time a query is executed. The amount of computation required on a base update can be reduced by sharing computation and cached data between universes. Implementing this as a joint partially-stateful dataflow is the key to doing this safely. By reasoning about all users’ queries as a joint dataflow, the system can detect such sharing: when identical dataflow paths exist, they can be merged. Logically distinct, but functionally equivalent dataflow vertices can also share a common backing store. Any record reaching such a vertex in a given universe implies that universe has access to it, so the system can safely expose the shared copy. Just as user universes can be created on demand, so inactive universes can be destroyed on demand as well. Under the covers, these are all manipulations of the dataflow graph, which partially-stateful dataflow can support without downtime. Prototype evaluation  The authors have built a prototype implementation of these ideas based on the Noria dataflow engine. It runs to about 2,000 lines of Rust. A Piazza-style class forum discussion application with 1M posts, 1,000 classes, and a privacy policy allowing TAs to see anonymous posts is used as the basis for benchmarking. The team compare the prototype with 5,000 active user universes, a MySQL implementation with inlined privacy policies (‘with AP’) and a MySQL implementation that does not enforce the privacy policy (‘without AP’):  Since the prototype is serving reads from a pre-computed universe stored in memory cached results are fast and make for a very favourable comparison against MySQL. Writes are significantly slower though (about 2x) – much of this overhead is in the implementation rather than essential. Memory footprint is 0.5GB with one universe, and 1.1GB with 5,000 universes, introduces a shared record store for identical queries reduces their space footprint by 94%. These results are encouraging, but a realistic multiverse database must further reduce memory overhead and efficiently run millions of user universes across machines. Neither Noria nor any other current dataflow system support execution of the huge dataflows that such a deployment requires. In particular, changes to the dataflow must avoid full traversals of the dataflow graph for faster universe creation. Support for write authorization policies (with some tricky consistency considerations for data-dependent policies) is future work, as is the development of a policy-checker (perhaps similar to Amazon’s SMT-based policy checker for AWS ) to help ensure policies themselves are consistent and complete. Our initial results indicate that a large, dynamic, and partially-stateful dataflow can support practical multiverse databases that are easy to use and achieve good performance and acceptable overheads. We are excited to further explore the multiverse database paradigm and associated research directions.", "pdf_url": "https://people.csail.mit.edu/malte/pub/papers/2019-hotos-multiversedb.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/towards-multiverse-databases.json"}
{"id": "84655302", "bin": "1300_1400", "summary_sentences": ["Skyway: connecting managed heaps in distributed big data systems Nguyen et al., ASPLOS’18  Yesterday we saw how to make Java objects persistent using NVM-backed heaps with Espresso.", "One of the drawbacks of using that as a persistence mechanism is that they’re only stored in the memory of a single node.", "If only there was some way to create a cluster of JVMs, and efficiently copy objects across remote heaps in the cluster… Meet Skyway!", "Skyway is aimed at JVM-based big data systems (think Spark, Flink) that end up spending a lot of their time serializing and deserializing objects to move them around the cluster (e.g., to and from workers – see ‘ Making sense of performance in data analytics frameworks ’).", "Java comes with a default serialization mechanism, and there are also many third party libraries.", "Kryo is the recommended library for use with Spark.", "Consider a small Spark cluster (3 worker nodes each with a 20 GB heap) running a triangle counting algorithm over the LiveJournal graph (about 1.2GB).", "With both the standard Java serializers and Kryo, serialization and deserialization combined account for a significant portion of the overall execution time (more than 30%).", "Where does all the time go?", "To transfer an object o from one JVM to another takes three steps:  A serialization procedure turns the whole object graph reachable from o into a binary sequence.", "During this process the serializer extracts the object data, strips the header, removes all references stored in an object, and changes the representation of certain metadata.", "The byte sequence is sent across the wire  A deserialization procedure reads the byte sequence, creates objects accordingly, and rebuilds the object graph in the managed heap of the receiver machine.", "In a big data system, a transfer can involve millions of objects, which means invoking, e.g., reflection APIs millions of times or more.", "Moreover, the Java serializer represents every type by a string containing the name of a class and all its superclasses.", "These type strings can consume a huge portion of the byte sequence transferred across the network, and reflection has to be used on the receiver end to resolve types from the string.", "Reflection is also heavily used when repairing object references in the graph on the receiving end.", "The key problem with existing S/D (serialization/deserialization) libraries is that, with an existing JVM, there are no alternative routes to transfer objects other than first disassembling and pushing them down to a (different) binary format, and the reassembling and pulling them back up into a remote heap.", "In this paper, we advocate to build a “skyway” between managed heaps so that data objects no longer need to pushed down to a lower level for transfer.", "At a high level, Skyway is fairly easy to understand.", "It extends the JVM (OpenJDK) to enable object graphs to be moved as is from one heap to another, and immediately used on a remote node right after the move.", "Given a root object o, Skyway performs a GC-like traversal copying every reachable object into an output buffer while performing only a very lightweight adjustment to machine-dependent metadata.", "Crucially, the object format is not changed and every object is transferred as a whole.", "This includes the hashcode, so that hash-based data structures can be used on the receiver node without rehashing.", "Types are represented by a global type-numbering procedure which assumes a master-workers pattern and keeps a registry of all types and their ids at the master.", "Workers communicate with the master to obtain ids for classes upon class loading.", "Absolute addresses in objects are turned into relative addresses when copied into the output buffer.", "The output buffer is streamed to an input buffer at the remote node, where the relative addresses are turned back into absolute addresses in the target heap.", "…data processing applications frequently shuffle many millions of objects and do so in strongly delimited phases.", "Hence, sending objects in batch without changing their formats provides significant execution efficiency.", "Second, the use of modern network technology enables extra bytes to be quickly transferred without incurring much overhead.", "Skyway under the covers  Skyway uses a GC-like mechanism to discover the object graph reachable from a set of root objects.", "Objects encountered during the traversal are copied into an output buffer (located in off-heap native memory so it doesn’t interfere with GC), which is streamed to the corresponding buffer(s) on the receiving node.", "Input buffers are allocated in the old generation (tenured) of the managed heap, and  can span multiple memory chunks – handy since you don’t always know the ultimate size of the buffer when streaming starts.", "Root objects in a stream are demarcated by special top marks which helps the receiver to efficiently read entire graphs without needing to parse all of their content.", "Once a data transfer is complete, Skyway updates the card table of the Parallel Scavenge GC making the new objects reachable via garbage collection.", "Skyway can support heterogeneous clusters, where JVMs may have different object formats, by adjusting the format of objects as they are copied into the output buffer.", "The implementation on top of OpenJDK 1.8.0 touches the classloader subsystem, the object/heap layout, and the Parallel Scavenge garbage collector.", "Skyway develops a distributed type-registration system that automatically allows different representations of the same class on different JVM instances to share the same integer ID.", "This system completely eliminates the need to represent types during data transfer…  The driver / master JVM maintains a complete type registry covering all classes that have been loaded in the cluster, initially populated by scanning its own loaded classes after JVM startup.", "When a worker JVM starts up it requests a copy of the registry from the driver, giving it a view of all classes loaded in the cluster to that point.", "If a worker JVM loads a class that does not yet exist in its local registry view it checks with the driver to obtain an ID for it.", "Whereas the standard Java serializer sends a type string over the network with every object, Skyway sends a type string at most once for every class on each machine during the entire computation.", "Performance evaluation  The first evaluation compares Skyway against 90 existing serialization libraries using the Java serializer benchmark set (JSBS).", "Results for the fastest 28 are shown below.", "Skyway is the fastest of the lot: 2.2x faster than Kryo-manual (intrusive), and 67x faster than the Java serializer.", "The next experiment modifies Spark (v1.2.0) to replace the use of Kryo-manual with the Skyway library.", "Four programs (Word Count, Page Rank, Connected Components, and Triangle Counting) are each run over four different graph inputs:  The following charts summarise the performance of Java serialisation, Kryo, and Skyway for each of the programs across each of the input graphs:  Skyway makes Spark run 36% faster than the Java serializer, and 16% faster than Kryo.", "Skyway is also evaluated against Flink 1.3.2 (released Aug 2017) in a batch processing mode.", "The TPC-H data generator is used to generate a 100GB input dataset, and 5 representative TPC-H queries are transformed into Flink applications.", "The performance of Skyway vs Flink’s built-in serializer is show below:  Skyway improves Flink’s performance by 19% on average.", "Our evaluation shows that Skyway outperforms all existing S/D libraries and improves widely-deployed systems such as Spark and Flink."], "summary_text": "Skyway: connecting managed heaps in distributed big data systems Nguyen et al., ASPLOS’18  Yesterday we saw how to make Java objects persistent using NVM-backed heaps with Espresso. One of the drawbacks of using that as a persistence mechanism is that they’re only stored in the memory of a single node. If only there was some way to create a cluster of JVMs, and efficiently copy objects across remote heaps in the cluster… Meet Skyway! Skyway is aimed at JVM-based big data systems (think Spark, Flink) that end up spending a lot of their time serializing and deserializing objects to move them around the cluster (e.g., to and from workers – see ‘ Making sense of performance in data analytics frameworks ’). Java comes with a default serialization mechanism, and there are also many third party libraries. Kryo is the recommended library for use with Spark. Consider a small Spark cluster (3 worker nodes each with a 20 GB heap) running a triangle counting algorithm over the LiveJournal graph (about 1.2GB). With both the standard Java serializers and Kryo, serialization and deserialization combined account for a significant portion of the overall execution time (more than 30%). Where does all the time go? To transfer an object o from one JVM to another takes three steps:  A serialization procedure turns the whole object graph reachable from o into a binary sequence. During this process the serializer extracts the object data, strips the header, removes all references stored in an object, and changes the representation of certain metadata. The byte sequence is sent across the wire  A deserialization procedure reads the byte sequence, creates objects accordingly, and rebuilds the object graph in the managed heap of the receiver machine. In a big data system, a transfer can involve millions of objects, which means invoking, e.g., reflection APIs millions of times or more. Moreover, the Java serializer represents every type by a string containing the name of a class and all its superclasses. These type strings can consume a huge portion of the byte sequence transferred across the network, and reflection has to be used on the receiver end to resolve types from the string. Reflection is also heavily used when repairing object references in the graph on the receiving end. The key problem with existing S/D (serialization/deserialization) libraries is that, with an existing JVM, there are no alternative routes to transfer objects other than first disassembling and pushing them down to a (different) binary format, and the reassembling and pulling them back up into a remote heap. In this paper, we advocate to build a “skyway” between managed heaps so that data objects no longer need to pushed down to a lower level for transfer. At a high level, Skyway is fairly easy to understand. It extends the JVM (OpenJDK) to enable object graphs to be moved as is from one heap to another, and immediately used on a remote node right after the move. Given a root object o, Skyway performs a GC-like traversal copying every reachable object into an output buffer while performing only a very lightweight adjustment to machine-dependent metadata. Crucially, the object format is not changed and every object is transferred as a whole. This includes the hashcode, so that hash-based data structures can be used on the receiver node without rehashing. Types are represented by a global type-numbering procedure which assumes a master-workers pattern and keeps a registry of all types and their ids at the master. Workers communicate with the master to obtain ids for classes upon class loading. Absolute addresses in objects are turned into relative addresses when copied into the output buffer. The output buffer is streamed to an input buffer at the remote node, where the relative addresses are turned back into absolute addresses in the target heap. …data processing applications frequently shuffle many millions of objects and do so in strongly delimited phases. Hence, sending objects in batch without changing their formats provides significant execution efficiency. Second, the use of modern network technology enables extra bytes to be quickly transferred without incurring much overhead. Skyway under the covers  Skyway uses a GC-like mechanism to discover the object graph reachable from a set of root objects. Objects encountered during the traversal are copied into an output buffer (located in off-heap native memory so it doesn’t interfere with GC), which is streamed to the corresponding buffer(s) on the receiving node. Input buffers are allocated in the old generation (tenured) of the managed heap, and  can span multiple memory chunks – handy since you don’t always know the ultimate size of the buffer when streaming starts. Root objects in a stream are demarcated by special top marks which helps the receiver to efficiently read entire graphs without needing to parse all of their content. Once a data transfer is complete, Skyway updates the card table of the Parallel Scavenge GC making the new objects reachable via garbage collection. Skyway can support heterogeneous clusters, where JVMs may have different object formats, by adjusting the format of objects as they are copied into the output buffer. The implementation on top of OpenJDK 1.8.0 touches the classloader subsystem, the object/heap layout, and the Parallel Scavenge garbage collector. Skyway develops a distributed type-registration system that automatically allows different representations of the same class on different JVM instances to share the same integer ID. This system completely eliminates the need to represent types during data transfer…  The driver / master JVM maintains a complete type registry covering all classes that have been loaded in the cluster, initially populated by scanning its own loaded classes after JVM startup. When a worker JVM starts up it requests a copy of the registry from the driver, giving it a view of all classes loaded in the cluster to that point. If a worker JVM loads a class that does not yet exist in its local registry view it checks with the driver to obtain an ID for it. Whereas the standard Java serializer sends a type string over the network with every object, Skyway sends a type string at most once for every class on each machine during the entire computation. Performance evaluation  The first evaluation compares Skyway against 90 existing serialization libraries using the Java serializer benchmark set (JSBS). Results for the fastest 28 are shown below. Skyway is the fastest of the lot: 2.2x faster than Kryo-manual (intrusive), and 67x faster than the Java serializer. The next experiment modifies Spark (v1.2.0) to replace the use of Kryo-manual with the Skyway library. Four programs (Word Count, Page Rank, Connected Components, and Triangle Counting) are each run over four different graph inputs:  The following charts summarise the performance of Java serialisation, Kryo, and Skyway for each of the programs across each of the input graphs:  Skyway makes Spark run 36% faster than the Java serializer, and 16% faster than Kryo. Skyway is also evaluated against Flink 1.3.2 (released Aug 2017) in a batch processing mode. The TPC-H data generator is used to generate a 100GB input dataset, and 5 representative TPC-H queries are transformed into Flink applications. The performance of Skyway vs Flink’s built-in serializer is show below:  Skyway improves Flink’s performance by 19% on average. Our evaluation shows that Skyway outperforms all existing S/D libraries and improves widely-deployed systems such as Spark and Flink.", "pdf_url": "https://people.cs.uchicago.edu/~shanlu/paper/asplos18_skyway.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/skyway-connecting-managed-heaps-in-distributed-big-data-systems.json"}
{"id": "36051826", "bin": "1300_1400", "summary_sentences": ["Compress objects, not cache lines: an object-based compressed memory hierarchy Tsai & Sanchez, ASPLOS’19  Last time out we saw how Google have been able to save millions of dollars though memory compression enabled via zswap.", "One of the important attributes of their design was easy and rapid deployment across an existing fleet.", "Today’s paper introduces Zippads, which compared to a state of the art compressed memory hierarchy is able to achieve a 1.63x higher compression ratio and improve performance by 17%.", "The big idea behind zippads is simple and elegant, but the ramifications go deep: all the way down to a modified instruction set (ISA)!", "So while you probably won’t be using Zippads in practice anytime soon, it’s a wonderful example of what’s possible when you’re prepared to take a fresh look at “the way we’ve always done things.”  The big idea  Existing cache and main memory compression techniques compress data in small fixed-size blocks, typically cache lines.", "Moreover, they use simple compression algorithms that focus on exploiting redundancy within a block.", "These techniques work well for scientific programs that are dominated by arrays.", "However, they are ineffective on object-based programs because objects do not fall neatly into fixed-size blocks and have a more irregular layout.", "Scientific computing and machine learning applications may be heavily array dominated, but many applications are dominated by objects, which have a much more irregular layout and very different access patterns.", "Looking across a set of eight Java benchmarks, we find that only two of them are array dominated, the rest having between 40% to 75% of the heap footprint allocated to objects, the vast majority of which are small.", "Existing compression algorithms that rely on similarities between nearby words won’t work as well on these applications.", "There are two big “Aha!” moments that underpin this work.", "The first one is that object-based applications perform memory accesses within objects and follow pointers to other objects:  Therefore objects, not cache lines, are the right compression unit.", "The second insight is that although nearby words may not look similar, different instances of the same object type do.", "Consider a B-Tree node from the B-tree Java benchmark:  Uncompressed, it’s memory layout looks like (a) below.", "Compressed using LCP (a hybrid BDI + FPC compression technique) we can achieve a 10% compression ratio as shown in (b).", "If we compress objects instead of cache lines though, we can get to a 56% compression ratio (c).", "Finally, if we also use cross-object compression in which a base object is stored for each object type, and only the delta from the base object is stored for every instance, then we can get the compression ratio up as high as 95% (d)!", "Implications  … to realize these insights, hardware needs to access data at object granularity and must have control over pointers between objects.", "This is where Hotpads comes in.", "Hotpads is a hardware-managed hierarchy of scratchpad-like memories called pads.", "Pads are designed to store variable sized objects efficiently, and a key feature is that they transfer objects across pad levels implicitly (just like cache levels) based on memory accesses.", "Objects are first allocated in the L1 pad and move up the hierarchy as they are evicted.", "Short-lived objects may be garbage collected before they ever reach main memory.", "The highest hierarchy level an object has reached is called its canonical level, and this level acts as the object’s backing store.", "Hotpad modifies the ISA to make all this work (see Table 2 below).", "The ISA changes are transparent to application programmers, but require runtime and compiler modifications.", "Collection evictions that move objects up the hierarchy occur entirely in hardware and are much faster than software GT because pads are small.", "Eviction cost is proportional to pad size, due to the constraint that objects may only point to objects at the same or higher canonical level.", "Thus we know when evicting an object from a smaller pad that no object at a larger pad can be holding a reference to it.", "Zippads  Zippads extends Hotpads with compression.", "It can work with conventional compression algorithms such as BDI and FPC, but it works best with the COCO cross-object compression scheme.", "New objects start their lifetime uncompressed, and if and when they make to to the last-level pad they are compressed there.", "This is a key difference from conventional hierarchies, where objects are mapped to a main memory address to begin with, forcing the problem of translating from uncompressed to compressed addresses.", "Compression information is encoded directly into pointers:  On dirty writebacks we have to consider the scenario whereby the new size of the compressed object no longer fits in its location.", "In this case Zippads allocates a new location and leaves behind a forwarding thunk in the old location.", "Overflows are rare in practice.", "Zippads also enhances Hotpad’s periodic garbage collection to work on compressed objects.", "COCO  COCO compresses an object by storing only the bytes that differ from a given base object.", "The compressed object format has three elements:  A base object id  A diff bitmap with one bit per byte of the object.", "Bit i is set if the ith byte differs from the base object.", "A string of byte diffs containing the bytes that are different from the base object.", "All of this happens in hardware:  COCO compression/decompression circuits are simple to implement and only require narrow comparators and multiplexors.", "Our implementations compress/decompress one word (8 bytes) per cycle… We have written the RTL for these circuits and synthesized them at 45nm using yosys and the FreePDK45 standard cell library.", "The process for selecting the base object for each type is simple: COCO simply uses the first object of each type that it sees.", "Base objects themselves are kept in a small and fast base object cache so that COCO has easy and fast access to them.", "What about arrays?", "We said at the start of all this that objects and arrays work best with different kinds of compression.", "We want Zippads to compress both well.", "Zippads addresses this by using different compression algorithms: COCO for objects and a conventional BDI+FPC algorithm for arrays.", "Metadata encoding the compression algorithm used is encoded in the pointers.", "Evaluation  Zippads and COCO are evaluated on a mix of array-based and object-based workloads, including eight Java and two C/C++ benchmarks.", "Zippads achieves the best compression ratios across all of the Java benchmarks.", "In the figure below, CMH is a state-of-art compressed memory hierarchy, hotpads is plain hotpads (no compression) and a three-level cache hierarchy, Zippads-BF is Zippads without COCO, and Zippads is the full Zippads with dynamic selection of compression algorithm including COCO.", "Zippads also has the best reduction in main memory traffic : halving the amount of traffic compared to the baseline.", "Not only that, but Zippads improves performance over the baseline by 30% as well (and by 17% when compared against CMH).", "As the following plots show, it’s also highly effective on C/C++ benchmarks.", "The bottom line:  Zippads + COCO improves compression ratio over a combination of state-of-the-are techniques by up to 2x and by 1.63x on average.", "It also reduces memory traffic by 56% and improves performance by 17%."], "summary_text": "Compress objects, not cache lines: an object-based compressed memory hierarchy Tsai & Sanchez, ASPLOS’19  Last time out we saw how Google have been able to save millions of dollars though memory compression enabled via zswap. One of the important attributes of their design was easy and rapid deployment across an existing fleet. Today’s paper introduces Zippads, which compared to a state of the art compressed memory hierarchy is able to achieve a 1.63x higher compression ratio and improve performance by 17%. The big idea behind zippads is simple and elegant, but the ramifications go deep: all the way down to a modified instruction set (ISA)! So while you probably won’t be using Zippads in practice anytime soon, it’s a wonderful example of what’s possible when you’re prepared to take a fresh look at “the way we’ve always done things.”  The big idea  Existing cache and main memory compression techniques compress data in small fixed-size blocks, typically cache lines. Moreover, they use simple compression algorithms that focus on exploiting redundancy within a block. These techniques work well for scientific programs that are dominated by arrays. However, they are ineffective on object-based programs because objects do not fall neatly into fixed-size blocks and have a more irregular layout. Scientific computing and machine learning applications may be heavily array dominated, but many applications are dominated by objects, which have a much more irregular layout and very different access patterns. Looking across a set of eight Java benchmarks, we find that only two of them are array dominated, the rest having between 40% to 75% of the heap footprint allocated to objects, the vast majority of which are small. Existing compression algorithms that rely on similarities between nearby words won’t work as well on these applications. There are two big “Aha!” moments that underpin this work. The first one is that object-based applications perform memory accesses within objects and follow pointers to other objects:  Therefore objects, not cache lines, are the right compression unit. The second insight is that although nearby words may not look similar, different instances of the same object type do. Consider a B-Tree node from the B-tree Java benchmark:  Uncompressed, it’s memory layout looks like (a) below. Compressed using LCP (a hybrid BDI + FPC compression technique) we can achieve a 10% compression ratio as shown in (b). If we compress objects instead of cache lines though, we can get to a 56% compression ratio (c). Finally, if we also use cross-object compression in which a base object is stored for each object type, and only the delta from the base object is stored for every instance, then we can get the compression ratio up as high as 95% (d)! Implications  … to realize these insights, hardware needs to access data at object granularity and must have control over pointers between objects. This is where Hotpads comes in. Hotpads is a hardware-managed hierarchy of scratchpad-like memories called pads. Pads are designed to store variable sized objects efficiently, and a key feature is that they transfer objects across pad levels implicitly (just like cache levels) based on memory accesses. Objects are first allocated in the L1 pad and move up the hierarchy as they are evicted. Short-lived objects may be garbage collected before they ever reach main memory. The highest hierarchy level an object has reached is called its canonical level, and this level acts as the object’s backing store. Hotpad modifies the ISA to make all this work (see Table 2 below). The ISA changes are transparent to application programmers, but require runtime and compiler modifications. Collection evictions that move objects up the hierarchy occur entirely in hardware and are much faster than software GT because pads are small. Eviction cost is proportional to pad size, due to the constraint that objects may only point to objects at the same or higher canonical level. Thus we know when evicting an object from a smaller pad that no object at a larger pad can be holding a reference to it. Zippads  Zippads extends Hotpads with compression. It can work with conventional compression algorithms such as BDI and FPC, but it works best with the COCO cross-object compression scheme. New objects start their lifetime uncompressed, and if and when they make to to the last-level pad they are compressed there. This is a key difference from conventional hierarchies, where objects are mapped to a main memory address to begin with, forcing the problem of translating from uncompressed to compressed addresses. Compression information is encoded directly into pointers:  On dirty writebacks we have to consider the scenario whereby the new size of the compressed object no longer fits in its location. In this case Zippads allocates a new location and leaves behind a forwarding thunk in the old location. Overflows are rare in practice. Zippads also enhances Hotpad’s periodic garbage collection to work on compressed objects. COCO  COCO compresses an object by storing only the bytes that differ from a given base object. The compressed object format has three elements:  A base object id  A diff bitmap with one bit per byte of the object. Bit i is set if the ith byte differs from the base object. A string of byte diffs containing the bytes that are different from the base object. All of this happens in hardware:  COCO compression/decompression circuits are simple to implement and only require narrow comparators and multiplexors. Our implementations compress/decompress one word (8 bytes) per cycle… We have written the RTL for these circuits and synthesized them at 45nm using yosys and the FreePDK45 standard cell library. The process for selecting the base object for each type is simple: COCO simply uses the first object of each type that it sees. Base objects themselves are kept in a small and fast base object cache so that COCO has easy and fast access to them. What about arrays? We said at the start of all this that objects and arrays work best with different kinds of compression. We want Zippads to compress both well. Zippads addresses this by using different compression algorithms: COCO for objects and a conventional BDI+FPC algorithm for arrays. Metadata encoding the compression algorithm used is encoded in the pointers. Evaluation  Zippads and COCO are evaluated on a mix of array-based and object-based workloads, including eight Java and two C/C++ benchmarks. Zippads achieves the best compression ratios across all of the Java benchmarks. In the figure below, CMH is a state-of-art compressed memory hierarchy, hotpads is plain hotpads (no compression) and a three-level cache hierarchy, Zippads-BF is Zippads without COCO, and Zippads is the full Zippads with dynamic selection of compression algorithm including COCO. Zippads also has the best reduction in main memory traffic : halving the amount of traffic compared to the baseline. Not only that, but Zippads improves performance over the baseline by 30% as well (and by 17% when compared against CMH). As the following plots show, it’s also highly effective on C/C++ benchmarks. The bottom line:  Zippads + COCO improves compression ratio over a combination of state-of-the-are techniques by up to 2x and by 1.63x on average. It also reduces memory traffic by 56% and improves performance by 17%.", "pdf_url": "https://people.csail.mit.edu/poantsai/papers/2019.zippads.asplos.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/zippads.json"}
{"id": "73515355", "bin": "1300_1400", "summary_sentences": ["Choosing a cloud DBMS: architectures and tradeoffs Tan et al., VLDB’19  If you’re moving an OLAP workload to the cloud (AWS in the context of this paper), what DBMS setup should you go with?", "There’s a broad set of choices including where you store the data, whether you run your own DBMS nodes or use a service, the kinds of instance types to go for if you do run your own, and so forth.", "Tan et al. use the TPC-H benchmark to assess Redshift, Redshift Spectrum, Athena, Presto, Hive, and Vertica to find out what works best and the trade-offs involved.", "We focused on OLAP-oriented parallel data warehouse products available for AWS and restricted our attention to commercially available systems.", "As it is infeasible to test every OLAP system runnable on AWS, we chose widely-used systems that represented a variety of architectures and cost models.", "My key takeaways as a TL;DR:  Store your data in S3  Use portable data format that gives you future flexibility to process it with multiple different systems (e.g. ORC or Parquet)  Use Athena for workloads it can support (Athena could not run 4 of the 22 TPC-H queries, and Spectrum could not run 2 of them), especially if you are doing less frequent ad-hoc queries.", "Which I’m quite happy to see as my most recent data pipeline is based around Lambda, S3, and Athena, and it’s been working great for my use case.", "The design space  We group the DBMS design choices and tradeoffs into three broad categories, which result from the need for dealing with (A) external storage; (B) query executors that are spun on demand; and (C) DBMS-as-a-service offerings.", "With regard to external storage, you could use S3 with remote storage accessible over a REST API, or block-based storage with EBS and Instance Store (InS), with EBS being the closest match for traditional database systems.", "InS does now offer an NVMe variant too, and the authors perform limited testing on that as well.", "For query executors that can be frequently started and stopped the authors explore performance with cold and warm caches (where applicable), and also the horizontal and vertical scaling performance.", "For the as-a-service offerings, what levels of flexibility do they offer compared to running your own systems, and how do they compare on cost?", "The test  The tests were done using 1000 scale factor TPC-H data (1TB uncompressed) – large enough to be I/O constrained yet still enabling queries to complete in seconds to minutes.", "Each systems begins from a cold start unless explicitly stated otherwise in the results.", "For S3 tests, data  was stored in ORC format, apart from for Vertica which used its own proprietary format.", "For those systems where you provide your own compute instances, the default configuration tested used a 4-node r4.8xlarge cluster with 10Gb/s networking.", "For cost calculations, the costs are a combination of compute costs, storage costs, data scan costs, and software license costs.", "Key findings  The experimental results focus on six main areas of comparison:  query restrictions  system initialisation time  query performance  cost  data compatibility with other systems  scalability  Query restrictions  Neither Spectrum nor Athena could run the full TPC-H query suite.", "On Athena, Q2 timed out after an hour, and Q8, Q9 and Q21 failed after exhausting resources.", "On Spectrum, Q15 failed because views are not supported, and Q22 failed because complex subqueries are not supported.", "(See §2.4 in the TPC-H Benchmark Standard for details of the queries).", "System initialisation time  Initialisation time measures how easy it is to launch and use a given DBMS (doesn’t apply to Athena, which is ‘always-on’).", "Most systems have initialisation times in the range of 5-7 minutes, with Redshift an outlier at around 12 minutes.", "It is advantageous in the cloud to shut down compute resources when they are not being used, but there is then a query latency cost.", "All cloud nodes require time to initialize and pass system checks before a DBMS can be started, with systems using local storage like Redshift taking longest to initialize.", "Serverless oﬀerings like Athena provide an alternative “instant on” query service.", "Query performance  Query performance is measured from both warm and cold caches.", "Of the 16/22 queries that can be run across all of the system under test, Athena and Redshift are the best performers (though interestingly, not Redshift Spectrum).", "If you can afford to keep it running (no initialisation costs on subsequent queries), and with a warm cache, Redshift offers the best performance for frequent querying.", "The warm cache benefits across the board are not as great as might be expected, but this is a characteristic of the workload where most queries are not I/O bound and those that are tend to be shorter.", "Another interesting experiment here compared the effects on performance of different storage types.", "In general, InS performance is faster than EBS, and EBS is faster than S3 on the same system, as expected.", "However, the magnitude of the difference may be exaggerated depending on how well that system utilizes a specific storage type.", "The results found that there is a performance advantage from faster storage, but it’s not as big as you might think.", "Using cheap remote object storage instead of more expensive EBS block storage seems fine, and even on heavily I/O bound workloads the cost advantage of S3 far exceeds its performance disadvantage.", "Cost  The following charts show the query costs for both cold start and subsequent runs:  Redshift Spectrum stands out as the expensive option here!", "“Redshift is not a cost-effective system if one relies heavily on pulling data from S3.“  Athena’s cost-per-query is on a par with other systems, which coupled with its good query performance gives it very competitive cost/performance.", "Therefore, a cloud DBMS user should consider Athena as an option for select workloads and utilize storage that can be accessed by serverless options.", "When it comes to the storage part of the cost equation, using EBS is much more expensive than S3.", "“Our setup found a 50x storage cost increase for EBS while only providing a 2x performance speedup.“  Data compatibility  Because the cloud offers the ability to easily launch new systems, being able to leverage different systems for different workloads is advantageous.", "However, ETL costs can make some system types infeasible to use when workloads change, thereby limiting the cloud offerings one can leverage.", "Green cells in the compatibility matrix below show where systems can use compatible formats from the same storage system, e.g. ORC on S3.", "Scalability  We analyzed performance when scaling horizontally and vertically… The main theme is that horizontal scaling is generally advantageous, while vertical scaling is generally disadvantageous.", "Scaling tests on AWS proprietary system are limited by the options they provide to the end user.", "The last word  Each of these findings poses opportunities for future work to explore specific architectural tradeoffs further.", "Additionally, future studies could analyze concurrency, test a different suite such as TPC-DS, evaluate different data sizes, and evaluate more systems."], "summary_text": "Choosing a cloud DBMS: architectures and tradeoffs Tan et al., VLDB’19  If you’re moving an OLAP workload to the cloud (AWS in the context of this paper), what DBMS setup should you go with? There’s a broad set of choices including where you store the data, whether you run your own DBMS nodes or use a service, the kinds of instance types to go for if you do run your own, and so forth. Tan et al. use the TPC-H benchmark to assess Redshift, Redshift Spectrum, Athena, Presto, Hive, and Vertica to find out what works best and the trade-offs involved. We focused on OLAP-oriented parallel data warehouse products available for AWS and restricted our attention to commercially available systems. As it is infeasible to test every OLAP system runnable on AWS, we chose widely-used systems that represented a variety of architectures and cost models. My key takeaways as a TL;DR:  Store your data in S3  Use portable data format that gives you future flexibility to process it with multiple different systems (e.g. ORC or Parquet)  Use Athena for workloads it can support (Athena could not run 4 of the 22 TPC-H queries, and Spectrum could not run 2 of them), especially if you are doing less frequent ad-hoc queries. Which I’m quite happy to see as my most recent data pipeline is based around Lambda, S3, and Athena, and it’s been working great for my use case. The design space  We group the DBMS design choices and tradeoffs into three broad categories, which result from the need for dealing with (A) external storage; (B) query executors that are spun on demand; and (C) DBMS-as-a-service offerings. With regard to external storage, you could use S3 with remote storage accessible over a REST API, or block-based storage with EBS and Instance Store (InS), with EBS being the closest match for traditional database systems. InS does now offer an NVMe variant too, and the authors perform limited testing on that as well. For query executors that can be frequently started and stopped the authors explore performance with cold and warm caches (where applicable), and also the horizontal and vertical scaling performance. For the as-a-service offerings, what levels of flexibility do they offer compared to running your own systems, and how do they compare on cost? The test  The tests were done using 1000 scale factor TPC-H data (1TB uncompressed) – large enough to be I/O constrained yet still enabling queries to complete in seconds to minutes. Each systems begins from a cold start unless explicitly stated otherwise in the results. For S3 tests, data  was stored in ORC format, apart from for Vertica which used its own proprietary format. For those systems where you provide your own compute instances, the default configuration tested used a 4-node r4.8xlarge cluster with 10Gb/s networking. For cost calculations, the costs are a combination of compute costs, storage costs, data scan costs, and software license costs. Key findings  The experimental results focus on six main areas of comparison:  query restrictions  system initialisation time  query performance  cost  data compatibility with other systems  scalability  Query restrictions  Neither Spectrum nor Athena could run the full TPC-H query suite. On Athena, Q2 timed out after an hour, and Q8, Q9 and Q21 failed after exhausting resources. On Spectrum, Q15 failed because views are not supported, and Q22 failed because complex subqueries are not supported. (See §2.4 in the TPC-H Benchmark Standard for details of the queries). System initialisation time  Initialisation time measures how easy it is to launch and use a given DBMS (doesn’t apply to Athena, which is ‘always-on’). Most systems have initialisation times in the range of 5-7 minutes, with Redshift an outlier at around 12 minutes. It is advantageous in the cloud to shut down compute resources when they are not being used, but there is then a query latency cost. All cloud nodes require time to initialize and pass system checks before a DBMS can be started, with systems using local storage like Redshift taking longest to initialize. Serverless oﬀerings like Athena provide an alternative “instant on” query service. Query performance  Query performance is measured from both warm and cold caches. Of the 16/22 queries that can be run across all of the system under test, Athena and Redshift are the best performers (though interestingly, not Redshift Spectrum). If you can afford to keep it running (no initialisation costs on subsequent queries), and with a warm cache, Redshift offers the best performance for frequent querying. The warm cache benefits across the board are not as great as might be expected, but this is a characteristic of the workload where most queries are not I/O bound and those that are tend to be shorter. Another interesting experiment here compared the effects on performance of different storage types. In general, InS performance is faster than EBS, and EBS is faster than S3 on the same system, as expected. However, the magnitude of the difference may be exaggerated depending on how well that system utilizes a specific storage type. The results found that there is a performance advantage from faster storage, but it’s not as big as you might think. Using cheap remote object storage instead of more expensive EBS block storage seems fine, and even on heavily I/O bound workloads the cost advantage of S3 far exceeds its performance disadvantage. Cost  The following charts show the query costs for both cold start and subsequent runs:  Redshift Spectrum stands out as the expensive option here! “Redshift is not a cost-effective system if one relies heavily on pulling data from S3.“  Athena’s cost-per-query is on a par with other systems, which coupled with its good query performance gives it very competitive cost/performance. Therefore, a cloud DBMS user should consider Athena as an option for select workloads and utilize storage that can be accessed by serverless options. When it comes to the storage part of the cost equation, using EBS is much more expensive than S3. “Our setup found a 50x storage cost increase for EBS while only providing a 2x performance speedup.“  Data compatibility  Because the cloud offers the ability to easily launch new systems, being able to leverage different systems for different workloads is advantageous. However, ETL costs can make some system types infeasible to use when workloads change, thereby limiting the cloud offerings one can leverage. Green cells in the compatibility matrix below show where systems can use compatible formats from the same storage system, e.g. ORC on S3. Scalability  We analyzed performance when scaling horizontally and vertically… The main theme is that horizontal scaling is generally advantageous, while vertical scaling is generally disadvantageous. Scaling tests on AWS proprietary system are limited by the options they provide to the end user. The last word  Each of these findings poses opportunities for future work to explore specific architectural tradeoffs further. Additionally, future studies could analyze concurrency, test a different suite such as TPC-DS, evaluate different data sizes, and evaluate more systems.", "pdf_url": "http://vldb.org/pvldb/vol12/p2170-tan.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/choosing-a-cloud-dbms.json"}
{"id": "41840073", "bin": "1300_1400", "summary_sentences": ["GAN dissection: visualizing and understanding generative adversarial networks Bau et al., arXiv’18  Earlier this week we looked at visualisations to aid understanding and interpretation of RNNs, today’s paper choice gives us a fascinating look at what happens inside a GAN (generative adversarial network).", "In addition to the paper, the code is available on GitHub and video demonstrations can be found on the project home page .", "We’re interested in GANs that generate images.", "To a human observer, a well-trained GAN appears to have learned facts about the objects in the image: for example, a door can appear on a building but not on a tree.", "We wish to understand how a GAN represents such a structure.", "Do the objects emerge as pure pixel patterns without any explicit representation of objects such as doors and trees, or does the GAN contain internal variables that correspond to the objects that humans perceive?", "If the GAN does contain variables for doors and trees, do those variables cause the generation of those objects, or do they merely correlate?", "How are relationships between objects represented?", "The basis for the study is three variants of Progressive GANs trained on LSUN scene datasets.", "To understand what’s going on inside these GANs the authors develop a technique involving a combination of dissection and intervention.", "Given a trained segmentation model (i.e., a model that can map pixels in an image to one of a set of pre-defined object classes),  we can dissect the intermediate layers of the GAN to identify the level of agreement between individual units and each object class.", "The segmentation model used in the paper was trained on the ADE20K scene dataset and can segment an input image into 336 object classes, 29 parts of large objects, and 25 materials.", "Dissection can reveal units that correlate with the appearance of objects of certain classes, but is the relationship causal?", "Two difference types of intervention help us to understand this better.", "First, we can ablate those units (switch them off), and see if the correlated objects disappear from an image in which they were previously present.", "Second, we can force the units on and see if the correlated objects appear in an image in which they were previously absent.", "The very first figure in the paper provides an excellent overview.", "Here we can see (a) a set of generated images of churches; and (b) the results of dissection identify GAN units matching trees.", "When we ablate those units ( c ) the trees largely disappear, and when we deliberately activate them (d) trees re-appear.", "The same insights can be used for human-guided model improvements.", "Here we see generated images with artefacts (f).", "If we identify the GAN units that cause those artefacts (e), and ablate them we can remove unwanted artefacts from generated images (g).", "Characterising units by dissection  For dissection we take a upsampled and thresholded feature map of a unit and compare it to the segmentation map of a given object class.", "The extent of agreement is captured using an intersection-over-union (IoU) measure.", "We take the intersection of the thresholded image and the pixels defined as belonging to the segment class, and divide it by their union.", "The result tells us what fraction of the combined pixels are correlated with the class.", "The following examples show units with high IoU scores for the classes ‘table’ and ‘sofa’.", "Finding causal relationships through intervention  We can say that a given hidden unit causes the generation of object(s) of a given class if ablating that unit causes the object to disappear, and activating it causes the object to appear.", "Averaging effects over all locations and images gives us the average causal effect (ACE) of a unit on the generation of a given class.", "While these measures can be applied to a single unit, we have found that objects tend to depend on more than one unit.", "Thus we need to identify a set of units U that maximize the average causal effect for an object class  .", "This set is found by optimising an objective that looks for a maximum class difference between images with partial ablation and images with partial insertion, using a parameter than controls the contribution of each unit.", "Here you can see the effects of increasing larger sets of hidden units, in this case identified as associated with the class ‘tree’.", "Findings from GAN analysis  Units emerge that correlate with instances of an object class, with diverse visual appearances.", "The units are learning abstractions.", "The set of all object classes matched by units of a GAN provide a map of what a GAN has learned about the data.", "The units that emerge are object classes appropriate to the scene type: for example, when we examine a GAN trained on kitchen scenes, we find units that match stoves, cabinets, and the legs of tall kitchen stools.", "Another striking phenomenon is that many units represent parts of objects: for example, the conference room GAN contains separate units for the body and head of a person.", "The type of information represented changes from layer to layer.", "Early layers  remain entangled, middle layers have many units matching semantic objects and object parts, and later layers have units matching pixel patterns such as materials, edges, and colors.", "Here’s an interesting layer-by-layer breakdown of a Progressive GAN trained to generate LSUN living room images:  Compared to a baseline Progressive GAN, adding minibatch stddev statistics increases the realism of the outputs.", "The unit analysis shows that it also increases the diversity of the concepts represented by units.", "Turning off (ablating) units identified as associated with common object classes causes the corresponding objects to mostly disappear from the generated scenes.", "Not every object can be erased though.", "Sometimes the object seems to be integral to the scene.", "For example, when generating conference rooms the size and density of tables and chairs can be reduced, but they cannot be eliminated entirely.", "By forcing units on we can try to insert objects into scenes.", "For example, activating the same ‘door units’ across a variety of scenes causes doors to appear – but the actual appearance of the door will vary in accordance with the surrounding scene.", "We also observe that doors cannot be added in most locations.", "The locations where a door can be added are highlighted by a yellow box… it is not possible to trigger a door in the sky or on trees.", "Interventions provide insight into how a GAN enforces relationships between objects.", "Even if we try to add a door in layer 4, that choice can be vetoed later if the object is not appropriate for the context.", "By carefully examining representation units, we have found many parts of GAN representations can be interpreted, not only as signals that correlate with object concepts but as variables that have a causal effect on the synthesis of objects in the output.", "These interpretable effects can be used to compare, debug, modify, and reason about a GAN model.", "There remain open questions for future work.", "For example, why can a door not be inserted in the sky?", "How does the GAN suppress the signal in the later layers?", "Understanding the relationships between the layers of a GAN is the next hurdle…"], "summary_text": "GAN dissection: visualizing and understanding generative adversarial networks Bau et al., arXiv’18  Earlier this week we looked at visualisations to aid understanding and interpretation of RNNs, today’s paper choice gives us a fascinating look at what happens inside a GAN (generative adversarial network). In addition to the paper, the code is available on GitHub and video demonstrations can be found on the project home page . We’re interested in GANs that generate images. To a human observer, a well-trained GAN appears to have learned facts about the objects in the image: for example, a door can appear on a building but not on a tree. We wish to understand how a GAN represents such a structure. Do the objects emerge as pure pixel patterns without any explicit representation of objects such as doors and trees, or does the GAN contain internal variables that correspond to the objects that humans perceive? If the GAN does contain variables for doors and trees, do those variables cause the generation of those objects, or do they merely correlate? How are relationships between objects represented? The basis for the study is three variants of Progressive GANs trained on LSUN scene datasets. To understand what’s going on inside these GANs the authors develop a technique involving a combination of dissection and intervention. Given a trained segmentation model (i.e., a model that can map pixels in an image to one of a set of pre-defined object classes),  we can dissect the intermediate layers of the GAN to identify the level of agreement between individual units and each object class. The segmentation model used in the paper was trained on the ADE20K scene dataset and can segment an input image into 336 object classes, 29 parts of large objects, and 25 materials. Dissection can reveal units that correlate with the appearance of objects of certain classes, but is the relationship causal? Two difference types of intervention help us to understand this better. First, we can ablate those units (switch them off), and see if the correlated objects disappear from an image in which they were previously present. Second, we can force the units on and see if the correlated objects appear in an image in which they were previously absent. The very first figure in the paper provides an excellent overview. Here we can see (a) a set of generated images of churches; and (b) the results of dissection identify GAN units matching trees. When we ablate those units ( c ) the trees largely disappear, and when we deliberately activate them (d) trees re-appear. The same insights can be used for human-guided model improvements. Here we see generated images with artefacts (f). If we identify the GAN units that cause those artefacts (e), and ablate them we can remove unwanted artefacts from generated images (g). Characterising units by dissection  For dissection we take a upsampled and thresholded feature map of a unit and compare it to the segmentation map of a given object class. The extent of agreement is captured using an intersection-over-union (IoU) measure. We take the intersection of the thresholded image and the pixels defined as belonging to the segment class, and divide it by their union. The result tells us what fraction of the combined pixels are correlated with the class. The following examples show units with high IoU scores for the classes ‘table’ and ‘sofa’. Finding causal relationships through intervention  We can say that a given hidden unit causes the generation of object(s) of a given class if ablating that unit causes the object to disappear, and activating it causes the object to appear. Averaging effects over all locations and images gives us the average causal effect (ACE) of a unit on the generation of a given class. While these measures can be applied to a single unit, we have found that objects tend to depend on more than one unit. Thus we need to identify a set of units U that maximize the average causal effect for an object class  . This set is found by optimising an objective that looks for a maximum class difference between images with partial ablation and images with partial insertion, using a parameter than controls the contribution of each unit. Here you can see the effects of increasing larger sets of hidden units, in this case identified as associated with the class ‘tree’. Findings from GAN analysis  Units emerge that correlate with instances of an object class, with diverse visual appearances. The units are learning abstractions. The set of all object classes matched by units of a GAN provide a map of what a GAN has learned about the data. The units that emerge are object classes appropriate to the scene type: for example, when we examine a GAN trained on kitchen scenes, we find units that match stoves, cabinets, and the legs of tall kitchen stools. Another striking phenomenon is that many units represent parts of objects: for example, the conference room GAN contains separate units for the body and head of a person. The type of information represented changes from layer to layer. Early layers  remain entangled, middle layers have many units matching semantic objects and object parts, and later layers have units matching pixel patterns such as materials, edges, and colors. Here’s an interesting layer-by-layer breakdown of a Progressive GAN trained to generate LSUN living room images:  Compared to a baseline Progressive GAN, adding minibatch stddev statistics increases the realism of the outputs. The unit analysis shows that it also increases the diversity of the concepts represented by units. Turning off (ablating) units identified as associated with common object classes causes the corresponding objects to mostly disappear from the generated scenes. Not every object can be erased though. Sometimes the object seems to be integral to the scene. For example, when generating conference rooms the size and density of tables and chairs can be reduced, but they cannot be eliminated entirely. By forcing units on we can try to insert objects into scenes. For example, activating the same ‘door units’ across a variety of scenes causes doors to appear – but the actual appearance of the door will vary in accordance with the surrounding scene. We also observe that doors cannot be added in most locations. The locations where a door can be added are highlighted by a yellow box… it is not possible to trigger a door in the sky or on trees. Interventions provide insight into how a GAN enforces relationships between objects. Even if we try to add a door in layer 4, that choice can be vetoed later if the object is not appropriate for the context. By carefully examining representation units, we have found many parts of GAN representations can be interpreted, not only as signals that correlate with object concepts but as variables that have a causal effect on the synthesis of objects in the output. These interpretable effects can be used to compare, debug, modify, and reason about a GAN model. There remain open questions for future work. For example, why can a door not be inserted in the sky? How does the GAN suppress the signal in the later layers? Understanding the relationships between the layers of a GAN is the next hurdle…", "pdf_url": "https://arxiv.org/pdf/1811.10597", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/gan-dissection-visualizing-and-understanding-generative-adversarial-networks.json"}
{"id": "40561438", "bin": "1300_1400", "summary_sentences": ["Functional Reactive Animation – Elliott & Hudak 1997  This is the paper widely acknowledged to have given birth to (Functional) Reactive Programming or FRP.", "The challenge that Elliott and Hudak faced was to provide an elegant and expressive way to specify animations without resorting to tedious frame-by-frame constructions.", "A key insight is that animations are all about how something changes over time, and time is conceptually continuous.", "Fran makes the notion of continuous behaviours first class in the programming model.", "The construction of richly interactive multimedia animations (involving audio, pictures, video, 2D and 3D graphics) has long been a complex and tedious job.", "Much of the difficulty, we believe, stems from the lack of sufficiently high-level abstractions, and in particular from the failure to clearly distinguish between modeling and presentation, or in other words, between what an animation is and how it should be presented… The benefits of a modeling approach to animation are similarto those in favor of a functional (or other declarative) programming paradigm, and include clarity, ease of construction, composability, and clean semantics.", "The modeling approach also makes animations easier to author, and easier to optimize.", "Fran is a collection of recursive data types, functions, and primitive graphics routines brought together around four central concepts: behaviors, events, declarative reactivity, and polymorphic media.", "The most novel aspect of Fran is its implicit treatment of time.", "Behaviours (temporal modeling)  A behaviour is a value that can vary over time.", "Behaviors are first-class values, and are built up compositionally; concurrency (parallel composition) is expressed naturally and implicitly.", "The simplest primitive behaviour is time itself.", "And cos time, for example,  is a behaviour  that varies as the cosine of time, created by lifting the cosine function to apply to behaviours.", "We can create a simple animation that changes the shape of a square over time as follows:  bigger (cos time) square  bigger scales its second argument by the amount specified in the first argument.", "Since the first argument is a behaviour, the result is also a behaviour.", "Using the over function as an infix operator we can place a circle on top whose size changes as the sine of time:  bigger (sin time) circle `over` bigger (cos time) square  Event Modeling  Events are also first class values in Fran.", "They can represent actual events in the external world (for example, a button press), and they can also be expressed as predicates (for example, based on proximity).", "Events are ‘what’, ‘when’ pairs.", "The when component is a lower bound on the time of the event – for examplelbp t0 represents the first left button press (‘what’), occurring after time t0.", "Declarative reactivity  Here we see the first use of the ‘reactive’ term…  Many behaviors are naturally expressed in terms of reactions to events.", "But even these “reactive behaviors” have declarative semantics in terms of temporal composition, rather than an imperative semantics in terms of the state changes often employed in event-based formalisms.", "b `untilB` e  Exhibits behaviour b until event e occurs, after which it behaves according to the behaviour associated with e. Using this we can describe a colour cycle as follows:  colorCycle t0 =        red `untilB` lbp t0 *=> \\t1 -> green `untilB` lbp t1 *=> \\t2 -> colorCycle t2  update: fixed &gt; vs > in the above, thanks David for pointing out the issue.", "Fran is implemented in Haskell, where \\x -> … defines a lambda function with bound variable x.", "This reactive behaviour can thus be interpreted as:  show red until the first left-button press after time t0, and then  show green until the first left-button press after time t1 (the time of the previous lbp), and then  repeat the color cycle starting at time t2 (the time of the second lbp)  Polymorphic Media  The variety of time-varying media (images, video, sound, 3D geometry) and parameters of those types (spatial transformations, colors, points, vectors, numbers) have their own type-specific operations (e.g. image rotation, sound mixing, and numerical addition), but fit into a common framework of behaviors and reactivity.", "For instance, the “untilB” operation used above is polymorphic, applying to all types of time-varying values.", "Examples  The paper includes a formal semantics of behaviours and events, as well as some implementation notes that describe an ‘interval analysis’ technique for detecting predicate events.", "We’ll look in more detail at design and implementation considerations for reactive frameworks later this week.", "For now, let’s just look at a few more code examples to get a feel for the expressivity of modeling that behaviours and events support.", "A behaviour that varies smoothly and cyclically between -1 and 1:  wiggle  = sin (pi * time)  And a behaviour that builds on this to smoothly vary between a high and low value:  wiggleRange lo hi =    lo + (hi - lo) * (wiggle + 1)/2  A red pulsating ball:  pBall = withColor red           (bigger (wiggleRange 0.5 1) circle)  Behaviours are composable, as we’ve been seeing already.", "Let’s use pBall as a building block…  rBall = move (vectorPolar 2.0 time)               (bigger 0.1 pBall)  “which yields a ball moving in a circular motion with radius 2.0 at a rate proportional to time.", "The ball itself is the same as pBall (red and pulsating), but 1/10 the original size.”  An image that tracks the position of the mouse:  followMouse im t0 = move (mouse t0) im  As a final example, let’s develop a modular program to describe “bouncing balls.” First note that the physical equations describing the position y and velocity v at time t of an object being accelerated by gravity g are:  where y0 and v0 are the initial position and velocity, respectively of the object at time t0.", "In Fran these equations are simply:  y = lift0 y0 + integral v t0 x = lift0 v0 + integral g t0  update: corrected typo in the x expression, thanks to zteve for pointing it out.", "Next we define a function bounce that, in addition to computing the position of an object based on the above equations, also determines when the ball has hit either the floor or the ceiling, and if so reverses the direction of the ball while reducing its velocity by a certain reciprocity, to account for loss of energy during the collision…  (See figure 1 in the paper for the bounce code).", "Using bounce we can also simulate horizontal movement if we simply use 0 for acceleration.", "Here’s a bouncing ball in a box:  moveXY x y   (withColor green circle) where   x = bounce xMin xMax x0 vx0 0 t0   y = bounce yMin yMax y0 vy0 g t0  See the full paper for further examples."], "summary_text": "Functional Reactive Animation – Elliott & Hudak 1997  This is the paper widely acknowledged to have given birth to (Functional) Reactive Programming or FRP. The challenge that Elliott and Hudak faced was to provide an elegant and expressive way to specify animations without resorting to tedious frame-by-frame constructions. A key insight is that animations are all about how something changes over time, and time is conceptually continuous. Fran makes the notion of continuous behaviours first class in the programming model. The construction of richly interactive multimedia animations (involving audio, pictures, video, 2D and 3D graphics) has long been a complex and tedious job. Much of the difficulty, we believe, stems from the lack of sufficiently high-level abstractions, and in particular from the failure to clearly distinguish between modeling and presentation, or in other words, between what an animation is and how it should be presented… The benefits of a modeling approach to animation are similarto those in favor of a functional (or other declarative) programming paradigm, and include clarity, ease of construction, composability, and clean semantics. The modeling approach also makes animations easier to author, and easier to optimize. Fran is a collection of recursive data types, functions, and primitive graphics routines brought together around four central concepts: behaviors, events, declarative reactivity, and polymorphic media. The most novel aspect of Fran is its implicit treatment of time. Behaviours (temporal modeling)  A behaviour is a value that can vary over time. Behaviors are first-class values, and are built up compositionally; concurrency (parallel composition) is expressed naturally and implicitly. The simplest primitive behaviour is time itself. And cos time, for example,  is a behaviour  that varies as the cosine of time, created by lifting the cosine function to apply to behaviours. We can create a simple animation that changes the shape of a square over time as follows:  bigger (cos time) square  bigger scales its second argument by the amount specified in the first argument. Since the first argument is a behaviour, the result is also a behaviour. Using the over function as an infix operator we can place a circle on top whose size changes as the sine of time:  bigger (sin time) circle `over` bigger (cos time) square  Event Modeling  Events are also first class values in Fran. They can represent actual events in the external world (for example, a button press), and they can also be expressed as predicates (for example, based on proximity). Events are ‘what’, ‘when’ pairs. The when component is a lower bound on the time of the event – for examplelbp t0 represents the first left button press (‘what’), occurring after time t0. Declarative reactivity  Here we see the first use of the ‘reactive’ term…  Many behaviors are naturally expressed in terms of reactions to events. But even these “reactive behaviors” have declarative semantics in terms of temporal composition, rather than an imperative semantics in terms of the state changes often employed in event-based formalisms. b `untilB` e  Exhibits behaviour b until event e occurs, after which it behaves according to the behaviour associated with e. Using this we can describe a colour cycle as follows:  colorCycle t0 =        red `untilB` lbp t0 *=> \\t1 -> green `untilB` lbp t1 *=> \\t2 -> colorCycle t2  update: fixed &gt; vs > in the above, thanks David for pointing out the issue. Fran is implemented in Haskell, where \\x -> … defines a lambda function with bound variable x. This reactive behaviour can thus be interpreted as:  show red until the first left-button press after time t0, and then  show green until the first left-button press after time t1 (the time of the previous lbp), and then  repeat the color cycle starting at time t2 (the time of the second lbp)  Polymorphic Media  The variety of time-varying media (images, video, sound, 3D geometry) and parameters of those types (spatial transformations, colors, points, vectors, numbers) have their own type-specific operations (e.g. image rotation, sound mixing, and numerical addition), but fit into a common framework of behaviors and reactivity. For instance, the “untilB” operation used above is polymorphic, applying to all types of time-varying values. Examples  The paper includes a formal semantics of behaviours and events, as well as some implementation notes that describe an ‘interval analysis’ technique for detecting predicate events. We’ll look in more detail at design and implementation considerations for reactive frameworks later this week. For now, let’s just look at a few more code examples to get a feel for the expressivity of modeling that behaviours and events support. A behaviour that varies smoothly and cyclically between -1 and 1:  wiggle  = sin (pi * time)  And a behaviour that builds on this to smoothly vary between a high and low value:  wiggleRange lo hi =    lo + (hi - lo) * (wiggle + 1)/2  A red pulsating ball:  pBall = withColor red           (bigger (wiggleRange 0.5 1) circle)  Behaviours are composable, as we’ve been seeing already. Let’s use pBall as a building block…  rBall = move (vectorPolar 2.0 time)               (bigger 0.1 pBall)  “which yields a ball moving in a circular motion with radius 2.0 at a rate proportional to time. The ball itself is the same as pBall (red and pulsating), but 1/10 the original size.”  An image that tracks the position of the mouse:  followMouse im t0 = move (mouse t0) im  As a final example, let’s develop a modular program to describe “bouncing balls.” First note that the physical equations describing the position y and velocity v at time t of an object being accelerated by gravity g are:  where y0 and v0 are the initial position and velocity, respectively of the object at time t0. In Fran these equations are simply:  y = lift0 y0 + integral v t0 x = lift0 v0 + integral g t0  update: corrected typo in the x expression, thanks to zteve for pointing it out. Next we define a function bounce that, in addition to computing the position of an object based on the above equations, also determines when the ball has hit either the floor or the ceiling, and if so reverses the direction of the ball while reducing its velocity by a certain reciprocity, to account for loss of energy during the collision…  (See figure 1 in the paper for the bounce code). Using bounce we can also simulate horizontal movement if we simply use 0 for acceleration. Here’s a bouncing ball in a box:  moveXY x y   (withColor green circle) where   x = bounce xMin xMax x0 vx0 0 t0   y = bounce yMin yMax y0 vy0 g t0  See the full paper for further examples.", "pdf_url": "http://conal.net/papers/icfp97/icfp97.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/fran.json"}
{"id": "25766140", "bin": "1300_1400", "summary_sentences": ["A static verification framework for message passing in Go using behavioural types Lange et al., ICSE 18  With thanks to Alexis Richardson who first forwarded this paper to me.", "We’re jumping ahead to ICSE 18 now, and a paper that has been accepted for publication there later this year.", "It fits with the theme we’ve been exploring this week though, so I thought I’d cover it now.", "We’ve seen verification techniques applied in the context of Rust and JavaScript , looked at the integration of linear types in Haskell , and today it is the turn of Go!", "Despite its popularity, the Go programming ecosystem offers little to no support for guaranteeing the correctness of message-passing concurrent programs.", "This work proposes a practical verification framework for message passing concurrency in Go…  Go’s channel-based concurrency model is inspired by process calculi.", "There is a rich body of work on process calculi-based verification for reasoning about safety and liveness properties of interactive systems.", "However, Go itself only enforces that messages exchange via communication channels adhere to the declared payload types, and at runtime offers just a “toy global deadlock detector.” Can we apply more of the process calculi based reasoning in the context of Go?", "It turns out that yes, we can.", "The Godel Checker enables verification that Go programs are free of global deadlocks, as well as several Go specific safety properties (including channel safety).", "There is also support for detecting potentially problematic loops and partial deadlocks.", "At the core of the system is a translation from Go source code to a behavioural type model of the program.", "A model checker mCRL2 and termination checker (based on KiTTeL ) can then be applied to the extracted behavioural types.", "Verification of key safety and liveness properties for a variety of programs shows that Godel Checker can complete its analysis in just a few seconds for smaller programs, and just over a minute for the larger code bases tackled (up to 16 kloc).", "As a user, what’s nice about all this is that you don’t have to get involved with any of the formal machinery yourself, just supply the source code!", "( Enlarge )  Common concurrency errors in Go programs  Godel Checker address three sources of common concurrency errors in Go programs: channel safety errors, global deadlocks, and partial deadlocks.", "After a channel is closed, receive actions always succeed but any send or close actions raise a runtime error.", "Hence, “channels should be closed at most once and no message should be sent on closed channels.”  Go does have a built-in global deadlock detector that will signal at runtime if all goroutines in a program are stuck.", "We’d like to find out about the possibility (or hopefully, the absence of the possibility) of global deadlocks ahead of time.", "Moreover, when certain common libraries are imported, the global deadlock detector is silently disabled and hence global deadlocks are just ignored.", "Then there’s the case when a program communication cannot progress even though only some of its goroutines are stuck.", "“This is known as a partial deadlock or as a failure of liveness.” Consider the following program:  Because ch1 is passed as both arguments to Consumer on line 16 the resulting system is not live: the second producer is not interacting with the consumer and its outputs will never be matched with their respective inputs.", "From Go to Behavioural types  Behavioural types are a typing discipline in which types express the possible actions of a program in a fine-grained way.", "When applied to communication and concurrency, behavioural types act as an abstract specification of all communication actions that may be performed in a program.", "Moreover, behavioural types are an executable specification.", "They have a natural operational meaning and evolve throughout program execution.", "For the program we saw above, the behavioural type looks like this:  Imperative control structures are transformed into recursive definitions, and data elements are erased.", "In terms of types, global deadlock freedom (GDF) requires that if a communication action is available to fire, the type can always make progress.", "Thus a type as a whole is never globally stuck.", "Liveness, or partial deadlock freedom, is a stronger condition (every live type is also global deadlock free).", "Liveness states that all communications that can become enabled in a type can always eventually fire.", "(Replacing the call to cons(ch1,ch1) with cons(ch1,ch2) makes the type main() satisfy liveness).", "In order to infer behavioural types from Go source code, the source is first converted to a static single assignment (SSA) intermediate representation (IR).", "The SSA IR conversion takes a Go program such as this:  And produces:  The main SSA instructions used in the IR are shown in the following table:  Given the SSA form, the next step is to soundly approximate the communication behaviour.", "A type signature is generated for every SSA block.", "The details of the algorithm are in section 3.2 of the paper.", "For our purposes we mostly just need to know that it is possible.", "At the end of this process, for Listing 1 above its SSA representation, the inferred behavioural type looks like this:  Model checking  We have our behavioural type model, and now we can proceed to verify its properties:  We proceed in three steps: (1) we generate a (finite) labelled transition system (LTS) for the types from a set of operational semantics rules; (2) we define properties of the states of the LTS in terms of the immediate actions behavioural types can take; and (3) we give safety and liveness properties expressed in the modal μ-calculus.", "Finiteness is defined by the restriction that types cannot feature parallel composition or channel creation under recursion.", "Semantics for the types follow definitions from CCS (concurrent communication systems) and CSP (communicating sequential processes).", "A labelled transition system is built for the entry point type (it basically tells you how to move between states in the system).", "Given this representation, we can encode (and hence check) a number of useful liveness and safety properties in the μ-calculus.", "They look like this:  You’ll find a very concise guide to decoding those symbols in section 4 of the paper!", "Defined this way, the properties can be verified using the mCRL2 model checker.", "When extracting behavioural types, conditionals are abstracted as a non-deterministic choice between the alternate behaviours in the then and else branches.", "This means that any data dependencies in the conditionals (e.g., testing the value of a variable) are not captured.", "This coarse abstraction introduces a subtle interaction between non-terminating program behaviour and data-dependent communication wrt.", "liveness.", "To address this, an additional termination analysis of loops is done using the KITTeL termination analyser.", "KITTeL actually targets C programs, but the syntax of Go is close enough to make it work with a translation to C functions.", "The analysis checks that the loop parameters are sufficient to make each loop eventually terminate.", "“_This enables us to pinpoint program locations where the liveness of types may not entail the analogue property in the program – if the termination analysis identifies the program as terminating, the liveness properties on types and programs coincide.”"], "summary_text": "A static verification framework for message passing in Go using behavioural types Lange et al., ICSE 18  With thanks to Alexis Richardson who first forwarded this paper to me. We’re jumping ahead to ICSE 18 now, and a paper that has been accepted for publication there later this year. It fits with the theme we’ve been exploring this week though, so I thought I’d cover it now. We’ve seen verification techniques applied in the context of Rust and JavaScript , looked at the integration of linear types in Haskell , and today it is the turn of Go! Despite its popularity, the Go programming ecosystem offers little to no support for guaranteeing the correctness of message-passing concurrent programs. This work proposes a practical verification framework for message passing concurrency in Go…  Go’s channel-based concurrency model is inspired by process calculi. There is a rich body of work on process calculi-based verification for reasoning about safety and liveness properties of interactive systems. However, Go itself only enforces that messages exchange via communication channels adhere to the declared payload types, and at runtime offers just a “toy global deadlock detector.” Can we apply more of the process calculi based reasoning in the context of Go? It turns out that yes, we can. The Godel Checker enables verification that Go programs are free of global deadlocks, as well as several Go specific safety properties (including channel safety). There is also support for detecting potentially problematic loops and partial deadlocks. At the core of the system is a translation from Go source code to a behavioural type model of the program. A model checker mCRL2 and termination checker (based on KiTTeL ) can then be applied to the extracted behavioural types. Verification of key safety and liveness properties for a variety of programs shows that Godel Checker can complete its analysis in just a few seconds for smaller programs, and just over a minute for the larger code bases tackled (up to 16 kloc). As a user, what’s nice about all this is that you don’t have to get involved with any of the formal machinery yourself, just supply the source code! ( Enlarge )  Common concurrency errors in Go programs  Godel Checker address three sources of common concurrency errors in Go programs: channel safety errors, global deadlocks, and partial deadlocks. After a channel is closed, receive actions always succeed but any send or close actions raise a runtime error. Hence, “channels should be closed at most once and no message should be sent on closed channels.”  Go does have a built-in global deadlock detector that will signal at runtime if all goroutines in a program are stuck. We’d like to find out about the possibility (or hopefully, the absence of the possibility) of global deadlocks ahead of time. Moreover, when certain common libraries are imported, the global deadlock detector is silently disabled and hence global deadlocks are just ignored. Then there’s the case when a program communication cannot progress even though only some of its goroutines are stuck. “This is known as a partial deadlock or as a failure of liveness.” Consider the following program:  Because ch1 is passed as both arguments to Consumer on line 16 the resulting system is not live: the second producer is not interacting with the consumer and its outputs will never be matched with their respective inputs. From Go to Behavioural types  Behavioural types are a typing discipline in which types express the possible actions of a program in a fine-grained way. When applied to communication and concurrency, behavioural types act as an abstract specification of all communication actions that may be performed in a program. Moreover, behavioural types are an executable specification. They have a natural operational meaning and evolve throughout program execution. For the program we saw above, the behavioural type looks like this:  Imperative control structures are transformed into recursive definitions, and data elements are erased. In terms of types, global deadlock freedom (GDF) requires that if a communication action is available to fire, the type can always make progress. Thus a type as a whole is never globally stuck. Liveness, or partial deadlock freedom, is a stronger condition (every live type is also global deadlock free). Liveness states that all communications that can become enabled in a type can always eventually fire. (Replacing the call to cons(ch1,ch1) with cons(ch1,ch2) makes the type main() satisfy liveness). In order to infer behavioural types from Go source code, the source is first converted to a static single assignment (SSA) intermediate representation (IR). The SSA IR conversion takes a Go program such as this:  And produces:  The main SSA instructions used in the IR are shown in the following table:  Given the SSA form, the next step is to soundly approximate the communication behaviour. A type signature is generated for every SSA block. The details of the algorithm are in section 3.2 of the paper. For our purposes we mostly just need to know that it is possible. At the end of this process, for Listing 1 above its SSA representation, the inferred behavioural type looks like this:  Model checking  We have our behavioural type model, and now we can proceed to verify its properties:  We proceed in three steps: (1) we generate a (finite) labelled transition system (LTS) for the types from a set of operational semantics rules; (2) we define properties of the states of the LTS in terms of the immediate actions behavioural types can take; and (3) we give safety and liveness properties expressed in the modal μ-calculus. Finiteness is defined by the restriction that types cannot feature parallel composition or channel creation under recursion. Semantics for the types follow definitions from CCS (concurrent communication systems) and CSP (communicating sequential processes). A labelled transition system is built for the entry point type (it basically tells you how to move between states in the system). Given this representation, we can encode (and hence check) a number of useful liveness and safety properties in the μ-calculus. They look like this:  You’ll find a very concise guide to decoding those symbols in section 4 of the paper! Defined this way, the properties can be verified using the mCRL2 model checker. When extracting behavioural types, conditionals are abstracted as a non-deterministic choice between the alternate behaviours in the then and else branches. This means that any data dependencies in the conditionals (e.g., testing the value of a variable) are not captured. This coarse abstraction introduces a subtle interaction between non-terminating program behaviour and data-dependent communication wrt. liveness. To address this, an additional termination analysis of loops is done using the KITTeL termination analyser. KITTeL actually targets C programs, but the syntax of Go is close enough to make it work with a translation to C functions. The analysis checks that the loop parameters are sufficient to make each loop eventually terminate. “_This enables us to pinpoint program locations where the liveness of types may not entail the analogue property in the program – if the termination analysis identifies the program as terminating, the liveness properties on types and programs coincide.”", "pdf_url": "http://mrg.doc.ic.ac.uk/publications/a-static-verification-framework-for-message-passing-in-go-using-behavioural-types/draft.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/a-static-verification-framework-for-message-passing-in-go-using-behavioural-types.json"}
{"id": "2899329", "bin": "1300_1400", "summary_sentences": ["A new paper presented in ICML 2018 describes some of the troubling trends in machine learning (ML) scholarship.", "The authors, Zachary C. Lipton and Jacob Steinhardt, discuss the implications of these trends on the field of ML and the proper distilling of machine learning research and knowledge.", "Machine learning (ML) research aims to disseminate knowledge about data-driven algorithms through theoretical characterizations and or empirical results.", "The field of ML is growing rapidly, at an unprecedented scale and speed, with certain troubling patterns emerging which could negatively affect meaningful progress in the field, poorly serving to researchers and enthusiasts.", "ML research is most valuable when communicated properly to the interested reader by avoiding the presentation of speculations as facts and other misleading interpretations and explanations.", "Lipton and Steinhardt propose several ways to combat some of the troubling trends occurring in the ML community, particularly as it relates to the poor dissemination of foundational knowledge and empirical results.", "The overall goal is to communicate research and findings with greater clarity and to pay closer attention to the following troubling patterns:  Explanation vs.", "Speculation  In writing, speculation encompasses a whole range of techniques used to explain concepts and intuitions without providing proper evidence or formal definitions.", "For example, we claim that X produces Y given some conditions Z.", "We know X and Y to be true, but we don’t know if all conditions Z apply because we haven’t formally defined them.", "Yet we claim that all conditions Z seem to work even when we haven’t defined them properly.", "Just because the reader isn’t aware or doesn’t care about all Zs it doesn’t mean the claim warrants validity — it’s “explanation disguised as speculation.”  This is a risky form of science communication since other researchers may blindly use them as facts in their work, further increasing the severity and reach of the initial speculation.", "It’s important to avoid asserting speculations as facts and explicitly separating them from facts when using them.", "(See actual examples in the paper)  Failure to Identify the Sources of Empirical Gains  Sometimes ML researchers propose complex models that combine several techniques to address a problem.", "Even though the proposed approach does well, authors sometimes fail to identify the source/s of empirical gains.", "The problem is that this may generate a false impression that the authors did a lot of work when in reality the improvements were incremental and only occurred due to a small change.", "This can sometimes be minimized and clarified by performing proper ablation studies.", "(See more examples in the paper)  Mathiness  Mathiness is a concept proposed by economist Paul Romer that describes the idea of using both natural language and mathematics to explain concepts but failing to provide tight links between statements and symbols.", "Mathiness manifests in many ways such as authors trying to pile math equations to convey technical depth without proper explanations and linkings.", "The problem with this approach is that clarity suffers, distracting the readers from acquiring the important knowledge and insights of the research.", "The best remedy seems to be avoiding the use of mathiness and providing clear explanations of the formulations so that it benefits all kinds of readers and not just ML researchers.", "Misuse of Language  Lipton and Steinhardt currently identified three types of language misuse in machine learning research: suggestive definitions, overloaded terminology, and suitcase words.", "Let’s briefly describe the three below:  Suggestive definitions — This occurs when authors coin suggestive terms that convey human qualities (i.e., anthropomorphic characterizations) such as “thought vectors.” These type of terms are not necessarily bad but if not qualified may only confuse readers.", "Overloaded terminology — This refers to the inappropriate or contradictory use of a technical term that already holds a very precise meaning in the literature.", "An example is the misuse of the term “deconvolution” to refer to transpose convolution as opposed to “reversing a convolution” which is what it originally describes.", "Suitcase words — This refers to terms that may pack many different meanings, often creating a confusion to readers with different backgrounds.", "These type of words could also manifest in the form of suggestive definitions and overloaded terminology as explained above.", "Some notorious examples include “generalization”, “interpretability”, and “bias.” Loosely using these type of words is common practice but often detracts and confuse readers.", "Potential Causes Behind Troubling Trends  Lipton and Steinhardt also discuss the potential causal factors to these troubling trends in ML scholarship:  Complacency in the face of progress — This describes the notion that authors feel entitled to make weak arguments because they are already providing strong results.", "This poses a problem since reviewers may feel pressured to accept potentially flawed papers only to compensate for the quantitative findings.", "Growing pains — ML is expanding rapidly and thus increases the entrance of inexperienced researchers which are more susceptible to the misuse of language.", "This is not to say that experienced researchers cannot fall into these patterns, especially when their reviewing responsibilities increase with the rapid growth of the field.", "This is not to discourage junior researchers, but spreading awareness of these issues is key to conducting proper research.", "Misaligned incentives — As ML becomes more popular, both the media and startup investors seek to provide incentives for things which may not be aligned with the objectives of the field.", "For instance, anthropomorphic descriptions, such as “simulated brain”, may be effective for popular coverage but could actually cause confusion in the ML community and literature.", "Suggestions  Besides recommending that authors — experienced and inexperienced — refrain from participating in these trends, Lipton and Steinhard outline a few preliminary suggestions:  What, Why, How — Identify “what worked” and provide clear details explaining “why it worked”, and not just “how well it worked.”  Insights — Authors are encouraged to provide error analysis, ablation studies, and robustness check.", "Besides identifying empirical gains, authors can also discuss insights with strong evidence to support them.", "Writing tips — Authors are recommended to revise important questions and theorems reported in the research, with clarity, flow, and precision in mind.", "Open problems can also be separated from closed ones when offering discussions to avoid confusion and to encourage follow-up research.", "Other suggestions are also provided for publishers and reviewers.", "(See paper for more details)  Overall, it’s important to avoid the use of confusing terminology (e.g. anthropomorphic characterization) and unsupported claims.", "Greater rigor is essential for the healthy growth of ML scholarship and scientific progress more broadly.", "There are special cases where the recommendations set forth above may actually hurt the fast pace of research.", "In some cases, original ideas may even be impeded.", "However, historically speaking, undisciplined scholarship affects both researchers and the public, which means there could potentially emerge a crisis and a “lack of meaningful progress.”  Final Words From the Editor  Machine learning has grown tremendously over the past few years and it promises to be at the epicenter of “everything technology.” It’s the responsibility of everyone — researchers, students, reviewers, industry, startups, lawmakers, and everyone involved — to participate in the debate and to ensure the proper distilling of machine learning research and knowledge in a transparent and accurate way.", "We are responsible for guiding the conversation in the right direction and to avoid harming the ML field and science more broadly.", "Ref:  [url]"], "summary_text": "A new paper presented in ICML 2018 describes some of the troubling trends in machine learning (ML) scholarship. The authors, Zachary C. Lipton and Jacob Steinhardt, discuss the implications of these trends on the field of ML and the proper distilling of machine learning research and knowledge. Machine learning (ML) research aims to disseminate knowledge about data-driven algorithms through theoretical characterizations and or empirical results. The field of ML is growing rapidly, at an unprecedented scale and speed, with certain troubling patterns emerging which could negatively affect meaningful progress in the field, poorly serving to researchers and enthusiasts. ML research is most valuable when communicated properly to the interested reader by avoiding the presentation of speculations as facts and other misleading interpretations and explanations. Lipton and Steinhardt propose several ways to combat some of the troubling trends occurring in the ML community, particularly as it relates to the poor dissemination of foundational knowledge and empirical results. The overall goal is to communicate research and findings with greater clarity and to pay closer attention to the following troubling patterns:  Explanation vs. Speculation  In writing, speculation encompasses a whole range of techniques used to explain concepts and intuitions without providing proper evidence or formal definitions. For example, we claim that X produces Y given some conditions Z. We know X and Y to be true, but we don’t know if all conditions Z apply because we haven’t formally defined them. Yet we claim that all conditions Z seem to work even when we haven’t defined them properly. Just because the reader isn’t aware or doesn’t care about all Zs it doesn’t mean the claim warrants validity — it’s “explanation disguised as speculation.”  This is a risky form of science communication since other researchers may blindly use them as facts in their work, further increasing the severity and reach of the initial speculation. It’s important to avoid asserting speculations as facts and explicitly separating them from facts when using them. (See actual examples in the paper)  Failure to Identify the Sources of Empirical Gains  Sometimes ML researchers propose complex models that combine several techniques to address a problem. Even though the proposed approach does well, authors sometimes fail to identify the source/s of empirical gains. The problem is that this may generate a false impression that the authors did a lot of work when in reality the improvements were incremental and only occurred due to a small change. This can sometimes be minimized and clarified by performing proper ablation studies. (See more examples in the paper)  Mathiness  Mathiness is a concept proposed by economist Paul Romer that describes the idea of using both natural language and mathematics to explain concepts but failing to provide tight links between statements and symbols. Mathiness manifests in many ways such as authors trying to pile math equations to convey technical depth without proper explanations and linkings. The problem with this approach is that clarity suffers, distracting the readers from acquiring the important knowledge and insights of the research. The best remedy seems to be avoiding the use of mathiness and providing clear explanations of the formulations so that it benefits all kinds of readers and not just ML researchers. Misuse of Language  Lipton and Steinhardt currently identified three types of language misuse in machine learning research: suggestive definitions, overloaded terminology, and suitcase words. Let’s briefly describe the three below:  Suggestive definitions — This occurs when authors coin suggestive terms that convey human qualities (i.e., anthropomorphic characterizations) such as “thought vectors.” These type of terms are not necessarily bad but if not qualified may only confuse readers. Overloaded terminology — This refers to the inappropriate or contradictory use of a technical term that already holds a very precise meaning in the literature. An example is the misuse of the term “deconvolution” to refer to transpose convolution as opposed to “reversing a convolution” which is what it originally describes. Suitcase words — This refers to terms that may pack many different meanings, often creating a confusion to readers with different backgrounds. These type of words could also manifest in the form of suggestive definitions and overloaded terminology as explained above. Some notorious examples include “generalization”, “interpretability”, and “bias.” Loosely using these type of words is common practice but often detracts and confuse readers. Potential Causes Behind Troubling Trends  Lipton and Steinhardt also discuss the potential causal factors to these troubling trends in ML scholarship:  Complacency in the face of progress — This describes the notion that authors feel entitled to make weak arguments because they are already providing strong results. This poses a problem since reviewers may feel pressured to accept potentially flawed papers only to compensate for the quantitative findings. Growing pains — ML is expanding rapidly and thus increases the entrance of inexperienced researchers which are more susceptible to the misuse of language. This is not to say that experienced researchers cannot fall into these patterns, especially when their reviewing responsibilities increase with the rapid growth of the field. This is not to discourage junior researchers, but spreading awareness of these issues is key to conducting proper research. Misaligned incentives — As ML becomes more popular, both the media and startup investors seek to provide incentives for things which may not be aligned with the objectives of the field. For instance, anthropomorphic descriptions, such as “simulated brain”, may be effective for popular coverage but could actually cause confusion in the ML community and literature. Suggestions  Besides recommending that authors — experienced and inexperienced — refrain from participating in these trends, Lipton and Steinhard outline a few preliminary suggestions:  What, Why, How — Identify “what worked” and provide clear details explaining “why it worked”, and not just “how well it worked.”  Insights — Authors are encouraged to provide error analysis, ablation studies, and robustness check. Besides identifying empirical gains, authors can also discuss insights with strong evidence to support them. Writing tips — Authors are recommended to revise important questions and theorems reported in the research, with clarity, flow, and precision in mind. Open problems can also be separated from closed ones when offering discussions to avoid confusion and to encourage follow-up research. Other suggestions are also provided for publishers and reviewers. (See paper for more details)  Overall, it’s important to avoid the use of confusing terminology (e.g. anthropomorphic characterization) and unsupported claims. Greater rigor is essential for the healthy growth of ML scholarship and scientific progress more broadly. There are special cases where the recommendations set forth above may actually hurt the fast pace of research. In some cases, original ideas may even be impeded. However, historically speaking, undisciplined scholarship affects both researchers and the public, which means there could potentially emerge a crisis and a “lack of meaningful progress.”  Final Words From the Editor  Machine learning has grown tremendously over the past few years and it promises to be at the epicenter of “everything technology.” It’s the responsibility of everyone — researchers, students, reviewers, industry, startups, lawmakers, and everyone involved — to participate in the debate and to ensure the proper distilling of machine learning research and knowledge in a transparent and accurate way. We are responsible for guiding the conversation in the right direction and to avoid harming the ML field and science more broadly. Ref:  [url]", "pdf_url": "https://arxiv.org/pdf/1807.03341", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/an-overview-of-troubling-trends-in-machine-learning-scholarship-582df3caa518.json"}
{"id": "90421095", "bin": "1300_1400", "summary_sentences": ["The truth, the whole truth, and nothing but the truth: A pragmatic guide to assessing empirical evaluations Blackburn et al. ACM Transactions on Programming Languages and Systems 2016  Yesterday we looked at some of the ways analysts may be fooled into thinking they’ve found a statistically significant result when in fact they haven’t.", "Today’s paper choice looks at what can go wrong with empirical evaluations.", "Let’s start with a couple of definitions:  An evaluation is either an experiment or an observational study, consisting of steps performed and data produced from those steps.", "A claim is an assertion about the significance and meaning of an evaluation; thus, unlike a hypothesis, which precedes an evaluation, a claim comes after the evaluation.", "A sound claim is one where the evaluation provides all the evidence necessary to support the claim, and does not provide any evidence that contradicts the claim.", "Assuming honest researchers, things can go wrong in one of two basic ways: sins of reasoning are those where the claim is not supported by the evaluation; and sins of exposition are those where the description of either the evaluation or the claim is not sufficient for readers to evaluate and/or reproduce the results.", "As the authors show, it’s not easy to avoid these traps even when you are doing your best.", "We’ll get into the details shortly, but for those authoring (or reviewing) evaluations and claims, here are five questions to help you avoid them:  Has all of the data from the evaluation been considered, and not just the data that supports the claim?", "Have any assumptions been made in forming the claim that are not justified by the evaluation?", "If the experimental evaluation is compared to prior work, is it an apples-to-oranges comparison?", "Has everything essential for getting the results been described?", "Has the claim been unambiguously and clearly stated?", "The two sins of exposition  The sin of inscrutability occurs when poor exposition obscures a claim.", "That is, we can’t be sure about what is really being claimed!", "The most basic form of inscrutability is simply omission – leaving the reader to figure out what the claims might be by themselves!", "Claims may also be ambiguous – for example, “improves performance” by 10% (throughput or latency?", "), or “reduces latency by 10%” (at what percentile and under what load?).", "Distorted claims are clear from the reader’s perspective, but don’t actually capture what the author intended.", "The sin of irreproducability occurs when poor exposition obscures an evaluation: the evaluation steps, the data, or both.", "The evaluation may not be reproducible because key information is omitted, imprecise language and lack of detail may lead to ambiguity.", "One important kind of omission that is hard to guard against is an incomplete understanding of the factors that are relevant to the evaluation.", "Experience shows that whole communities can ignore important, though non-obvious, aspects of the empirical environment, only to find out their significance much later, throwing into question years of published results.", "This situation invites two responses: (i) authors should be held to the community’s standard of what is known about the importance of the empirical environment, and (ii) the community should intentionally and actively improve this knowledge, for example, by promoting reproduction studies.", "A good example of an unknown significant factor turns out to be the size of environment variables when measuring speed-ups of gcc in Linux!", "The size of the environment affects memory layout, and thus the performance of the program.", "An evaluation only exploring one (unspecified) environment size leads to the sin of irreproducibility.", "Researchers knew of course that memory layout affects performance, but it wasn’t obvious that environment variables would affect it enough to make a difference… until someone measured it.", "The three sins of reasoning  The three sins of reasoning are the sin of ignorance, the sin of inappropriateness, and the sin of inconsistency.", "The sin of ignorance occurs when a claim is made that ignores elements of the evaluation supporting a contradictory alternative claim (i.e., selecting data points to substantiate a claim while ignoring other relevant points).", "In our experience, while the sin of ignorance seems obvious and easy to avoid, in reality, it is far from it.", "Many factors in the evaluation that seem irrelevant to a claim may actually be critical to the soundness of the claim.", "Consider the following latency histogram for a component of Gmail that has both a fast-path and a slow-path through it.", "The distribution is bimodal, which means that commonly used statistical techniques making assumptions the data is normally distributed do not apply.", "If a claim is made on the basis of such techniques, it is unsound and we have committed the sin of ignorance.", "This is easy to see when you have the histogram, but suppose the experiment calculated mean and standard-deviation on the fly – then it may not be so obvious to us at all!", "The sin of inappropriateness occurs when a claim is made that is predicated on some fact that is absent from the evaluation.", "In our experience, while the sin of inappropriateness seems obvious and easy to avoid, in reality, it is far from it.", "Many factors that may be unaccounted for in evaluation may actually be important to derive a sound claim.", "Consider an evaluation trying to measure energy consumption.", "Often execution time is measured as a proxy for this, but it turns out that other factors can also affect energy consumption, and thus execution time does not support a claim about energy consumption.", "Knowing that execution time is not always an accurate proxy for energy consumption is not obvious – until it was pointed out in a 2001 paper.", "As another example, before it was widely understood that heap sizes have a big impact on garbage collector performance, many papers derived a claim from an evaluation on only one heap size, and did not report it.", "The sin of inconsistency is a sin of faulty comparison – two systems are compared, but each is evaluated in a way that is inconsistent with the other.", "In our experience, while the sin of inconsistency seems obvious and easy to avoid, in reality, it is far from it.", "Many artifacts that seem comparable may actually be inconsistent.", "Consider the use of hardware performance counters to evaluate performance…  …the performance counters that are provided by different vendors may vary, and even the performance counters provided by the same vendor may vary across different generations of the same architecture.", "Comparing data from what may seem like similar performance counters within an architecture, across architectures, or between generations of the same architecture may result in the sin of inconsistency, because the hardware performance counters are counting different hardware events.", "Comparing against inconsistent workloads would be another way of falling foul of this sin: for example, testing one algorithm during the first half of a week, and another algorithm during the second half of a week, when the underlying distribution of workload is not uniform across the week."], "summary_text": "The truth, the whole truth, and nothing but the truth: A pragmatic guide to assessing empirical evaluations Blackburn et al. ACM Transactions on Programming Languages and Systems 2016  Yesterday we looked at some of the ways analysts may be fooled into thinking they’ve found a statistically significant result when in fact they haven’t. Today’s paper choice looks at what can go wrong with empirical evaluations. Let’s start with a couple of definitions:  An evaluation is either an experiment or an observational study, consisting of steps performed and data produced from those steps. A claim is an assertion about the significance and meaning of an evaluation; thus, unlike a hypothesis, which precedes an evaluation, a claim comes after the evaluation. A sound claim is one where the evaluation provides all the evidence necessary to support the claim, and does not provide any evidence that contradicts the claim. Assuming honest researchers, things can go wrong in one of two basic ways: sins of reasoning are those where the claim is not supported by the evaluation; and sins of exposition are those where the description of either the evaluation or the claim is not sufficient for readers to evaluate and/or reproduce the results. As the authors show, it’s not easy to avoid these traps even when you are doing your best. We’ll get into the details shortly, but for those authoring (or reviewing) evaluations and claims, here are five questions to help you avoid them:  Has all of the data from the evaluation been considered, and not just the data that supports the claim? Have any assumptions been made in forming the claim that are not justified by the evaluation? If the experimental evaluation is compared to prior work, is it an apples-to-oranges comparison? Has everything essential for getting the results been described? Has the claim been unambiguously and clearly stated? The two sins of exposition  The sin of inscrutability occurs when poor exposition obscures a claim. That is, we can’t be sure about what is really being claimed! The most basic form of inscrutability is simply omission – leaving the reader to figure out what the claims might be by themselves! Claims may also be ambiguous – for example, “improves performance” by 10% (throughput or latency? ), or “reduces latency by 10%” (at what percentile and under what load?). Distorted claims are clear from the reader’s perspective, but don’t actually capture what the author intended. The sin of irreproducability occurs when poor exposition obscures an evaluation: the evaluation steps, the data, or both. The evaluation may not be reproducible because key information is omitted, imprecise language and lack of detail may lead to ambiguity. One important kind of omission that is hard to guard against is an incomplete understanding of the factors that are relevant to the evaluation. Experience shows that whole communities can ignore important, though non-obvious, aspects of the empirical environment, only to find out their significance much later, throwing into question years of published results. This situation invites two responses: (i) authors should be held to the community’s standard of what is known about the importance of the empirical environment, and (ii) the community should intentionally and actively improve this knowledge, for example, by promoting reproduction studies. A good example of an unknown significant factor turns out to be the size of environment variables when measuring speed-ups of gcc in Linux! The size of the environment affects memory layout, and thus the performance of the program. An evaluation only exploring one (unspecified) environment size leads to the sin of irreproducibility. Researchers knew of course that memory layout affects performance, but it wasn’t obvious that environment variables would affect it enough to make a difference… until someone measured it. The three sins of reasoning  The three sins of reasoning are the sin of ignorance, the sin of inappropriateness, and the sin of inconsistency. The sin of ignorance occurs when a claim is made that ignores elements of the evaluation supporting a contradictory alternative claim (i.e., selecting data points to substantiate a claim while ignoring other relevant points). In our experience, while the sin of ignorance seems obvious and easy to avoid, in reality, it is far from it. Many factors in the evaluation that seem irrelevant to a claim may actually be critical to the soundness of the claim. Consider the following latency histogram for a component of Gmail that has both a fast-path and a slow-path through it. The distribution is bimodal, which means that commonly used statistical techniques making assumptions the data is normally distributed do not apply. If a claim is made on the basis of such techniques, it is unsound and we have committed the sin of ignorance. This is easy to see when you have the histogram, but suppose the experiment calculated mean and standard-deviation on the fly – then it may not be so obvious to us at all! The sin of inappropriateness occurs when a claim is made that is predicated on some fact that is absent from the evaluation. In our experience, while the sin of inappropriateness seems obvious and easy to avoid, in reality, it is far from it. Many factors that may be unaccounted for in evaluation may actually be important to derive a sound claim. Consider an evaluation trying to measure energy consumption. Often execution time is measured as a proxy for this, but it turns out that other factors can also affect energy consumption, and thus execution time does not support a claim about energy consumption. Knowing that execution time is not always an accurate proxy for energy consumption is not obvious – until it was pointed out in a 2001 paper. As another example, before it was widely understood that heap sizes have a big impact on garbage collector performance, many papers derived a claim from an evaluation on only one heap size, and did not report it. The sin of inconsistency is a sin of faulty comparison – two systems are compared, but each is evaluated in a way that is inconsistent with the other. In our experience, while the sin of inconsistency seems obvious and easy to avoid, in reality, it is far from it. Many artifacts that seem comparable may actually be inconsistent. Consider the use of hardware performance counters to evaluate performance…  …the performance counters that are provided by different vendors may vary, and even the performance counters provided by the same vendor may vary across different generations of the same architecture. Comparing data from what may seem like similar performance counters within an architecture, across architectures, or between generations of the same architecture may result in the sin of inconsistency, because the hardware performance counters are counting different hardware events. Comparing against inconsistent workloads would be another way of falling foul of this sin: for example, testing one algorithm during the first half of a week, and another algorithm during the second half of a week, when the underlying distribution of workload is not uniform across the week.", "pdf_url": "https://kar.kent.ac.uk/55171/1/Blackburn+2016TOPLAS.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/the-truth-the-whole-truth-and-nothing-but-the-truth-a-pragmatic-guide-to-assessing-empirical-evaluations.json"}
{"id": "28037079", "bin": "1300_1400", "summary_sentences": ["CORALS: who are my potential new customers?", "Tapping into the wisdom of customers’ decisions Li et al., WSDM’19  The authors of this paper won round 9 of the Yelp dataset challenge for their work.", "The goal is to find new target customers for local businesses by mining location-based checkins of users, user preferences, and online reviews.", "Location-based social networks attract millions of users to share their social friendship and their locations via check-ins.", "For example, an average of 142 million users check in at local businesses via Yelp every month.", "Foursquare and 55 million monthly active users and 8 million daily check-ins on the Swarm application.", "Facebook Local, powered by 70 million businesses, facilitates the discovery of local events and places for over one billion active daily users.", "(And of course these are just the explicit check-ins, location tracking has proved tempting to many businesses without requiring explicit user checkin actions.", "For example: Facebook is tracking your phone’s location… , Google’s location history is still recording your every move… , Google tracks your movements, like it or not , …).", "Check-ins give us a location history.", "Preference for a given business will be influenced by the proximity of that business to the places a given user frequents, in accordance with Tobler’s first law of geography : “everything is related to everything else, but near things are more related than distant things.” Which translates into:  … the propensity of a customer for a local business in inversely proportional to the distance between the customer and the business.", "It’s not quite that straightforward though.", "Customers will be prepared to travel more or less far depending on the type of business.", "For example, customers at the Phoenix art museum travel farther to get there on average than customers at a McDonald’s.", "For most people, their lives gravitate around two location centres (‘exploration centres’): home and work:  This is what that looks like for two individual users:  In addition to location and their general preferences, customers are also increasingly influenced by reviews.", "A 2016 study by BrightLocal found that 92% of customers regularly or occasionally read online reviews, which help them judge the quality of services offered.", "… the impact of online reviews is non-negligible and growing.", "One of the interesting findings from the evaluation is that to a reasonable extent, “you’re only as good as your last review!” The chart below shows a big jump in MAP when including information from the most recent review, and only a marginal performance gain with additional reviews.", "This is mainly due to the fact that customers only read a few latest reviews to perceive the reputation of the local business.", "Anyway, we’re getting ahead of ourselves…  CORALS  CORALS is a customer recommendation model based on historical check-in information, which integrates customer personal preferences, geographical influence, and business reputation (reviews).", "These are modelled by:  , the preference of customer i for business b,  , the geographical convenience of business b for customer i  , the reliance of customer i on the reputation  of business b  The overall tendency of a customer to visit a given business is just a linear combination of these three factors:  .", "The probability that a given customer will check-in at a given business location is learned by a series of pairwise comparisons.", "Say we know that customer i has checked in at location b.", "Sample another customer j at random.", "If j has not checked in at location b, then intuitively it ought to be that the probability for i to check in at location b should be higher than the probability of j to check in there.", "The probability that customer i is more likely to visit a business b than customer j is denoted by  , with  representing the model parameters.", "It’s just a linear combination of the same three factors, but this time looking at the distances between them (and then using the sigmoid function, represented by  in this case, to turn the sum into a probability).", "The overall model is constructed by maximising over all observed and sampled checkins:  where  models the parameters  with a Gaussian prior.", "Optimisation proceeds as follows.", "First the parameters  are initialised using the normal distribution.", "Then we iterate for a fixed number of iterations, performing  the following steps in each iteration:  For each observed check in (b, i),  sample random customers j who have not visited b, making up to $latex s_{max} attempts to find a _j_ such that the preference order between _i_ and _j_ is _not_ predicted correctly.", "If such a violation is found, update the corresponding parameter  .", "After iterating over every observed checkin, evaluate performance on the validation set.", "Accept the updates to  from this iteration if performance has improved on the validation set, otherwise reject them.", "Modelling preferences  The personal preference of customer i for business b is given by  where  and  are business and customer vector representations in the preference hidden space.", "(Learned by e.g. collaborative filtering or matrix factorisation techniques).", "Modelling reputations  The reliance of a customer i on the reputation of a business b is given by  where  and  are business and customer vector representations in the reputation hidden space.", "For business reputation vectors, word and sentence embeddings are used to turn review text into a vector.", "The n most recent reviews are used to form the final reputation vector, with n = 1 being the default.", "(Because ratings on a 0-5 scale weren’t available?", "I presume so.", "Someone must have studied the relative influence of words and rating numbers in online reviews… this infographic from Vendasta that turned up in a quick Google search certainly suggests stars are more important than the text).", "Modelling geographical convenience  The final missing component is the geographical convenience of a business b for customer i,  .", "To learn this the authors use a Gaussian mixture model taking a weighted sum over 2-vectors representing the latitude and longitude of locations visited by i.  Expectation-Maximization is used to estimate the parameters of the model to maximise the likelihood of sequences of check-ins made by users.", "How well does it work?", "The evaluation is conducted on the Yelp challenge dataset and on a Foursquare dataset.", "CORALS is compared against 12 other recommendation algorithms, as well as versions of itself using alternative optimisation strategies (see §3.2 for the full list).", "The bottom line is that CORALS is consistently one of the top scoring models across a range of cities:  ( Enlarge )  We can also see for example how geography and reputation have differing levels of influence for different kinds of businesses:  The results demonstrate that CORALS outperforms all these baselines by a significant margin in most scenarios.", "In addition to identifying potential new customers, we also break down the analysis for different types of businesses to evaluate the impact of various factors that may affect customers’ decisions.", "This information, in turn, provides a great resource for local businesses to adjust their advertising strategies and business services to attract more prospective customers."], "summary_text": "CORALS: who are my potential new customers? Tapping into the wisdom of customers’ decisions Li et al., WSDM’19  The authors of this paper won round 9 of the Yelp dataset challenge for their work. The goal is to find new target customers for local businesses by mining location-based checkins of users, user preferences, and online reviews. Location-based social networks attract millions of users to share their social friendship and their locations via check-ins. For example, an average of 142 million users check in at local businesses via Yelp every month. Foursquare and 55 million monthly active users and 8 million daily check-ins on the Swarm application. Facebook Local, powered by 70 million businesses, facilitates the discovery of local events and places for over one billion active daily users. (And of course these are just the explicit check-ins, location tracking has proved tempting to many businesses without requiring explicit user checkin actions. For example: Facebook is tracking your phone’s location… , Google’s location history is still recording your every move… , Google tracks your movements, like it or not , …). Check-ins give us a location history. Preference for a given business will be influenced by the proximity of that business to the places a given user frequents, in accordance with Tobler’s first law of geography : “everything is related to everything else, but near things are more related than distant things.” Which translates into:  … the propensity of a customer for a local business in inversely proportional to the distance between the customer and the business. It’s not quite that straightforward though. Customers will be prepared to travel more or less far depending on the type of business. For example, customers at the Phoenix art museum travel farther to get there on average than customers at a McDonald’s. For most people, their lives gravitate around two location centres (‘exploration centres’): home and work:  This is what that looks like for two individual users:  In addition to location and their general preferences, customers are also increasingly influenced by reviews. A 2016 study by BrightLocal found that 92% of customers regularly or occasionally read online reviews, which help them judge the quality of services offered. … the impact of online reviews is non-negligible and growing. One of the interesting findings from the evaluation is that to a reasonable extent, “you’re only as good as your last review!” The chart below shows a big jump in MAP when including information from the most recent review, and only a marginal performance gain with additional reviews. This is mainly due to the fact that customers only read a few latest reviews to perceive the reputation of the local business. Anyway, we’re getting ahead of ourselves…  CORALS  CORALS is a customer recommendation model based on historical check-in information, which integrates customer personal preferences, geographical influence, and business reputation (reviews). These are modelled by:  , the preference of customer i for business b,  , the geographical convenience of business b for customer i  , the reliance of customer i on the reputation  of business b  The overall tendency of a customer to visit a given business is just a linear combination of these three factors:  . The probability that a given customer will check-in at a given business location is learned by a series of pairwise comparisons. Say we know that customer i has checked in at location b. Sample another customer j at random. If j has not checked in at location b, then intuitively it ought to be that the probability for i to check in at location b should be higher than the probability of j to check in there. The probability that customer i is more likely to visit a business b than customer j is denoted by  , with  representing the model parameters. It’s just a linear combination of the same three factors, but this time looking at the distances between them (and then using the sigmoid function, represented by  in this case, to turn the sum into a probability). The overall model is constructed by maximising over all observed and sampled checkins:  where  models the parameters  with a Gaussian prior. Optimisation proceeds as follows. First the parameters  are initialised using the normal distribution. Then we iterate for a fixed number of iterations, performing  the following steps in each iteration:  For each observed check in (b, i),  sample random customers j who have not visited b, making up to $latex s_{max} attempts to find a _j_ such that the preference order between _i_ and _j_ is _not_ predicted correctly. If such a violation is found, update the corresponding parameter  . After iterating over every observed checkin, evaluate performance on the validation set. Accept the updates to  from this iteration if performance has improved on the validation set, otherwise reject them. Modelling preferences  The personal preference of customer i for business b is given by  where  and  are business and customer vector representations in the preference hidden space. (Learned by e.g. collaborative filtering or matrix factorisation techniques). Modelling reputations  The reliance of a customer i on the reputation of a business b is given by  where  and  are business and customer vector representations in the reputation hidden space. For business reputation vectors, word and sentence embeddings are used to turn review text into a vector. The n most recent reviews are used to form the final reputation vector, with n = 1 being the default. (Because ratings on a 0-5 scale weren’t available? I presume so. Someone must have studied the relative influence of words and rating numbers in online reviews… this infographic from Vendasta that turned up in a quick Google search certainly suggests stars are more important than the text). Modelling geographical convenience  The final missing component is the geographical convenience of a business b for customer i,  . To learn this the authors use a Gaussian mixture model taking a weighted sum over 2-vectors representing the latitude and longitude of locations visited by i.  Expectation-Maximization is used to estimate the parameters of the model to maximise the likelihood of sequences of check-ins made by users. How well does it work? The evaluation is conducted on the Yelp challenge dataset and on a Foursquare dataset. CORALS is compared against 12 other recommendation algorithms, as well as versions of itself using alternative optimisation strategies (see §3.2 for the full list). The bottom line is that CORALS is consistently one of the top scoring models across a range of cities:  ( Enlarge )  We can also see for example how geography and reputation have differing levels of influence for different kinds of businesses:  The results demonstrate that CORALS outperforms all these baselines by a significant margin in most scenarios. In addition to identifying potential new customers, we also break down the analysis for different types of businesses to evaluate the impact of various factors that may affect customers’ decisions. This information, in turn, provides a great resource for local businesses to adjust their advertising strategies and business services to attract more prospective customers.", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3289600.3290995?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/corals-who-are-my-potential-new-customers-tapping-into-the-wisdom-of-customers-decisions.json"}
{"id": "40771210", "bin": "1300_1400", "summary_sentences": ["DDSketch: a fast and fully-mergeable quantile sketch with relative-error guarantees Masson et al., VLDB’19  Datadog handles a ton of metrics – some customers have endpoints generating over 10M points per second!", "For response times (latencies) reporting a simple metric such as ‘average’ is next to useless.", "Instead we want to understand what’s happening at different latency percentiles (e.g p99).", "The ability to compute quantiles over aggregated metrics has been recognized to be an essential feature of any monitoring system… Given how expensive calculating exact quantiles can be for both storage and network bandwidth, most monitoring system will compress the data into sketches and compute approximate quantiles.", "Fortunately there are plenty of quantile sketching algorithms available including the GK-sketch, the t-digest, the HDR histogram, and the Moments sketch that we looked at last year.", "For reasons we’ll see shortly though, none of those were good enough for Datadog, so they developed their own sketching data structure, DDSketch.", "Officially in the paper DDSketch stands for ‘Distributed Distribution Sketch’ but that seems a bit of a stretch… surely it’s the ‘Datadog Sketch’ !", "A glance at the code repository for the Python implementation confirms my suspicion: there are several references to ‘DogSketch’ in the commit history and the codebase still ;).", "In putting together this write-up I’m grateful to the first author, Charles Masson, who gave a great talk on this paper at the VLDB conference and was kind enough to share his slides with me.", "Some of the images in this post are taken from Charles’ slide deck with permission.", "Why do we need a new sketch?", "A characteristic of latency distributions is that they have a long tail.", "This can play havoc with quantile sketches that are based on rank error.", "At the 50th percentile, an accuracy within 0.5% is still pretty tightly bound in absolute terms.", "But the further out into the tail we go, the more spread apart things get:  In this example, you can see that the GK-sketch, which uses rank error, is out by 1300ms compared to the ground truth.", "And look how noisy the sketch is!", "In another example in the paper, a rank accuracy of 0.5% at the 99%-ile guarantees us a value between the 98.5th and 99.5th percentiles…  In this case, this is anywhere from 2 to 20 seconds, which from an end user’s perspective is the difference between an annoying delay and giving up on the request.", "t-digest sketches are also rank based, but give lower errors on quantiles further away from the medium than uniform rank-error sketches.", "But even so, the error is still relatively high on heavily-tailed datasets.", "The HDR Histogram uses relative error, which retains accuracy into the tail (you’d expect nothing less from it’s creator, Gil Tene, who surely knows a thing or two about performance analysis !).", "Its two weaknesses are that it can only handle a bounded (but large!)", "range, and that it has no published guarantees.", "The Moments sketch only guarantees it’s accuracy for the average rank error (not the worst case).", "We want a fast-to-insert, fully mergeable (we can combine sketches in a distributed manner), space-efficient quantile sketch with relative error guarantees.", "So it looks like we’re going to need a new sketch!", "How DDSketch works  First let’s define what we mean by relative error.", "An α-accurate  -sketch outputs quantiles within  of the true value  for all quantiles  ,  .", "E.g., if α is 1%, then we’ll be within 1% of the true value.", "The basic version of the sketch gives (0,1) α-accurate sketches (i.e. they’re accurate over the full range).", "We’re going to divide response times into buckets, and count the number of points that fall into each bucket.", "The trick is to assign the bucket sizes (boundaries) such that when we compute quantiles using the bucket counts we retain the desired accuracy.", "Let  .", "So for  we have  .", "Bucket  will be used to store values between  and  .", "In practical terms, when we see a value  we take  and round up to the nearest whole number to get the bucket number.", "Then we just increment the counter for that bucket.", "Insertion is therefore relatively fast, and merging two sketches with the same  is as simple as summing up bucket counts by index.", "Now, when we want to know the estimate of the qth quantile we add up the bucket counts until we find the bucket containing the qth quantile value.", "With n total elements counted, it looks like this:  However buckets are stored in memory (e.g., as a dictionary that maps indices to bucket counters, or as a list of bucket counters for contiguous indices), the memory size of the sketch is at least linear in the number of non-empty buckets.", "So given a pathological input of n points each falling into a different bucket, we have worse case size O(N).", "To keep a bound on the size we can make one small tweak, yielding the the full DDSketch in all its glory:  The full version of DDSketch is a simple modification that addresses its unbounded growth by imposing a limit of  on the number of buckets it keeps track of.", "It does so by collapsing the buckets for the smallest indices:  When we merge two sketches that have both independently kept their number of buckets within the limit, we might end up with more buckets than we wanted.", "So we do the same bucket collapsing trick on merging as well to fix that.", "The paper of course has proofs that all this provides the desired guarantees, but I’m just going to take their word for it ;).", "For response times we don’t need to worry about negative values.", "But if you have a use case where you do, the solution is to keep one DDSketch for positive numbers, and another one for negative numbers.", "DDSketch in action  How well does it work?", "How about this:  Beautiful tracking of the true values without any discernible noise :).", "In the evaluation DDSketch (fast) stores buckets in a contiguous way for fast addition, and DDSketch (vanilla) stores buckets in a sparse way for smaller memory footprint.", "DDSketch is pretty good in terms of memory usage:  Has fast insert times:  And super-fast merges:  [DDSketch is] the first fully-mergeable, relative-error quantile sketching algorithm with formal guarantees.", "The sketch is extremely fast and accurate, and is currently being used by Datadog at a wide-scale.", "Yes, the academic literature will probably continue to refer to it as the ‘Distributed Distribution’ sketch.", "But you and I can just call it the “Datadog” sketch :).", "The kind folks at Datadog have even open sourced implementations for you in Python , Go , and Java ."], "summary_text": "DDSketch: a fast and fully-mergeable quantile sketch with relative-error guarantees Masson et al., VLDB’19  Datadog handles a ton of metrics – some customers have endpoints generating over 10M points per second! For response times (latencies) reporting a simple metric such as ‘average’ is next to useless. Instead we want to understand what’s happening at different latency percentiles (e.g p99). The ability to compute quantiles over aggregated metrics has been recognized to be an essential feature of any monitoring system… Given how expensive calculating exact quantiles can be for both storage and network bandwidth, most monitoring system will compress the data into sketches and compute approximate quantiles. Fortunately there are plenty of quantile sketching algorithms available including the GK-sketch, the t-digest, the HDR histogram, and the Moments sketch that we looked at last year. For reasons we’ll see shortly though, none of those were good enough for Datadog, so they developed their own sketching data structure, DDSketch. Officially in the paper DDSketch stands for ‘Distributed Distribution Sketch’ but that seems a bit of a stretch… surely it’s the ‘Datadog Sketch’ ! A glance at the code repository for the Python implementation confirms my suspicion: there are several references to ‘DogSketch’ in the commit history and the codebase still ;). In putting together this write-up I’m grateful to the first author, Charles Masson, who gave a great talk on this paper at the VLDB conference and was kind enough to share his slides with me. Some of the images in this post are taken from Charles’ slide deck with permission. Why do we need a new sketch? A characteristic of latency distributions is that they have a long tail. This can play havoc with quantile sketches that are based on rank error. At the 50th percentile, an accuracy within 0.5% is still pretty tightly bound in absolute terms. But the further out into the tail we go, the more spread apart things get:  In this example, you can see that the GK-sketch, which uses rank error, is out by 1300ms compared to the ground truth. And look how noisy the sketch is! In another example in the paper, a rank accuracy of 0.5% at the 99%-ile guarantees us a value between the 98.5th and 99.5th percentiles…  In this case, this is anywhere from 2 to 20 seconds, which from an end user’s perspective is the difference between an annoying delay and giving up on the request. t-digest sketches are also rank based, but give lower errors on quantiles further away from the medium than uniform rank-error sketches. But even so, the error is still relatively high on heavily-tailed datasets. The HDR Histogram uses relative error, which retains accuracy into the tail (you’d expect nothing less from it’s creator, Gil Tene, who surely knows a thing or two about performance analysis !). Its two weaknesses are that it can only handle a bounded (but large!) range, and that it has no published guarantees. The Moments sketch only guarantees it’s accuracy for the average rank error (not the worst case). We want a fast-to-insert, fully mergeable (we can combine sketches in a distributed manner), space-efficient quantile sketch with relative error guarantees. So it looks like we’re going to need a new sketch! How DDSketch works  First let’s define what we mean by relative error. An α-accurate  -sketch outputs quantiles within  of the true value  for all quantiles  ,  . E.g., if α is 1%, then we’ll be within 1% of the true value. The basic version of the sketch gives (0,1) α-accurate sketches (i.e. they’re accurate over the full range). We’re going to divide response times into buckets, and count the number of points that fall into each bucket. The trick is to assign the bucket sizes (boundaries) such that when we compute quantiles using the bucket counts we retain the desired accuracy. Let  . So for  we have  . Bucket  will be used to store values between  and  . In practical terms, when we see a value  we take  and round up to the nearest whole number to get the bucket number. Then we just increment the counter for that bucket. Insertion is therefore relatively fast, and merging two sketches with the same  is as simple as summing up bucket counts by index. Now, when we want to know the estimate of the qth quantile we add up the bucket counts until we find the bucket containing the qth quantile value. With n total elements counted, it looks like this:  However buckets are stored in memory (e.g., as a dictionary that maps indices to bucket counters, or as a list of bucket counters for contiguous indices), the memory size of the sketch is at least linear in the number of non-empty buckets. So given a pathological input of n points each falling into a different bucket, we have worse case size O(N). To keep a bound on the size we can make one small tweak, yielding the the full DDSketch in all its glory:  The full version of DDSketch is a simple modification that addresses its unbounded growth by imposing a limit of  on the number of buckets it keeps track of. It does so by collapsing the buckets for the smallest indices:  When we merge two sketches that have both independently kept their number of buckets within the limit, we might end up with more buckets than we wanted. So we do the same bucket collapsing trick on merging as well to fix that. The paper of course has proofs that all this provides the desired guarantees, but I’m just going to take their word for it ;). For response times we don’t need to worry about negative values. But if you have a use case where you do, the solution is to keep one DDSketch for positive numbers, and another one for negative numbers. DDSketch in action  How well does it work? How about this:  Beautiful tracking of the true values without any discernible noise :). In the evaluation DDSketch (fast) stores buckets in a contiguous way for fast addition, and DDSketch (vanilla) stores buckets in a sparse way for smaller memory footprint. DDSketch is pretty good in terms of memory usage:  Has fast insert times:  And super-fast merges:  [DDSketch is] the first fully-mergeable, relative-error quantile sketching algorithm with formal guarantees. The sketch is extremely fast and accurate, and is currently being used by Datadog at a wide-scale. Yes, the academic literature will probably continue to refer to it as the ‘Distributed Distribution’ sketch. But you and I can just call it the “Datadog” sketch :). The kind folks at Datadog have even open sourced implementations for you in Python , Go , and Java .", "pdf_url": "http://www.vldb.org/pvldb/vol12/p2195-masson.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/ddsketch.json"}
{"id": "66357293", "bin": "1300_1400", "summary_sentences": ["Smoke: fine-grained lineage at interactive speed Psallidas et al., VLDB’18  Data lineage connects the input and output data items of a computation.", "Given a set of output records, a backward lineage query selects a subset of the output records and asks “which input records contributed to these results?” A forward lineage query selects a subset of the input records and asks, “which output records depend on these inputs?”.", "Lineage-enabled systems capture record-level relationships throughout a workflow and support lineage queries.", "Data lineage is useful in lots of different applications; this paper uses as its main example interactive visualisation systems.", "This domain requires fast answers to queries and is typically dominated by hand-written implementations.", "Consider the two views in the figure below.", "When the user selects a set of marks in  , marks derived from the same records are highlighted in  (linked brushing).", "A typical visualisation system implements this manually, but it can equally be viewed as a backward lineage query from the selection points in  , followed by a forward lineage query from the resulting input records to  .", "(See ‘ Explaining outputs in modern data analytics’ which we looked at last year for an introduction to lineage and provenance principles.", "Chotia et al. use a shadow dataflow graph mapping the original computation but flowing in the opposite direction…).", "Challenges with existing lineage capture systems  We have the usual space/time trade-offs to consider.", "We can slow down the base query in order to capture lineage information during query execution (and store that information somewhere).", "This speeds up answering lineage queries later on.", "Or we can keep base queries fast and lazily materialize lineage information later when lineage queries are asked (making them slower).", "As data processing engines become faster, an important question —and the main focus of this paper— is whether it is possible to achieve the best of both worlds: negligible lineage capture overhead, as well as fast lineage query execution.", "Smoke’s four principles  Smoke employs four central design principles to try and pull off this trick.", "Tight integration of lineage capture into query execution itself, using write-efficient data structures.", "Where apriori knowledge of lineage queries is available (e.g., the set of explorations supported by a visualisation tool), this information is used to minimise the amount of lineage that needs to be materialized.", "Again, if we know lineage queries in advance, and those queries are only interested in aggregated information then we can potentially materialize aggregate statistics as we process queries, and prune or re-partition lineage indices.", "Wherever possible, data structures constructed during normal operation execution are augmented and reused for lineage purposes rather than introducing separated dedicated structures.", "…Smoke is an in-memory query compilation database engine that tightly integrates the lineage capture logic within query execution and uses simple, write-efficient lineage indexes for low-overhead lineage capture.", "In addition, Smoke enables workload-aware optimizations that prune captured lineage and push the logic of lineage consuming queries down into the lineage capture phase.", "Lineage capture  Smoke uses read- and write-efficient index structures based on row ids to capture lineage information.", "1-N relationships (between input and output tuples) are represented as inverted indexes.", "The index’s ith entry corresponds to the ith output group, and points to a row id array containing the ids of all input records that belong to the group.", "1-1 relationships between input and output are represented as a single array.", "These indices are populated through a tight integration of lineage capture and relation operator logic to avoid additional API calls, and to facilitate co-optimisation.", "There are two basic strategies depending on the operator and circumstances: defer and inject.", "Inject strategies incur the full cost of index generation during base query execution, whereas defer strategies defer (portions of) the lineage capture until after the operation execution.", "Consider selection: both forward and backward lineage capture use row-id arrays, with the forward one pre-allocated based on the cardinality of the input relation.", "While iterating over the relation evaluating the predicate, the inject strategy adds two counters to track the row ids of the current input and output and uses these to update the indices when an output row is emitted.", "There is no defer strategy for selection, because it is strictly inferior to inject.", "As another example consider hash joins.", "Smoke will generate both backward row id arrays, and forward row-id indexes.", "Under the inject strategy a build phase augments each hash table entry with a row id array containing the input row ids for that entry’s join key.", "The probe phase tracks the row id for each output record and populates the forward and backward indexes.", "One drawback of this strategy is that we might trigger multiple re-allocations (growing the size) of the forward indexes if an input record has many matches.", "The defer strategy for hash joins takes advantage of the fact that we know the size of the forward indexes after the probe phase.", "The build phase maintains an additional output row id list, storing the first output record for each match (output records are emitted contiguously).", "After the probe phase, the forward and backward indexes can be pre-allocated and populated in a final scan of the hash table.", "For multi-operator plans Smoke propagates lineage information through plan execution so that only a single set of lineage indexes connecting the input and final output relations are emitted.", "It also takes advantage of pipelines (that merge multiple operators into a single pipeline as part of normal query execution) to eliminate intermediate lineage materialization points where possible.", "Workload-aware optimisations  When the set of lineage queries is known in advance, Smoke can go further than the baseline lineage capture describe above and also apply instrumentation pruning and optimisation push-down.", "Instrumentation pruning disables lineage capture for lineage indexes that will not be used by the workload.", "Lineage queries that apply summarisation / aggregation before presenting results provide an opportunity to push-down optimisations.", "For example, selection push-down when using a static predicate, and partitioning of index arrays by predicate attributes for dynamic selections.", "Group-by aggregation can also be pushed down into lineage capture.", "We observe that popular provenance semantics (e.g. which and why provenance) can be expressed as lineage consuming queries and pushed down using the above optimizations.", "In other words, Smoke can operate as a system with alternative provenance semantics depending on the given lineage consuming query.", "Evaluation  Smoke is compared to state-of-the-art logical and physical lineage capture and query approaches using a combination of microbenchmarks, TPC-H queries, and two real-world applications.", "Smoke’s lineage capture techniques outperform both logical and physical approaches by up to two orders of magnitude.", "For example:  Smoke also sped-up lineage query evaluation by multiple orders of magnitude, especially for low-selectivity lineage queries.", "The two real-world applications used in the evaluation were Crossfilter visualisation and data profiling with UGuide.", "The results show that :  Lineage can express many real-world tasks from visualisation and data profiling, that are currently implemented by hand in ad-hoc ways, and  the lineage capture mechanism is fast enough to avoid sacrificing performance vs the hand implementations, and in many cases may even perform better.", "Smoke illustrates that it is possible to both capture lineage with low overhead and enable fast lineage query performance… Our capture techniques and workload-aware optimization make Smoke well-suited for online; adaptive; and offline physical database design settings."], "summary_text": "Smoke: fine-grained lineage at interactive speed Psallidas et al., VLDB’18  Data lineage connects the input and output data items of a computation. Given a set of output records, a backward lineage query selects a subset of the output records and asks “which input records contributed to these results?” A forward lineage query selects a subset of the input records and asks, “which output records depend on these inputs?”. Lineage-enabled systems capture record-level relationships throughout a workflow and support lineage queries. Data lineage is useful in lots of different applications; this paper uses as its main example interactive visualisation systems. This domain requires fast answers to queries and is typically dominated by hand-written implementations. Consider the two views in the figure below. When the user selects a set of marks in  , marks derived from the same records are highlighted in  (linked brushing). A typical visualisation system implements this manually, but it can equally be viewed as a backward lineage query from the selection points in  , followed by a forward lineage query from the resulting input records to  . (See ‘ Explaining outputs in modern data analytics’ which we looked at last year for an introduction to lineage and provenance principles. Chotia et al. use a shadow dataflow graph mapping the original computation but flowing in the opposite direction…). Challenges with existing lineage capture systems  We have the usual space/time trade-offs to consider. We can slow down the base query in order to capture lineage information during query execution (and store that information somewhere). This speeds up answering lineage queries later on. Or we can keep base queries fast and lazily materialize lineage information later when lineage queries are asked (making them slower). As data processing engines become faster, an important question —and the main focus of this paper— is whether it is possible to achieve the best of both worlds: negligible lineage capture overhead, as well as fast lineage query execution. Smoke’s four principles  Smoke employs four central design principles to try and pull off this trick. Tight integration of lineage capture into query execution itself, using write-efficient data structures. Where apriori knowledge of lineage queries is available (e.g., the set of explorations supported by a visualisation tool), this information is used to minimise the amount of lineage that needs to be materialized. Again, if we know lineage queries in advance, and those queries are only interested in aggregated information then we can potentially materialize aggregate statistics as we process queries, and prune or re-partition lineage indices. Wherever possible, data structures constructed during normal operation execution are augmented and reused for lineage purposes rather than introducing separated dedicated structures. …Smoke is an in-memory query compilation database engine that tightly integrates the lineage capture logic within query execution and uses simple, write-efficient lineage indexes for low-overhead lineage capture. In addition, Smoke enables workload-aware optimizations that prune captured lineage and push the logic of lineage consuming queries down into the lineage capture phase. Lineage capture  Smoke uses read- and write-efficient index structures based on row ids to capture lineage information. 1-N relationships (between input and output tuples) are represented as inverted indexes. The index’s ith entry corresponds to the ith output group, and points to a row id array containing the ids of all input records that belong to the group. 1-1 relationships between input and output are represented as a single array. These indices are populated through a tight integration of lineage capture and relation operator logic to avoid additional API calls, and to facilitate co-optimisation. There are two basic strategies depending on the operator and circumstances: defer and inject. Inject strategies incur the full cost of index generation during base query execution, whereas defer strategies defer (portions of) the lineage capture until after the operation execution. Consider selection: both forward and backward lineage capture use row-id arrays, with the forward one pre-allocated based on the cardinality of the input relation. While iterating over the relation evaluating the predicate, the inject strategy adds two counters to track the row ids of the current input and output and uses these to update the indices when an output row is emitted. There is no defer strategy for selection, because it is strictly inferior to inject. As another example consider hash joins. Smoke will generate both backward row id arrays, and forward row-id indexes. Under the inject strategy a build phase augments each hash table entry with a row id array containing the input row ids for that entry’s join key. The probe phase tracks the row id for each output record and populates the forward and backward indexes. One drawback of this strategy is that we might trigger multiple re-allocations (growing the size) of the forward indexes if an input record has many matches. The defer strategy for hash joins takes advantage of the fact that we know the size of the forward indexes after the probe phase. The build phase maintains an additional output row id list, storing the first output record for each match (output records are emitted contiguously). After the probe phase, the forward and backward indexes can be pre-allocated and populated in a final scan of the hash table. For multi-operator plans Smoke propagates lineage information through plan execution so that only a single set of lineage indexes connecting the input and final output relations are emitted. It also takes advantage of pipelines (that merge multiple operators into a single pipeline as part of normal query execution) to eliminate intermediate lineage materialization points where possible. Workload-aware optimisations  When the set of lineage queries is known in advance, Smoke can go further than the baseline lineage capture describe above and also apply instrumentation pruning and optimisation push-down. Instrumentation pruning disables lineage capture for lineage indexes that will not be used by the workload. Lineage queries that apply summarisation / aggregation before presenting results provide an opportunity to push-down optimisations. For example, selection push-down when using a static predicate, and partitioning of index arrays by predicate attributes for dynamic selections. Group-by aggregation can also be pushed down into lineage capture. We observe that popular provenance semantics (e.g. which and why provenance) can be expressed as lineage consuming queries and pushed down using the above optimizations. In other words, Smoke can operate as a system with alternative provenance semantics depending on the given lineage consuming query. Evaluation  Smoke is compared to state-of-the-art logical and physical lineage capture and query approaches using a combination of microbenchmarks, TPC-H queries, and two real-world applications. Smoke’s lineage capture techniques outperform both logical and physical approaches by up to two orders of magnitude. For example:  Smoke also sped-up lineage query evaluation by multiple orders of magnitude, especially for low-selectivity lineage queries. The two real-world applications used in the evaluation were Crossfilter visualisation and data profiling with UGuide. The results show that :  Lineage can express many real-world tasks from visualisation and data profiling, that are currently implemented by hand in ad-hoc ways, and  the lineage capture mechanism is fast enough to avoid sacrificing performance vs the hand implementations, and in many cases may even perform better. Smoke illustrates that it is possible to both capture lineage with low overhead and enable fast lineage query performance… Our capture techniques and workload-aware optimization make Smoke well-suited for online; adaptive; and offline physical database design settings.", "pdf_url": "http://www.vldb.org/pvldb/vol11/p719-psallidas.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/smoke-fine-grained-lineage-at-interactive-speed.json"}
{"id": "66112363", "bin": "1300_1400", "summary_sentences": ["A New Metaheuristic Bat-Inspired Algorithm – Xin-She Yang, 2010  Today it’s the turn of bats!", "The bat algorithm is an attempt to combine some of the major advantages of previous algorithms such as the firefly algorithm and harmony search (inspired by music composition).", "It is based on the echo-location behaviour of bats.", "Bats are fascinating animals.", "They are the only mammals with wings and they also have advanced capability of echolocation.", "It is estimated that there are about 996 different species which account for up to 20% of all mammal species.", "Their size ranges from the tiny bumblebee bat (of about 1.5 to 2g) to the giant bats with wingspan of about 2m and weight up to about 1 kg.", "Microbats typically have forearm length of about 2.2 to 11cm.", "Most bats uses echolocation to a certain degree; among all the species, microbats are a famous example as microbats use echolocation extensively while megabats do not.", "(I think they missed a trick by not calling the megabats monoliths ;) ).", "Microbats use a type of sonar, called, echolocation, to detect prey, avoid obstacles, and locate their roosting crevices in the dark.", "These bats emit a very loud sound pulse and listen for the echo that bounces back from the surrounding objects.", "Their pulses vary in properties and can be correlated with their hunting strategies, depending on the species.", "Most bats use short, frequency-modulated signals to sweep through about an octave, while others more often use constant-frequency signals for echolocation.", "Their signal bandwidth varies depends on the species, and often increased by using more harmonics.", "Microbats emit about 10-20 sound bursts per second.", "When hunting this can go up to about 200 pulses a second as they get near their prey.", "“Such short sound bursts imply the fantastic ability of the signal processing power of bats.", "In fact, studies shows the integration time of the bat ear is typically about 300 to 400 µs.” The wavelengths are in the same order as their prey sizes.", "Fortunately these pulses are in the ultrasonic region because they are astonishingly loud – up to about 110 dB.", "(That’s roughly the same volume as a live rock concert, or a jackhammer).", "The loudness also varies from the loudest when searching for prey and to a quieter base when homing towards the prey.", "The travelling range of such short pulses are typically a few metres, depending on the actual frequencies.", "Microbats can manage to avoid obstacles as small as thin human hairs.", "Studies show that microbats use the time delay from the emission and detection of the echo, the time difference between their two ears, and the loudness variations of the echoes to build up three dimensional scenario of the surrounding.", "They can detect the distance and orientation of the target, the type of prey, and even the moving speed of the prey such as small insects.", "Indeed, studies suggested that bats seem to be able to discriminate targets by the variations of the Doppler effect induced by the wing-flutter rates of the target insects.", "Bats are pretty amazing really!", "By equating the echo-location behaviour of bats with an objective function to be optimised, we can formulate new optimisation algorithms.", "We will use the following rules/simplifications for our idealized bats:  Echolocation is used to sense distance (ignore any eyesight etc.).", "Bats fly randomly with a velocity vi , at position xi.", "They emit pulses at a fixed wavelength λ, with varying frequency f and loudness A to search for prey.", "The rate of pulse emission r ∈ [0,1] varies depending on the proximity of the target  Loudness varies from a large positive A0 to a minimum constant value Amin  Another obvious simplification is that no ray tracing is used in estimating the time delay and three dimensional topography (!).", "Though this might be a good feature for the application in computational geometry, however, we will not use this as it is more computationally extensive in multidimensional cases.", "Starting out with an initial population of bats spread over the solution space, the algorithm proceeds in iterations.", "Each bat is randomly assigned a start frequency drawn from [fmin,fmax] – for example, 0..100.", "Each bat is also given a random initial loudness and pulse emission rate r ∈ [0,1] close to zero.", "For each iteration, we update the frequency, position and velocity of each bat in the search space as follows.", "Draw a random number between 0 and 1, if it is less than or equal to the current rate of bat i, then update the bat’s location using:  fi = fmin + β(fmax – fmin) , where β is a random number between 0 and 1.  vi = vi + fi(xi – xG),  where xG is the current global best location (solution)  xi = xi + vi  However, if the random number is greater than ri, then instead move the bat to a new location generated by taking a random walk from the current best solution:  xi = xG + εA , where A is the average loudness of all bats at this time, and ε is a random number drawn from [-1,1].", "Now we evaluate the objective function at the new location for the bat.", "If the fitness has improved and a random number drawn between A0 and Amin is less than the current loudness for the bat, then we accept (move it to) the the location.", "Finally we update the loudness and rate of pulse emission for the bat:  Ai = αAi  rit+1 = ri0[1 – exp(-λt)], where t represents the time step (iteration)  α and λ are constants, both set to 0.9 in the simulation in the paper.", "At the end of our chosen number of iterations, the bat at the best location (solution) is the winner.", "Note that the algorithm description given in the paper I found quite confusing, and ended up piecing together the above from a combination of the paper and the pseudo-code in Xin-She Yang’s book .", "[In tests,] the Bat Algorithm is much superior to other algorithms [Genetic Algorithms and PSO] in terms of accuracy and efficiency.", "This is not surprising as the aim of developing the new algorithm was to try to use the advantages of existing algorithms and other interesting feature inspired by the fantastic behaviour of echolocation of microbats.", "The Bat Algorithm subsumes (is more powerful than) PSO and Harmony Search:  If we replace the variations of the frequency fi by a random parameter and setting Ai = 0 and ri= 1, the bat algorithm essentially becomes the standard Particle Swarm Optimization (PSO).", "Similarly, if we do not use the velocities, but we use fixed loudness and rate: Ai and ri – for example, Ai = ri= 0.7 – this algorithm is virtually reduced to a simple Harmony Search (HS) as the frequency/wavelength change is essentially the pitch adjustment, while the rate of pulse emission is similar to the harmonic acceptance rate (here with a twist) in the harmony search algorithm.", "The current studies implies that the proposed new algorithm is potentially more powerful and thus should be investigated further in many applications of engineering and industrial optimization problems."], "summary_text": "A New Metaheuristic Bat-Inspired Algorithm – Xin-She Yang, 2010  Today it’s the turn of bats! The bat algorithm is an attempt to combine some of the major advantages of previous algorithms such as the firefly algorithm and harmony search (inspired by music composition). It is based on the echo-location behaviour of bats. Bats are fascinating animals. They are the only mammals with wings and they also have advanced capability of echolocation. It is estimated that there are about 996 different species which account for up to 20% of all mammal species. Their size ranges from the tiny bumblebee bat (of about 1.5 to 2g) to the giant bats with wingspan of about 2m and weight up to about 1 kg. Microbats typically have forearm length of about 2.2 to 11cm. Most bats uses echolocation to a certain degree; among all the species, microbats are a famous example as microbats use echolocation extensively while megabats do not. (I think they missed a trick by not calling the megabats monoliths ;) ). Microbats use a type of sonar, called, echolocation, to detect prey, avoid obstacles, and locate their roosting crevices in the dark. These bats emit a very loud sound pulse and listen for the echo that bounces back from the surrounding objects. Their pulses vary in properties and can be correlated with their hunting strategies, depending on the species. Most bats use short, frequency-modulated signals to sweep through about an octave, while others more often use constant-frequency signals for echolocation. Their signal bandwidth varies depends on the species, and often increased by using more harmonics. Microbats emit about 10-20 sound bursts per second. When hunting this can go up to about 200 pulses a second as they get near their prey. “Such short sound bursts imply the fantastic ability of the signal processing power of bats. In fact, studies shows the integration time of the bat ear is typically about 300 to 400 µs.” The wavelengths are in the same order as their prey sizes. Fortunately these pulses are in the ultrasonic region because they are astonishingly loud – up to about 110 dB. (That’s roughly the same volume as a live rock concert, or a jackhammer). The loudness also varies from the loudest when searching for prey and to a quieter base when homing towards the prey. The travelling range of such short pulses are typically a few metres, depending on the actual frequencies. Microbats can manage to avoid obstacles as small as thin human hairs. Studies show that microbats use the time delay from the emission and detection of the echo, the time difference between their two ears, and the loudness variations of the echoes to build up three dimensional scenario of the surrounding. They can detect the distance and orientation of the target, the type of prey, and even the moving speed of the prey such as small insects. Indeed, studies suggested that bats seem to be able to discriminate targets by the variations of the Doppler effect induced by the wing-flutter rates of the target insects. Bats are pretty amazing really! By equating the echo-location behaviour of bats with an objective function to be optimised, we can formulate new optimisation algorithms. We will use the following rules/simplifications for our idealized bats:  Echolocation is used to sense distance (ignore any eyesight etc.). Bats fly randomly with a velocity vi , at position xi. They emit pulses at a fixed wavelength λ, with varying frequency f and loudness A to search for prey. The rate of pulse emission r ∈ [0,1] varies depending on the proximity of the target  Loudness varies from a large positive A0 to a minimum constant value Amin  Another obvious simplification is that no ray tracing is used in estimating the time delay and three dimensional topography (!). Though this might be a good feature for the application in computational geometry, however, we will not use this as it is more computationally extensive in multidimensional cases. Starting out with an initial population of bats spread over the solution space, the algorithm proceeds in iterations. Each bat is randomly assigned a start frequency drawn from [fmin,fmax] – for example, 0..100. Each bat is also given a random initial loudness and pulse emission rate r ∈ [0,1] close to zero. For each iteration, we update the frequency, position and velocity of each bat in the search space as follows. Draw a random number between 0 and 1, if it is less than or equal to the current rate of bat i, then update the bat’s location using:  fi = fmin + β(fmax – fmin) , where β is a random number between 0 and 1.  vi = vi + fi(xi – xG),  where xG is the current global best location (solution)  xi = xi + vi  However, if the random number is greater than ri, then instead move the bat to a new location generated by taking a random walk from the current best solution:  xi = xG + εA , where A is the average loudness of all bats at this time, and ε is a random number drawn from [-1,1]. Now we evaluate the objective function at the new location for the bat. If the fitness has improved and a random number drawn between A0 and Amin is less than the current loudness for the bat, then we accept (move it to) the the location. Finally we update the loudness and rate of pulse emission for the bat:  Ai = αAi  rit+1 = ri0[1 – exp(-λt)], where t represents the time step (iteration)  α and λ are constants, both set to 0.9 in the simulation in the paper. At the end of our chosen number of iterations, the bat at the best location (solution) is the winner. Note that the algorithm description given in the paper I found quite confusing, and ended up piecing together the above from a combination of the paper and the pseudo-code in Xin-She Yang’s book . [In tests,] the Bat Algorithm is much superior to other algorithms [Genetic Algorithms and PSO] in terms of accuracy and efficiency. This is not surprising as the aim of developing the new algorithm was to try to use the advantages of existing algorithms and other interesting feature inspired by the fantastic behaviour of echolocation of microbats. The Bat Algorithm subsumes (is more powerful than) PSO and Harmony Search:  If we replace the variations of the frequency fi by a random parameter and setting Ai = 0 and ri= 1, the bat algorithm essentially becomes the standard Particle Swarm Optimization (PSO). Similarly, if we do not use the velocities, but we use fixed loudness and rate: Ai and ri – for example, Ai = ri= 0.7 – this algorithm is virtually reduced to a simple Harmony Search (HS) as the frequency/wavelength change is essentially the pitch adjustment, while the rate of pulse emission is similar to the harmonic acceptance rate (here with a twist) in the harmony search algorithm. The current studies implies that the proposed new algorithm is potentially more powerful and thus should be investigated further in many applications of engineering and industrial optimization problems.", "pdf_url": "http://arxiv.org/pdf/1004.4170v1.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/a-new-metaheuristic-bat-inspired-algorithm.json"}
{"id": "65756728", "bin": "1300_1400", "summary_sentences": ["Graph Neural Network (GNN) is a family of powerful machine learning (ML) models for graphs that can combine node information with the structural information.", "One downside of GNNs is that their predictions are hard to interpret.", "The paper proposes GNN Explainer model for solving the problem of interpretability.", "Paper  Desiderata for GNN explanations  Local edge fidelity - identify the subgraph structure (ideally the smallest) that significantly affected the predictions of the GNN.", "ie identify the important edges in the graph (for a given prediction).", "Local node fidelity - identify the import node features and correlations in the features of the neighboring nodes.", "Single instance and multi-instance explanations - Support both single instance prediction tasks and multi-instance prediction tasks.", "Model Agnostic - Support a large family of models (ideally all)  Task Agnostic - Support a large family of tasks (ideally all)  Approach  I first describe the single instance prediction case and use that as the base to describe the multiple instance prediction cases.", "All the discussion in this section assumes a single instance prediction task.", "Input: Trained GNN, a single instance whose prediction is to be explained.", "Task: Identify the small subgraph and the small subset of features that explain the prediction.", "Idea: Maximize the mutual information (MI) between the GNN and the explanation by learning a graph mask which can be used for selecting the relevant subgraph (from the GNN’s computational graph) and features (from all layers of the GNN).", "Computational graph of GNN (corresponding to a node) refers to the approx L-hop neighborhood of the node in the graph ie the subgraph formed by nodes and edges whose representation affected the representation of the given node.", "Single-Instance Explanations  For a node v, the information used to predict its label y is completely described by its computation graph Gc(v) and the associated feature set Xc(v).", "The feature set includes the features of all the nodes in the computation graph.", "When constructing the explaination, only Gc(v) and Xc(v) are used.", "The task can be reformulated as identifying a subgraph GS (subset of Gc(v)) with associated features XS which are important when predicting the label y for node v.  “Importance” is measured in terms of MI  MI(Y, (GS, XS)) = H(Y) - H(Y | G = GS, X = XS) where H is the entropy and Y is a random variable representing the prediction.", "A further constraint, | GS| < k is imposed to obtain consise explaintations.", "Since H(Y) is fixed (recall that the network has already been trained and is now being used in the inference mode), maximizing MI is equivalent to minimizing the conditional entropy H(Y | G = GS, X = XS)  This is equivalent to selecting the subgraph that minimizes the uncertainty in the prediction of y when the computational graph is Gc(v)  Optimiation Process  Given the exponentially large number of possible subgraphs, we can not directly optimize the given equation.", "A “relaxed”-adjacency matrix (whose values are real numbers in the range 0 to 1) is introduced where each element of this fractional adjacency matrix is smaller than the corresponding element of the original adjacency matrix.", "Gradient descent can be performed on this adjacency matrix.", "The “relaxed” GS can be interpreted as a variational approximation of the subgraph distributions of Gc(v) and the objective can be written as min EGSH(Y | G = GS, X = XS)  Now the paper makes a big approximation that the GNN is convex so as to leverage the Jensen inequality and push the expectation inside the entropy term to get an upper bound and then minimize that ie min H(Y | G = Es[GS], X = XS)  The paper reports that the convexity approximation (along with discreteness constraint) works in practice.", "Next, mean field approximation is used to decompose P(GS) as a multivariate Bernoulli distrbitution ie product of AS(i, j) for all (i, j) belonging to Gc(v).", "AS can be optimized directly and its values represent the expectation of the Bernoulli distrbitution on wether the edge ei, j exists.", "Given the constraints on AS, it is easier to learn a mask matrix M and optimize that such that AS = M * Ac* Additionally, the sigmod operator can be applied on M.  Once M is learned, only the top k values are retained.", "Including Node Features in the Explanation  Similar to the previous approach, another feature mask is learned (either one for entire GNN or one per node of the GNN) and is used as a feature selector.", "The mask could either be learned such that same set of node features (in terms of dimensions) are selected or a different set of features are selected per node.", "The paper uses the former as it is more straightforward.", "Just like before, a “relaxed” mask MT is trained to select features as MT * XS.", "One tricky case is where one feature is important but its value is set to 0.", "In the case, the value will be masked even though it should not be  The workaround is to use Monte Carlo (MC) estimates of marginals of the missing features.", "This gives a way to assign importance scores to each feature dimension and a form of reparameterization trick is used to perform end-to-end learning.", "Masks are encouraged to be discrete by regularizing their element-wise entropy.", "Resulting computation graph is valid as in it allows message passing towards the central node v.  Multi-Instance Explanations  Given a set of nodes (having the label say y),  the task is to obtain a global explanation of the predictions.", "For the given class, a prototypical reference node is chosen by computing the mean of embeddings of all the nodes in the class and then selecting the node which is closest to the mean.", "Now, compute the important computational graph corresponding to this node and align the computational subgraphs of all the other nodes (in the given class) to reference.", "Let A* be the adjacency matrix and X* be the feature matrix for the explanation corresponding to the reference node.", "Let Av and Xv be the adjacency matrix and feature matrix of the to-ber-aligned computational graph.", "A relaed alignment matrix P is optimized to align the nodes and features in the two graphs ie we minimize |PTAvP - A*| + *|PTXvP - X*|  Choosing concise explanations helps in efficient graph matching.", "For GNNs that compute attention over the entire graph, edges with low attention weight can be pruned to increase efficiency.", "Experiments  Datasets  Node classification: BA-Shapes, BA-Community, Tree-Cycles, Tree-Grid  Graph classification: MUTAG, Reddit-Binary  Baselines  GRAD - Compute the gradient of the model loss with respect to the adjacency matrix and the node features to be classified and fix the edges with the highest absolute gradient.", "GAT - Graph Attention Network  The proposed model seems to outperform the baselines both qualitatively and quantitatively.", "But the results should be taken with a grain of salt as only 2 baselines are considered."], "summary_text": "Graph Neural Network (GNN) is a family of powerful machine learning (ML) models for graphs that can combine node information with the structural information. One downside of GNNs is that their predictions are hard to interpret. The paper proposes GNN Explainer model for solving the problem of interpretability. Paper  Desiderata for GNN explanations  Local edge fidelity - identify the subgraph structure (ideally the smallest) that significantly affected the predictions of the GNN. ie identify the important edges in the graph (for a given prediction). Local node fidelity - identify the import node features and correlations in the features of the neighboring nodes. Single instance and multi-instance explanations - Support both single instance prediction tasks and multi-instance prediction tasks. Model Agnostic - Support a large family of models (ideally all)  Task Agnostic - Support a large family of tasks (ideally all)  Approach  I first describe the single instance prediction case and use that as the base to describe the multiple instance prediction cases. All the discussion in this section assumes a single instance prediction task. Input: Trained GNN, a single instance whose prediction is to be explained. Task: Identify the small subgraph and the small subset of features that explain the prediction. Idea: Maximize the mutual information (MI) between the GNN and the explanation by learning a graph mask which can be used for selecting the relevant subgraph (from the GNN’s computational graph) and features (from all layers of the GNN). Computational graph of GNN (corresponding to a node) refers to the approx L-hop neighborhood of the node in the graph ie the subgraph formed by nodes and edges whose representation affected the representation of the given node. Single-Instance Explanations  For a node v, the information used to predict its label y is completely described by its computation graph Gc(v) and the associated feature set Xc(v). The feature set includes the features of all the nodes in the computation graph. When constructing the explaination, only Gc(v) and Xc(v) are used. The task can be reformulated as identifying a subgraph GS (subset of Gc(v)) with associated features XS which are important when predicting the label y for node v.  “Importance” is measured in terms of MI  MI(Y, (GS, XS)) = H(Y) - H(Y | G = GS, X = XS) where H is the entropy and Y is a random variable representing the prediction. A further constraint, | GS| < k is imposed to obtain consise explaintations. Since H(Y) is fixed (recall that the network has already been trained and is now being used in the inference mode), maximizing MI is equivalent to minimizing the conditional entropy H(Y | G = GS, X = XS)  This is equivalent to selecting the subgraph that minimizes the uncertainty in the prediction of y when the computational graph is Gc(v)  Optimiation Process  Given the exponentially large number of possible subgraphs, we can not directly optimize the given equation. A “relaxed”-adjacency matrix (whose values are real numbers in the range 0 to 1) is introduced where each element of this fractional adjacency matrix is smaller than the corresponding element of the original adjacency matrix. Gradient descent can be performed on this adjacency matrix. The “relaxed” GS can be interpreted as a variational approximation of the subgraph distributions of Gc(v) and the objective can be written as min EGSH(Y | G = GS, X = XS)  Now the paper makes a big approximation that the GNN is convex so as to leverage the Jensen inequality and push the expectation inside the entropy term to get an upper bound and then minimize that ie min H(Y | G = Es[GS], X = XS)  The paper reports that the convexity approximation (along with discreteness constraint) works in practice. Next, mean field approximation is used to decompose P(GS) as a multivariate Bernoulli distrbitution ie product of AS(i, j) for all (i, j) belonging to Gc(v). AS can be optimized directly and its values represent the expectation of the Bernoulli distrbitution on wether the edge ei, j exists. Given the constraints on AS, it is easier to learn a mask matrix M and optimize that such that AS = M * Ac* Additionally, the sigmod operator can be applied on M.  Once M is learned, only the top k values are retained. Including Node Features in the Explanation  Similar to the previous approach, another feature mask is learned (either one for entire GNN or one per node of the GNN) and is used as a feature selector. The mask could either be learned such that same set of node features (in terms of dimensions) are selected or a different set of features are selected per node. The paper uses the former as it is more straightforward. Just like before, a “relaxed” mask MT is trained to select features as MT * XS. One tricky case is where one feature is important but its value is set to 0. In the case, the value will be masked even though it should not be  The workaround is to use Monte Carlo (MC) estimates of marginals of the missing features. This gives a way to assign importance scores to each feature dimension and a form of reparameterization trick is used to perform end-to-end learning. Masks are encouraged to be discrete by regularizing their element-wise entropy. Resulting computation graph is valid as in it allows message passing towards the central node v.  Multi-Instance Explanations  Given a set of nodes (having the label say y),  the task is to obtain a global explanation of the predictions. For the given class, a prototypical reference node is chosen by computing the mean of embeddings of all the nodes in the class and then selecting the node which is closest to the mean. Now, compute the important computational graph corresponding to this node and align the computational subgraphs of all the other nodes (in the given class) to reference. Let A* be the adjacency matrix and X* be the feature matrix for the explanation corresponding to the reference node. Let Av and Xv be the adjacency matrix and feature matrix of the to-ber-aligned computational graph. A relaed alignment matrix P is optimized to align the nodes and features in the two graphs ie we minimize |PTAvP - A*| + *|PTXvP - X*|  Choosing concise explanations helps in efficient graph matching. For GNNs that compute attention over the entire graph, edges with low attention weight can be pruned to increase efficiency. Experiments  Datasets  Node classification: BA-Shapes, BA-Community, Tree-Cycles, Tree-Grid  Graph classification: MUTAG, Reddit-Binary  Baselines  GRAD - Compute the gradient of the model loss with respect to the adjacency matrix and the node features to be classified and fix the edges with the highest absolute gradient. GAT - Graph Attention Network  The proposed model seems to outperform the baselines both qualitatively and quantitatively. But the results should be taken with a grain of salt as only 2 baselines are considered.", "pdf_url": "https://www.mitpressjournals.org/doi/pdf/10.1162/089976602753712972", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/gnn-explainer-a-tool-for-post-hoc-explanation-of-graph-neural-networks.json"}
{"id": "25710556", "bin": "1300_1400", "summary_sentences": ["They suggest a new method to train GANs.", "They start training them at low resolution (4x4), wait until \"convergence\", then add more convolutions to the existing model to generate and discriminate higher resolutions.", "Each new block of convolutions is slowly blended in, instead of being added from one batch to the next.", "Combined with two new normalization techniques, they get good-looking images at up to 1024x1024 on their new CelebA-HQ dataset (CelebA in high resolution).", "They also suggest a new scoring method based on the approximated Wasserstein distance between real and generated image patches.", "According to that score, their progressive training method improves results significantly.", "What  They suggest a new, progressive training method for GANs.", "The method enables the training of high resolution GANs (1024x1024) that still produce good-looking, diverse images.", "They also introduce two new normalization techniques.", "They also suggest a new method to estimate/score the quality of the generated images.", "They introduce CelebA-HQ, a variation of CelebA containing high resolution images.", "How  Progressive growing/training  They train their GANs resolution by resolution, starting with 4x4 and going up to 1024x1024 (a bit similar to LAPGAN).", "Visualization:  Initially, their generator produces 4x4 images and the discriminator receives 4x4 images.", "Once training at 4x4 does not improve any more (measured by their new score, see below), they add an upscaling module (to 8x8) to the generator and add a downscaling one to the discriminator.", "They don't switch to the added convolutions instantly/suddenly, but give the model a grace period during which the upscaled features are computed from (1-alpha)*A + alpha*B, where A are the features after just upscaling, B are the features after upscaling AND the convolutions and alpha is the overlay factor, which is gradually increased over time.", "This is done for both the generator and the discriminator and at all resolutions.", "Visualization:  Note that all layers are always trained (after they were added to the models).", "Training for the earlier layers does not stop.", "Training in this way focuses most of the computation on the earlier resolutions.", "It also seems to increase stability, as the model does not have to learn all features of all resolutions at the same time.", "Minibatch Standard Deviation  They try to improve diversity by adding a method very similar to minibatch discrimination.", "They compute the standard deviation of each feature per spatial location (for one of the disciminator's last layers).", "They do this per example in each minibatch, resulting in B*H*W*C standard deviations.", "(B = batch size, H = height, W = width, C = channels/filters)  They average these values to one value, then replicate them to size H*W and concatenate that to the layer's output.", "This adds a channel with one constant value to each example in the minibatch.", "The value is the same for all examples.", "Equalized Learning Rate  They use Adam for their training.", "Adam updates weights roughly based on mean(gradient)/variance(gradient) (per weight).", "They argue that this has the downside of equalizing all weight's stepsizes.", "But some weights might require larger stepsizes and other smaller ones (large/small \"dynamic range\").", "As a result, the learning rate will be too small for some weights and too large for others.", "To evade this problem, they first stop using modern weight initialization techniques and instead simply sample weights from the standard normal distribution N(0,1).", "Then, they rescale each weight w_i continuously during runtime to w_i/c, where c is the per-layer normalization from He's initializer.", "(TODO exact formula for c?)", "(This looks an aweful lot like weight normalization .)", "Using simpler weight initialization equalizes the dynamic range of parameters.", "Doing the normalization then fixes problems related to the simpler weight initialization.", "Pixelwise Feature Vector Normalization in the Generator  They argue that collapses in GANs come from the discriminator making some temporary error, leading to high gradients, leading to bad outputs of the generator, leading to more problems in the discriminator and ultimately making both spiral out of control.", "They fix this by normalizing feature vectors in the generator, similar to local response normalization.", "They apply the following equation in the generator (per spatial location (x, y) with N = number of filters):  Scoring Images  They suggest a new method to score images generated by the generator.", "They perform the following steps:  Sample 16384 images from the generator and the dataset.", "Build a Laplacian Pyramid of each image.", "It begins at a 16x16 resolution of the image and progressively doubles that until the final image resolution.", "Each level of the pyramid only contains the difference between the sum of the previous scales and the final image (i.e. each step is a difference image, containing a frequency band).", "Sample per image 128 7x7 neighbourhoods/patches (randomly?)", "from each pyramid level.", "Per image set (generator/real) and pyramid level, compute the mean and standard deviations of each color channels of the sampled patches.", "Normalize each patch with respect to the computed means and standard deviations.", "Use Sliced Wasserstein Distance (SWD) to compute the similarity between the image sets (generator/real).", "The result is one value.", "Lower values are better.", "CelebA-HQ  They derive from CelebA images a new dataset containing 30k 1024x1024 images of celebrity faces.", "They use a convolutional autoencoder to remove JPEG artifacts from the CelebA images.", "They use an adversarially-trained superresolution model to upscale the images.", "They crop faces from the dataset based on their facial landmarks, so that each final face has a normalized position and rotation.", "They rescale the images to 1024x1024 using bilinear sampling and box filters.", "They manually select the 30k best looking images.", "Other stuff  They use Adam for training (alpha=0.001, beta1=0, beta2=0.99).", "They use the WGAN-WP method for training, but LSGAN also works.", "They set gamma to 750 (from 1) for CIFAR-10, incentivizing fast transitions.", "They also add regularization loss on the discriminator, punishing outputs that are very far away from 0.", "Their model for CelebA-HQ training is similar to a standard DCGAN model.", "The generator uses two convolutions after each upscaling, the discriminator analogously two convolutions after each downscaling.", "They start with 512 filters in the generator and end in 16 (before the output) - same for the discriminator.", "They use leaky ReLUs in the generator and discriminator.", "They remove batch normalization everywhere.", "Results  Scores  Results, according to their new scoring measure (Sliced Wasserstein Distance) and MS-SSIM measure:  So progressive growing (b) significantly improves results.", "Same -- to a smaller degree -- for minibatch standard deviation (e), equalized learning rate (f) and pixelwise normalization (g).", "Minibatch discrimination worsened the results.", "Using small batch sizes also worsened the results.", "In (d) they \"adjusted the hyperparameters\" (??)", "and removed batch normalization.", "They generate 1024x1024 CelebA images, while maintaining pixelwise quality compared to previous models.", "They achieve an Inception Score of 8.80 on CIFAR-10.", "Images look improved.", "CelebA-HQ example results:  LSUN dining room, horse, kitchen, churches:"], "summary_text": "They suggest a new method to train GANs. They start training them at low resolution (4x4), wait until \"convergence\", then add more convolutions to the existing model to generate and discriminate higher resolutions. Each new block of convolutions is slowly blended in, instead of being added from one batch to the next. Combined with two new normalization techniques, they get good-looking images at up to 1024x1024 on their new CelebA-HQ dataset (CelebA in high resolution). They also suggest a new scoring method based on the approximated Wasserstein distance between real and generated image patches. According to that score, their progressive training method improves results significantly. What  They suggest a new, progressive training method for GANs. The method enables the training of high resolution GANs (1024x1024) that still produce good-looking, diverse images. They also introduce two new normalization techniques. They also suggest a new method to estimate/score the quality of the generated images. They introduce CelebA-HQ, a variation of CelebA containing high resolution images. How  Progressive growing/training  They train their GANs resolution by resolution, starting with 4x4 and going up to 1024x1024 (a bit similar to LAPGAN). Visualization:  Initially, their generator produces 4x4 images and the discriminator receives 4x4 images. Once training at 4x4 does not improve any more (measured by their new score, see below), they add an upscaling module (to 8x8) to the generator and add a downscaling one to the discriminator. They don't switch to the added convolutions instantly/suddenly, but give the model a grace period during which the upscaled features are computed from (1-alpha)*A + alpha*B, where A are the features after just upscaling, B are the features after upscaling AND the convolutions and alpha is the overlay factor, which is gradually increased over time. This is done for both the generator and the discriminator and at all resolutions. Visualization:  Note that all layers are always trained (after they were added to the models). Training for the earlier layers does not stop. Training in this way focuses most of the computation on the earlier resolutions. It also seems to increase stability, as the model does not have to learn all features of all resolutions at the same time. Minibatch Standard Deviation  They try to improve diversity by adding a method very similar to minibatch discrimination. They compute the standard deviation of each feature per spatial location (for one of the disciminator's last layers). They do this per example in each minibatch, resulting in B*H*W*C standard deviations. (B = batch size, H = height, W = width, C = channels/filters)  They average these values to one value, then replicate them to size H*W and concatenate that to the layer's output. This adds a channel with one constant value to each example in the minibatch. The value is the same for all examples. Equalized Learning Rate  They use Adam for their training. Adam updates weights roughly based on mean(gradient)/variance(gradient) (per weight). They argue that this has the downside of equalizing all weight's stepsizes. But some weights might require larger stepsizes and other smaller ones (large/small \"dynamic range\"). As a result, the learning rate will be too small for some weights and too large for others. To evade this problem, they first stop using modern weight initialization techniques and instead simply sample weights from the standard normal distribution N(0,1). Then, they rescale each weight w_i continuously during runtime to w_i/c, where c is the per-layer normalization from He's initializer. (TODO exact formula for c?) (This looks an aweful lot like weight normalization .) Using simpler weight initialization equalizes the dynamic range of parameters. Doing the normalization then fixes problems related to the simpler weight initialization. Pixelwise Feature Vector Normalization in the Generator  They argue that collapses in GANs come from the discriminator making some temporary error, leading to high gradients, leading to bad outputs of the generator, leading to more problems in the discriminator and ultimately making both spiral out of control. They fix this by normalizing feature vectors in the generator, similar to local response normalization. They apply the following equation in the generator (per spatial location (x, y) with N = number of filters):  Scoring Images  They suggest a new method to score images generated by the generator. They perform the following steps:  Sample 16384 images from the generator and the dataset. Build a Laplacian Pyramid of each image. It begins at a 16x16 resolution of the image and progressively doubles that until the final image resolution. Each level of the pyramid only contains the difference between the sum of the previous scales and the final image (i.e. each step is a difference image, containing a frequency band). Sample per image 128 7x7 neighbourhoods/patches (randomly?) from each pyramid level. Per image set (generator/real) and pyramid level, compute the mean and standard deviations of each color channels of the sampled patches. Normalize each patch with respect to the computed means and standard deviations. Use Sliced Wasserstein Distance (SWD) to compute the similarity between the image sets (generator/real). The result is one value. Lower values are better. CelebA-HQ  They derive from CelebA images a new dataset containing 30k 1024x1024 images of celebrity faces. They use a convolutional autoencoder to remove JPEG artifacts from the CelebA images. They use an adversarially-trained superresolution model to upscale the images. They crop faces from the dataset based on their facial landmarks, so that each final face has a normalized position and rotation. They rescale the images to 1024x1024 using bilinear sampling and box filters. They manually select the 30k best looking images. Other stuff  They use Adam for training (alpha=0.001, beta1=0, beta2=0.99). They use the WGAN-WP method for training, but LSGAN also works. They set gamma to 750 (from 1) for CIFAR-10, incentivizing fast transitions. They also add regularization loss on the discriminator, punishing outputs that are very far away from 0. Their model for CelebA-HQ training is similar to a standard DCGAN model. The generator uses two convolutions after each upscaling, the discriminator analogously two convolutions after each downscaling. They start with 512 filters in the generator and end in 16 (before the output) - same for the discriminator. They use leaky ReLUs in the generator and discriminator. They remove batch normalization everywhere. Results  Scores  Results, according to their new scoring measure (Sliced Wasserstein Distance) and MS-SSIM measure:  So progressive growing (b) significantly improves results. Same -- to a smaller degree -- for minibatch standard deviation (e), equalized learning rate (f) and pixelwise normalization (g). Minibatch discrimination worsened the results. Using small batch sizes also worsened the results. In (d) they \"adjusted the hyperparameters\" (??) and removed batch normalization. They generate 1024x1024 CelebA images, while maintaining pixelwise quality compared to previous models. They achieve an Inception Score of 8.80 on CIFAR-10. Images look improved. CelebA-HQ example results:  LSUN dining room, horse, kitchen, churches:", "pdf_url": "https://arxiv.org/pdf/1710.10196.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/progressive_growing_of_gans.json"}
{"id": "4830217", "bin": "1300_1400", "summary_sentences": ["Mining High-Speed Data Streams – Domingos & Hulten 2000  This paper won a ‘test of time’ award at KDD’15 as an ‘outstanding paper from a past KDD Conference beyond the last decade that has had an important impact on the data mining community.’  Here’s what the test-of-time committee have to say about it:  This paper proposes a decision tree learner for data streams, the Hoeffding Tree algorithm, which comes with the guarantee that the learned decision tree is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples.", "This work constitutes a significant step in developing methodology suitable for modern ‘big data’ challenges and has initiated a lot of follow-up research.", "The Hoeffding Tree algorithm has been covered in various textbooks and is available in several public domain tools, including the WEKA Data Mining platform.", "The goal is to create a knowledge discovery system that can cope with large volumes of data (perhaps an unbounded stream) without needing to fit everything in memory (40MB was the allotted amount used in their evaluation tests – remember this was 2000).", "Ideally, we would like to have KDD systems that operate continuously and indefinitely, incorporating examples as they arrive, and never losing potentially valuable information.", "Such desiderata are fulfilled by incremental learning methods (also known as online, successive or sequential methods), on which a substantial literature exists.", "However, the available algorithms of this type have significant shortcomings from the KDD point of view.", "Some are reasonably efficient, but do not guarantee that the model learned will be similar to the one obtained by learning on the same data in batch mode.", "They are highly sensitive to example ordering, potentially never recovering from an unfavorable set of early examples.", "Others produce the same model as the batch version, but at a high cost in efficiency, often to the point of being slower than the batch algorithm.", "Based on a statistical result known as the Hoeffding bound, the authors show how to create Hoeffding (decision) trees and build a Very Fast Decision Tree (VFDT) system based on them.", "A key property of the Hoeffding tree algorithm is that it is possible to guarantee under realistic assumptions that the trees it produces are asymptotically arbitrarily close to the ones produced by a batch learner (i.e., a learner that uses all the examples to choose a test at each node).", "In other words, the incremental nature of the Hoeffding tree algorithm does not significantly affect the quality of the trees it produces.", "In a classification problem, a set of N training examples of the form (x⃗,y) is given, where y is a discrete class label and x⃗ is a vector of d attributes.", "From these examples we need to produce a model y = f(x⃗) that will predict the class of future examples x⃗ with high accuracy.", "Decision tree learners create models in the form of decision trees, where each node contains a test on an attribute, each branch corresponds to a possible outcome of the test, and each leaf contains a class prediction.", "To learn a decision tree you recursively replace leaves by test nodes, starting at the root.", "Our goal is to design a decision tree learner for extremely large (potentially infinite) datasets.", "This learner should require each example to be read at most once, and only a small constant time to process it.", "This will make it possible to directly mine online data sources (i.e., without ever storing the examples), and to build potentially very complex trees with acceptable computational cost.", "In Hoeffding trees, in order to find the best attribute to test at a given node, only a small subset of the training examples that pass through that node are used.", "The key of course, is to determine how small that subset can be, and what guarantees we can give concerning it.", "Thus, given a stream of examples, the first ones will be used to choose the root test; once the root attribute is chosen, the succeeding examples will be passed down to the corresponding leaves and used to choose the appropriate attributes there, and so on recursively.", "We solve the difficult problem of deciding exactly how many examples are necessary at each node by using a statistical result known as the Hoeffding bound (or additive Chernoff bound).", "Given a real-valued random variable r with range R (e.g. 0-1 for a probability), and n independent observations of the variable, we can compute the mean of those observations, r̄.", "The Hoeffding bound tells us that with probability 1 – δ, the true mean of the variable is at least r̄ – ε, where:  The Hoeffding bound has the very attractive property that it is independent of the probability distribution generating the observations.", "The price of this generality is that the bound is more conservative than distribution-dependent ones (i.e., it will take more observations to reach the same δ and ε).", "If G(Xi) is the heuristic used to choose test attributes, then we want to ensure with high probability the attribute chosen using n examples (where n is as small as possible) is the same that would be chosen using infinite examples.", "Suppose that we’ve seen n examples so far, and the best attribute predicted by G is Xa and the second best is Xb.", "Call the difference between the observed heuristic values of Xa and Xb ΔḠ  Now, given a desired δ, the Hoeffding bound tells us that Xa is the correct choice with probability 1 – δ if n examples have been seen at this node and ΔḠ > ε2.", "Thus a node needs to accumulate examples from the streamuntil ε becomes smaller than ∆G.", "(Notice that ε is a monotonically decreasing function of n.) At this point the node can be split using the current best attribute, and succeeding examples will be passed to the new leaves.", "Pseudo-code for a Hoeffding tree algorithm based on this is given in table 1 of the paper.", "The VFDT system was built using this algorithm, and included a number of additional optimisations:  When two or more attributes have very similar scores, lots of examples may be needed to decide between them with confidence.", "But if they are very similar, it probably doesn’t matter too much which one we choose, so let’s just pick one after we reach some user-defined threshold and move on…  We don’t need to recompute G after every example since it is unlikely the decision to split will be made at that specific point.", "So we can micro-batch and accept a minimum number of new examples before recomputing G.  Under memory pressure, VFDT deactivates the least promising leaves in order to make room for new ones.", "Likewise VFDT can also drop early on attributes that do not look promising.", "VFDT can be initialised with a tree produced offline by a traditional batch learner.", "(Trained on a subset of the overall data).", "VFDT can rescan previously seen examples if desired.", "This option can be activated if either the data arrives slowly enough that there is time for it, or if the dataset is finite and small enough that it is feasible to scan it multiple times.", "This means that VFDT need never grow a smaller (and potentially less accurate) tree than other algorithms because of using each example only once."], "summary_text": "Mining High-Speed Data Streams – Domingos & Hulten 2000  This paper won a ‘test of time’ award at KDD’15 as an ‘outstanding paper from a past KDD Conference beyond the last decade that has had an important impact on the data mining community.’  Here’s what the test-of-time committee have to say about it:  This paper proposes a decision tree learner for data streams, the Hoeffding Tree algorithm, which comes with the guarantee that the learned decision tree is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples. This work constitutes a significant step in developing methodology suitable for modern ‘big data’ challenges and has initiated a lot of follow-up research. The Hoeffding Tree algorithm has been covered in various textbooks and is available in several public domain tools, including the WEKA Data Mining platform. The goal is to create a knowledge discovery system that can cope with large volumes of data (perhaps an unbounded stream) without needing to fit everything in memory (40MB was the allotted amount used in their evaluation tests – remember this was 2000). Ideally, we would like to have KDD systems that operate continuously and indefinitely, incorporating examples as they arrive, and never losing potentially valuable information. Such desiderata are fulfilled by incremental learning methods (also known as online, successive or sequential methods), on which a substantial literature exists. However, the available algorithms of this type have significant shortcomings from the KDD point of view. Some are reasonably efficient, but do not guarantee that the model learned will be similar to the one obtained by learning on the same data in batch mode. They are highly sensitive to example ordering, potentially never recovering from an unfavorable set of early examples. Others produce the same model as the batch version, but at a high cost in efficiency, often to the point of being slower than the batch algorithm. Based on a statistical result known as the Hoeffding bound, the authors show how to create Hoeffding (decision) trees and build a Very Fast Decision Tree (VFDT) system based on them. A key property of the Hoeffding tree algorithm is that it is possible to guarantee under realistic assumptions that the trees it produces are asymptotically arbitrarily close to the ones produced by a batch learner (i.e., a learner that uses all the examples to choose a test at each node). In other words, the incremental nature of the Hoeffding tree algorithm does not significantly affect the quality of the trees it produces. In a classification problem, a set of N training examples of the form (x⃗,y) is given, where y is a discrete class label and x⃗ is a vector of d attributes. From these examples we need to produce a model y = f(x⃗) that will predict the class of future examples x⃗ with high accuracy. Decision tree learners create models in the form of decision trees, where each node contains a test on an attribute, each branch corresponds to a possible outcome of the test, and each leaf contains a class prediction. To learn a decision tree you recursively replace leaves by test nodes, starting at the root. Our goal is to design a decision tree learner for extremely large (potentially infinite) datasets. This learner should require each example to be read at most once, and only a small constant time to process it. This will make it possible to directly mine online data sources (i.e., without ever storing the examples), and to build potentially very complex trees with acceptable computational cost. In Hoeffding trees, in order to find the best attribute to test at a given node, only a small subset of the training examples that pass through that node are used. The key of course, is to determine how small that subset can be, and what guarantees we can give concerning it. Thus, given a stream of examples, the first ones will be used to choose the root test; once the root attribute is chosen, the succeeding examples will be passed down to the corresponding leaves and used to choose the appropriate attributes there, and so on recursively. We solve the difficult problem of deciding exactly how many examples are necessary at each node by using a statistical result known as the Hoeffding bound (or additive Chernoff bound). Given a real-valued random variable r with range R (e.g. 0-1 for a probability), and n independent observations of the variable, we can compute the mean of those observations, r̄. The Hoeffding bound tells us that with probability 1 – δ, the true mean of the variable is at least r̄ – ε, where:  The Hoeffding bound has the very attractive property that it is independent of the probability distribution generating the observations. The price of this generality is that the bound is more conservative than distribution-dependent ones (i.e., it will take more observations to reach the same δ and ε). If G(Xi) is the heuristic used to choose test attributes, then we want to ensure with high probability the attribute chosen using n examples (where n is as small as possible) is the same that would be chosen using infinite examples. Suppose that we’ve seen n examples so far, and the best attribute predicted by G is Xa and the second best is Xb. Call the difference between the observed heuristic values of Xa and Xb ΔḠ  Now, given a desired δ, the Hoeffding bound tells us that Xa is the correct choice with probability 1 – δ if n examples have been seen at this node and ΔḠ > ε2. Thus a node needs to accumulate examples from the streamuntil ε becomes smaller than ∆G. (Notice that ε is a monotonically decreasing function of n.) At this point the node can be split using the current best attribute, and succeeding examples will be passed to the new leaves. Pseudo-code for a Hoeffding tree algorithm based on this is given in table 1 of the paper. The VFDT system was built using this algorithm, and included a number of additional optimisations:  When two or more attributes have very similar scores, lots of examples may be needed to decide between them with confidence. But if they are very similar, it probably doesn’t matter too much which one we choose, so let’s just pick one after we reach some user-defined threshold and move on…  We don’t need to recompute G after every example since it is unlikely the decision to split will be made at that specific point. So we can micro-batch and accept a minimum number of new examples before recomputing G.  Under memory pressure, VFDT deactivates the least promising leaves in order to make room for new ones. Likewise VFDT can also drop early on attributes that do not look promising. VFDT can be initialised with a tree produced offline by a traditional batch learner. (Trained on a subset of the overall data). VFDT can rescan previously seen examples if desired. This option can be activated if either the data arrives slowly enough that there is time for it, or if the dataset is finite and small enough that it is feasible to scan it multiple times. This means that VFDT need never grow a smaller (and potentially less accurate) tree than other algorithms because of using each example only once.", "pdf_url": "http://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/mining-high-speed-data-streams.json"}
{"id": "9563928", "bin": "1300_1400", "summary_sentences": ["Using word embedding to enable semantic queries in relational databases Bordawekar and Shmeuli, DEEM’17  As I’m sure some of you have figured out, I’ve started to work through a collection of papers from SIGMOD’17.", "Strictly speaking, this paper comes from the DEEM workshop held in conjunction with SIGMOD, but it sparked my imagination and I hope you’ll enjoy it too.", "Plus, as a bonus it’s only four pages long!", "What do you get if you cross word embedding vectors with a relational database?", "The ability to ask a new class of queries, which the authors term cognitive intelligence (CI) queries, that ask about the semantic relationship between tokens in the database, rather than just syntactic matching as is supported by current queries.", "It’s a really interesting example of AI infusing everyday systems.", "We begin with a simple observation: there is a large amount of untapped latent information within a database relation.", "This is intuitively clear for columns that contain unstructured text.", "But even columns that contain different types of data, e.g., strings, numerical values, images, dates, etc., possess significant latent information in the form of inter- and intra-column relationships.", "If we understood the meaning of these tokens in the database (at least in some abstract way that was comparable), we could ask queries such as “show me all the rows similar to this.” That’s something you can’t easily do with relational databases today – excepting perhaps for range queries on specific types such as dates.", "Where can we get comparable abstract representations of meaning though?", "The answer is already given away in the paper title of course – this is exactly what word embedding vectors do for us!", "If you’re not familiar with word embedding vectors, we covered word2vec and GloVe in The Morning Paper a while back.", "In fact, “ The Amazing Power of Word Vectors ” continues to be one of the most read pieces on this blog.", "In short:  The idea of word embedding is to fix a d-dimensional vector space and for each word in a text corpus associate a dimension d vector of reals numbers that encodes the meaning of that word… If two words have similar meaning, their word vectors point in very similar directions.", "The authors use word2vec in their work, though as they point out they could equally have used GloVe.", "How do we get word embedding vectors for database content?", "One approach is to use word vectors that have been pre-trained from external sources.", "You can also learn directly from the database itself.", "Think of each row as corresponding to a sentence, and a relation as a document.", "Word embedding then can extract latent semantic information in terms of word (and in general, token) associations and co-occurrences and encode it in word vectors.", "Thus, these vectors capture first inter- and intra-attribute relationships within a row (sentence) and then aggregate these relationships across the relation (document) to compute the collective semantic relationships.", "In their prototype implementation, the authors first textify (!)", "the data in a database table (e.g., using a view), and then use a modified version of word2vec to learn vectors for the words (database tokens) in the extracted text.", "This phase can also use an external source (e.g. Wikipedia articles) for model training.", "We use word as a synonym to token although some tokens may not be valid words in any natural language.", "Following vector training, the resultant vectors are stored in a relational system table.", "At runtime, the system (built on Spark using Spark SQL and the DataFrames API) uses UDFs to fetch trained vectors from the system and answer CI queries.", "CI Queries  Broadly, there are two classes of cognitive intelligence queries: similarity and prediction queries… The key characteristic of the CI queries is that these queries are executed, in part, using the vectors in the word embedding model.", "If the word embedding model is generated using the database being queried, it captures meaning in the context of the associated relational table, as specified by the relational view.", "If a model is rebuilt using a different relational view, a CI query may return different results for the new model.", "It’s time to look at some concrete examples to make all this a bit clearer.", "Given a similarityUDF that can tell us how similar two sets of word vectors are, we can ask a query such as:  In this case, the vector sets correspond to the items purchased by the corresponding customers.", "What this query will return is pairs of customers that have similar purchasing histories!", "The pattern observed in this query can be applied to other domains as well, e.g., identifying patients that are taking similar drugs, but with different brand names or identifying food items with similar ingredients, or finding mutual funds with similar investment strategies.", "The key difference to a traditional query is that we’re matching by semantic similarity, not by values.", "Recall that word embeddings also support inductive reasoning (e.g., the classic King is to Man as Queen is to ?", "style queries).", "You can exploit this capability in CI queries too.", "In the following toy example, we’re looking for food product pairs that relate to each other as ‘peanut-butter’ relates to ‘jelly’.", "(For example, the query may return the pair ‘chips’, ‘salsa’).", "The analogyUDF computes the differences (peanut butter – jelly) and (p1 – p2) and looks at the cosine similarity of those differences.", "The analogy capabilities of CI queries have several applications in the enterprise space, e.g., associating customers with either most-common or least-common purchases in a given domain (e.g., books, electronics, etc.).", "I understand the analogy query mechanism, but I’m not sure I quite get the example the authors are trying to give above.", "Neither finding product popularity, nor seeing whether a customer has purchased a low-popularity (high popularity) item seems to need an analogy?", "Here’s an example of my own – recommendation by analogy: razor is to blade as [product the customer just put in their basket] is to ?.", "(Probably not about to replace frequent itemset mining anytime soon!)", "Our final example shows how embeddings trained using external data can be used in queries.", "Suppose we trained word embeddings with a data set that reveals information about fruits and their allergenic properties.", "We would have a relationship between the vector for ‘allergenic’ and the vectors for allergenic fruit names.", "Now we can ask:  This example demonstrates a very powerful ability of CI queries that enables users to query a database using a token (e.g., allergenic) not present in the database.", "The last word  Will all relational databases one day come with CI querying capabilities built-in?", "In summary, we believe this work is a step towards empowering database systems with built-in AI capabilities… We believe CI queries are applicable to a broad class of application domains including healthcare, bio-informatics, document searching, retail analysis, and data integration.", "We are currently working on applying the CI capabilities to some of these domains."], "summary_text": "Using word embedding to enable semantic queries in relational databases Bordawekar and Shmeuli, DEEM’17  As I’m sure some of you have figured out, I’ve started to work through a collection of papers from SIGMOD’17. Strictly speaking, this paper comes from the DEEM workshop held in conjunction with SIGMOD, but it sparked my imagination and I hope you’ll enjoy it too. Plus, as a bonus it’s only four pages long! What do you get if you cross word embedding vectors with a relational database? The ability to ask a new class of queries, which the authors term cognitive intelligence (CI) queries, that ask about the semantic relationship between tokens in the database, rather than just syntactic matching as is supported by current queries. It’s a really interesting example of AI infusing everyday systems. We begin with a simple observation: there is a large amount of untapped latent information within a database relation. This is intuitively clear for columns that contain unstructured text. But even columns that contain different types of data, e.g., strings, numerical values, images, dates, etc., possess significant latent information in the form of inter- and intra-column relationships. If we understood the meaning of these tokens in the database (at least in some abstract way that was comparable), we could ask queries such as “show me all the rows similar to this.” That’s something you can’t easily do with relational databases today – excepting perhaps for range queries on specific types such as dates. Where can we get comparable abstract representations of meaning though? The answer is already given away in the paper title of course – this is exactly what word embedding vectors do for us! If you’re not familiar with word embedding vectors, we covered word2vec and GloVe in The Morning Paper a while back. In fact, “ The Amazing Power of Word Vectors ” continues to be one of the most read pieces on this blog. In short:  The idea of word embedding is to fix a d-dimensional vector space and for each word in a text corpus associate a dimension d vector of reals numbers that encodes the meaning of that word… If two words have similar meaning, their word vectors point in very similar directions. The authors use word2vec in their work, though as they point out they could equally have used GloVe. How do we get word embedding vectors for database content? One approach is to use word vectors that have been pre-trained from external sources. You can also learn directly from the database itself. Think of each row as corresponding to a sentence, and a relation as a document. Word embedding then can extract latent semantic information in terms of word (and in general, token) associations and co-occurrences and encode it in word vectors. Thus, these vectors capture first inter- and intra-attribute relationships within a row (sentence) and then aggregate these relationships across the relation (document) to compute the collective semantic relationships. In their prototype implementation, the authors first textify (!) the data in a database table (e.g., using a view), and then use a modified version of word2vec to learn vectors for the words (database tokens) in the extracted text. This phase can also use an external source (e.g. Wikipedia articles) for model training. We use word as a synonym to token although some tokens may not be valid words in any natural language. Following vector training, the resultant vectors are stored in a relational system table. At runtime, the system (built on Spark using Spark SQL and the DataFrames API) uses UDFs to fetch trained vectors from the system and answer CI queries. CI Queries  Broadly, there are two classes of cognitive intelligence queries: similarity and prediction queries… The key characteristic of the CI queries is that these queries are executed, in part, using the vectors in the word embedding model. If the word embedding model is generated using the database being queried, it captures meaning in the context of the associated relational table, as specified by the relational view. If a model is rebuilt using a different relational view, a CI query may return different results for the new model. It’s time to look at some concrete examples to make all this a bit clearer. Given a similarityUDF that can tell us how similar two sets of word vectors are, we can ask a query such as:  In this case, the vector sets correspond to the items purchased by the corresponding customers. What this query will return is pairs of customers that have similar purchasing histories! The pattern observed in this query can be applied to other domains as well, e.g., identifying patients that are taking similar drugs, but with different brand names or identifying food items with similar ingredients, or finding mutual funds with similar investment strategies. The key difference to a traditional query is that we’re matching by semantic similarity, not by values. Recall that word embeddings also support inductive reasoning (e.g., the classic King is to Man as Queen is to ? style queries). You can exploit this capability in CI queries too. In the following toy example, we’re looking for food product pairs that relate to each other as ‘peanut-butter’ relates to ‘jelly’. (For example, the query may return the pair ‘chips’, ‘salsa’). The analogyUDF computes the differences (peanut butter – jelly) and (p1 – p2) and looks at the cosine similarity of those differences. The analogy capabilities of CI queries have several applications in the enterprise space, e.g., associating customers with either most-common or least-common purchases in a given domain (e.g., books, electronics, etc.). I understand the analogy query mechanism, but I’m not sure I quite get the example the authors are trying to give above. Neither finding product popularity, nor seeing whether a customer has purchased a low-popularity (high popularity) item seems to need an analogy? Here’s an example of my own – recommendation by analogy: razor is to blade as [product the customer just put in their basket] is to ?. (Probably not about to replace frequent itemset mining anytime soon!) Our final example shows how embeddings trained using external data can be used in queries. Suppose we trained word embeddings with a data set that reveals information about fruits and their allergenic properties. We would have a relationship between the vector for ‘allergenic’ and the vectors for allergenic fruit names. Now we can ask:  This example demonstrates a very powerful ability of CI queries that enables users to query a database using a token (e.g., allergenic) not present in the database. The last word  Will all relational databases one day come with CI querying capabilities built-in? In summary, we believe this work is a step towards empowering database systems with built-in AI capabilities… We believe CI queries are applicable to a broad class of application domains including healthcare, bio-informatics, document searching, retail analysis, and data integration. We are currently working on applying the CI capabilities to some of these domains.", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3076246.3076251?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/using-word-embedding-to-enable-semantic-queries-on-relational-databases.json"}
{"id": "202387", "bin": "1300_1400", "summary_sentences": ["Snorkel: rapid training data creation with weak supervision Ratner et al., VLDB’18  Earlier this week we looked at Sparser, which comes from the Stanford Dawn project , “a five-year research project to democratize AI by making it dramatically easier to build AI-powered applications.” Today’s paper choice, Snorkel, is from the same stable.", "It tackles one of central questions in supervised machine learning: how do you get a large enough set of training data to power modern deep models?", "…deep learning has a major upfront cost: these methods need massive training sets of labeled examples to learn from – often tens of thousands to millions to reach peak predictive performance.", "Such training sets are enormously expensive to create…  Snorkel lets you throw everything you’ve got at the problem.", "Heuristics, external knowledge bases, crowd-sourced workers, you name it.", "These are known as weak supervision sources because they may be limited in accuracy and coverage.", "All of these get combined in a principled manner to produce a set of probability-weighted labels.", "The authors call this process ‘data programming’.", "The end model is then trained on the generated labels.", "Snorkel is the first system to implement our recent work on data programming… While programming weak supervision seems superficially similar to feature engineering, we observe that users approach the two processes very differently.", "Our vision – weak supervision as the sole port of interaction for machine learning – implies radically different workflows…  The big picture  There are three main stages in the Snorkel workflow:  Instead of hand-labelling large quantities of training data, users write labelling functions which capture patterns and heuristics, connect with external knowledge bases (distant supervision), and so on.", "A labelling function is a Python method which given an input can either output a label or abstain.", "Snorkel also includes a number of declarative labelling functions that can be used out of the box.", "Snorkel learns a generative model over all of the labelling functions, so that it can estimated their accuracies and correlations.", "“This step uses no ground-truth data, learning instead from the agreements and disagreements of the labeling functions.”  Snorkel outputs a set of probabilistic labels which can then be used to train a wide variety of machine learning models.", "( Enlarge )  While the generative model is essentially a re-weighted combination of the user-provided labeling functions – which tend to be precise but low coverage – modern discriminative models can retain this precision while learning to generalize beyond the labelling functions, increasing coverage and robustness on unseen data.", "Labelling functions  Say we’re interested in a binary classifier text-relation extraction task, in which a (chemical, disease) input tuple maps to true iff the chemical causes the disease.", "Snorkel breaks input documents (PubMed abstracts) down into a context hierarchy made up of context types.", "The set of context types that make sense will be data dependent.", "Here we might extract documents, sentences, spans, and entities.", "Tuples of relevant entities are then passed to labelling functions as candidates.", "Writing in Python, a labelling function encoding the heuristic that the word ‘causes’ in-between a chemical and a disease indicates a causal relationship would look like this:  For simple cases, there are built-in declarative labelling functions.", "In this case, we could have used a pattern-based function instead of writing our own:  Labeling function generators create multiple labelling functions from a single resource.", "We could use the Comparative Toxicogenomics Database as a distant supervision source for example, and label candidates as ‘true’ if they appear in the “Causes” subset, and ‘false’ if they appear in the “Treats” subset.", "One neat example in the evaluation is using a set of crowdworkers to crowdsource annotations, and then representing each crowdworker as a distinct labelling function.", "Snorkel will automatically learn to adapt to the different skill levels and accuracy of the workers.", "The generative model  Once we have a collection of labelling functions, an obvious thing to do would be to ask each function to label a candidate and use majority voting to determine the resulting label.", "In fact, in situations where we don’t have many votes on an input (e.g., most of the labelling functions abstain), and in situations where we have lots of votes, then majority voting works really well.", "But in-between these two extremes, taking a weighted vote based on modelling labelling function accuracy works better.", "Snorkel uses a heuristic based on the ratio of positive to negative labels for each data point to decide whether to use majority voting or to build a generative model of function accuracy in order to perform weighted voting.", "Essentially, we are taking the expected counts of instances in which a weighted majority vote could possibly flip the incorrect predictions of unweighted majority vote under best case conditions, which is an upper bound for the expected advantage.", "When a generative model is called for it is built as a factor graph, applying all labelling functions to the unlabelled data points and capturing the labelling propensity, accuracy, and pairwise correlations of the functions.", "The details of learning the model are given in an earlier paper, ‘ Learning the structure of generative models without labeled data .’  Dealing with correlated labels  Often the provided labelling functions are not independent.", "For example functions could be simple variations of each other, or they could depend on a common source of distant supervision.", "If we don’t account for the dependencies between labelling functions, we can get into all sorts of trouble:  Getting users to somehow indicate dependencies by hand is difficult and error-prone.", "We therefore turn to our method for automatically selecting which dependencies to model without access to ground truth (See ‘ Learning the structure of generative models without labeled data .’ It uses a pseudo-likelihood estimator, which does not require any sampling or other approximations to compute the objective gradient exactly.", "It is much faster than maximum likelihood estimation, taking 15 seconds to select pairwise correlations to be modeled among 100 labeling functions with 10,000 data points.", "The estimator does rely on a hyperparameter  though, which trades-off between predictive performance and computational cost.", "With large values of  no correlations are included and as we reduce the value progressively more correlations are added, starting with the strongest.", "The following plots show examples of the numbers of correlations added for different values of the correlation threshold  in three different tasks.", "Generally, the number of correlations grows slowly at first, then hits an “elbow point” beyond which the number explodes… setting  to this elbow point is a safe tradeoff between predictive performance and computational cost.", "Snorkel in action  Snorkel is evaluated across six different applications and in a user study to determine how quickly subject-matter experts could learn to write labelling functions.", "In the user study, participants were given 4.5 hours of instruction on how to use and evaluate models developed using Snorkel, and then had 2.5 hours to write labelling functions, for a total time invested of 7 hours.", "(Workshop materials are available at  [url]"], "summary_text": "Snorkel: rapid training data creation with weak supervision Ratner et al., VLDB’18  Earlier this week we looked at Sparser, which comes from the Stanford Dawn project , “a five-year research project to democratize AI by making it dramatically easier to build AI-powered applications.” Today’s paper choice, Snorkel, is from the same stable. It tackles one of central questions in supervised machine learning: how do you get a large enough set of training data to power modern deep models? …deep learning has a major upfront cost: these methods need massive training sets of labeled examples to learn from – often tens of thousands to millions to reach peak predictive performance. Such training sets are enormously expensive to create…  Snorkel lets you throw everything you’ve got at the problem. Heuristics, external knowledge bases, crowd-sourced workers, you name it. These are known as weak supervision sources because they may be limited in accuracy and coverage. All of these get combined in a principled manner to produce a set of probability-weighted labels. The authors call this process ‘data programming’. The end model is then trained on the generated labels. Snorkel is the first system to implement our recent work on data programming… While programming weak supervision seems superficially similar to feature engineering, we observe that users approach the two processes very differently. Our vision – weak supervision as the sole port of interaction for machine learning – implies radically different workflows…  The big picture  There are three main stages in the Snorkel workflow:  Instead of hand-labelling large quantities of training data, users write labelling functions which capture patterns and heuristics, connect with external knowledge bases (distant supervision), and so on. A labelling function is a Python method which given an input can either output a label or abstain. Snorkel also includes a number of declarative labelling functions that can be used out of the box. Snorkel learns a generative model over all of the labelling functions, so that it can estimated their accuracies and correlations. “This step uses no ground-truth data, learning instead from the agreements and disagreements of the labeling functions.”  Snorkel outputs a set of probabilistic labels which can then be used to train a wide variety of machine learning models. ( Enlarge )  While the generative model is essentially a re-weighted combination of the user-provided labeling functions – which tend to be precise but low coverage – modern discriminative models can retain this precision while learning to generalize beyond the labelling functions, increasing coverage and robustness on unseen data. Labelling functions  Say we’re interested in a binary classifier text-relation extraction task, in which a (chemical, disease) input tuple maps to true iff the chemical causes the disease. Snorkel breaks input documents (PubMed abstracts) down into a context hierarchy made up of context types. The set of context types that make sense will be data dependent. Here we might extract documents, sentences, spans, and entities. Tuples of relevant entities are then passed to labelling functions as candidates. Writing in Python, a labelling function encoding the heuristic that the word ‘causes’ in-between a chemical and a disease indicates a causal relationship would look like this:  For simple cases, there are built-in declarative labelling functions. In this case, we could have used a pattern-based function instead of writing our own:  Labeling function generators create multiple labelling functions from a single resource. We could use the Comparative Toxicogenomics Database as a distant supervision source for example, and label candidates as ‘true’ if they appear in the “Causes” subset, and ‘false’ if they appear in the “Treats” subset. One neat example in the evaluation is using a set of crowdworkers to crowdsource annotations, and then representing each crowdworker as a distinct labelling function. Snorkel will automatically learn to adapt to the different skill levels and accuracy of the workers. The generative model  Once we have a collection of labelling functions, an obvious thing to do would be to ask each function to label a candidate and use majority voting to determine the resulting label. In fact, in situations where we don’t have many votes on an input (e.g., most of the labelling functions abstain), and in situations where we have lots of votes, then majority voting works really well. But in-between these two extremes, taking a weighted vote based on modelling labelling function accuracy works better. Snorkel uses a heuristic based on the ratio of positive to negative labels for each data point to decide whether to use majority voting or to build a generative model of function accuracy in order to perform weighted voting. Essentially, we are taking the expected counts of instances in which a weighted majority vote could possibly flip the incorrect predictions of unweighted majority vote under best case conditions, which is an upper bound for the expected advantage. When a generative model is called for it is built as a factor graph, applying all labelling functions to the unlabelled data points and capturing the labelling propensity, accuracy, and pairwise correlations of the functions. The details of learning the model are given in an earlier paper, ‘ Learning the structure of generative models without labeled data .’  Dealing with correlated labels  Often the provided labelling functions are not independent. For example functions could be simple variations of each other, or they could depend on a common source of distant supervision. If we don’t account for the dependencies between labelling functions, we can get into all sorts of trouble:  Getting users to somehow indicate dependencies by hand is difficult and error-prone. We therefore turn to our method for automatically selecting which dependencies to model without access to ground truth (See ‘ Learning the structure of generative models without labeled data .’ It uses a pseudo-likelihood estimator, which does not require any sampling or other approximations to compute the objective gradient exactly. It is much faster than maximum likelihood estimation, taking 15 seconds to select pairwise correlations to be modeled among 100 labeling functions with 10,000 data points. The estimator does rely on a hyperparameter  though, which trades-off between predictive performance and computational cost. With large values of  no correlations are included and as we reduce the value progressively more correlations are added, starting with the strongest. The following plots show examples of the numbers of correlations added for different values of the correlation threshold  in three different tasks. Generally, the number of correlations grows slowly at first, then hits an “elbow point” beyond which the number explodes… setting  to this elbow point is a safe tradeoff between predictive performance and computational cost. Snorkel in action  Snorkel is evaluated across six different applications and in a user study to determine how quickly subject-matter experts could learn to write labelling functions. In the user study, participants were given 4.5 hours of instruction on how to use and evaluate models developed using Snorkel, and then had 2.5 hours to write labelling functions, for a total time invested of 7 hours. (Workshop materials are available at  [url]", "pdf_url": "https://arxiv.org/pdf/1711.10160", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/snorkel-rapid-training-data-creation-with-weak-supervision.json"}
{"id": "35928796", "bin": "1300_1400", "summary_sentences": ["Moving Fast with Software Verification – Calcagno et al. 2015  This is a story of transporting ideas from recent theoretical research in reasoning about programs into the fast-moving engineering culture of Facebook.", "The context is that most of the authors landed at Facebook in September of 2013, when we brought the INFER static analyser with us from the verification startup Monoidics.", "INFER itself is based on recent academic research in program analysis, which applied a relatively recent development in logics of programs, separation logic.", "As of this writing INFER is deployed and running continuously to verify select properties of every code modification in Facebook’s mobile apps; these include the main Facebook apps for Android and iOS, Facebook Messenger, Instagram, and other apps which are used by over a billion people in total.", "How do you mesh formal verification “proponents of which sometimes even used to argue that programs should be developed only after a prior specifications had been written down,” with a continuous delivery model?", "This strikes me as very similar to the problem of integrating security into a continuous delivery pipeline too.", "On the web, Facebook pushes new changes to code twice a day – but mobile platforms are now even more important than the web.", "With the mobile platforms (iOS and Android), you can’t just push new features and bug fixes the minute they are ready – Facebook can only distribute a new version to the Apple App Store or Google Play but the user controls if/when they update.", "This difference has dramatic impact on bug fixes.", "On the web when a bug is discovered a fix can be shipped to the servers as part of a periodic release or in exceptional cases immediately via a “hotfix”.", "And on the web mechanisms exist to automatically update the JavaScript client software running in the browser, allowing fixes to quickly and automatically be deployed as soon as they have been developed.", "On current mobile platforms updates must typically be explicitly authorised by the device owner, so there is no guarantee that a fix will be deployed in a timely manner, if ever, once it is developed.", "Furthermore mobile platforms have relatively low fault tolerance so a runtime error can often cause termination of the entire app.", "Thus mobile development at Facebook presents a strong dichotomy: on one hand it employs continuous development; on the other hand it could benefit from techniques like formal verification to prevent bugs before apps are shipped.", "The first safety properties the INFER team decided to tackle were the implicit safety properties that null pointer exceptions and resource leaks cannot occur in Android-based code, and additionally memory leaks in iOS code.", "The formal verification is integrated with regression testing as part of the continuous delivery pipeline.", "The authors identify four (challenging) requirements for verification in such a scenario:  It must be fully automated and integrated  It must scale to millions of lines of code  It must give precise and useful results: “a developer’s time is an importance resource, an imprecise tool providing poor results would be seen as a waste of that resource.”  It must generate results quickly: “it has to report in minutes, before programmers commit or make further changes.”  These requirements are challenging.", "In our context we are talking about analysis of large Android and iPhone apps (millions of lines of code are involved in the codebases).", "The analysis must be able to run on thousands of code diffs in a day, and it should report in under 10 minutes on average to fit in with the developer workflow.", "There are intra-procedural analyses and linters which fit these scaling requirements, and which are routinely deployed at Facebook and other companies with similar scale codebases and workflows.", "But if an analysis is to detect or exclude bugs involving chains of procedure calls, as one minimally expects of verification techniques, then an inter-procedural analysis is needed, and making inter-procedural analyses scale to this degree while maintaining any degree of accuracy has long been a challenge.", "To address these challenges INFER exploits several recent advances in automatic verification:  It’s underlying formalism is separation logic.", "It implements a compositional, bottom-up variant of the classic RHS inter-procedural analysis algorithm based on procedure summaries.", "There are two main novelties.", "First, it uses compact summaries, based on the ideas of footprints and frame inference from separation logic, to avoid the need for huge summaries that explicitly tabulate most of the input-output possibilities.", "Second, it uses a variation on the notion of abductive inference to discover those summaries.", "The overall development process works as follows:  The programmer creates a diff  The diff goes through a phase of peer-reviews: “a loop of interactions aimed at making the code change robust and efficient as well as being understandable, readable, and maintainable by others.”  When the reviewers are satisfied they accept the code-change and the diff is pushed to the main code-base.", "Every two weeks a version of the code is frozen into the release candidate.", "This goes into a testing period where it is available to Facebook’s employees for internal use.", "After two weeks of internal use, the release candidate is rolled out to Facebook users.", "During phase 2, regression tests are automatically run and before accepting any code change a reviewer requires that all the tests pass.", "Tests run asynchronously and the results are automatically available in the collaboration tool phabricator used for peer review.", "INFER is run at phase 2.", "The process is completely automatic.", "Once the code is submitted for peer review, an analysis is run asynchronously in one of Facebook’s datacenters and results are reported on phabricator in the form of comments.", "To be able to comment on a diff within 10 minutes, INFER uses incremental analysis and a caching system for analysis results.", "The full Android and iOS code bases are fully analysed nightly as well.", "This full analysis can take over 4 hours, and produces the cache used when processing diffs.", "Ultimately, one of the biggest challenges we faced was a social challenge: to get programmers to react to bugs reported by the tool and fix genuine errors.", "Programmers need to accumulate trust in the analyser and they should see it as something helping them to build better software rather than something slowing them down  To build this trust, the team started conservatively concentrating on out-of-memory errors and null pointer exceptions and training INFER on the existing database of crashes and bugs to target false positives and negatives.", "“Having a dedicated static analysis team within Facebook helps tremendously with the social challenge.", "INFER is a good start, but the authors believe there is much more the research community can do as a whole:  Finally, although there have been some successes, we should say that from an industrial perspective advanced program analysis techniques are generally under- developed.", "Simplistic techniques based on context insensitive pattern matching (“linters”) are deployed often and do provide value, and it is highly non-trivial to determine when or where many of the ingenious ideas being proposed in the scientific literature can be deployed practically.", "Part of the problem, we suggest, is that academic research has focused too much on whole-program analysis, or on specify-first, both of which severely limit the number of use cases.", "There are of course many other relevant problem areas – error reporting, fix suggestion, precision of abstract domains, to name a few – but we believe that automatic formal verification techniques have the potential for much greater impact if compositional analyses can become better developed and understood."], "summary_text": "Moving Fast with Software Verification – Calcagno et al. 2015  This is a story of transporting ideas from recent theoretical research in reasoning about programs into the fast-moving engineering culture of Facebook. The context is that most of the authors landed at Facebook in September of 2013, when we brought the INFER static analyser with us from the verification startup Monoidics. INFER itself is based on recent academic research in program analysis, which applied a relatively recent development in logics of programs, separation logic. As of this writing INFER is deployed and running continuously to verify select properties of every code modification in Facebook’s mobile apps; these include the main Facebook apps for Android and iOS, Facebook Messenger, Instagram, and other apps which are used by over a billion people in total. How do you mesh formal verification “proponents of which sometimes even used to argue that programs should be developed only after a prior specifications had been written down,” with a continuous delivery model? This strikes me as very similar to the problem of integrating security into a continuous delivery pipeline too. On the web, Facebook pushes new changes to code twice a day – but mobile platforms are now even more important than the web. With the mobile platforms (iOS and Android), you can’t just push new features and bug fixes the minute they are ready – Facebook can only distribute a new version to the Apple App Store or Google Play but the user controls if/when they update. This difference has dramatic impact on bug fixes. On the web when a bug is discovered a fix can be shipped to the servers as part of a periodic release or in exceptional cases immediately via a “hotfix”. And on the web mechanisms exist to automatically update the JavaScript client software running in the browser, allowing fixes to quickly and automatically be deployed as soon as they have been developed. On current mobile platforms updates must typically be explicitly authorised by the device owner, so there is no guarantee that a fix will be deployed in a timely manner, if ever, once it is developed. Furthermore mobile platforms have relatively low fault tolerance so a runtime error can often cause termination of the entire app. Thus mobile development at Facebook presents a strong dichotomy: on one hand it employs continuous development; on the other hand it could benefit from techniques like formal verification to prevent bugs before apps are shipped. The first safety properties the INFER team decided to tackle were the implicit safety properties that null pointer exceptions and resource leaks cannot occur in Android-based code, and additionally memory leaks in iOS code. The formal verification is integrated with regression testing as part of the continuous delivery pipeline. The authors identify four (challenging) requirements for verification in such a scenario:  It must be fully automated and integrated  It must scale to millions of lines of code  It must give precise and useful results: “a developer’s time is an importance resource, an imprecise tool providing poor results would be seen as a waste of that resource.”  It must generate results quickly: “it has to report in minutes, before programmers commit or make further changes.”  These requirements are challenging. In our context we are talking about analysis of large Android and iPhone apps (millions of lines of code are involved in the codebases). The analysis must be able to run on thousands of code diffs in a day, and it should report in under 10 minutes on average to fit in with the developer workflow. There are intra-procedural analyses and linters which fit these scaling requirements, and which are routinely deployed at Facebook and other companies with similar scale codebases and workflows. But if an analysis is to detect or exclude bugs involving chains of procedure calls, as one minimally expects of verification techniques, then an inter-procedural analysis is needed, and making inter-procedural analyses scale to this degree while maintaining any degree of accuracy has long been a challenge. To address these challenges INFER exploits several recent advances in automatic verification:  It’s underlying formalism is separation logic. It implements a compositional, bottom-up variant of the classic RHS inter-procedural analysis algorithm based on procedure summaries. There are two main novelties. First, it uses compact summaries, based on the ideas of footprints and frame inference from separation logic, to avoid the need for huge summaries that explicitly tabulate most of the input-output possibilities. Second, it uses a variation on the notion of abductive inference to discover those summaries. The overall development process works as follows:  The programmer creates a diff  The diff goes through a phase of peer-reviews: “a loop of interactions aimed at making the code change robust and efficient as well as being understandable, readable, and maintainable by others.”  When the reviewers are satisfied they accept the code-change and the diff is pushed to the main code-base. Every two weeks a version of the code is frozen into the release candidate. This goes into a testing period where it is available to Facebook’s employees for internal use. After two weeks of internal use, the release candidate is rolled out to Facebook users. During phase 2, regression tests are automatically run and before accepting any code change a reviewer requires that all the tests pass. Tests run asynchronously and the results are automatically available in the collaboration tool phabricator used for peer review. INFER is run at phase 2. The process is completely automatic. Once the code is submitted for peer review, an analysis is run asynchronously in one of Facebook’s datacenters and results are reported on phabricator in the form of comments. To be able to comment on a diff within 10 minutes, INFER uses incremental analysis and a caching system for analysis results. The full Android and iOS code bases are fully analysed nightly as well. This full analysis can take over 4 hours, and produces the cache used when processing diffs. Ultimately, one of the biggest challenges we faced was a social challenge: to get programmers to react to bugs reported by the tool and fix genuine errors. Programmers need to accumulate trust in the analyser and they should see it as something helping them to build better software rather than something slowing them down  To build this trust, the team started conservatively concentrating on out-of-memory errors and null pointer exceptions and training INFER on the existing database of crashes and bugs to target false positives and negatives. “Having a dedicated static analysis team within Facebook helps tremendously with the social challenge. INFER is a good start, but the authors believe there is much more the research community can do as a whole:  Finally, although there have been some successes, we should say that from an industrial perspective advanced program analysis techniques are generally under- developed. Simplistic techniques based on context insensitive pattern matching (“linters”) are deployed often and do provide value, and it is highly non-trivial to determine when or where many of the ingenious ideas being proposed in the scientific literature can be deployed practically. Part of the problem, we suggest, is that academic research has focused too much on whole-program analysis, or on specify-first, both of which severely limit the number of use cases. There are of course many other relevant problem areas – error reporting, fix suggestion, precision of abstract domains, to name a few – but we believe that automatic formal verification techniques have the potential for much greater impact if compositional analyses can become better developed and understood.", "pdf_url": "https://research.fb.com/wp-content/uploads/2016/11/publication00124_download0001.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/moving-fast-with-software-verification.json"}
{"id": "29703573", "bin": "1300_1400", "summary_sentences": ["Incorporating copying mechanism in sequence to sequence learning Gu et al. 2016, with a side-helping of Neural machine translation by jointly learning to align and translate Bahdanau et al.", "ICLR 2015  Today’s paper shows how the sequence-to-sequence conversational model we looked at yesterday can be made to seem more natural by including a “copying mechanism” that is an important part of human conversation.", "Consider the following examples where the blue subsequence from the input (I) is repeated in the response (R):  It’s easy for humans to do, but it turns out we require quite a lot of additional machinery over and above the base sequence-to-sequence approach in order to teach computers to do it.", "We argue that it will benefit many Seq2Seq tasks to have an elegant unified model that can accommodate both understanding and rote memorization (copying).", "Towards this goal, we propose CopyNet, which is not only capable of the regular generation of words but also the operation of copying appropriate segments of the input sequence.", "Despite the seemingly “hard” operation of copying, CopyNet can be trained in an end-to-end fashion.", "One of the areas where the copying mechanism seems to add a lot of value is in coping with “out of vocabulary” (OOV) words.", "Words not included in the vocabulary could be entity names for example, and echoing these in responses is often appropriate.", "Background: adding an attention mechanism to seq2seq  CopyNet itself builds on the work of Bahdanau et al. in “Neural Machine Translation by Jointly Learning to Align and Translate.” So let’s take a brief detour to understand the basic attention mechanism idea in that paper.", "The setting is machine translation using sequence-to-sequence , which struggles to cope with long sentences where all of the necessary information in the source sentence needs to be compressed into a fixed-length vector.", "The performance of a basic encoder-decoder pair deteriorates rapidly as the length of the input sentence increases.", "So the trick is not to try and compress the whole sentence when looking for a translation, but instead to focus attention on different parts of the source sentence at different points in the translation.", "The most important distinguishing feature of this approach from the basic encoder–decoder is that it does not attempt to encode a whole input sentence into a single fixed-length vector.", "Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation.", "This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector.", "We show this allows a model to cope better with long sentences.", "The resulting architecture uses a bi-directional RNN as an encoder (more on that shortly), and a decoder that ‘emulates searching through a source sentence during decoding a translation.’  The decoder relies on a series of context annotations, one for each word in the source sentence.", "Each annotation hi contains information about the whole input sequence, with a strong emphasis on the parts surrounding the ith word of the input sequence.", "A learned alignment model weights these annotations to score how well the inputs around position j and the outputs at position i match.", "Intuitively, this implements a mechanism of attention in the decoder.", "The decoder decides parts of the source sentence to pay attention to.", "By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixed- length vector.", "With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.", "In normal encoding, we just read each input symbol starting from the first symbol and proceeding to the last one.", "But we want the annotation for each word to summarize not only what has come before it, but what follows it.", "The solution is simple – encode the sequence both forwards and backwards (a bi-directional RNN) and concatenate the forward and backward state for each symbol.", "CopyNet overview  CopyNet follows the general encoder-decoder pattern, and uses a bi-directional RNN to encode the source sequence.", "The encoded representation can be thought of as a short-term memory, M.  The decoder reads M and predicts the target sequence.", "It uses a similar attention mechanism to Bahdanau et al., with a few important differences:  Words are predicted based on a probabilistic model combining two modes: a generate mode and a copy mode.", "The generate mode uses the same scoring function as a generic encoder-decoder (see e.g. Bahdanau et al.).", "Copy mode picks words from the source sequence using the hidden states in M to represent each word, using a non-linear activation function (tanh).", "The two modes are combined with a shared normalization term, and so are basically competing through a softmax function.", "See section 3.2 for further details, I’m not sure I can do them justice here…  CopyNet uses both the previous state and a weighted sum of the hidden states in M in order to update each decoding state at every step.", "This selective read is designed for the copy mode and focuses attention on the source sequence encoded in the hidden state.", "A properly trained encoder will have captured both the semantics of a word and its location in the input in the hidden states in M.  We hypothesize that COPYNET uses a hybrid strategy for fetching the content in M, which combines both content-based and location-based addressing.", "Both addressing strategies are coordinated by the decoder RNN in managing the attentive read and selective read, as well as determining when to enter/quit the copy-mode… Unlike the explicit design for hybrid addressing in the Neural Turing Machine , COPYNET is more subtle; it provides the architecture that can facilitate some particular location-based addressing and lets the model figure out the details from the training data for specific tasks.", "CopyNet experiments  The authors use CopyNet in three different tasks: a synthetic dataset to show that it can learn rules requiring copying of symbols outside of its vocabulary (it can); a text summarization task; and simple single-turn dialogues.", "On text summarization, CopyNet ‘beats the competitor models by a big margin.’ But it’s the dialogue performance we’re most interested in this week.", "Dialogue data is collected from Baidu Tieba with real-life conversations covering greetings and sports etc.", "Patterns are mined from the set, e.g. “Hi, my name is Adrian” followed by “Hi, Adrian” can lead to the pattern “hi, my name is X -> hi, X” simply by looking for copied subsequences.", "The dataset is enlarged by filling the slots with suitable subsequences (e.g. name entities, dates etc.).", "Using this slot filling, two datasets are created based on 173 collected patterns.", "CopyNet is able to accurately replicate critical segments from the input using copy mode, and generates the rest of the answers smoothly using the generate mode.", "(Click to enlarge)."], "summary_text": "Incorporating copying mechanism in sequence to sequence learning Gu et al. 2016, with a side-helping of Neural machine translation by jointly learning to align and translate Bahdanau et al. ICLR 2015  Today’s paper shows how the sequence-to-sequence conversational model we looked at yesterday can be made to seem more natural by including a “copying mechanism” that is an important part of human conversation. Consider the following examples where the blue subsequence from the input (I) is repeated in the response (R):  It’s easy for humans to do, but it turns out we require quite a lot of additional machinery over and above the base sequence-to-sequence approach in order to teach computers to do it. We argue that it will benefit many Seq2Seq tasks to have an elegant unified model that can accommodate both understanding and rote memorization (copying). Towards this goal, we propose CopyNet, which is not only capable of the regular generation of words but also the operation of copying appropriate segments of the input sequence. Despite the seemingly “hard” operation of copying, CopyNet can be trained in an end-to-end fashion. One of the areas where the copying mechanism seems to add a lot of value is in coping with “out of vocabulary” (OOV) words. Words not included in the vocabulary could be entity names for example, and echoing these in responses is often appropriate. Background: adding an attention mechanism to seq2seq  CopyNet itself builds on the work of Bahdanau et al. in “Neural Machine Translation by Jointly Learning to Align and Translate.” So let’s take a brief detour to understand the basic attention mechanism idea in that paper. The setting is machine translation using sequence-to-sequence , which struggles to cope with long sentences where all of the necessary information in the source sentence needs to be compressed into a fixed-length vector. The performance of a basic encoder-decoder pair deteriorates rapidly as the length of the input sentence increases. So the trick is not to try and compress the whole sentence when looking for a translation, but instead to focus attention on different parts of the source sentence at different points in the translation. The most important distinguishing feature of this approach from the basic encoder–decoder is that it does not attempt to encode a whole input sentence into a single fixed-length vector. Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector. We show this allows a model to cope better with long sentences. The resulting architecture uses a bi-directional RNN as an encoder (more on that shortly), and a decoder that ‘emulates searching through a source sentence during decoding a translation.’  The decoder relies on a series of context annotations, one for each word in the source sentence. Each annotation hi contains information about the whole input sequence, with a strong emphasis on the parts surrounding the ith word of the input sequence. A learned alignment model weights these annotations to score how well the inputs around position j and the outputs at position i match. Intuitively, this implements a mechanism of attention in the decoder. The decoder decides parts of the source sentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixed- length vector. With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly. In normal encoding, we just read each input symbol starting from the first symbol and proceeding to the last one. But we want the annotation for each word to summarize not only what has come before it, but what follows it. The solution is simple – encode the sequence both forwards and backwards (a bi-directional RNN) and concatenate the forward and backward state for each symbol. CopyNet overview  CopyNet follows the general encoder-decoder pattern, and uses a bi-directional RNN to encode the source sequence. The encoded representation can be thought of as a short-term memory, M.  The decoder reads M and predicts the target sequence. It uses a similar attention mechanism to Bahdanau et al., with a few important differences:  Words are predicted based on a probabilistic model combining two modes: a generate mode and a copy mode. The generate mode uses the same scoring function as a generic encoder-decoder (see e.g. Bahdanau et al.). Copy mode picks words from the source sequence using the hidden states in M to represent each word, using a non-linear activation function (tanh). The two modes are combined with a shared normalization term, and so are basically competing through a softmax function. See section 3.2 for further details, I’m not sure I can do them justice here…  CopyNet uses both the previous state and a weighted sum of the hidden states in M in order to update each decoding state at every step. This selective read is designed for the copy mode and focuses attention on the source sequence encoded in the hidden state. A properly trained encoder will have captured both the semantics of a word and its location in the input in the hidden states in M.  We hypothesize that COPYNET uses a hybrid strategy for fetching the content in M, which combines both content-based and location-based addressing. Both addressing strategies are coordinated by the decoder RNN in managing the attentive read and selective read, as well as determining when to enter/quit the copy-mode… Unlike the explicit design for hybrid addressing in the Neural Turing Machine , COPYNET is more subtle; it provides the architecture that can facilitate some particular location-based addressing and lets the model figure out the details from the training data for specific tasks. CopyNet experiments  The authors use CopyNet in three different tasks: a synthetic dataset to show that it can learn rules requiring copying of symbols outside of its vocabulary (it can); a text summarization task; and simple single-turn dialogues. On text summarization, CopyNet ‘beats the competitor models by a big margin.’ But it’s the dialogue performance we’re most interested in this week. Dialogue data is collected from Baidu Tieba with real-life conversations covering greetings and sports etc. Patterns are mined from the set, e.g. “Hi, my name is Adrian” followed by “Hi, Adrian” can lead to the pattern “hi, my name is X -> hi, X” simply by looking for copied subsequences. The dataset is enlarged by filling the slots with suitable subsequences (e.g. name entities, dates etc.). Using this slot filling, two datasets are created based on 173 collected patterns. CopyNet is able to accurately replicate critical segments from the input using copy mode, and generates the rest of the answers smoothly using the generate mode. (Click to enlarge).", "pdf_url": "https://arxiv.org/pdf/1603.06393v3.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/incorporating-a-copying-mechanism-in-sequence-to-sequence-learning.json"}
{"id": "66912872", "bin": "1400_1500", "summary_sentences": ["Boosted race trees for low energy classification Tzimpragos et al., ASPLOS’19  We don’t talk about energy as often as we probably should on this blog, but it’s certainly true that our data centres and various IT systems consume an awful lot of it.", "So it’s interesting to see a paper using nano-Joules per prediction as an evaluation metric.", "The goal is to produce a low-energy hardware classifier for embedded applications doing local processing of sensor data.", "To get there, the authors question a whole bunch of received wisdom, beginning with this: do we really need to convert the analog sensor data into a digital signal?!", "Here’s another fun one: what if instead of being something you worked hard to avoid, you had to build your whole application based on the outcomes of data races??!", "Typically, a sensor gathers analog information from the physical world and then converts it into a conventional digital signal… While this binary-represented integer is perfectly efficient for storage as bits in memory and for typical general purpose computing operations, it is unclear that this is the most efficient for our target application.", "One such possible representation is pure analog signalling.", "Of course analog signalling comes with a whole bunch of challenges of its own, which is one of the reasons we tend to convert to digital.", "But here the authors seem to have found a perfect marriage of a class of logic called race logic, a natural encoding for sensor data, and the classification use case.", "We argue that race logic fits nicely with temporally-coded sensing systems,, such as 3D depth perception systems and dynamic vision sensors, where the time that a signal gets “excited” to a logic-level “1” depends on the magnitude of the sensor reading.", "Furthermore, we demonstrate that the structure of race logic makes it a natural encoding for decision tree-based approaches.", "The resulting system can integrate seamlessly into a scikit-learn based development process, and dramatically reduces the total energy usage required for classification with very low latency.", "Introducing race logic  Race logic encodes values by delaying signals.", "Signals can still be low or high (0 or 1), but it’s the timing of the transition between those states which is all important.", "For example, a value of ‘2’ can be represented by a signal that is low until time 2 and then high from then on.", "Race logic has four primary operations that are easy to implement in hardware: MAX, MIN, ADD-CONSTANT, and INHIBIT.", "For MAX the output of a gate should go high only when all of the inputs have arrived.", "A single AND gate between two wires is therefore all we need to implement MAX in the race domain.", "(That’s kind of cool isn’t it :) ).", "MIN is actually pretty straightforward as well – here we want the first signal to arrive so a single OR gate does the job.", "Since values are encoded by the time of the rising edge from 0 to 1, adding a constant value to a signal (ADD-CONSTANT) amounts to introducing the proportionate amount of delay to the signal.", "One efficient way of doing that in analog hardware is the use of current-starved inverters.", "INHIBIT takes two inputs: an inhibiting signal and a data signal.", "If the inhibiting signal arrives first, the output is prevented from ever going high.", "If the data signal arrives at the same time as, or before, the inhibiting signal, it is allowed to pass through unchanged.", "An INHIBIT function can be encoded using a single PMOS pass gate.", "Together this set of four operations allow us to deliberately engineer “race conditions” in a circuit to perform computation at low energy.", "Encoding decision trees using race logic  Take a decision tree, and turn it upside down so that instead of computation starting at the root and proceeding to an ultimately selected leaf, we start at the leaves and work our way to the root.", "One idea is to assign a unique delay-encoded label to each leaf, and then let the labels race each other on their journey to the root.", "The first one to arrive at the root is the winner, i.e., the correct label for this classification.", "Another way of looking at the problem is that each path from the tree root to a leaf corresponds to a unique conjunction of attribute tests.", "We can execute those tests in parallel rather than sequentially.", "When it comes to making a decision at a node in the tree, each node has a fixed threshold t, learned during the training phase, against which it compares the input value.", "So decision trees are essentially networks of threshold functions.", "This fits rather nicely with an INHIBIT gate.", "An end-to-end architecture  It’s now time to put our race-logic encoded decision trees in the context of an end-to-end system.", "The first stage of analog-to-digital converters (ADC) often involves converting the analog signal into the temporal domain.", "“Such an approach leverages the small delays and high temporal resolution of nano-meter scale devices to get superior performance over their voltage domain counterparts.” For our use case, we simply don’t need the rest of the conversion (temporal to digital, TDC).", "Examples of systems providing directly time-encoded outputs without TDCs, range from visual sensors, such as Dynamic Vision Sensors (DVS), Asynchronous Time-based Image Sensors (ATIS), Time to First Spike (TTFS) and Time-of-Flight (ToF) cameras, to sound sensors; e.g. the AER (Address Event Representation) Ear, which is a silicon-based cochlea that converts auditory signals from various frequency bands to precise temporally coded outputs….", "the presented architecture can work with any temporally-coded input provided to it.", "A programmable race-logic accelerator is used for the decision trees, with threshold values encoded using a shift register.", "A decoder reads the race-tree output and turns it into a one-hot encoded memory address.", "For an ensemble learner, we have an ensemble of such trees, with a weighted voting scheme based on general binary adders.", "Integration with scikit-learn  The Race Trees accelerator is fully integrated into a sci-kit learn based development flow.", "Once the model is trained, our tool analyzes the importance of input features, explores the learners performance against lower resolution data, proceeds with votes (content of tree leaves) quantization.", "Finally, the user has to choose one of the following options: (a) the generation of a configuration file for the crossbar networks of a programmable Race Trees accelerator, or (b) the generation of a custom RTL-level design with hardwired units.", "Performance evaluation  The evaluation is performed using the MNIST dataset, since that has the most results available in the literature for comparison.", "Here’s how Race Trees (green dots in the top LH corner) compare against other state-of-the-art low power classifiers when we look at energy vs accuracy:  An open-source implementation of race-logic primitives and example Race Trees can be found on GitHub.", "Using GradientBoosting, a classifier consisting of 1,000 race trees of depth 6 gets 97.45% accuracy at 31.35nJ/prediction.", "Using just 200 race trees it is still possible to achieve 95.7% accuracy, but now at just 7.8nJ/prediction.", "A comparison of Race Trees to other machine learning implementations in an accuracy vs energy-delay product scatter plot is presented in Figure 14 (below), and shows that RaceTrees achieve both lower delay and energy per operation than their counterparts.", "The paper’s conclusion lists a number of areas for future work which indicate there’s still plenty of room for further exploration and improvement, including for example the integration of other circuits such as vector-matrix multipliers that operate purely in the time domain.", "The quest for energy-efficient machine learning systems looks like it may take us to some very interesting places indeed!"], "summary_text": "Boosted race trees for low energy classification Tzimpragos et al., ASPLOS’19  We don’t talk about energy as often as we probably should on this blog, but it’s certainly true that our data centres and various IT systems consume an awful lot of it. So it’s interesting to see a paper using nano-Joules per prediction as an evaluation metric. The goal is to produce a low-energy hardware classifier for embedded applications doing local processing of sensor data. To get there, the authors question a whole bunch of received wisdom, beginning with this: do we really need to convert the analog sensor data into a digital signal?! Here’s another fun one: what if instead of being something you worked hard to avoid, you had to build your whole application based on the outcomes of data races??! Typically, a sensor gathers analog information from the physical world and then converts it into a conventional digital signal… While this binary-represented integer is perfectly efficient for storage as bits in memory and for typical general purpose computing operations, it is unclear that this is the most efficient for our target application. One such possible representation is pure analog signalling. Of course analog signalling comes with a whole bunch of challenges of its own, which is one of the reasons we tend to convert to digital. But here the authors seem to have found a perfect marriage of a class of logic called race logic, a natural encoding for sensor data, and the classification use case. We argue that race logic fits nicely with temporally-coded sensing systems,, such as 3D depth perception systems and dynamic vision sensors, where the time that a signal gets “excited” to a logic-level “1” depends on the magnitude of the sensor reading. Furthermore, we demonstrate that the structure of race logic makes it a natural encoding for decision tree-based approaches. The resulting system can integrate seamlessly into a scikit-learn based development process, and dramatically reduces the total energy usage required for classification with very low latency. Introducing race logic  Race logic encodes values by delaying signals. Signals can still be low or high (0 or 1), but it’s the timing of the transition between those states which is all important. For example, a value of ‘2’ can be represented by a signal that is low until time 2 and then high from then on. Race logic has four primary operations that are easy to implement in hardware: MAX, MIN, ADD-CONSTANT, and INHIBIT. For MAX the output of a gate should go high only when all of the inputs have arrived. A single AND gate between two wires is therefore all we need to implement MAX in the race domain. (That’s kind of cool isn’t it :) ). MIN is actually pretty straightforward as well – here we want the first signal to arrive so a single OR gate does the job. Since values are encoded by the time of the rising edge from 0 to 1, adding a constant value to a signal (ADD-CONSTANT) amounts to introducing the proportionate amount of delay to the signal. One efficient way of doing that in analog hardware is the use of current-starved inverters. INHIBIT takes two inputs: an inhibiting signal and a data signal. If the inhibiting signal arrives first, the output is prevented from ever going high. If the data signal arrives at the same time as, or before, the inhibiting signal, it is allowed to pass through unchanged. An INHIBIT function can be encoded using a single PMOS pass gate. Together this set of four operations allow us to deliberately engineer “race conditions” in a circuit to perform computation at low energy. Encoding decision trees using race logic  Take a decision tree, and turn it upside down so that instead of computation starting at the root and proceeding to an ultimately selected leaf, we start at the leaves and work our way to the root. One idea is to assign a unique delay-encoded label to each leaf, and then let the labels race each other on their journey to the root. The first one to arrive at the root is the winner, i.e., the correct label for this classification. Another way of looking at the problem is that each path from the tree root to a leaf corresponds to a unique conjunction of attribute tests. We can execute those tests in parallel rather than sequentially. When it comes to making a decision at a node in the tree, each node has a fixed threshold t, learned during the training phase, against which it compares the input value. So decision trees are essentially networks of threshold functions. This fits rather nicely with an INHIBIT gate. An end-to-end architecture  It’s now time to put our race-logic encoded decision trees in the context of an end-to-end system. The first stage of analog-to-digital converters (ADC) often involves converting the analog signal into the temporal domain. “Such an approach leverages the small delays and high temporal resolution of nano-meter scale devices to get superior performance over their voltage domain counterparts.” For our use case, we simply don’t need the rest of the conversion (temporal to digital, TDC). Examples of systems providing directly time-encoded outputs without TDCs, range from visual sensors, such as Dynamic Vision Sensors (DVS), Asynchronous Time-based Image Sensors (ATIS), Time to First Spike (TTFS) and Time-of-Flight (ToF) cameras, to sound sensors; e.g. the AER (Address Event Representation) Ear, which is a silicon-based cochlea that converts auditory signals from various frequency bands to precise temporally coded outputs…. the presented architecture can work with any temporally-coded input provided to it. A programmable race-logic accelerator is used for the decision trees, with threshold values encoded using a shift register. A decoder reads the race-tree output and turns it into a one-hot encoded memory address. For an ensemble learner, we have an ensemble of such trees, with a weighted voting scheme based on general binary adders. Integration with scikit-learn  The Race Trees accelerator is fully integrated into a sci-kit learn based development flow. Once the model is trained, our tool analyzes the importance of input features, explores the learners performance against lower resolution data, proceeds with votes (content of tree leaves) quantization. Finally, the user has to choose one of the following options: (a) the generation of a configuration file for the crossbar networks of a programmable Race Trees accelerator, or (b) the generation of a custom RTL-level design with hardwired units. Performance evaluation  The evaluation is performed using the MNIST dataset, since that has the most results available in the literature for comparison. Here’s how Race Trees (green dots in the top LH corner) compare against other state-of-the-art low power classifiers when we look at energy vs accuracy:  An open-source implementation of race-logic primitives and example Race Trees can be found on GitHub. Using GradientBoosting, a classifier consisting of 1,000 race trees of depth 6 gets 97.45% accuracy at 31.35nJ/prediction. Using just 200 race trees it is still possible to achieve 95.7% accuracy, but now at just 7.8nJ/prediction. A comparison of Race Trees to other machine learning implementations in an accuracy vs energy-delay product scatter plot is presented in Figure 14 (below), and shows that RaceTrees achieve both lower delay and energy per operation than their counterparts. The paper’s conclusion lists a number of areas for future work which indicate there’s still plenty of room for further exploration and improvement, including for example the integration of other circuits such as vector-matrix multipliers that operate purely in the time domain. The quest for energy-efficient machine learning systems looks like it may take us to some very interesting places indeed!", "pdf_url": "https://sites.cs.ucsb.edu/~sherwood/pubs/ASPLOS-19-racetree.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/race-trees.json"}
{"id": "45601881", "bin": "1400_1500", "summary_sentences": ["The Log-Structured Merge-Tree (LSM Tree) – O’Neil et al. ’96.", "Log-Structured Merge is an important technique used in many modern data stores (for example, BigTable, Cassandra, HBase, Riak, …).", "Suppose you have a hierarchy of storage options for data – for example, RAM, SSDs, Spinning disks, with different price/performance characteristics.", "Furthermore, you have a large dataset experiencing frequent writes, and need to maintain an index on that dataset that can be queried at any time – how can you best exploit the tiers of storage available to you to make keeping this up-to-date as efficient as possible?", "For example:  High-performance transaction system applications typically insert rows in a History table to provide an activity trace;  at the same time the transaction system generates log records for purposes of system recovery.", "Both types of generated information can benefit from efficient indexing.", "The LSM tree is a data structure designed to provide low-cost indexing for files experiencing a high rate of inserts (and deletes) over an extended period.", "A good modern example might be incoming streaming data being written to a table.", "LSM trees cascade data over time from smaller, higher performing (but more expensive) stores to larger less performant (and less expensive) stores.", "The LSM-tree uses an algorithm that defers and batches index changes, cascading the changes from a memory-based component through one or more disk components in an efficient manner reminiscent of merge sort.", "… it is most useful in applications where index inserts are more common than finds that retrieve the entries.", "This seems to be a common property for History tables and log files, for example.", "The only thing that is required for LSM trees to give an advantage is a high update rate compared to the retrieval rate.", "Although many examples involve time-series data, this is not a necessary feature.", "The high update:retrieval rate ratio makes the efficiency of maintaining the index paramount.", "At the same time find access needs to be  frequent enough that an index of some kind must be maintained, because a sequential search through all the records would be out of the question.", "An LSM-tree is composed of two or more tree-like component data structures.", "For example,  a two component LSM-tree has a smaller component which is entirely memory resident, known as the C0 tree (or C0 component), and a larger component which is resident on disk, known as the C1 tree (or C1 component).", "Note that although the C1 tree resides on disk, frequently referenced page nodes will still reside in memory in memory buffers.", "Dath is first inserted into C0, and from there it migrates to C1.", "The index entry for [an insert] is inserted into the memory resident C0 tree, after which it will in time migrate out to the C1 tree on disk;  any search for an index entry will look first in C0 and then in C1.", "There is a certain amount of latency (delay) before entries in the C0 tree migrate out to the disk resident C1 tree, implying a need for recovery of index entries that don’t get out to disk prior to a crash.", "It’s very cheap to insert an entry into the memory-resident C0 tree, but  the cost / capacity of memory compared to disk limits the size of what it makes sense to keep in memory.", "At the heart of the LSM algorithm is a rolling merge process:  We need an efficient way to migrate entries out to the C1 tree that resides on the lower cost disk medium.", "To achieve this, whenever the C0 tree as a result of an insert reaches a threshold size near the maximum allotted, an ongoing rolling merge  process serves to delete some contiguous segment of entries from the C0 tree and merge it into the C1 tree on disk.", "The rolling merge proceeds one block at at time from the downstream tree (C1 in our example).", "A block is read in and  entries from the upstream tree (C0) are merged with it.", "This reduces the size of the C0 tree, and creates a new merged C1 block that is written out to disk.", "The rolling merge acts in a series of merge steps.", "A read of a multi-page block containing leaf nodes of the C1 tree makes a range of entries in C1 buffer resident.", "Each merge step then reads a disk page sized leaf node of the C1 tree buffered in this block, merges entries from the leaf node with entries taken from the leaf level of the C0 tree, thus decreasing the size of C0, and creates a newly merged leaf node of the C1 tree.", "Newly merged blocks are written to new disk positions, so that the old blocks will not be over-written and will be available for recovery in case of a crash….", "We picture the rolling merge process in a two component LSM-tree as having a conceptual cursor which slowly circulates in quantized steps through equal key values of the C0 tree and C1 tree components, drawing indexing data out from the C0 tree to the C1 tree on disk.", "The best efficiency gains over B-tree based systems (the prior art) come when reads and writes are in multi-page blocks thus eliminating seek time.", "Finds simply need to work through the trees in order:  When an exact-match find or range find requiring immediate response is performed through the LSM-tree index, first the C0 tree and then the C1 tree is searched for the value or values desired.", "We can generalize from a two-component LSM tree to one with k components:  …we define a multi component LSM-tree as having components C0, C1, C2, .", ".", ".,   CK-1 and CK, indexed tree structures of increasing size, where C0 is memory resident and all other components are disk resident.", "There are asynchronous rolling merge processes in train between all component pairs (Ci-1, Ci) that move entries out from the smaller to the larger component each time the smaller component, Ci-1, exceeds its threshold size.", "Section 3 of the paper contains an analysis of the cost-performance of LSM trees.", "This is based on the cost per unit of storage in each component (i.e. memory in C0, and disk in C1), and the cost per unit of I/O in each component.", "The Five-Minute Rule , which we examined earlier in the series (together with its updates at 10 and 20 years later) determines the inflection points where the dominant factors change.", "This section also shows how to calculate the optimal threshold sizes for the various components in an LSM tree.", "The answer turns out to be to arrange things in a geometric progression whereby the ratio of the size of component(i) to the size of component(i+1) is a fixed value r for all adjacent components in the tree.", "The three variables ultimately affecting overall cost are therefore r, the size of component 0, and the number of components, k.  there are costs associated with increasing the number of components:  a CPU cost to perform the additional rolling merges and a memory cost to buffer the nodes of those merges (which will actually swamp the memory cost of C0 in common cost regimes).", "In addition, indexed finds requiring immediate response will sometimes have to perform retrieval from all component trees.", "These considerations put a strong constraint on the appropriate number of components, and three components are probably the most that will be seen in practice.", "There’s plenty more in the paper (which runs to 32 pages) that we haven’t had space to cover here, including an analysis of the concurrency and recovery implications of LSM-trees."], "summary_text": "The Log-Structured Merge-Tree (LSM Tree) – O’Neil et al. ’96. Log-Structured Merge is an important technique used in many modern data stores (for example, BigTable, Cassandra, HBase, Riak, …). Suppose you have a hierarchy of storage options for data – for example, RAM, SSDs, Spinning disks, with different price/performance characteristics. Furthermore, you have a large dataset experiencing frequent writes, and need to maintain an index on that dataset that can be queried at any time – how can you best exploit the tiers of storage available to you to make keeping this up-to-date as efficient as possible? For example:  High-performance transaction system applications typically insert rows in a History table to provide an activity trace;  at the same time the transaction system generates log records for purposes of system recovery. Both types of generated information can benefit from efficient indexing. The LSM tree is a data structure designed to provide low-cost indexing for files experiencing a high rate of inserts (and deletes) over an extended period. A good modern example might be incoming streaming data being written to a table. LSM trees cascade data over time from smaller, higher performing (but more expensive) stores to larger less performant (and less expensive) stores. The LSM-tree uses an algorithm that defers and batches index changes, cascading the changes from a memory-based component through one or more disk components in an efficient manner reminiscent of merge sort. … it is most useful in applications where index inserts are more common than finds that retrieve the entries. This seems to be a common property for History tables and log files, for example. The only thing that is required for LSM trees to give an advantage is a high update rate compared to the retrieval rate. Although many examples involve time-series data, this is not a necessary feature. The high update:retrieval rate ratio makes the efficiency of maintaining the index paramount. At the same time find access needs to be  frequent enough that an index of some kind must be maintained, because a sequential search through all the records would be out of the question. An LSM-tree is composed of two or more tree-like component data structures. For example,  a two component LSM-tree has a smaller component which is entirely memory resident, known as the C0 tree (or C0 component), and a larger component which is resident on disk, known as the C1 tree (or C1 component). Note that although the C1 tree resides on disk, frequently referenced page nodes will still reside in memory in memory buffers. Dath is first inserted into C0, and from there it migrates to C1. The index entry for [an insert] is inserted into the memory resident C0 tree, after which it will in time migrate out to the C1 tree on disk;  any search for an index entry will look first in C0 and then in C1. There is a certain amount of latency (delay) before entries in the C0 tree migrate out to the disk resident C1 tree, implying a need for recovery of index entries that don’t get out to disk prior to a crash. It’s very cheap to insert an entry into the memory-resident C0 tree, but  the cost / capacity of memory compared to disk limits the size of what it makes sense to keep in memory. At the heart of the LSM algorithm is a rolling merge process:  We need an efficient way to migrate entries out to the C1 tree that resides on the lower cost disk medium. To achieve this, whenever the C0 tree as a result of an insert reaches a threshold size near the maximum allotted, an ongoing rolling merge  process serves to delete some contiguous segment of entries from the C0 tree and merge it into the C1 tree on disk. The rolling merge proceeds one block at at time from the downstream tree (C1 in our example). A block is read in and  entries from the upstream tree (C0) are merged with it. This reduces the size of the C0 tree, and creates a new merged C1 block that is written out to disk. The rolling merge acts in a series of merge steps. A read of a multi-page block containing leaf nodes of the C1 tree makes a range of entries in C1 buffer resident. Each merge step then reads a disk page sized leaf node of the C1 tree buffered in this block, merges entries from the leaf node with entries taken from the leaf level of the C0 tree, thus decreasing the size of C0, and creates a newly merged leaf node of the C1 tree. Newly merged blocks are written to new disk positions, so that the old blocks will not be over-written and will be available for recovery in case of a crash…. We picture the rolling merge process in a two component LSM-tree as having a conceptual cursor which slowly circulates in quantized steps through equal key values of the C0 tree and C1 tree components, drawing indexing data out from the C0 tree to the C1 tree on disk. The best efficiency gains over B-tree based systems (the prior art) come when reads and writes are in multi-page blocks thus eliminating seek time. Finds simply need to work through the trees in order:  When an exact-match find or range find requiring immediate response is performed through the LSM-tree index, first the C0 tree and then the C1 tree is searched for the value or values desired. We can generalize from a two-component LSM tree to one with k components:  …we define a multi component LSM-tree as having components C0, C1, C2, . . .,   CK-1 and CK, indexed tree structures of increasing size, where C0 is memory resident and all other components are disk resident. There are asynchronous rolling merge processes in train between all component pairs (Ci-1, Ci) that move entries out from the smaller to the larger component each time the smaller component, Ci-1, exceeds its threshold size. Section 3 of the paper contains an analysis of the cost-performance of LSM trees. This is based on the cost per unit of storage in each component (i.e. memory in C0, and disk in C1), and the cost per unit of I/O in each component. The Five-Minute Rule , which we examined earlier in the series (together with its updates at 10 and 20 years later) determines the inflection points where the dominant factors change. This section also shows how to calculate the optimal threshold sizes for the various components in an LSM tree. The answer turns out to be to arrange things in a geometric progression whereby the ratio of the size of component(i) to the size of component(i+1) is a fixed value r for all adjacent components in the tree. The three variables ultimately affecting overall cost are therefore r, the size of component 0, and the number of components, k.  there are costs associated with increasing the number of components:  a CPU cost to perform the additional rolling merges and a memory cost to buffer the nodes of those merges (which will actually swamp the memory cost of C0 in common cost regimes). In addition, indexed finds requiring immediate response will sometimes have to perform retrieval from all component trees. These considerations put a strong constraint on the appropriate number of components, and three components are probably the most that will be seen in practice. There’s plenty more in the paper (which runs to 32 pages) that we haven’t had space to cover here, including an analysis of the concurrency and recovery implications of LSM-trees.", "pdf_url": "http://paperhub.s3.amazonaws.com/18e91eb4db2114a06ea614f0384f2784.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/the-log-structured-merge-tree-lsm-tree.json"}
{"id": "518237", "bin": "1400_1500", "summary_sentences": ["Overview  This work proposes models that combine information from different modalities (e.g., images and text) to be able to classify social media content.", "Information from different modalities are combined using neural network models through a pooling layer.", "In addition, an auxiliary learning task is used to learn a common feature space for all modalities (more on this later).", "Motivation  Multimodal approaches become more important as social media networks allow for users to post multimodal posts (e.g., gifs, videos, audio segments, text, etc.).", "Analysis of multimodal information allows for better understanding of users (user profiling) and can be used to effectively run ad campaigns on the social network.", "In addition, it can be used to better understand other emotion-related behaviors such as mental health disorders, etc.", "Example  Consider the examples of multimodal posts in the pictures below.", "If we only paid attention to the images (left to right), we would predict emotions such as joy, fear, and contentment.", "If we considered both the image and text: the first example remains as joy; the second example is probably mixed emotion (the text convey joy); and the third example is also mixed emotion (the text convey sadness).", "These simple examples emphasize on the importance of considering both modalities to deduce the overall emotion conveyed in the social post.", "Contribution  The main problem with previous multimodal approaches is the inability to deal with the absence of some important modality.", "For instance, let’s assume we can obtain text and video from a piece of content, but we can’t obtain the audio because it is corrupted or unavailable.", "In such cases, previous methods did not address this important problem (i.e., missing modality).", "The proposed model aims to address this problem and proves its robustness through an emotion classification task.", "Besides dealing with the “missing modality” problem, the authors claim that their approach can also scale to other tasks and modalities.", "Challenge Addressed  As previously mentioned, the proposed model can handle situations where there is a missing modality.", "In other words, there system supports the following cases: only image or text or both.", "Hand-Crafted Features vs. Automatic Feature Learning  I believe this is an important discussion that this paper highlights in the related work.", "As it relates to emotion recognition, it is challenging to manually create features as we cannot guarantee that all aspects of emotions (features) that can capture the emotions are covered.", "Convolutional neural networks (CNNs) are used in place so as to automatically learn representations that can generalize to the problem of emotion recognition.", "I couldn’t help but commenting that even though this argument is strong, hand-crafting features also offer better intuition of what is being learned, something deep learning models may not offer, yet!", "However, some good people are tirelessly working on this problem ( Feature Visualization ).", "Concepts to Know  Late fusion — combination of results obtained by different classifiers (trained on different modalities); i.e., fusion is done at the decision level.", "Early fusion — information from different modalities are combined at the feature level, and classification is done on the combined representations.", "Multimodal Classification  This work employs an adaptation of early fusion for combining modalities for emotion recognition through CNNs.", "Two prominent modalities of social media are used, i.e. text and image.", "If both image and text are available for a social post, they are assumed to have semantic relation — the text describes the image.", "Images are represented by vectors, which are obtained after feeding images into a CNN trained on ImageNet .", "Texts are represented through pre-trained word embeddings (GloVe).", "Model  In the figure above, all types of fusion techniques for combining features to be fed to a classifier are shown.", "This work proposes two fusion approaches which enjoy the simplicity of early fusion (a) and the flexibility of late fusion (b).", "These approaches are called joint fusion © and common space fusion (d).", "In the joint fusion model, text and images are fused in the fusion layer, which applies a pooling operation to the text and image vector to obtain a combined feature vector.", "The pooling operations require both vectors to be of the same size.", "Typically, the image vector has a higher dimension than the text vector, therefore, an extra linear layer is added to map the original image vector to a vector of the same dimension as the text vector.", "The joint fusion neural network is trained by minimizing the negative log-likelihood using stochastic gradient descent (SGD).", "(See paper for additional details)  The second approach, common feature space fusion, aims to enforce visual and textual vectors of a post to be in the same feature space.", "Note that this was motivated by the fact that the joint fusion model considers these signals (visual and textual) as different, i.e., no relationship between them.", "An auxiliary task is employed, which enforces similarity between both a text and image vector belonging to a post, ensuring that the rest of text vectors from different classes are different from the image vector.", "(See paper for details on how this objective is trained and combined with the main classification task).", "Task  An emotion classification task is used to evaluate the proposed multimodal approaches.", "Different discrete emotion categories from Plutchik’s wheel of emotions are employed to label two types of datasets.", "Datasets  A flickr image dataset was crawled and assigned to Amazon Mechanical Turk workers for annotations; i.e., human workers were asked to annotate the emotion they perceived from the images.", "Title and descriptions are also obtained for each image from the flickr website.", "In addition, a Reddit dataset was also collected; subreddits related to emotion (happy, creepy, rage, gore) were used to collect data for 4 emotions, joy, fear, anger, and disgust, respectively.", "(See paper for more details on collecting and preparing datasets)  Experiments  Unimodal baselines (FastText model for text and InceptionNet model for images), traditional multimodal approaches (early and late fusion), and the proposed multimodal models (joint fusion and common space fusion) are trained on the datasets.", "For the embedding layer, GloVe pre-trained word vectors are considered.", "From the results in the table above, we can observe that the proposed fusion models (joint fusion and common space fusion) outperform all the other models in both datasets, including the traditional fusion techniques.", "The common space fusion model, in particular, performs extremely well even when only one modality for a post existed (see results below).", "Analysis  Classification results on several examples are provided for error analysis (see figure below).", "We can observe that for the highlighted example, the common space fusion model can detect fear when using both the text and image information.", "Somehow, the model can detect that in this particular example, the image descriptor expresses sarcasm, which is obvious from the creepy doll in the closet show in the image.", "Kind on interesting and weird at the same time, no pun intended.", "(See paper for more interesting analysis and examples)  Conclusion and Future Work  Experimental results show that by combining textual and visual data, fusion models can improve emotion classification accuracy on two types of datasets, with very significant gains in some cases.", "The common space fusion model performs well even when only one modality existed because this model contains information from two modalities.", "These type of models proof ideal for cases where more modalities may exist in the data.", "However, the authors point out that these models may suffer from information loss as one modality may be less informative than the other.", "I honestly enjoyed reading this paper.", "It is well written and it provides a new idea on how to combine modalities for understanding social media data.", "Other applications may include sarcasm detection, which is a more complex task as summarized here .", "References  Datasets and code for this work are available here  Ref:  [url]"], "summary_text": "Overview  This work proposes models that combine information from different modalities (e.g., images and text) to be able to classify social media content. Information from different modalities are combined using neural network models through a pooling layer. In addition, an auxiliary learning task is used to learn a common feature space for all modalities (more on this later). Motivation  Multimodal approaches become more important as social media networks allow for users to post multimodal posts (e.g., gifs, videos, audio segments, text, etc.). Analysis of multimodal information allows for better understanding of users (user profiling) and can be used to effectively run ad campaigns on the social network. In addition, it can be used to better understand other emotion-related behaviors such as mental health disorders, etc. Example  Consider the examples of multimodal posts in the pictures below. If we only paid attention to the images (left to right), we would predict emotions such as joy, fear, and contentment. If we considered both the image and text: the first example remains as joy; the second example is probably mixed emotion (the text convey joy); and the third example is also mixed emotion (the text convey sadness). These simple examples emphasize on the importance of considering both modalities to deduce the overall emotion conveyed in the social post. Contribution  The main problem with previous multimodal approaches is the inability to deal with the absence of some important modality. For instance, let’s assume we can obtain text and video from a piece of content, but we can’t obtain the audio because it is corrupted or unavailable. In such cases, previous methods did not address this important problem (i.e., missing modality). The proposed model aims to address this problem and proves its robustness through an emotion classification task. Besides dealing with the “missing modality” problem, the authors claim that their approach can also scale to other tasks and modalities. Challenge Addressed  As previously mentioned, the proposed model can handle situations where there is a missing modality. In other words, there system supports the following cases: only image or text or both. Hand-Crafted Features vs. Automatic Feature Learning  I believe this is an important discussion that this paper highlights in the related work. As it relates to emotion recognition, it is challenging to manually create features as we cannot guarantee that all aspects of emotions (features) that can capture the emotions are covered. Convolutional neural networks (CNNs) are used in place so as to automatically learn representations that can generalize to the problem of emotion recognition. I couldn’t help but commenting that even though this argument is strong, hand-crafting features also offer better intuition of what is being learned, something deep learning models may not offer, yet! However, some good people are tirelessly working on this problem ( Feature Visualization ). Concepts to Know  Late fusion — combination of results obtained by different classifiers (trained on different modalities); i.e., fusion is done at the decision level. Early fusion — information from different modalities are combined at the feature level, and classification is done on the combined representations. Multimodal Classification  This work employs an adaptation of early fusion for combining modalities for emotion recognition through CNNs. Two prominent modalities of social media are used, i.e. text and image. If both image and text are available for a social post, they are assumed to have semantic relation — the text describes the image. Images are represented by vectors, which are obtained after feeding images into a CNN trained on ImageNet . Texts are represented through pre-trained word embeddings (GloVe). Model  In the figure above, all types of fusion techniques for combining features to be fed to a classifier are shown. This work proposes two fusion approaches which enjoy the simplicity of early fusion (a) and the flexibility of late fusion (b). These approaches are called joint fusion © and common space fusion (d). In the joint fusion model, text and images are fused in the fusion layer, which applies a pooling operation to the text and image vector to obtain a combined feature vector. The pooling operations require both vectors to be of the same size. Typically, the image vector has a higher dimension than the text vector, therefore, an extra linear layer is added to map the original image vector to a vector of the same dimension as the text vector. The joint fusion neural network is trained by minimizing the negative log-likelihood using stochastic gradient descent (SGD). (See paper for additional details)  The second approach, common feature space fusion, aims to enforce visual and textual vectors of a post to be in the same feature space. Note that this was motivated by the fact that the joint fusion model considers these signals (visual and textual) as different, i.e., no relationship between them. An auxiliary task is employed, which enforces similarity between both a text and image vector belonging to a post, ensuring that the rest of text vectors from different classes are different from the image vector. (See paper for details on how this objective is trained and combined with the main classification task). Task  An emotion classification task is used to evaluate the proposed multimodal approaches. Different discrete emotion categories from Plutchik’s wheel of emotions are employed to label two types of datasets. Datasets  A flickr image dataset was crawled and assigned to Amazon Mechanical Turk workers for annotations; i.e., human workers were asked to annotate the emotion they perceived from the images. Title and descriptions are also obtained for each image from the flickr website. In addition, a Reddit dataset was also collected; subreddits related to emotion (happy, creepy, rage, gore) were used to collect data for 4 emotions, joy, fear, anger, and disgust, respectively. (See paper for more details on collecting and preparing datasets)  Experiments  Unimodal baselines (FastText model for text and InceptionNet model for images), traditional multimodal approaches (early and late fusion), and the proposed multimodal models (joint fusion and common space fusion) are trained on the datasets. For the embedding layer, GloVe pre-trained word vectors are considered. From the results in the table above, we can observe that the proposed fusion models (joint fusion and common space fusion) outperform all the other models in both datasets, including the traditional fusion techniques. The common space fusion model, in particular, performs extremely well even when only one modality for a post existed (see results below). Analysis  Classification results on several examples are provided for error analysis (see figure below). We can observe that for the highlighted example, the common space fusion model can detect fear when using both the text and image information. Somehow, the model can detect that in this particular example, the image descriptor expresses sarcasm, which is obvious from the creepy doll in the closet show in the image. Kind on interesting and weird at the same time, no pun intended. (See paper for more interesting analysis and examples)  Conclusion and Future Work  Experimental results show that by combining textual and visual data, fusion models can improve emotion classification accuracy on two types of datasets, with very significant gains in some cases. The common space fusion model performs well even when only one modality existed because this model contains information from two modalities. These type of models proof ideal for cases where more modalities may exist in the data. However, the authors point out that these models may suffer from information loss as one modality may be less informative than the other. I honestly enjoyed reading this paper. It is well written and it provides a new idea on how to combine modalities for understanding social media data. Other applications may include sarcasm detection, which is a more complex task as summarized here . References  Datasets and code for this work are available here  Ref:  [url]", "pdf_url": "https://arxiv.org/pdf/1708.02099", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/detecting-emotions-with-cnn-fusion-models-b066944969c8.json"}
{"id": "28727146", "bin": "1400_1500", "summary_sentences": ["Algorithmic glass ceiling in social networks: the effects of social recommendations on network diversity Stoica et al., WWW’18  (If you don’t have ACM Digital Library access, the paper can be accessed either by following the link above directly from The Morning Paper blog site, or from the WWW 2018 proceedings page).", "Social networks were meant to connect us and bring us together.", "This paper shows that while they might be quite successful at doing this in the small, on a macro scale they’re actually doing the opposite.", "Not only do they reinforce and sustain disparities among groups, but they actually reinforce the rate at which disparity grows.", "I.e., they’re driving us apart.", "This happens due to the rich-get-richer phenomenon resulting from friend/follow recommendation algorithms.", "… we find that prominent social recommendation algorithms can exacerbate the under-representation of certain demographic groups at the top of the social hierarchy… Our mathematical analysis demonstrates the existence of an algorithmic glass ceiling that exhibits all the properties of the metaphorical social barrier that hinders groups like women or people of colour from attaining equal representation.", "Organic growth vs algorithmic growth  “In the social networks now governing the knowledge, jobs and deals one can seek, what matters most today is one’s position in the graph of advantageous connections.” It takes time and effort to build and maintain  your connections.", "One of the key tools that social networks provide to help with this is algorithm recommendation of connections.", "I.e., “people you may know” on Facebook, “who to follow” on Twitter, and “suggested accounts” on Instagram.", "Unsurprisingly, and by design, these suggestions influence the networks that people end up building.", "Homophily — a tendency of individuals to favour interactions with similar peers — influences even organic connection growth (i.e., connections that are built up in the absence of algorithmic recommendations).", "When we combine homophily with algorithmic connection growth (i.e., connections that are built up in the presence of algorithmic recommendations), the advantage of the majority group is exacerbated.", "We build on a growing body of evidence that online services (including Twitter, TaskRabbit, and Airbnb) can reproduce well-know prejudices against historically under-represented groups.", "Issues raised include disparate treatment and evidence of a metaphorical glass ceiling.", "The latter denotes an invisible barrier preventing given demographics (most commonly females) from reaching superior levels of recognition.", "The conditions for declaring the presence of a glass ceiling impacting a given demographic group are:  The chances of advancement to a higher level are uneven for members of that group  The disparity is not explained by task-relevant characteristics  The inequality increases for higher and higher levels.", "Consider the following chart, concentrating just on the blue circles to start with.", "You’re looking at Instagram data collected over multiple months of 2014 and 2015, before Instagram’s “suggested accounts” feature was introduced.", "That is, it’s the result of organic growth.", "The horizontal dashed line shows the fraction of female users in the overall dataset (54.43%).", "On the x-axis we have increasing degree, and on the y-axis the percentage of female users among nodes with that degree.", "Women appear under-represented at the top of the hierarchy, but they are not completely excluded.", "The paper just rolls straight past this factual observation, since the authors are mostly interested in the comparison to algorithmic growth, but it’s worth pausing for a moment to think about what’s causing this result: it’s not what you’d expect from a straightforward ‘rich get richer’ model.", "It’s due to differential homophily between men and women.", "Men show a stronger bias towards connecting to other men than women do for connecting to other women.", "This happens even when controlling for content productivity.", "Now take a second look at the chart, this time focusing on the blue and green circles.", "The x-axis is now representing the frequency with which a node is recommended as a connection, under approximations to two common recommendation strategies (the Adamic-Adar index and a random walk of length 2).", "There’s a much sharper drop-off for recommendations of female users under these algorithmic growth conditions.", "You can see it even more clearly in this log-log scale plot:  This gap, exhibited to grow in log-log scale, is a sign of different power coefficients governing the statistical chance to reach at least x recommendations, depending on gender.", "As one progressively selects to retain only the most successful individuals, the aforementioned effect translates into a sudden and accelerating drop of the observed fraction of females.", "Again, simple homophily would suggest that females should be recommended more throughout, but differential homophily coupled with recommendation can rapidly overturn and invert the inequality.", "(Of course, if females showed stronger homophily than males, then the effect would be to even more rapidly increase the female advantage instead).", "Modelling growth  Consider a network with red and blue labelled nodes.", "In the organic growth model we have the following rules;  A new node  enters the network and receives label R with probability r, and B with probability 1-r, where r is less than 0.5.", "Then repeat the following steps until an edge is formed:  With probability  , the new node  choose an existing node  at random.", "With probability  ,  choose a node uniformly at random and copy one of its edges.", "This model the ‘rich get richer’ effect.", "If the new node has a different label than the node it chooses to connect to, the connection is accepted with probability  .", "(If it has the same label, it is always accepted).", "For algorithmic growth we make the following changes.", "Once a new node has entered the network as above, it connects through organic growth with probability  .", "With probability  it will instead select a node  uniformly at random and follow a random walk of length 2 to choose a node.", "The chosen node is accepted if it is the same colour, and accepted with probability  if it is a different colour.", "( Enlarge )  A power inequality effect exists for the red nodes if the average degree of a red node ends up lower than the average degree of a blue node.", "A tail glass ceiling effect exists if there exists an increasing function k(n) for short k such that:  The algorithmic glass ceiling  … the algorithms we analyzed do not create bias ex nihilo, but simply amplify bias in networks where it exists.", "In section 5 of the paper the authors show the following under the organic model:  The rate of growth of edges towards the red population converges towards a constant  .", "The in-degree distributions of the two populations follow power laws with different coefficients  And under the algorithmic model:  The sum of degrees of the red nodes converges to a constant that is smaller than r, and even smaller than in the organic case: $latex \\alpha_2 < \\alpha … the sharp amplification of the glass ceiling effect by an algorithm is an entirely new result, which has no equivalent that we know of.", "It is a special case of a widely open problem: how to correct a seemingly neutral algorithm when the structure it exploits is not fair to begin with… Unfortunately, without a deep understanding of the cause and reverberation of bias, any post-hoc correction can be harshly debated.", "Our paper offers an alternative way: identify some structural causes of the emerging unfairness, and require algorithms to be designed in a way that leverages structure while carefully avoiding those biases in the presence of the above conditions."], "summary_text": "Algorithmic glass ceiling in social networks: the effects of social recommendations on network diversity Stoica et al., WWW’18  (If you don’t have ACM Digital Library access, the paper can be accessed either by following the link above directly from The Morning Paper blog site, or from the WWW 2018 proceedings page). Social networks were meant to connect us and bring us together. This paper shows that while they might be quite successful at doing this in the small, on a macro scale they’re actually doing the opposite. Not only do they reinforce and sustain disparities among groups, but they actually reinforce the rate at which disparity grows. I.e., they’re driving us apart. This happens due to the rich-get-richer phenomenon resulting from friend/follow recommendation algorithms. … we find that prominent social recommendation algorithms can exacerbate the under-representation of certain demographic groups at the top of the social hierarchy… Our mathematical analysis demonstrates the existence of an algorithmic glass ceiling that exhibits all the properties of the metaphorical social barrier that hinders groups like women or people of colour from attaining equal representation. Organic growth vs algorithmic growth  “In the social networks now governing the knowledge, jobs and deals one can seek, what matters most today is one’s position in the graph of advantageous connections.” It takes time and effort to build and maintain  your connections. One of the key tools that social networks provide to help with this is algorithm recommendation of connections. I.e., “people you may know” on Facebook, “who to follow” on Twitter, and “suggested accounts” on Instagram. Unsurprisingly, and by design, these suggestions influence the networks that people end up building. Homophily — a tendency of individuals to favour interactions with similar peers — influences even organic connection growth (i.e., connections that are built up in the absence of algorithmic recommendations). When we combine homophily with algorithmic connection growth (i.e., connections that are built up in the presence of algorithmic recommendations), the advantage of the majority group is exacerbated. We build on a growing body of evidence that online services (including Twitter, TaskRabbit, and Airbnb) can reproduce well-know prejudices against historically under-represented groups. Issues raised include disparate treatment and evidence of a metaphorical glass ceiling. The latter denotes an invisible barrier preventing given demographics (most commonly females) from reaching superior levels of recognition. The conditions for declaring the presence of a glass ceiling impacting a given demographic group are:  The chances of advancement to a higher level are uneven for members of that group  The disparity is not explained by task-relevant characteristics  The inequality increases for higher and higher levels. Consider the following chart, concentrating just on the blue circles to start with. You’re looking at Instagram data collected over multiple months of 2014 and 2015, before Instagram’s “suggested accounts” feature was introduced. That is, it’s the result of organic growth. The horizontal dashed line shows the fraction of female users in the overall dataset (54.43%). On the x-axis we have increasing degree, and on the y-axis the percentage of female users among nodes with that degree. Women appear under-represented at the top of the hierarchy, but they are not completely excluded. The paper just rolls straight past this factual observation, since the authors are mostly interested in the comparison to algorithmic growth, but it’s worth pausing for a moment to think about what’s causing this result: it’s not what you’d expect from a straightforward ‘rich get richer’ model. It’s due to differential homophily between men and women. Men show a stronger bias towards connecting to other men than women do for connecting to other women. This happens even when controlling for content productivity. Now take a second look at the chart, this time focusing on the blue and green circles. The x-axis is now representing the frequency with which a node is recommended as a connection, under approximations to two common recommendation strategies (the Adamic-Adar index and a random walk of length 2). There’s a much sharper drop-off for recommendations of female users under these algorithmic growth conditions. You can see it even more clearly in this log-log scale plot:  This gap, exhibited to grow in log-log scale, is a sign of different power coefficients governing the statistical chance to reach at least x recommendations, depending on gender. As one progressively selects to retain only the most successful individuals, the aforementioned effect translates into a sudden and accelerating drop of the observed fraction of females. Again, simple homophily would suggest that females should be recommended more throughout, but differential homophily coupled with recommendation can rapidly overturn and invert the inequality. (Of course, if females showed stronger homophily than males, then the effect would be to even more rapidly increase the female advantage instead). Modelling growth  Consider a network with red and blue labelled nodes. In the organic growth model we have the following rules;  A new node  enters the network and receives label R with probability r, and B with probability 1-r, where r is less than 0.5. Then repeat the following steps until an edge is formed:  With probability  , the new node  choose an existing node  at random. With probability  ,  choose a node uniformly at random and copy one of its edges. This model the ‘rich get richer’ effect. If the new node has a different label than the node it chooses to connect to, the connection is accepted with probability  . (If it has the same label, it is always accepted). For algorithmic growth we make the following changes. Once a new node has entered the network as above, it connects through organic growth with probability  . With probability  it will instead select a node  uniformly at random and follow a random walk of length 2 to choose a node. The chosen node is accepted if it is the same colour, and accepted with probability  if it is a different colour. ( Enlarge )  A power inequality effect exists for the red nodes if the average degree of a red node ends up lower than the average degree of a blue node. A tail glass ceiling effect exists if there exists an increasing function k(n) for short k such that:  The algorithmic glass ceiling  … the algorithms we analyzed do not create bias ex nihilo, but simply amplify bias in networks where it exists. In section 5 of the paper the authors show the following under the organic model:  The rate of growth of edges towards the red population converges towards a constant  . The in-degree distributions of the two populations follow power laws with different coefficients  And under the algorithmic model:  The sum of degrees of the red nodes converges to a constant that is smaller than r, and even smaller than in the organic case: $latex \\alpha_2 < \\alpha … the sharp amplification of the glass ceiling effect by an algorithm is an entirely new result, which has no equivalent that we know of. It is a special case of a widely open problem: how to correct a seemingly neutral algorithm when the structure it exploits is not fair to begin with… Unfortunately, without a deep understanding of the cause and reverberation of bias, any post-hoc correction can be harshly debated. Our paper offers an alternative way: identify some structural causes of the emerging unfairness, and require algorithms to be designed in a way that leverages structure while carefully avoiding those biases in the presence of the above conditions.", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3178876.3186140?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/algorithmic-glass-ceiling-in-social-networks-the-effects-of-recommendation-on-social-diversity.json"}
{"id": "78412080", "bin": "1400_1500", "summary_sentences": ["Fail-slow at scale: evidence of hardware performance faults in large production systems Gunawi et al., FAST’18  The first thing that strikes you about this paper is the long list of authors from multiple different establishments.", "That’s because it’s actually a study of 101 different fail-slow hardware incidents collected across large-scale cluster deployments in 12 different institutions.", "Last year we looked at ‘ Gray failure: the Achilles’ heel of cloud-scale systems ,’ in which partial failures and system components running in degraded mode were revealed to be involved in many production incidents, and difficult to detect and diagnose with today’s systems.", "In ‘Fail-slow at scale’ we have a catalog of ways in which hardware has been shown to run in degraded mode, causing all sorts of problems.", "Fail-slow hardware, like file system faults and network partitions , should be added to the list of things that occur much more frequently than we’d like to think, and cause a lot of damage when they do occur.", "This paper highlights an under-studied “new” failure type: fail-slow hardware, hardware that is still running and functional but in a degraded mode, slower than its expected performance.", "We found that all major hardware components can exhibit fail-slow faults.", "These faults may be comparatively rare, but they can trigger chains of cascading events, resulting visible failures some distance from the original cause.", "Perhaps because of this, and because the partial failures are often deliberately masked in the hardware without enough metrics to surface them, troubleshooting fail-slow hardware based incidents can be very time-consuming:  The fail-slow hardware incidents in our report took hours or even months to detect (pinpoint).", "More specifically, 1% of the cases are detected in minutes, 13% in hours, 13% in days, 11% in weeks, and 17% in months (and unknown time in 45%).", "One of the conclusions is that many modern deployed systems do not anticipate this failure mode.", "The authors offer a set of suggestions to hardware vendors, operators, and systems designers to improve the state of affairs.", "Guidelines for vendors  Instead of implicitly masking errors whenever possible, consider throwing explicit error signals when error rates far exceed the expected rate.", "Expose device-level performance statistics at fine enough granularity to be able to pinpoint device degradation.", "(The information from S.M.A.R.T.", "was deemed insufficient to act upon).", "Guidelines for operators  Enable online (in-situ) diagnosis, since many problems cannot be reproduced offline (back in the office).", "Monitor all hardware components.", "Capture full-stack performance data and use statistical approaches to pinpoint and isolate root causes.", "Guidelines for system designers  Make implicit error masking explicit (this is the software equivalent of the advice given above to hardware vendors).", "“Software systems should not just silently work around fail-slow hardware, but need to expose enough information to help troubleshooting.”  Consider converting fail-slow faults to fail-stop.", "One example given is skipping a caching layer altogether when SSDs are misbehaving.", "Another example is to fail-stop after sufficient recurrence of fail-slow incidents (e.g., tripping a circuit breaker after too many retries).", "Doing this requires an ability to shut-off devices at a fine-grained level.", "Use fault-injection to explore fail-slow scenarios:  … we strongly believe that injecting root causes reported in this paper will reveal many flaws in existing systems.", "Furthermore, all forms of fail-slow hardware such as slow NIC’s, switches, disks, SSD, NVDIMM, and CPUs need to be exercised as they lead to differrent symptoms.", "The challenge is then to build future systems that enable various fail-slow behaviors to be injected easily.", "Let’s now take a look at some of the characteristics and causes of fail-slow incidents.", "Root causes  There can be a variety of root causes — these can be internal to the hardware component (for example, device wear or firmware issues), or external (for example, configuration, environment, temperature or power issues).", "Section 4 in the paper describes the myriad ways that SSDs, Disks, Memory, and Network components can suffer from internal failures.", "And even when a component would otherwise function just fine, section 5 catalogues a long list of external factors that have been known to cause partial failures:  Temperature: clogged air filters, cold-air-under-floor systems, broken fans, buggy fan firmware, poor assembly/design such that heat-sinks are ineffective.", "For example, “there was a case where a fan in a compute node stopped working, and to compensate this failing fan, fans in other compute nods started to operate at their maximal speed, which then generated heavy noise and vibration that degraded the disk performance.”  Power: reduced power can easily trigger fail-slow hardware faults.", "Causes include insufficient capacitors, PCU firmware bugs, fail-partial power supplies, power hungry neighbours (drawing too much power under periods of heavy load), and faulty motherboard sensors.", "Enviroment: altitude (failure to provide enough cooling for high altitude), cosmic events, loose interconnects, vibrations (e.g. from fans).", "Configuration errors due to buggy BIOS firmware and simple human mistakes (e.g., in mapping PCIe cards to slots).", "Fault conversion  Fail-stops in some components can cause others to exhibit fail-slow behaviour:  For example, a dead power supply throttled the CPUs by 50% as the backup supply did not deliver enough power; a single bad disk exhausted the entire RAID card’s performance; and a vendor’s buggy firmware made a batch of SSDs stop for seconds, disabling the flash cache layer and making the entire storage stack slow.", "Likewise frequent transient failures can result in fail-slow behaviour due to the overheads of error masking (retries, repairs, etc..).", "When errors ceases to be rare, the masking overhead becomes the normal case performance.", "Partial internal failures whereby only some part of the device is unusable can also lead to fail-slow behaviour.", "Symptoms  Fail-slow conditions can manifest in several different ways.", "The slowdown may be permanent, or the device may fluctuate between normal and degraded performance.", "Some parts of the device may continue to offer full performance while others are degraded, and in other situations a device may periodically reboot itself, leading to periods of complete unavailability.", "Cascading failures  …between the actual root cause and the hardware’s fail-slow symptom, there is a chain of cascading root causes… the fail-slow symptom then creates cascading impacts to the high-level software stack, and potentially to the entire cluster.", "We’ve already seen the example of the failing fan leading to a cascade of events ending in degraded disk performance.", "Another example is a faulty sensor in a motherboard reporting a false value to the OS, thus making the CPUs run slower in energy saving mode.", "Fail-slow hardware problems can cascade into the software stack – for example, in an HBase deployment a memory card at 25% of normal speed caused backlogs, out-of-memory errors, and crashes.", "The last word  The paper is packed with anecdotes that I didn’t have space to cover here.", "It’s well worth a read to get a finer appreciation of the kinds of hazards that await when building and operating systems at scale.", "We believe fail-slow hardware is a fundamentally harder problem to solve [than fail-stop].", "It is very hard to distinguish such cases from ones that are caused by software performance issues.", "It is also evident that many modern, advanced deployed systems do not anticipate this failure mode.", "We hope that our study can influence vendors, operators, and system-designers to treat fail-slow hardware as a separate class of failures and start addressing them more robustly in future system."], "summary_text": "Fail-slow at scale: evidence of hardware performance faults in large production systems Gunawi et al., FAST’18  The first thing that strikes you about this paper is the long list of authors from multiple different establishments. That’s because it’s actually a study of 101 different fail-slow hardware incidents collected across large-scale cluster deployments in 12 different institutions. Last year we looked at ‘ Gray failure: the Achilles’ heel of cloud-scale systems ,’ in which partial failures and system components running in degraded mode were revealed to be involved in many production incidents, and difficult to detect and diagnose with today’s systems. In ‘Fail-slow at scale’ we have a catalog of ways in which hardware has been shown to run in degraded mode, causing all sorts of problems. Fail-slow hardware, like file system faults and network partitions , should be added to the list of things that occur much more frequently than we’d like to think, and cause a lot of damage when they do occur. This paper highlights an under-studied “new” failure type: fail-slow hardware, hardware that is still running and functional but in a degraded mode, slower than its expected performance. We found that all major hardware components can exhibit fail-slow faults. These faults may be comparatively rare, but they can trigger chains of cascading events, resulting visible failures some distance from the original cause. Perhaps because of this, and because the partial failures are often deliberately masked in the hardware without enough metrics to surface them, troubleshooting fail-slow hardware based incidents can be very time-consuming:  The fail-slow hardware incidents in our report took hours or even months to detect (pinpoint). More specifically, 1% of the cases are detected in minutes, 13% in hours, 13% in days, 11% in weeks, and 17% in months (and unknown time in 45%). One of the conclusions is that many modern deployed systems do not anticipate this failure mode. The authors offer a set of suggestions to hardware vendors, operators, and systems designers to improve the state of affairs. Guidelines for vendors  Instead of implicitly masking errors whenever possible, consider throwing explicit error signals when error rates far exceed the expected rate. Expose device-level performance statistics at fine enough granularity to be able to pinpoint device degradation. (The information from S.M.A.R.T. was deemed insufficient to act upon). Guidelines for operators  Enable online (in-situ) diagnosis, since many problems cannot be reproduced offline (back in the office). Monitor all hardware components. Capture full-stack performance data and use statistical approaches to pinpoint and isolate root causes. Guidelines for system designers  Make implicit error masking explicit (this is the software equivalent of the advice given above to hardware vendors). “Software systems should not just silently work around fail-slow hardware, but need to expose enough information to help troubleshooting.”  Consider converting fail-slow faults to fail-stop. One example given is skipping a caching layer altogether when SSDs are misbehaving. Another example is to fail-stop after sufficient recurrence of fail-slow incidents (e.g., tripping a circuit breaker after too many retries). Doing this requires an ability to shut-off devices at a fine-grained level. Use fault-injection to explore fail-slow scenarios:  … we strongly believe that injecting root causes reported in this paper will reveal many flaws in existing systems. Furthermore, all forms of fail-slow hardware such as slow NIC’s, switches, disks, SSD, NVDIMM, and CPUs need to be exercised as they lead to differrent symptoms. The challenge is then to build future systems that enable various fail-slow behaviors to be injected easily. Let’s now take a look at some of the characteristics and causes of fail-slow incidents. Root causes  There can be a variety of root causes — these can be internal to the hardware component (for example, device wear or firmware issues), or external (for example, configuration, environment, temperature or power issues). Section 4 in the paper describes the myriad ways that SSDs, Disks, Memory, and Network components can suffer from internal failures. And even when a component would otherwise function just fine, section 5 catalogues a long list of external factors that have been known to cause partial failures:  Temperature: clogged air filters, cold-air-under-floor systems, broken fans, buggy fan firmware, poor assembly/design such that heat-sinks are ineffective. For example, “there was a case where a fan in a compute node stopped working, and to compensate this failing fan, fans in other compute nods started to operate at their maximal speed, which then generated heavy noise and vibration that degraded the disk performance.”  Power: reduced power can easily trigger fail-slow hardware faults. Causes include insufficient capacitors, PCU firmware bugs, fail-partial power supplies, power hungry neighbours (drawing too much power under periods of heavy load), and faulty motherboard sensors. Enviroment: altitude (failure to provide enough cooling for high altitude), cosmic events, loose interconnects, vibrations (e.g. from fans). Configuration errors due to buggy BIOS firmware and simple human mistakes (e.g., in mapping PCIe cards to slots). Fault conversion  Fail-stops in some components can cause others to exhibit fail-slow behaviour:  For example, a dead power supply throttled the CPUs by 50% as the backup supply did not deliver enough power; a single bad disk exhausted the entire RAID card’s performance; and a vendor’s buggy firmware made a batch of SSDs stop for seconds, disabling the flash cache layer and making the entire storage stack slow. Likewise frequent transient failures can result in fail-slow behaviour due to the overheads of error masking (retries, repairs, etc..). When errors ceases to be rare, the masking overhead becomes the normal case performance. Partial internal failures whereby only some part of the device is unusable can also lead to fail-slow behaviour. Symptoms  Fail-slow conditions can manifest in several different ways. The slowdown may be permanent, or the device may fluctuate between normal and degraded performance. Some parts of the device may continue to offer full performance while others are degraded, and in other situations a device may periodically reboot itself, leading to periods of complete unavailability. Cascading failures  …between the actual root cause and the hardware’s fail-slow symptom, there is a chain of cascading root causes… the fail-slow symptom then creates cascading impacts to the high-level software stack, and potentially to the entire cluster. We’ve already seen the example of the failing fan leading to a cascade of events ending in degraded disk performance. Another example is a faulty sensor in a motherboard reporting a false value to the OS, thus making the CPUs run slower in energy saving mode. Fail-slow hardware problems can cascade into the software stack – for example, in an HBase deployment a memory card at 25% of normal speed caused backlogs, out-of-memory errors, and crashes. The last word  The paper is packed with anecdotes that I didn’t have space to cover here. It’s well worth a read to get a finer appreciation of the kinds of hazards that await when building and operating systems at scale. We believe fail-slow hardware is a fundamentally harder problem to solve [than fail-stop]. It is very hard to distinguish such cases from ones that are caused by software performance issues. It is also evident that many modern, advanced deployed systems do not anticipate this failure mode. We hope that our study can influence vendors, operators, and system-designers to treat fail-slow hardware as a separate class of failures and start addressing them more robustly in future system.", "pdf_url": "https://www.usenix.org/system/files/conference/fast18/fast18-gunawi.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/fail-slow-at-scale-evidence-of-hardware-performance-faults-in-large-production-systems.json"}
{"id": "87208689", "bin": "1400_1500", "summary_sentences": ["Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples Athalye et al., ICML’18  There has been a lot of back and forth in the research community on adversarial attacks and defences in machine learning.", "Today’s paper examines a number of recently proposed defences and shows that most of them rely on forms of gradient masking.", "The authors develop attack techniques to overcome such defences, and 9 analyse defences from ICLR 2018 claiming to protect against white-box attacks.", "7 of these turn out to rely on obfuscated gradients, and 6 of these fall to the new attacks (and the other one partially succumbs).", "Athalye et al. won a best paper award at ICML’18 for this work.", "One of the great things about work on adversarial attacks and defences, as we’ve looked at before, is that they illuminate the strengths and weaknesses of current technology.", "Depending on the threat model you choose, for my own part I’m currently of the opinion that we’re unlikely to find a robust adversarial defence without a more radical re-think of how we’re doing image classification.", "If we’re talking about the task of ‘find an image that doesn’t fool a human, but does fool a neural network’ then I think there will always be gaps to exploit until we incorporate more human-like (e.g., higher level semantic understanding) reasoning into our classifiers.", "Not that the human system itself can’t be fooled of course (e.g., optical illusions).", "Breaking gradient descent  A defense is said to cause gradient masking if it “does not have useful gradients” for generating adversarial examples; gradient masking is known to be an incomplete defense to adversarial examples.", "Despite this, we observe that 7 of the ICLR 2018 defenses rely on this effect.", "Some defenses break gradient descent deliberately, others may do it unintentionally.", "Here are five clues that something isn’t right:  One-step attacks perform better than iterative attacks.", "Iterative attacks are strictly stronger, so this shouldn’t be the case.", "If single-step methods are working better, it’s a sign the iterative attack is becoming stuck at a local minimum.", "Black-box attacks work better than white-box attacks.", "The black-box threat model is a strict subset of white-box attacks, so white-box attacks should perform better.", "When a defense obfuscates gradients, then black-box attacks (which don’t use it) often perform better.", "(Since practical black-box attacks are possible and we can also find e.g. universal adversarial perturbations the utility of a defense that excludes such attack modes seems rather limited anyway to me).", "Unbounded attacks do not reach 100% success.", "With unbounded distortion, any classifier should eventually fail.", "An attack that doesn’t achieve this should be improved (i.e., it’s a weak attack, not necessarily a strong defense).", "Random sampling finds adversarial examples (where a gradient-based attack does not).", "Increasing the distortion bound does not increase success.", "We expect a monotonically increasing attack success rate with increasing distortion bound.", "Three ways that a defense might break gradient descent are shattering, stochastic gradients, and exploding & vanishing gradients:  Shattered gradients are caused when a defense is “non-differentiable, introduces number instability, or otherwise causes a gradient to be nonexistent or incorrect.”  Stochastic gradients are a result of randomization – either introduced in the network itself, or in the inputs being fed to the network  Exploding and vanishing gradients are often caused by defenses that consist of multiple iterations of neural network evaluation, feeding the output of one computation into the next.", "Overcoming masked gradients  Shattered gradients can be overcome using a technique the authors call ‘Backward Pass Differentiable Approximation’ (BPDA).", "Think of a secured neural network (i.e., one that has been hardened against adversarial attacks) as being a composition of some hardening function  and a regular pre-trained classifier  such that the hardened classifier  .", "For example,  might be a denoising function designed to remove perturbations introduced by an adversary.", "If  itself is smooth and differentiable we can often compute gradients through the combined network  .", "Even if it isn’t differentiable, we know that by construction in a local area it is close to the identify function (i.e. ).", "So we conduct an attack by performing forward propagation through the neural network as usual, but replacing  with the identity function (or even simpler, just the evaluation of  ) during backpropagation.", "The full (general) version of BPDA operates by finding and replacing any non-differentiable layer  with a differentiable approximation  such that  .", "As before, we use the vanilla network on the forward pass, and substitute  on the backward pass.", "To attack a network that relies on stochastic gradients, it is necessary to estimate the gradient of the stochastic function.", "‘ Expectation over Transformation ’ (EOT) can be used to compute the gradient over an expected transformation to an input.", "Given a transformation function  EOT optimises the expectation over  .", "“The optimization problem can be solved through gradient descent… differentiating through the classifier and transformation, and approximating the expectation with samples at each gradient descent step.”  Vanishing or exploding gradients can be addressed through reparameterization.", "Suppose differentiating  leads to a problem.", "We can introduce a new differentiable function  such that  and  for all  .", "We can now compute gradients through  and circumvent the defense.", "(§4.3).", "How the ICLR 2018 defenses stack up  Section 5 of the paper works through nine defenses from ICRL 2018 that claim robustness in a white-box threat model.", "Seven of the nine turn out to rely on some form of obfuscated gradients.", "For these the authors use the techniques described above to successfully attack them.", "Thermometer encoding is an example of a defense that relies on gradient shattering at its core.", "Image pixels are mapped to l-dimensional vectors such as ‘111110000’, where the number of leading 1’s indicates the temperature (colour value) of a pixel.", "A BPDA attack reduces model accuracy to 30% (worse than if thermometer encoding were not used at all!).", "Examples of transformation based defenses include image cropping and rescaling, bit-depth reduction, JPEG compression, randomly dropping pixels and replacing them using total variance minimization, and image quilting (reconstructing images by replaces small patches with patches taken from ‘clean’ images).", "It is possible to bypass each defense independently (and ensembles of defenses usually are not much stronger than the strongest sub-component)… with our attack, we achieve 100% targeted attack success rate and accuracy drops to 0% for the strongest defense under the smallest perturbation budget…  Image cropping and rescaling are circumvented using EOT.", "Bit depth reduction and JPEG compression are circumvented using BPDA and the identity function on the backward pass.", "Random pixel replacement and quilting are defeated using a combination of EOT and BPDA.", "PixelDefend and Defense-GAN both use generators to project input images back onto a data manifold before feeding them into a classifier.", "Informally, this is intended to ‘purify’ them by avoiding low-probability regions in the data distribution.", "PixelDefend can be defeated using BPDA, and DefenseGAN can also be evaded with BPDA, though only at a 45% success rate.", "See section 5 in the paper for more details and discussion of the other defenses.", "… we hope that future work will be able to avoid relying on obfuscated gradients (and other methods that only prevent gradient descent-based attacks) for perceived robustness, and use our evaluation approach to detect when this occurs."], "summary_text": "Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples Athalye et al., ICML’18  There has been a lot of back and forth in the research community on adversarial attacks and defences in machine learning. Today’s paper examines a number of recently proposed defences and shows that most of them rely on forms of gradient masking. The authors develop attack techniques to overcome such defences, and 9 analyse defences from ICLR 2018 claiming to protect against white-box attacks. 7 of these turn out to rely on obfuscated gradients, and 6 of these fall to the new attacks (and the other one partially succumbs). Athalye et al. won a best paper award at ICML’18 for this work. One of the great things about work on adversarial attacks and defences, as we’ve looked at before, is that they illuminate the strengths and weaknesses of current technology. Depending on the threat model you choose, for my own part I’m currently of the opinion that we’re unlikely to find a robust adversarial defence without a more radical re-think of how we’re doing image classification. If we’re talking about the task of ‘find an image that doesn’t fool a human, but does fool a neural network’ then I think there will always be gaps to exploit until we incorporate more human-like (e.g., higher level semantic understanding) reasoning into our classifiers. Not that the human system itself can’t be fooled of course (e.g., optical illusions). Breaking gradient descent  A defense is said to cause gradient masking if it “does not have useful gradients” for generating adversarial examples; gradient masking is known to be an incomplete defense to adversarial examples. Despite this, we observe that 7 of the ICLR 2018 defenses rely on this effect. Some defenses break gradient descent deliberately, others may do it unintentionally. Here are five clues that something isn’t right:  One-step attacks perform better than iterative attacks. Iterative attacks are strictly stronger, so this shouldn’t be the case. If single-step methods are working better, it’s a sign the iterative attack is becoming stuck at a local minimum. Black-box attacks work better than white-box attacks. The black-box threat model is a strict subset of white-box attacks, so white-box attacks should perform better. When a defense obfuscates gradients, then black-box attacks (which don’t use it) often perform better. (Since practical black-box attacks are possible and we can also find e.g. universal adversarial perturbations the utility of a defense that excludes such attack modes seems rather limited anyway to me). Unbounded attacks do not reach 100% success. With unbounded distortion, any classifier should eventually fail. An attack that doesn’t achieve this should be improved (i.e., it’s a weak attack, not necessarily a strong defense). Random sampling finds adversarial examples (where a gradient-based attack does not). Increasing the distortion bound does not increase success. We expect a monotonically increasing attack success rate with increasing distortion bound. Three ways that a defense might break gradient descent are shattering, stochastic gradients, and exploding & vanishing gradients:  Shattered gradients are caused when a defense is “non-differentiable, introduces number instability, or otherwise causes a gradient to be nonexistent or incorrect.”  Stochastic gradients are a result of randomization – either introduced in the network itself, or in the inputs being fed to the network  Exploding and vanishing gradients are often caused by defenses that consist of multiple iterations of neural network evaluation, feeding the output of one computation into the next. Overcoming masked gradients  Shattered gradients can be overcome using a technique the authors call ‘Backward Pass Differentiable Approximation’ (BPDA). Think of a secured neural network (i.e., one that has been hardened against adversarial attacks) as being a composition of some hardening function  and a regular pre-trained classifier  such that the hardened classifier  . For example,  might be a denoising function designed to remove perturbations introduced by an adversary. If  itself is smooth and differentiable we can often compute gradients through the combined network  . Even if it isn’t differentiable, we know that by construction in a local area it is close to the identify function (i.e. ). So we conduct an attack by performing forward propagation through the neural network as usual, but replacing  with the identity function (or even simpler, just the evaluation of  ) during backpropagation. The full (general) version of BPDA operates by finding and replacing any non-differentiable layer  with a differentiable approximation  such that  . As before, we use the vanilla network on the forward pass, and substitute  on the backward pass. To attack a network that relies on stochastic gradients, it is necessary to estimate the gradient of the stochastic function. ‘ Expectation over Transformation ’ (EOT) can be used to compute the gradient over an expected transformation to an input. Given a transformation function  EOT optimises the expectation over  . “The optimization problem can be solved through gradient descent… differentiating through the classifier and transformation, and approximating the expectation with samples at each gradient descent step.”  Vanishing or exploding gradients can be addressed through reparameterization. Suppose differentiating  leads to a problem. We can introduce a new differentiable function  such that  and  for all  . We can now compute gradients through  and circumvent the defense. (§4.3). How the ICLR 2018 defenses stack up  Section 5 of the paper works through nine defenses from ICRL 2018 that claim robustness in a white-box threat model. Seven of the nine turn out to rely on some form of obfuscated gradients. For these the authors use the techniques described above to successfully attack them. Thermometer encoding is an example of a defense that relies on gradient shattering at its core. Image pixels are mapped to l-dimensional vectors such as ‘111110000’, where the number of leading 1’s indicates the temperature (colour value) of a pixel. A BPDA attack reduces model accuracy to 30% (worse than if thermometer encoding were not used at all!). Examples of transformation based defenses include image cropping and rescaling, bit-depth reduction, JPEG compression, randomly dropping pixels and replacing them using total variance minimization, and image quilting (reconstructing images by replaces small patches with patches taken from ‘clean’ images). It is possible to bypass each defense independently (and ensembles of defenses usually are not much stronger than the strongest sub-component)… with our attack, we achieve 100% targeted attack success rate and accuracy drops to 0% for the strongest defense under the smallest perturbation budget…  Image cropping and rescaling are circumvented using EOT. Bit depth reduction and JPEG compression are circumvented using BPDA and the identity function on the backward pass. Random pixel replacement and quilting are defeated using a combination of EOT and BPDA. PixelDefend and Defense-GAN both use generators to project input images back onto a data manifold before feeding them into a classifier. Informally, this is intended to ‘purify’ them by avoiding low-probability regions in the data distribution. PixelDefend can be defeated using BPDA, and DefenseGAN can also be evaded with BPDA, though only at a 45% success rate. See section 5 in the paper for more details and discussion of the other defenses. … we hope that future work will be able to avoid relying on obfuscated gradients (and other methods that only prevent gradient descent-based attacks) for perceived robustness, and use our evaluation approach to detect when this occurs.", "pdf_url": "https://arxiv.org/pdf/1802.00420", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/obfuscated-gradients-give-a-false-sense-of-security-circumventing-defenses-to-adversarial-examples.json"}
{"id": "71546396", "bin": "1400_1500", "summary_sentences": ["Master of web puppets: abusing web browsers for persistent and stealthy computation Papadopoulus et al., NDSS’19  UPDATE 2019-04-14: An author update has been published for this paper which details that with current browser versions, ServiceWorkers can only stay alive for about a minute after the user navigates away from the site.", "This mitigates the main risk detailed in the paper of long running botnet membership.", "With thanks to Alex Russell ( @slightylate ) for highlighting this to me.", "You’ve probably heard about crypto-currency mining and the like in hijacked browsers.", "From a security perspective, a fundamental problem of web applications is that by default their publisher is considered as trusted, and thus allowed to run JavaScript code (even from third parties) on the user side without any restrictions… On the positive side JavaScript execution so far has been constrained chronologically to the lifetime of the browser window or tab that rendered the compromised or malicious website.", "Not any more!", "This paper shows how modern browsers with support for Service Workers can be stealthily connected into a botnet, with a connection that persists until the user closes the browser completely: “in contrast to previous approaches for browser hijacking, a key feature of MarioNet is that it remains operational even after the user browses away from the malicious webpage.”  MarioNet building blocks: Service Workers and WebRTC  Service Workers are non-blocking modules that reside in the user’s browser.", "Once registered they can run in the background without requiring the user to continue browsing on the originating site.", "In addition, service workers have the ability to intercept and handle network requests (for e.g., caching and pre-loading purposes).", "No user permission is required to register and maintain a service worker.", "When the user browses away from a website, the service worker of that website is typically paused by the browser; it is then restarted and reactivated once the parent domain is visited again.", "However, it is possible for the publisher of a website to keep its service worker alive by implementing periodic synchronization.", "If the user permits the publishing site to send push notifications, then it is even possible to have the service worker restart when the browser is restarted.", "Browser extensions are not permitted to use HTML5 APIs such as the Service Workers API and Push API, and hence cannot interact with or see deployed service workers in any way (e.g., modify their code, monitor their outgoing traffic etc.).", "Thus no extension-based mechanism or blocker (save turning off JavaScript completely) can offer protection.", "Once browsers have been hijacked, the Web Real-Time Communication (WebRTC) API is also very useful for implementing peer-to-peer communication between browsers.", "Key features of MarioNet  MarioNet aims to establish an in-browser botnet that requires only that the user visit a target website (e.g. one owned by the attacker, or one that the attacker has compromised) one time in order to takeover the browser.", "The design has the following goals:  Isolation: operation is independent of any browsing session thread or process (so that more heavyweight computation can be done with less chance of detection)  Persistence: operation is completely detached from any ephemeral browsing session, so that the browser remains under the attackers control for periods of time much longer than a website visit  Evasiveness: operations should be performed in a stealthy manner to evade detection and maintain the infection for as long as possible.There are three main components to the system, as shown in the figure below.", "The user simply needs to visit one time a page under the attackers control, hosted on a website.", "This page installs a service worker (the Servant) when loaded in the browser, and uses background sync registrations to keep the Servant always alive.", "As part of its initialization, the Servant establishes a communication channel with its remote command and control server (Puppeteer) and requests the initial set of tasks.", "The Puppeteer can send tasks to Servants at any time over the established communications channels.", "One easy way to package them is as JavaScript scripts that the servant simply evals.", "Because extensions cannot see service workers, the connection between a Servant and the Puppeteer is hidden from all extensions.", "The connection is also TLS-encrypted such that it cannot be eavesdropped outside of the browser either.", "The only request that reveals the existence of the service worker is the initial GET request at the time of the user’s first website visit, when the service worker gets initially registered.", "To further evade detection, the Servant monitors the device’s current status (CPU utilisation, battery, etc.)", "and can throttle or pause execution of the malicious workload to minimise risk of detection.", "If the user allows the distributor to send push notifications (more likely perhaps when the servant is distributed via a compromised site the user trusts), then MarioNet can send asynchronous notifications and updates to its service workers to re-activate them after browser restarts.", "MarioNet use cases  MarioNet can conduct a subset of DDoS attacks by instructing Servants to connect to a specific Internet host.", "There is no low-level networking access, but HTTP request methods and the HTTP body can of course be controlled.", "If the victim site has enabled WebSockets then this is another attack vector.", "UsingXMLHttpRequest.send(), jQuery’s ajax() and WebSocket’s send() methods, we can continually send a flood of messages to a targeted host.", "MarioNet can be used for cryptocurrency mining, where it is much more efficient than current Web Worker based approaches since it can continue mining even once the user has left the initial site.", "MarioNet can instruct Servants to assist in distributed password cracking.", "“A major advantage of MarioNet is that it can be agnostic to the hashing function used, since the function code is transferred from the Puppeteer and executed from the MarioNet nodes through eval().”  A network of MarioNet nodes can form a content distribution network (presumably for content of an unsavoury nature) using WebRTC.", "The authors implemented a WebTorrent proof-of-concept for this idea.", "In a similar manner, WebRTC can be used to construct obfuscating relay proxies.", "Another classic use case would be click fraud, where MarioNet is used to surf targeted websites, stream online videos to increase views, manipulate online polls, and so on.", "The Puppeteer would simply need to periodically send a list of online links to visit…  Defenses?", "MarioNet works across a wide range of modern desktop and mobile browsers, although in the evaluation Service Worker performance on Safari was found to be very poor (presumably Apple will improve this over time?).", "The prime defence against MarioNet would require a mechanism to restrict or disable service workers.", "Since many sites now depend on them this has the potential to degrade user experience for legitimate use cases.", "Forbidding service workers to use eval and friends would make it more difficult for the Puppeteer.", "A same-origin policy for service worker network requests would also help in the case of a compromised distribution site.", "Users could also be required to explicitly give permission for a service worker to be installed by a site (currently user consent is only needed for Push notifications).", "Network traffic monitors may be able to pick up message exchange patterns, in environments where these are deployed and appropriately configured.", "Beyond never accepting push notification requests, and periodically restarting your browser, it seems there’s not much more you can do as an end user to protect against this class of attacks.", "Essentially, our work demonstrates that the trust model of web, which considers web publishers as trusted and allows them to execute code on the client-side without any restrictions is ﬂawed and needs reconsideration.", "Furthermore, this work aims to increase the awareness regarding the powerful capabilities that modern browser APIs provide to attackers, and to initiate a serious discussion about implementing restrictions while offering such capabilities that can be easily abused."], "summary_text": "Master of web puppets: abusing web browsers for persistent and stealthy computation Papadopoulus et al., NDSS’19  UPDATE 2019-04-14: An author update has been published for this paper which details that with current browser versions, ServiceWorkers can only stay alive for about a minute after the user navigates away from the site. This mitigates the main risk detailed in the paper of long running botnet membership. With thanks to Alex Russell ( @slightylate ) for highlighting this to me. You’ve probably heard about crypto-currency mining and the like in hijacked browsers. From a security perspective, a fundamental problem of web applications is that by default their publisher is considered as trusted, and thus allowed to run JavaScript code (even from third parties) on the user side without any restrictions… On the positive side JavaScript execution so far has been constrained chronologically to the lifetime of the browser window or tab that rendered the compromised or malicious website. Not any more! This paper shows how modern browsers with support for Service Workers can be stealthily connected into a botnet, with a connection that persists until the user closes the browser completely: “in contrast to previous approaches for browser hijacking, a key feature of MarioNet is that it remains operational even after the user browses away from the malicious webpage.”  MarioNet building blocks: Service Workers and WebRTC  Service Workers are non-blocking modules that reside in the user’s browser. Once registered they can run in the background without requiring the user to continue browsing on the originating site. In addition, service workers have the ability to intercept and handle network requests (for e.g., caching and pre-loading purposes). No user permission is required to register and maintain a service worker. When the user browses away from a website, the service worker of that website is typically paused by the browser; it is then restarted and reactivated once the parent domain is visited again. However, it is possible for the publisher of a website to keep its service worker alive by implementing periodic synchronization. If the user permits the publishing site to send push notifications, then it is even possible to have the service worker restart when the browser is restarted. Browser extensions are not permitted to use HTML5 APIs such as the Service Workers API and Push API, and hence cannot interact with or see deployed service workers in any way (e.g., modify their code, monitor their outgoing traffic etc.). Thus no extension-based mechanism or blocker (save turning off JavaScript completely) can offer protection. Once browsers have been hijacked, the Web Real-Time Communication (WebRTC) API is also very useful for implementing peer-to-peer communication between browsers. Key features of MarioNet  MarioNet aims to establish an in-browser botnet that requires only that the user visit a target website (e.g. one owned by the attacker, or one that the attacker has compromised) one time in order to takeover the browser. The design has the following goals:  Isolation: operation is independent of any browsing session thread or process (so that more heavyweight computation can be done with less chance of detection)  Persistence: operation is completely detached from any ephemeral browsing session, so that the browser remains under the attackers control for periods of time much longer than a website visit  Evasiveness: operations should be performed in a stealthy manner to evade detection and maintain the infection for as long as possible.There are three main components to the system, as shown in the figure below. The user simply needs to visit one time a page under the attackers control, hosted on a website. This page installs a service worker (the Servant) when loaded in the browser, and uses background sync registrations to keep the Servant always alive. As part of its initialization, the Servant establishes a communication channel with its remote command and control server (Puppeteer) and requests the initial set of tasks. The Puppeteer can send tasks to Servants at any time over the established communications channels. One easy way to package them is as JavaScript scripts that the servant simply evals. Because extensions cannot see service workers, the connection between a Servant and the Puppeteer is hidden from all extensions. The connection is also TLS-encrypted such that it cannot be eavesdropped outside of the browser either. The only request that reveals the existence of the service worker is the initial GET request at the time of the user’s first website visit, when the service worker gets initially registered. To further evade detection, the Servant monitors the device’s current status (CPU utilisation, battery, etc.) and can throttle or pause execution of the malicious workload to minimise risk of detection. If the user allows the distributor to send push notifications (more likely perhaps when the servant is distributed via a compromised site the user trusts), then MarioNet can send asynchronous notifications and updates to its service workers to re-activate them after browser restarts. MarioNet use cases  MarioNet can conduct a subset of DDoS attacks by instructing Servants to connect to a specific Internet host. There is no low-level networking access, but HTTP request methods and the HTTP body can of course be controlled. If the victim site has enabled WebSockets then this is another attack vector. UsingXMLHttpRequest.send(), jQuery’s ajax() and WebSocket’s send() methods, we can continually send a flood of messages to a targeted host. MarioNet can be used for cryptocurrency mining, where it is much more efficient than current Web Worker based approaches since it can continue mining even once the user has left the initial site. MarioNet can instruct Servants to assist in distributed password cracking. “A major advantage of MarioNet is that it can be agnostic to the hashing function used, since the function code is transferred from the Puppeteer and executed from the MarioNet nodes through eval().”  A network of MarioNet nodes can form a content distribution network (presumably for content of an unsavoury nature) using WebRTC. The authors implemented a WebTorrent proof-of-concept for this idea. In a similar manner, WebRTC can be used to construct obfuscating relay proxies. Another classic use case would be click fraud, where MarioNet is used to surf targeted websites, stream online videos to increase views, manipulate online polls, and so on. The Puppeteer would simply need to periodically send a list of online links to visit…  Defenses? MarioNet works across a wide range of modern desktop and mobile browsers, although in the evaluation Service Worker performance on Safari was found to be very poor (presumably Apple will improve this over time?). The prime defence against MarioNet would require a mechanism to restrict or disable service workers. Since many sites now depend on them this has the potential to degrade user experience for legitimate use cases. Forbidding service workers to use eval and friends would make it more difficult for the Puppeteer. A same-origin policy for service worker network requests would also help in the case of a compromised distribution site. Users could also be required to explicitly give permission for a service worker to be installed by a site (currently user consent is only needed for Push notifications). Network traffic monitors may be able to pick up message exchange patterns, in environments where these are deployed and appropriately configured. Beyond never accepting push notification requests, and periodically restarting your browser, it seems there’s not much more you can do as an end user to protect against this class of attacks. Essentially, our work demonstrates that the trust model of web, which considers web publishers as trusted and allows them to execute code on the client-side without any restrictions is ﬂawed and needs reconsideration. Furthermore, this work aims to increase the awareness regarding the powerful capabilities that modern browser APIs provide to attackers, and to initiate a serious discussion about implementing restrictions while offering such capabilities that can be easily abused.", "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_01B-2_Papadopoulos_paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/master-of-web-puppets-abusing-web-browsers-for-persistent-and-stealthy-computation.json"}
{"id": "11112277", "bin": "1400_1500", "summary_sentences": ["Most of the visual question-answering (VQA) models perform poorly on the task of counting objects in an image.", "The main reasons are:  Most VQA models use a soft attention mechanism to perform a weighted sum over the spatial features to obtain a single feature vector.", "These aggregated features helps in most category of questions but seems to hurt for counting based questions.", "For the counting questions, we do not have a ground truth segmentation of where the objects to be counted are present on the image.", "This limits the scope of supervision.", "Additionally, we need to ensure that any modification in the architecture, to enhance the performance on the counting questions, should not degrade the performance on other classes of questions.", "The paper proposes to overcome these challenges by using the attention maps (and not the aggregated feature vectors) as input to a separate count module.", "Notes  The basic idea is quite intuitive: when we perform weighted averaging based on different attention maps, we end up averaging the features corresponding to the difference instances of an object.", "This makes the feature vectors indistinguishable from the scenario where we had just one instance of the object in the image.", "Even multiple glimpses (multiple attention steps) can not resolve this problem as the weights given to one feature vector would not depend on the other feature vectors (that are attended to).", "Hard attention could be more useful than soft-attention but there is not much empirical evidence in support of this hypothesis.", "The proposed count module is a separate pipeline that can be integrated with most of the existing attention based VQA models without affecting the performance on non-count based questions.", "The inputs to the count module are the attention maps and the object proposals (coming from some pre-trained model like the RCNN model) and the output is an count-feature vector which is used to answer the count based question.", "The top level idea is the following - given the object proposals and the attention maps, create a graph where nodes are objects (object proposals) and edges capture how similar two object proposals are (how much do they overlap).", "The graph is transformed (by removing and scaling edges) so that the count of the object can be obtained easily.", "To explain their methodology, the paper simplifies the setting by making two assumptions:  The first assumption is that the attention weights are either 1 (when the object is present in the proposal) or 0 (when the object is absent from the proposal).", "The second assumption is that any two object proposals either overlap completely (in which case, they are corresponding to the exact same object and hence receive the exact same weights) or the two proposals have zero overlap (in which case, they must be corresponding to completely different objects).", "These simplifying assumptions are made only for the sake of exposition and do not limit the capabilities of the count module.", "Given the assumptions, the task of the count module is to handle the exact duplicates to prevent double-counting of objects.", "As the first step, the attention weights (a) are used to generate an attention matrix (A) by performing an outer product between a and aT.", "This corresponds to the step of creating a graph from the input.", "A corresponds to the adjacency matrix of that graph.", "The attention weight for the ith proposal corresponds to the ith node in the graph and the edge between the nodes i and j has the weight ai*aj.", "Also note that the graph is a weighted directed graph and the subgraph of vertices satisfying the condition ai = 1 is a complete directed graph with self-loops.", "Given such a graph, the number of vertices, V = sqrt(E) where E could be computed by summing over the adjacency matrix.This implies that if the proposals are distinct, then the count can be obtained trivially by performing a sum over the adjacency matrix.", "The objective is now to eliminate the edges such that the underlying objects are the vertices of a complete subgraph.", "This requires removing two type of duplicate edges - intra-object edges and inter-object edges.", "Intra-object edges can be removed by computing a distance matrix, D, defined as 1 - IoU, where IoU matrix corresponds to the Intersection-over-Union matrix.", "A modified adjacency matrix A’ is obtained by performing the element-wise product between f1(A) and f2(D) where f1 and f2 are piece-wise linear functions that are learnt via backpropogation.", "The inter-object edges are removed in the following manner:  Count the number of proposals that correspond of each instance of an object and then scale down the edges corresponding to the different instances by that number.", "This creates the effect of reducing the weights of multiple proposals equivalent to a single proposal.", "The number of proposals corresponding to an object is not available as an annotation in the training pipeline and is estimated based on the similarity between the different proposals (measured via the attention weights a, adjacency matrix A and distance matrix D).", "The matrix corresponding to the similarity between proposals  (simi, j) is transformed into a vector corresponding to the scaling factor of each node (si)  s can be converted into a matrix (by doing outer-product with itself) so as to scale both the incoming and the outgoing edges.", "The self edges (which were removed while computing A’ are added back (after scaling with s) to obtain a new transformed matrix C.  The transformed matrix C is a complete graph with self-loops where the nodes corresponds to all the relevant object instances and not to object proposals.", "The actual count can be obtained from C by performing a sum over all its values as described earlier.", "The original count problem was a regression problem but it is transformed into a classification problem to avoid scale issues.", "The network produces a k-hot n-dimensional vector called o where n is the number of object proposals that were feed into the module (and hence the upper limit on upto how large a number could the module count).", "In the ideal setting, k should be one, as the network would produce an integer value but in practice, the network produces a real number so k can be upto 2.", "If c is an exact integer, the output is a 1-hot vector with the value in index corresponding to c set to 1.", "If c is a real number, the output is a linear interpolation between two one-hot vectors (the one-hot vectors correspond to the two integers between  which c lies).", "count module supports computing the confidence of a prediction by defining two variables pa and pD which compute the average distance of f6(a) and $f7(D) from 0.5.", "The final output o’ is defined as f8(pa + pD) .", "o  All the different f functions are piece wise linear functions and are learnt via backpropagation.", "Experiments  The authors created a new category of count-based questions by filtering the number-type questions to remove questions like “What is the time right now”.", "These questions do have a neumerical answer but do not fall under the purview of count based questions and hence are not targeted by the count model.", "The authors augmented a state of the art VQA model with their count module and show substantial gains over the count-type questions for the VQA-v2 dataset .", "This augmentation does not drastically impact the performance on non-count questions.", "The overall idea is quite crisp and intutive and the paper is easy to follow.", "It would be even better if there were some more abalation studies.", "For example, why are the piece-wise linear functions assumed to have 16 linear components?", "Would a smaller or larger number be better?"], "summary_text": "Most of the visual question-answering (VQA) models perform poorly on the task of counting objects in an image. The main reasons are:  Most VQA models use a soft attention mechanism to perform a weighted sum over the spatial features to obtain a single feature vector. These aggregated features helps in most category of questions but seems to hurt for counting based questions. For the counting questions, we do not have a ground truth segmentation of where the objects to be counted are present on the image. This limits the scope of supervision. Additionally, we need to ensure that any modification in the architecture, to enhance the performance on the counting questions, should not degrade the performance on other classes of questions. The paper proposes to overcome these challenges by using the attention maps (and not the aggregated feature vectors) as input to a separate count module. Notes  The basic idea is quite intuitive: when we perform weighted averaging based on different attention maps, we end up averaging the features corresponding to the difference instances of an object. This makes the feature vectors indistinguishable from the scenario where we had just one instance of the object in the image. Even multiple glimpses (multiple attention steps) can not resolve this problem as the weights given to one feature vector would not depend on the other feature vectors (that are attended to). Hard attention could be more useful than soft-attention but there is not much empirical evidence in support of this hypothesis. The proposed count module is a separate pipeline that can be integrated with most of the existing attention based VQA models without affecting the performance on non-count based questions. The inputs to the count module are the attention maps and the object proposals (coming from some pre-trained model like the RCNN model) and the output is an count-feature vector which is used to answer the count based question. The top level idea is the following - given the object proposals and the attention maps, create a graph where nodes are objects (object proposals) and edges capture how similar two object proposals are (how much do they overlap). The graph is transformed (by removing and scaling edges) so that the count of the object can be obtained easily. To explain their methodology, the paper simplifies the setting by making two assumptions:  The first assumption is that the attention weights are either 1 (when the object is present in the proposal) or 0 (when the object is absent from the proposal). The second assumption is that any two object proposals either overlap completely (in which case, they are corresponding to the exact same object and hence receive the exact same weights) or the two proposals have zero overlap (in which case, they must be corresponding to completely different objects). These simplifying assumptions are made only for the sake of exposition and do not limit the capabilities of the count module. Given the assumptions, the task of the count module is to handle the exact duplicates to prevent double-counting of objects. As the first step, the attention weights (a) are used to generate an attention matrix (A) by performing an outer product between a and aT. This corresponds to the step of creating a graph from the input. A corresponds to the adjacency matrix of that graph. The attention weight for the ith proposal corresponds to the ith node in the graph and the edge between the nodes i and j has the weight ai*aj. Also note that the graph is a weighted directed graph and the subgraph of vertices satisfying the condition ai = 1 is a complete directed graph with self-loops. Given such a graph, the number of vertices, V = sqrt(E) where E could be computed by summing over the adjacency matrix.This implies that if the proposals are distinct, then the count can be obtained trivially by performing a sum over the adjacency matrix. The objective is now to eliminate the edges such that the underlying objects are the vertices of a complete subgraph. This requires removing two type of duplicate edges - intra-object edges and inter-object edges. Intra-object edges can be removed by computing a distance matrix, D, defined as 1 - IoU, where IoU matrix corresponds to the Intersection-over-Union matrix. A modified adjacency matrix A’ is obtained by performing the element-wise product between f1(A) and f2(D) where f1 and f2 are piece-wise linear functions that are learnt via backpropogation. The inter-object edges are removed in the following manner:  Count the number of proposals that correspond of each instance of an object and then scale down the edges corresponding to the different instances by that number. This creates the effect of reducing the weights of multiple proposals equivalent to a single proposal. The number of proposals corresponding to an object is not available as an annotation in the training pipeline and is estimated based on the similarity between the different proposals (measured via the attention weights a, adjacency matrix A and distance matrix D). The matrix corresponding to the similarity between proposals  (simi, j) is transformed into a vector corresponding to the scaling factor of each node (si)  s can be converted into a matrix (by doing outer-product with itself) so as to scale both the incoming and the outgoing edges. The self edges (which were removed while computing A’ are added back (after scaling with s) to obtain a new transformed matrix C.  The transformed matrix C is a complete graph with self-loops where the nodes corresponds to all the relevant object instances and not to object proposals. The actual count can be obtained from C by performing a sum over all its values as described earlier. The original count problem was a regression problem but it is transformed into a classification problem to avoid scale issues. The network produces a k-hot n-dimensional vector called o where n is the number of object proposals that were feed into the module (and hence the upper limit on upto how large a number could the module count). In the ideal setting, k should be one, as the network would produce an integer value but in practice, the network produces a real number so k can be upto 2. If c is an exact integer, the output is a 1-hot vector with the value in index corresponding to c set to 1. If c is a real number, the output is a linear interpolation between two one-hot vectors (the one-hot vectors correspond to the two integers between  which c lies). count module supports computing the confidence of a prediction by defining two variables pa and pD which compute the average distance of f6(a) and $f7(D) from 0.5. The final output o’ is defined as f8(pa + pD) . o  All the different f functions are piece wise linear functions and are learnt via backpropagation. Experiments  The authors created a new category of count-based questions by filtering the number-type questions to remove questions like “What is the time right now”. These questions do have a neumerical answer but do not fall under the purview of count based questions and hence are not targeted by the count model. The authors augmented a state of the art VQA model with their count module and show substantial gains over the count-type questions for the VQA-v2 dataset . This augmentation does not drastically impact the performance on non-count questions. The overall idea is quite crisp and intutive and the paper is easy to follow. It would be even better if there were some more abalation studies. For example, why are the piece-wise linear functions assumed to have 16 linear components? Would a smaller or larger number be better?", "pdf_url": "https://arxiv.org/pdf/1802.05766", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/learning-to-count-objects-in-natural-images-for-visual-question-answering.json"}
{"id": "32338938", "bin": "1400_1500", "summary_sentences": ["Cardinality estimation done right: Index-based join sampling  Cardinality estimation done right: Index-based join sampling Leis et al., CIDR 2017  Let’s finish up our brief look at CIDR 2017 with something closer to the core of database systems research – query optimisation.", "For good background on this topic a great place to start is Selinger’s 1979 classic ‘ Access path selection in a relational database management system ‘ (System R).", "One of the trickiest areas in query optimisation is determining the best join strategy, and in the almost 40 years since that System R paper, a lot of work has been done on this problem .", "If you are joining n tables, the number of possible join orders grows with n!", ".", "When estimating the potential costs of different join orders, a fundamental input is an estimation of the number of matching tuples in each of the relations to be joined (aka cardinality estimation).", "Virtually all industrial-strength systems estimate cardinalities by combining some fixed-size, per-attribute summary statistics (histograms) with strong assumptions (uniformity, independency, inclusion, ad hoc constants).", "In other words, most databases try to approximate an arbitrarily large database in a constant amount of space… For real-world data sets, cardinality estimation errors are large and occur frequently.", "These errors lead to slow queries and unpredictable performance.", "Instead of using histograms to try and guess, for example, how many tuples a relation might have where  an alternative approach would be to sample a subset of the rows and see how many matches there are in the sampled subset.", "Sampling has not traditionally been used for two reasons:  The disk I/O involved in going and fetching sample tuples is too slow, compounded by…  You need to sample a surprisingly large number of tuples in order to ensure that you’re still left with enough tuples to carry on estimating after joining.", "Say we’re joining A, B, and C. We want to estimate the cost of joining A and B, and then joining the result with C. We need enough samples remaining after the first join to be able to do a good enough estimation of the join with C. It turns out that with random sampling without replacement, in order to have n expected result tuples after the first join, we need to sample on the order of  tuples.", "The more joins we have, the larger the initial samples need to be….", "For in-memory databases, the first consideration largely goes away – we can sample reasonable numbers of tuples in a small amount of time (though presumably we always still have some kind of space budget we also have to adhere to).", "To address the second consideration it would be ideal if the samples we took were somehow more likely to ‘survive’ the joining process, so that a sufficient number of tuples flow through to the next join…  In this work we propose a novel cardinality estimation technique that produces accurate results but is much cheaper than joining random samples.", "The basic building block is an efficient sampling operator that utilizes existing index structures: to get an estimate for  , we obtain a random sample of  and then look up the samples’ join partners in the index for  (we could also start with  using an index on  ).", "The resulting sample for  can be used as a starting point for obtaining a sample for  by using an index on the join attribute  and so on.", "Suppose we have a sample S as a result of sampling some relation T, and for the next step we want to create a sample of  .", "For each tuple in S we use the index to look up how many matching tuples are expected in A. Summing these gives us the total expected number of matching tuples, and the required number of samples is then drawn from this set.", "The index-based sampling operator can cheaply compute a sample for a join result, but it is not a full solution by itself.", "We also need a join enumeration strategy which can systematically explore the intermediate results of a query using the sampling operator, while also ensuring that the overall sampling time is limited.", "If we sampled every possible combination, it would take too long for queries with many joins.", "In the Join Order Benchmark (JOB), queries with 7 joins have 84-107 intermediate results, and queries with 13 joins have 1,517-2,032.", "A time limit is set on the sampling phase, after which the algorithm falls back to traditional estimation.", "The advantage is that one quickly obtains accurate estimates for large intermediate results.", "The disadvantage is that many small intermediate results are not sampled and thus have to be estimated using traditional estimation.", "It is well known that—due to the independence assumption—traditional estimators tend to underestimate result sizes.", "Therefore, when this mix of (accurate) sampling-based estimates and traditional (under-)estimates are injected into a query optimizer, it will often pick a plan based on the traditional estimates (as they appear to be very cheap).", "This phenomenon has been called “fleeing from knowledge to ignorance” and—paradoxically—causes additional, accurate information to decrease plan quality.", "To address this issue, joins are sampled ‘bottom-up’ – i.e., first all 2-way joins are computed, then all 3-way joins, and so on.", "“A cost-based query optimizer will thus have precise knowledge for the costs of the early (and often crucial) joins.”  Integrating this approach into an existing DBMS is pretty straightforward since you just need to inject the results into the cardinality estimation component of the query optimizer, and no changes to the cost model or plan space enumeration algorithm are necessary.", "Any query optimizer change that increases the performance for the vast majority of queries, will also decrease performance for some queries, which is very undesirable in production systems.", "Existing database systems are therefore very conservative with query optimizer changes.", "Thus, one could use our approach as an optional tuning feature for queries that are slower than expected.", "In other words, if a user is not satisfied with the performance of a particular query, to get better performance she may turn on index-based sampling only for that query.", "How well does it work?", "Evaluation is based on the Join Order Benchmark, based on the Internet Movie Database.", "There are 113 queries with 3 to 16 joins.", "As a worst case for planning complexity, indices are created on all primary key and foreign key columns.", "To compare with traditional cardinality estimates, the test harness supports injection of cardinality estimates from an outside system – in this case obtained from PostgreSQL using the EXPLAIN command.", "You can see that compared to the PostgreSQL estimates (column 1), index-based sampling with even just 10K samples produces much more accurate estimates.", "Also note how estimation accuracy decreases with the number of joins.", "The more samples we have of course, the closer we get to the true cardinalities.", "Do these more accurate cardinality estimates actually lead to better plans?", "In figure 6 below you can see the plan quality (log scale) of the plans produced.", "The cost of each query is normalized by the cost of the optimal plan that would have been chosen if the true cardinalities were known.", "Using PostgreSQL’s estimates, only around one quarter of the plans are close to the optimum, 42% of the plans are off by a factor of 2 or more, and 12% are off by a factor of 10 or more…  With a budget of 100,000 index lookups, index-based sampling improves performance for many queries – only 17% are off by a factor of 2 or more, and only 3% by a factor of 10 or more.", "As expected, these better plans lead (mostly!)", "to faster runtimes:  A small number of plans are actually faster (up to 3x) with inaccurate estimates rather than with the true cardinalities.", "This effect is caused by cost model errors rather than inaccurate cardinalities and explains the hesitation of many commercial database systems to change their query optimizers."], "summary_text": "Cardinality estimation done right: Index-based join sampling  Cardinality estimation done right: Index-based join sampling Leis et al., CIDR 2017  Let’s finish up our brief look at CIDR 2017 with something closer to the core of database systems research – query optimisation. For good background on this topic a great place to start is Selinger’s 1979 classic ‘ Access path selection in a relational database management system ‘ (System R). One of the trickiest areas in query optimisation is determining the best join strategy, and in the almost 40 years since that System R paper, a lot of work has been done on this problem . If you are joining n tables, the number of possible join orders grows with n! . When estimating the potential costs of different join orders, a fundamental input is an estimation of the number of matching tuples in each of the relations to be joined (aka cardinality estimation). Virtually all industrial-strength systems estimate cardinalities by combining some fixed-size, per-attribute summary statistics (histograms) with strong assumptions (uniformity, independency, inclusion, ad hoc constants). In other words, most databases try to approximate an arbitrarily large database in a constant amount of space… For real-world data sets, cardinality estimation errors are large and occur frequently. These errors lead to slow queries and unpredictable performance. Instead of using histograms to try and guess, for example, how many tuples a relation might have where  an alternative approach would be to sample a subset of the rows and see how many matches there are in the sampled subset. Sampling has not traditionally been used for two reasons:  The disk I/O involved in going and fetching sample tuples is too slow, compounded by…  You need to sample a surprisingly large number of tuples in order to ensure that you’re still left with enough tuples to carry on estimating after joining. Say we’re joining A, B, and C. We want to estimate the cost of joining A and B, and then joining the result with C. We need enough samples remaining after the first join to be able to do a good enough estimation of the join with C. It turns out that with random sampling without replacement, in order to have n expected result tuples after the first join, we need to sample on the order of  tuples. The more joins we have, the larger the initial samples need to be…. For in-memory databases, the first consideration largely goes away – we can sample reasonable numbers of tuples in a small amount of time (though presumably we always still have some kind of space budget we also have to adhere to). To address the second consideration it would be ideal if the samples we took were somehow more likely to ‘survive’ the joining process, so that a sufficient number of tuples flow through to the next join…  In this work we propose a novel cardinality estimation technique that produces accurate results but is much cheaper than joining random samples. The basic building block is an efficient sampling operator that utilizes existing index structures: to get an estimate for  , we obtain a random sample of  and then look up the samples’ join partners in the index for  (we could also start with  using an index on  ). The resulting sample for  can be used as a starting point for obtaining a sample for  by using an index on the join attribute  and so on. Suppose we have a sample S as a result of sampling some relation T, and for the next step we want to create a sample of  . For each tuple in S we use the index to look up how many matching tuples are expected in A. Summing these gives us the total expected number of matching tuples, and the required number of samples is then drawn from this set. The index-based sampling operator can cheaply compute a sample for a join result, but it is not a full solution by itself. We also need a join enumeration strategy which can systematically explore the intermediate results of a query using the sampling operator, while also ensuring that the overall sampling time is limited. If we sampled every possible combination, it would take too long for queries with many joins. In the Join Order Benchmark (JOB), queries with 7 joins have 84-107 intermediate results, and queries with 13 joins have 1,517-2,032. A time limit is set on the sampling phase, after which the algorithm falls back to traditional estimation. The advantage is that one quickly obtains accurate estimates for large intermediate results. The disadvantage is that many small intermediate results are not sampled and thus have to be estimated using traditional estimation. It is well known that—due to the independence assumption—traditional estimators tend to underestimate result sizes. Therefore, when this mix of (accurate) sampling-based estimates and traditional (under-)estimates are injected into a query optimizer, it will often pick a plan based on the traditional estimates (as they appear to be very cheap). This phenomenon has been called “fleeing from knowledge to ignorance” and—paradoxically—causes additional, accurate information to decrease plan quality. To address this issue, joins are sampled ‘bottom-up’ – i.e., first all 2-way joins are computed, then all 3-way joins, and so on. “A cost-based query optimizer will thus have precise knowledge for the costs of the early (and often crucial) joins.”  Integrating this approach into an existing DBMS is pretty straightforward since you just need to inject the results into the cardinality estimation component of the query optimizer, and no changes to the cost model or plan space enumeration algorithm are necessary. Any query optimizer change that increases the performance for the vast majority of queries, will also decrease performance for some queries, which is very undesirable in production systems. Existing database systems are therefore very conservative with query optimizer changes. Thus, one could use our approach as an optional tuning feature for queries that are slower than expected. In other words, if a user is not satisfied with the performance of a particular query, to get better performance she may turn on index-based sampling only for that query. How well does it work? Evaluation is based on the Join Order Benchmark, based on the Internet Movie Database. There are 113 queries with 3 to 16 joins. As a worst case for planning complexity, indices are created on all primary key and foreign key columns. To compare with traditional cardinality estimates, the test harness supports injection of cardinality estimates from an outside system – in this case obtained from PostgreSQL using the EXPLAIN command. You can see that compared to the PostgreSQL estimates (column 1), index-based sampling with even just 10K samples produces much more accurate estimates. Also note how estimation accuracy decreases with the number of joins. The more samples we have of course, the closer we get to the true cardinalities. Do these more accurate cardinality estimates actually lead to better plans? In figure 6 below you can see the plan quality (log scale) of the plans produced. The cost of each query is normalized by the cost of the optimal plan that would have been chosen if the true cardinalities were known. Using PostgreSQL’s estimates, only around one quarter of the plans are close to the optimum, 42% of the plans are off by a factor of 2 or more, and 12% are off by a factor of 10 or more…  With a budget of 100,000 index lookups, index-based sampling improves performance for many queries – only 17% are off by a factor of 2 or more, and only 3% by a factor of 10 or more. As expected, these better plans lead (mostly!) to faster runtimes:  A small number of plans are actually faster (up to 3x) with inaccurate estimates rather than with the true cardinalities. This effect is caused by cost model errors rather than inaccurate cardinalities and explains the hesitation of many commercial database systems to change their query optimizers.", "pdf_url": "http://cidrdb.org/cidr2017/papers/p9-leis-cidr17.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/cardinality-estimation-done-right-index-based-join-sampling.json"}
{"id": "36197895", "bin": "1400_1500", "summary_sentences": ["Exploring Complex Networks – Strogatz 2001  Network anatomy is important to characterize because structure always affects function…  Written in 2001, this article – recently recommended by Werner Vogels in his ‘ Back-to-Basics ‘ series – explores the topic of complex networks.", "It turns out that the behaviour of individual nodes, and the way that we connect them together, tells us a lot about the emergent properties of the resulting system.", "Non-linear dynamics primer  Let the state of some system be modelled by a vector of state variables, x, with x(t) representing the state at time t. Let v(x) be a vector of functions that encode the dynamics of the system (how it changes over time).", "The behaviour of the resulting dynamical system can be modelled by differential equations dx/dt = v(x).", "As the system evolves x(t) flows through state space, guided by the ‘velocity’ field dx/dt = v(x) like a speck carried along in a steady, viscous fluid…  There are three possibilities:  If x(t) comes to rest at some point, then the velocity there must be zero and we call it a fixed point and it corresponds to an equilibrium state for the system.", "If small disturbances from this point ‘damp out’, then it is known as a stable fixed point.", "x(t) may flow towards a closed loop and eventually circulate around it forever.", "This is known as a limit cycle.", "x(t) might settle onto a strange attractor – a set of states on which it wanders forever, never stopping or repeating.", "If two nearby states flow away from each other exponentially fast we call the system chaotic.", "Networks of dynamical systems  Consider a network in which each node is a dynamical system.", "The long-term behaviour of each node in isolation will be given by stable fixed points, limit cycles, or attractors.", "But what can we say about their collective behaviour when we couple them together?", "If the dynamical system at each node has stable fixed points and no other attractors, the network tends to lock into a static pattern.", "Many such patterns may coexist, especially if the nodes have competing interactions.", "In that case the network may become frustrated and display enormous numbers of locally stable equilibria.", "This kind of complex static behaviour is seen in models of spin glasses, associative memory neural networks and combinatorial optimization problems.", "If each node has a chaotic attractor, then there are few rules.", "But one curious phenomenon exists for networks of identical chaotic systems:  It is known that networks of identical chaotic systems can synchronize their erratic fluctuations… For a wide range of network topologies, synchronized chaos requires that the coupling be neither too weak nor too strong; otherwise spatial instabilities are triggered.", "The intermediate case, in which each node has a stable limit cycle, is more fruitful.", "When nodes are coupled by smooth interactions similar to diffusion then:  Arrays of identical oscillators often synchronize, or else form patterns that depend on the symmetry of the underlying network.", "Other common modes of organization are travelling waves in one spatial dimension, rotating spirals in two dimensions and scroll waves in three dimensions.", "For fully connected networks where each node is coupled equally to all the others, the completely synchronized state becomes likely.", "Instead of smooth interactions though, “Many biological oscillators communicate by sudden impulses: a neuron fires, a firefly flashes, a cricket chirps…” – and many computer networks communicate by sending messages.", "Peskin investigated this scenario.", "If each oscillator is connected to all others, they end up firing in unison.", "It is conjectured that this is also true if the oscillators are not quite identical.", "With weaker coupling than in the fully connected model, Winfree discovered that there is a ‘temporal analogue of a phase transition.’ With a network of weakly coupled, nearly identical limit-cycle oscillators the system behaves incoherently…  As the coupling is increased, the incoherence persists until a certain threshold is crossed – then a small cluster of oscillators suddenly ‘freezes’ into synchrony.", "For still greater coupling, all the oscillators become locked in phase and amplitude.", "This model was further refined by Kuramoto.", "Network architectures  In a simple random graph with n nodes and m links the expected structure of the graph changes as a function of m.  Erdös and Rényi studied how the expected topology of this random graph changes as a function of m. When m is small, the graph is likely to be fragmented into many small clusters of nodes, called components.", "As m increases, the components grow, at first by linking to isolated nodes and later by coalescing with other components.", "A phase transition occurs at m=n/2, where many clusters crosslink spontaneously to form a single giant component.", "For m > n/2, this giant component contains on the order of n nodes (more precisely, its size scales linearly with n, as n → ∞), while its closest rival contains only about log n nodes.", "Furthermore, all nodes in the giant component are connected to each other by short paths: the maximum number of ‘degrees of separation’ between any two nodes grows slowly, like log n  Many real world networks contain a mixture of order and randomness.", "Watts and Strogatz studied this model by starting with a regular lattice and then replacing the original links with random ones with some probability p.  They found that the slightest bit of rewiring transforms the network into a ‘small world,’ with short paths between any two nodes, just as in the giant component of a random graph.", "Yet the network is much more highly clustered than a random graph, in the sense that if A is linked to B and B is linked to C, there is a greatly increased probability that A will also be linked to C.  The short path and high clustering properties also hold for many natural and technological networks.", "Furthermore, they conjectured that dynamical systems coupled in this way would display enhanced signal propagation speed, sychronizability and computational power, as compared with regular lattices of the same size.", "The intuition is that the short paths could provide high-speed communication channels between distant parts of the system, thereby facilitating any dynamical process (like synchronization or computation) that requires global coordination and information flow.", "In real networks, some nodes are more highly connected than others.", "The degree of a node is the number of links that it has.", "Many networks have a degree distribution that is highly skewed and decays as a power law.", "For example, the world-wide web has a small number of nodes with many links, and a long tail of nodes with very few links.", "Albert and Jeong have dubbed these networks ‘scale-free,’ by analogy with fractals, phase transitions, and other situations where power laws arise and no single characteristic scale can be defined.", "An interesting property of such networks is their resistance to random failures (e.g. of nodes on AWS!).", "Albert, Jeong and Barabási suggested that scale-free networks are resistant to random failures because a few hubs dominate their topology Any node that fails probably has small degree (like most nodes) and so is expendable.", "The flip side is that such networks are vulnerable to deliberate attacks on the hubs.", "These intuitive ideas have been confirmed numerically and analytically by examining how the average path length and size of the giant component depend on the number and degree of the nodes removed.", "Let the power law governing degree distribution be pk ~ k-γ.", "Then if γ < 3.47 (which holds for most scale-free networks measured so far) then a giant component will exist.", "A component is a set of connected nodes with a path from each node to every other node.", "If γ < 1 then the network forms one huge connected piece.", "For the world wide web (at least as of 2001) γ is approximately 2.2.", "If this topic interests you, I can also recommend Matthew Jackson’s book on Social and Economic Networks from 2010."], "summary_text": "Exploring Complex Networks – Strogatz 2001  Network anatomy is important to characterize because structure always affects function…  Written in 2001, this article – recently recommended by Werner Vogels in his ‘ Back-to-Basics ‘ series – explores the topic of complex networks. It turns out that the behaviour of individual nodes, and the way that we connect them together, tells us a lot about the emergent properties of the resulting system. Non-linear dynamics primer  Let the state of some system be modelled by a vector of state variables, x, with x(t) representing the state at time t. Let v(x) be a vector of functions that encode the dynamics of the system (how it changes over time). The behaviour of the resulting dynamical system can be modelled by differential equations dx/dt = v(x). As the system evolves x(t) flows through state space, guided by the ‘velocity’ field dx/dt = v(x) like a speck carried along in a steady, viscous fluid…  There are three possibilities:  If x(t) comes to rest at some point, then the velocity there must be zero and we call it a fixed point and it corresponds to an equilibrium state for the system. If small disturbances from this point ‘damp out’, then it is known as a stable fixed point. x(t) may flow towards a closed loop and eventually circulate around it forever. This is known as a limit cycle. x(t) might settle onto a strange attractor – a set of states on which it wanders forever, never stopping or repeating. If two nearby states flow away from each other exponentially fast we call the system chaotic. Networks of dynamical systems  Consider a network in which each node is a dynamical system. The long-term behaviour of each node in isolation will be given by stable fixed points, limit cycles, or attractors. But what can we say about their collective behaviour when we couple them together? If the dynamical system at each node has stable fixed points and no other attractors, the network tends to lock into a static pattern. Many such patterns may coexist, especially if the nodes have competing interactions. In that case the network may become frustrated and display enormous numbers of locally stable equilibria. This kind of complex static behaviour is seen in models of spin glasses, associative memory neural networks and combinatorial optimization problems. If each node has a chaotic attractor, then there are few rules. But one curious phenomenon exists for networks of identical chaotic systems:  It is known that networks of identical chaotic systems can synchronize their erratic fluctuations… For a wide range of network topologies, synchronized chaos requires that the coupling be neither too weak nor too strong; otherwise spatial instabilities are triggered. The intermediate case, in which each node has a stable limit cycle, is more fruitful. When nodes are coupled by smooth interactions similar to diffusion then:  Arrays of identical oscillators often synchronize, or else form patterns that depend on the symmetry of the underlying network. Other common modes of organization are travelling waves in one spatial dimension, rotating spirals in two dimensions and scroll waves in three dimensions. For fully connected networks where each node is coupled equally to all the others, the completely synchronized state becomes likely. Instead of smooth interactions though, “Many biological oscillators communicate by sudden impulses: a neuron fires, a firefly flashes, a cricket chirps…” – and many computer networks communicate by sending messages. Peskin investigated this scenario. If each oscillator is connected to all others, they end up firing in unison. It is conjectured that this is also true if the oscillators are not quite identical. With weaker coupling than in the fully connected model, Winfree discovered that there is a ‘temporal analogue of a phase transition.’ With a network of weakly coupled, nearly identical limit-cycle oscillators the system behaves incoherently…  As the coupling is increased, the incoherence persists until a certain threshold is crossed – then a small cluster of oscillators suddenly ‘freezes’ into synchrony. For still greater coupling, all the oscillators become locked in phase and amplitude. This model was further refined by Kuramoto. Network architectures  In a simple random graph with n nodes and m links the expected structure of the graph changes as a function of m.  Erdös and Rényi studied how the expected topology of this random graph changes as a function of m. When m is small, the graph is likely to be fragmented into many small clusters of nodes, called components. As m increases, the components grow, at first by linking to isolated nodes and later by coalescing with other components. A phase transition occurs at m=n/2, where many clusters crosslink spontaneously to form a single giant component. For m > n/2, this giant component contains on the order of n nodes (more precisely, its size scales linearly with n, as n → ∞), while its closest rival contains only about log n nodes. Furthermore, all nodes in the giant component are connected to each other by short paths: the maximum number of ‘degrees of separation’ between any two nodes grows slowly, like log n  Many real world networks contain a mixture of order and randomness. Watts and Strogatz studied this model by starting with a regular lattice and then replacing the original links with random ones with some probability p.  They found that the slightest bit of rewiring transforms the network into a ‘small world,’ with short paths between any two nodes, just as in the giant component of a random graph. Yet the network is much more highly clustered than a random graph, in the sense that if A is linked to B and B is linked to C, there is a greatly increased probability that A will also be linked to C.  The short path and high clustering properties also hold for many natural and technological networks. Furthermore, they conjectured that dynamical systems coupled in this way would display enhanced signal propagation speed, sychronizability and computational power, as compared with regular lattices of the same size. The intuition is that the short paths could provide high-speed communication channels between distant parts of the system, thereby facilitating any dynamical process (like synchronization or computation) that requires global coordination and information flow. In real networks, some nodes are more highly connected than others. The degree of a node is the number of links that it has. Many networks have a degree distribution that is highly skewed and decays as a power law. For example, the world-wide web has a small number of nodes with many links, and a long tail of nodes with very few links. Albert and Jeong have dubbed these networks ‘scale-free,’ by analogy with fractals, phase transitions, and other situations where power laws arise and no single characteristic scale can be defined. An interesting property of such networks is their resistance to random failures (e.g. of nodes on AWS!). Albert, Jeong and Barabási suggested that scale-free networks are resistant to random failures because a few hubs dominate their topology Any node that fails probably has small degree (like most nodes) and so is expendable. The flip side is that such networks are vulnerable to deliberate attacks on the hubs. These intuitive ideas have been confirmed numerically and analytically by examining how the average path length and size of the giant component depend on the number and degree of the nodes removed. Let the power law governing degree distribution be pk ~ k-γ. Then if γ < 3.47 (which holds for most scale-free networks measured so far) then a giant component will exist. A component is a set of connected nodes with a path from each node to every other node. If γ < 1 then the network forms one huge connected piece. For the world wide web (at least as of 2001) γ is approximately 2.2. If this topic interests you, I can also recommend Matthew Jackson’s book on Social and Economic Networks from 2010.", "pdf_url": "http://www.nature.com/nature/journal/v410/n6825/pdf/410268a0.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/exploring-complex-networks.json"}
{"id": "46507565", "bin": "1400_1500", "summary_sentences": ["Slim: OS kernel support for a low-overhead container overlay network Zhuo et al., NSDI’19  Container overlay networks rely on packet transformations, with each packet traversing the networking stack twice on its way from the sending container to the receiving container.", "There are CPU, throughput, and latency overheads associated with those traversals.", "In this paper, we ask whether we can design and implement a container overlay network, where packets go through the OS kernel’s network stack only once.", "This requires us to remove packet transformation from the overlay network’s data-plane.", "Instead, we implement network virtualization by manipulating connection-level metadata at connection setup time, saving CPU cycles and reducing packet latency.", "Slim comes with some caveats: it requires a kernel module for secure deployment, has longer connection establishment times, doesn’t fit with packet-based network policies, and only handles TCP traffic.", "For UDP, ICMP, and for its own service discovery, it also relies on an existing container overlay network ( Weave Net ).", "But for longer lasting connections managed using connection-based network policies it delivers some impressive results:  memcached throughput up by 71%, with latency reduced by 42%, and CPU utilisation reduced by 56%.", "Nginx CPU utilisation reduced by 22-24%  PostgreSQL CPU utilisation reduced by 22%  Apache Kafka CPU utilisation reduced by 10%  Since Slim both builds on and compares to Weave Net, I should say at this point that Weave Net was the very first open source project from Weaveworks , the “GitOps for Kubernetes” company.", "Accel is an investor in Weaveworks, and I am also a personal investor.", "If you’re using Kubernetes, you should definitely check them out.", "Anyway, on with the show…  Container networking  In theory there are four possible modes for container networking: a bridge mode for containers on the same host; host mode in which containers use the IP address of their host network interface; macvlan mode (or similar hardware mechanisms) to give each container its own IP address; and overlay mode in which each container is given its own own virtual network interface and each application has its own network namespace.", "In practice, there are management and deployment challenges with the bridge, host, and macvlan approaches, so overlay networks such as Weave Net are the preferred solution.", "Overlay packets are encapsulated with host network headers when routed on the host network.", "This lets the container overlay network have its own IP address space and network configuration that is disjoint from that of the host network; each can be managed completely independently.", "Many container overlay network solutions are available today— such as Weave, Flannel, and Docker Overlay— all of which share similar internal architectures.", "Overlay network overheads  The overheads in overlay networking come from the per-packet processing inside the OS kernel: delivering a packet on the overlay network requires one extra traversal of the network stack and also packet encapsulation and decapsulation.", "Here are some test measurements comparing Weave Net in fast dataplane mode to host networking to give an example:  In this test, compared to direct host networking, for two containers on the same host (Intra) the overlay network reduces throughput by 23% and increases latency by 34%.", "For containers communicating across hosts (Inter), throughput reduces by 48% and latency increases by 85%.", "The overheads are lower when communicating on the same host since packet encapsulation is not required.", "Compared to host networking, the CPU utilisation also increases by 93%.", "There are several known techniques to reduce the data plane overhead.", "Packet steering creates multiple queues, each per CPU core, for a network interface and uses consistent hashing to map packets to different queues.", "In this way, packets in the same network connection are processed only on a single core.", "Different cores therefore do not have access to the same queue, removing the overhead due to multi-core synchronization.", "The authors integrated the above Receive Packet Steering (RPS), and also an enhancement called Receive Flow Steering (RFS— which further ensures that interrupt processing occurs on the same core as the application— into Weave Net.", "With this enhancement, throughput is within 9% of that achieved with host networking, but it makes almost no difference to latency.", "Introducing Slim  The big idea in Slim is to reduce CPU utilisation and latency overheads by having packets traverse the network stack only once.", "That means you can’t do per-packet processing.", "Instead, Slim works at the connection level.", "Slim virtualizes the network by manipulating connection-level metadata.", "SlimSocket exposes the POSIX socket interface to application binaries to intercept invocations of socket-related system calls.", "When SlimSocket detects an application is trying to set up a connection, it sends a request to SlimRouter.", "After SlimRouter sets up the network connection, it passes access to the connection as a ﬁle descriptor to the process inside the container.", "The application inside the container then uses the host namespace ﬁle descriptor to send/receive packets directly to/from the host network.", "Because SlimSocket has the exact same interface as the POSIX socket, and Slim dynamically links SlimSocket into the application, the application binary need not be modiﬁed.", "Given that Slim is out of the picture once the connection is established, a separate mechanism is needed to support control plane and data plane policies.", "SlimRouter stores control plane policies and enforces them at connection setup time.", "If the policy changes, SlimRouter scans existing connections and removes the file descriptors for any connection violating the new policy.", "This requires the support of a kernel module, SlimKernModule.", "To avoid containers learning the IP addresses of host machines, SlimKernModule (in secure mode) also prohibits unsafe system calls on file descriptors (e.g. getpeername).", "Existing kernel mechanisms are used to enforce data plane policies.", "This is what it looks like when Slim is used with blocking I/O:  Calls to the POSIX socket interface are intercepted by the SlimSocket shim and forward to the SlimRouter.", "For non-blocking I/O (e.g., select, epoll) Slim also intercepts these API calls and maintains mappings for epoll file descriptor sets.", "The SlimRouter needs to know the IP address and port mappings in order to establish connections.", "It does this using a Weave Net overlay network!", "When the client calls connect, it actually creates an overlay network connection on the standard container overlay network.", "When the server receives an incoming connection on the standard overlay network, SlimSocket queries SlimRouter for the physical IP address and port and sends them to the client side inside the overlay connection.", "In secure mode (§4.3), the result queried from SlimRouter is encrypted.", "SlimSocket on the client side sends the physical IP address and port (encrypted if in secure mode) to its SlimRouter and the SlimRouter establishes the host connection.", "This means connection setup time is longer in Slim than that on container overlay networks based on packet transformation.", "Weave Net is also used for packets that require data plane handling such as ICMP and UDP.", "Evaluation  Microbenchmarks compare Slim to Weave Net with RFS.", "Creating on a TCP connection with Weave Net takes 270 µs.", "With Slim it takes 556µs (440µs in insecure mode).", "For applications with persistent connections, this additional overhead will not be significant.", "Compared to Weave Net, Slim reduces CPU overhead by 22-41%.", "Slim and Weave Net are then further compared on four application workloads based on Memcached, Nginx, PostgreSQL, and Apache Kafka respectively.", "For Memcached, Slim improves throughput by 71% and reduces latency by 42%, with 25% lower CPU utilisation.", "For Nginx, PostgreSQL the main advantage of Slim is reduced CPU utilisation (around 22% reduction).", "For Kafka the CPU utilisation reduction is around 10%, but latency is also reduced by 28%.", "Slim’s source code is available at  [url]"], "summary_text": "Slim: OS kernel support for a low-overhead container overlay network Zhuo et al., NSDI’19  Container overlay networks rely on packet transformations, with each packet traversing the networking stack twice on its way from the sending container to the receiving container. There are CPU, throughput, and latency overheads associated with those traversals. In this paper, we ask whether we can design and implement a container overlay network, where packets go through the OS kernel’s network stack only once. This requires us to remove packet transformation from the overlay network’s data-plane. Instead, we implement network virtualization by manipulating connection-level metadata at connection setup time, saving CPU cycles and reducing packet latency. Slim comes with some caveats: it requires a kernel module for secure deployment, has longer connection establishment times, doesn’t fit with packet-based network policies, and only handles TCP traffic. For UDP, ICMP, and for its own service discovery, it also relies on an existing container overlay network ( Weave Net ). But for longer lasting connections managed using connection-based network policies it delivers some impressive results:  memcached throughput up by 71%, with latency reduced by 42%, and CPU utilisation reduced by 56%. Nginx CPU utilisation reduced by 22-24%  PostgreSQL CPU utilisation reduced by 22%  Apache Kafka CPU utilisation reduced by 10%  Since Slim both builds on and compares to Weave Net, I should say at this point that Weave Net was the very first open source project from Weaveworks , the “GitOps for Kubernetes” company. Accel is an investor in Weaveworks, and I am also a personal investor. If you’re using Kubernetes, you should definitely check them out. Anyway, on with the show…  Container networking  In theory there are four possible modes for container networking: a bridge mode for containers on the same host; host mode in which containers use the IP address of their host network interface; macvlan mode (or similar hardware mechanisms) to give each container its own IP address; and overlay mode in which each container is given its own own virtual network interface and each application has its own network namespace. In practice, there are management and deployment challenges with the bridge, host, and macvlan approaches, so overlay networks such as Weave Net are the preferred solution. Overlay packets are encapsulated with host network headers when routed on the host network. This lets the container overlay network have its own IP address space and network configuration that is disjoint from that of the host network; each can be managed completely independently. Many container overlay network solutions are available today— such as Weave, Flannel, and Docker Overlay— all of which share similar internal architectures. Overlay network overheads  The overheads in overlay networking come from the per-packet processing inside the OS kernel: delivering a packet on the overlay network requires one extra traversal of the network stack and also packet encapsulation and decapsulation. Here are some test measurements comparing Weave Net in fast dataplane mode to host networking to give an example:  In this test, compared to direct host networking, for two containers on the same host (Intra) the overlay network reduces throughput by 23% and increases latency by 34%. For containers communicating across hosts (Inter), throughput reduces by 48% and latency increases by 85%. The overheads are lower when communicating on the same host since packet encapsulation is not required. Compared to host networking, the CPU utilisation also increases by 93%. There are several known techniques to reduce the data plane overhead. Packet steering creates multiple queues, each per CPU core, for a network interface and uses consistent hashing to map packets to different queues. In this way, packets in the same network connection are processed only on a single core. Different cores therefore do not have access to the same queue, removing the overhead due to multi-core synchronization. The authors integrated the above Receive Packet Steering (RPS), and also an enhancement called Receive Flow Steering (RFS— which further ensures that interrupt processing occurs on the same core as the application— into Weave Net. With this enhancement, throughput is within 9% of that achieved with host networking, but it makes almost no difference to latency. Introducing Slim  The big idea in Slim is to reduce CPU utilisation and latency overheads by having packets traverse the network stack only once. That means you can’t do per-packet processing. Instead, Slim works at the connection level. Slim virtualizes the network by manipulating connection-level metadata. SlimSocket exposes the POSIX socket interface to application binaries to intercept invocations of socket-related system calls. When SlimSocket detects an application is trying to set up a connection, it sends a request to SlimRouter. After SlimRouter sets up the network connection, it passes access to the connection as a ﬁle descriptor to the process inside the container. The application inside the container then uses the host namespace ﬁle descriptor to send/receive packets directly to/from the host network. Because SlimSocket has the exact same interface as the POSIX socket, and Slim dynamically links SlimSocket into the application, the application binary need not be modiﬁed. Given that Slim is out of the picture once the connection is established, a separate mechanism is needed to support control plane and data plane policies. SlimRouter stores control plane policies and enforces them at connection setup time. If the policy changes, SlimRouter scans existing connections and removes the file descriptors for any connection violating the new policy. This requires the support of a kernel module, SlimKernModule. To avoid containers learning the IP addresses of host machines, SlimKernModule (in secure mode) also prohibits unsafe system calls on file descriptors (e.g. getpeername). Existing kernel mechanisms are used to enforce data plane policies. This is what it looks like when Slim is used with blocking I/O:  Calls to the POSIX socket interface are intercepted by the SlimSocket shim and forward to the SlimRouter. For non-blocking I/O (e.g., select, epoll) Slim also intercepts these API calls and maintains mappings for epoll file descriptor sets. The SlimRouter needs to know the IP address and port mappings in order to establish connections. It does this using a Weave Net overlay network! When the client calls connect, it actually creates an overlay network connection on the standard container overlay network. When the server receives an incoming connection on the standard overlay network, SlimSocket queries SlimRouter for the physical IP address and port and sends them to the client side inside the overlay connection. In secure mode (§4.3), the result queried from SlimRouter is encrypted. SlimSocket on the client side sends the physical IP address and port (encrypted if in secure mode) to its SlimRouter and the SlimRouter establishes the host connection. This means connection setup time is longer in Slim than that on container overlay networks based on packet transformation. Weave Net is also used for packets that require data plane handling such as ICMP and UDP. Evaluation  Microbenchmarks compare Slim to Weave Net with RFS. Creating on a TCP connection with Weave Net takes 270 µs. With Slim it takes 556µs (440µs in insecure mode). For applications with persistent connections, this additional overhead will not be significant. Compared to Weave Net, Slim reduces CPU overhead by 22-41%. Slim and Weave Net are then further compared on four application workloads based on Memcached, Nginx, PostgreSQL, and Apache Kafka respectively. For Memcached, Slim improves throughput by 71% and reduces latency by 42%, with 25% lower CPU utilisation. For Nginx, PostgreSQL the main advantage of Slim is reduced CPU utilisation (around 22% reduction). For Kafka the CPU utilisation reduction is around 10%, but latency is also reduced by 28%. Slim’s source code is available at  [url]", "pdf_url": "https://www.usenix.org/system/files/nsdi19-zhuo.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/slim-os-kernel-support-for-a-low-overhead-container-overlay-network.json"}
{"id": "17349212", "bin": "1400_1500", "summary_sentences": ["Formal Requirements for Virtualizable Third Generation Architectures – Popek & Goldberg 1974.", "With thanks to Alfred Bratterud for pointing me at this paper.", "What exactly is a virtual machine?", "What does a virtual machine monitor do?", "And how do we now whether a given piece of hardware can support virtualization or not?", "In today’s paper choice, Popek and Goldberg set out the answers for us – and by the way, they had all this figured out way back in 1974!", "What is a Virtual Machine?", "There are currently (1974) a number of viewpoints suggesting what a virtual machine is, how it ought to be constructed, and what hardware and operating system implications result…  Here’s a very simple definition of a virtual machine:  A virtual machine is taken to be an efficient, isolated duplicate of the real machine.", "Though of course we need to dig further and understand what is implied by the three words efficient, isolated, and duplicate.", "To explain these, the authors introduce the notion of a virtual machine monitor…  What is a Virtual Machine Monitor?", "A virtual machine monitor (VMM) does three things:  It provides a duplicate, or essentially identical to the original machine, environment for programs.", "“Any program run under the VMM should exhibit an effect  identical with that demonstrated if the program had been run  on the original machine directly, with the possible exception of differences caused by the availability of system resources and differences caused by timing dependencies.”  It does so efficiently, requiring “a statistically dominant subset of the virtual processor’s instructions be executed directly by the real processor, with no software intervention by the VMM.", "This statement rules out traditional emulators and complete software interpreters (simulators) from the virtual machine umbrella.” Thus programs that run in this environment show only minor decreases in speed.", "It is in complete control of system resources (memory, peripherals, and the like).", "This requires two conditions: (i) it must not be possible for a program running in the created environment to access any resource not allocated to it (isolation), and (ii) it is possible under certain circumstances to regain control of resources already allocated.", "A virtual machine is the environment created by the virtual machine monitor.", "Does my Hardware Support Virtualization?", "This is the question the vast majority of the paper is dedicated to.", "Before we can get to the answer (which is actually a very simple and easy test), we need to understand what the authors mean by a ‘third generation architecture’ (per the paper title).", "Examples of a third generation architecture machine are the IBM 360, Honeywell 6000, or DEC PDP-10.", "Such machines have a processor, and linear uniformly addressable memory.", "The processor can operate in supervisor mode, or in user mode.", "Some instructions are only available in supervisor mode.", "Memory addressing is done relative to a relocation register R=(l,b) which is always active.", "The location parameter l gives the absolute address that corresponds to the apparent address zero, and the bounds parameter b gives the absolute size of the virtual memory.", "Suppose an instruction produces some address a, we check and then find the true address as follows:  if  a + l >= total-real-memory-size then       // out of real memory bounds     memorytrap  else if a >= b then     // out of virtual memory bounds    memorytrap else     use address a+l  The program status word PSW is a triplet (mode – user/supervisor, program counter, relocation register), and the overall state S of the machine can be modeled as (E,PSW) where E is the executable storage.", "E[0] and E[1] are used to store an old-PSW and fetch a new PSW respectively.", "Instructions  are simply modeled as a function from State -> State.", "In this model, for simplicity, we have departed slightly from most common relocation systems by assuming it to be active in the supervisor as well as user mode.", "This difference will not be important to the proof of our result.", "Note also that all references made by the processor to memory are assumed to be relocated.", "A trap, such as the memorytrap above, automatically saves the current state of the machine and passes control to a pre-specified control routine by changing the PSW to the values specified in E[1].", "Key to understand whether or not it is possible to virtualize a given piece of hardware is to divide the instructions into groups.", "In particular, privileged instructions are those that do not trap when the processor is in supervisor mode, but do trap (a privileged instruction trap) when in user mode.", "Privileged instructions are independent of the virtualization process.", "They are merely characteristics of the machine which may be determined from reading the principles of operation.", "Note, however, that the way we have defined privileged instructions requires them to trap.", "Merely NOPing the instruction without trapping is insufficient.", "The latter case should not be called a privileged instruction; maybe “user mode NOP” would be more accurate.", "Sensitive instructions may be either control sensitive, or behaviour sensitive.", "Control sensitive instructions are those that affect or can affect control over system resources – in our simplified model the only such resource is memory.", "A control sensitive instruction attempts to change the amount of resource (memory) available, or change processor mode, without going through a memory trap.", "A behaviour sensitive instruction is one whereby the effect of its execution is dependent on the value of the relocation bounds register (location in real memory) or processor mode.", "An instruction that is not sensitive is innocuous.", "A virtual machine monitor is a control program comprising a dispatcher, an allocator, and a set of interpreters, one per privileged instruction.", "The location of the control program (dispatcher) is placed in the program counter at E[1], it directs execution to the allocator or interpreters as needed.", "The allocator decides what system resources are to be provided (e.g. keeping the VMM and VM memory separate).", "The allocator will be invoked by the dispatcher whenever an attempted execution of a privileged instruction in a virtual machine environment occurs which would have the effect of changing the machine resources associated with that environment.", "Attempting to reset the R (relocation-bounds) register is the primary example in our skeletal model.", "If the processor were to be treated as a resource, a halt would be another.", "The job of the interpreters is to simulate the instruction that trapped.", "A virtual machine monitor [that satisfies the three properties of efficiency, resource control, and equivalence] may be constructed if the set of sensitive instructions for that computer is a subset of the privileged instructions.", "The proof of this statement is given in the paper and the appendices – it rests on showing a one-one homomorphism f between real machine states and virtual machine states, and that if the real machine halts in state S, then the virtual machine halts in state f(S).", "The final step is an existence argument (i.e. there is at least one way) to build the privileged instruction interpreters – using a lookup table.", "Furthermore, recursive virtualization (a VM that runs a copy of itself under the VMM) is possible if (a) a VMM can be constructed for the hardware as above, and (b) the VMM does not have any timing dependencies.", "The theorem provides a fairly simple condition sufficient to guarantee virtualizability, assuming, of course, that the requisite features of “conventional third generation machines” are present.", "However, those features which have been assumed are fairly standard ones, so the relationship between the sets of sensitive and privileged instructions is the only constraint.", "It is a very modest one, easy to check.", "Further, it is also a simple matter for hardware designers to use as a design requirement."], "summary_text": "Formal Requirements for Virtualizable Third Generation Architectures – Popek & Goldberg 1974. With thanks to Alfred Bratterud for pointing me at this paper. What exactly is a virtual machine? What does a virtual machine monitor do? And how do we now whether a given piece of hardware can support virtualization or not? In today’s paper choice, Popek and Goldberg set out the answers for us – and by the way, they had all this figured out way back in 1974! What is a Virtual Machine? There are currently (1974) a number of viewpoints suggesting what a virtual machine is, how it ought to be constructed, and what hardware and operating system implications result…  Here’s a very simple definition of a virtual machine:  A virtual machine is taken to be an efficient, isolated duplicate of the real machine. Though of course we need to dig further and understand what is implied by the three words efficient, isolated, and duplicate. To explain these, the authors introduce the notion of a virtual machine monitor…  What is a Virtual Machine Monitor? A virtual machine monitor (VMM) does three things:  It provides a duplicate, or essentially identical to the original machine, environment for programs. “Any program run under the VMM should exhibit an effect  identical with that demonstrated if the program had been run  on the original machine directly, with the possible exception of differences caused by the availability of system resources and differences caused by timing dependencies.”  It does so efficiently, requiring “a statistically dominant subset of the virtual processor’s instructions be executed directly by the real processor, with no software intervention by the VMM. This statement rules out traditional emulators and complete software interpreters (simulators) from the virtual machine umbrella.” Thus programs that run in this environment show only minor decreases in speed. It is in complete control of system resources (memory, peripherals, and the like). This requires two conditions: (i) it must not be possible for a program running in the created environment to access any resource not allocated to it (isolation), and (ii) it is possible under certain circumstances to regain control of resources already allocated. A virtual machine is the environment created by the virtual machine monitor. Does my Hardware Support Virtualization? This is the question the vast majority of the paper is dedicated to. Before we can get to the answer (which is actually a very simple and easy test), we need to understand what the authors mean by a ‘third generation architecture’ (per the paper title). Examples of a third generation architecture machine are the IBM 360, Honeywell 6000, or DEC PDP-10. Such machines have a processor, and linear uniformly addressable memory. The processor can operate in supervisor mode, or in user mode. Some instructions are only available in supervisor mode. Memory addressing is done relative to a relocation register R=(l,b) which is always active. The location parameter l gives the absolute address that corresponds to the apparent address zero, and the bounds parameter b gives the absolute size of the virtual memory. Suppose an instruction produces some address a, we check and then find the true address as follows:  if  a + l >= total-real-memory-size then       // out of real memory bounds     memorytrap  else if a >= b then     // out of virtual memory bounds    memorytrap else     use address a+l  The program status word PSW is a triplet (mode – user/supervisor, program counter, relocation register), and the overall state S of the machine can be modeled as (E,PSW) where E is the executable storage. E[0] and E[1] are used to store an old-PSW and fetch a new PSW respectively. Instructions  are simply modeled as a function from State -> State. In this model, for simplicity, we have departed slightly from most common relocation systems by assuming it to be active in the supervisor as well as user mode. This difference will not be important to the proof of our result. Note also that all references made by the processor to memory are assumed to be relocated. A trap, such as the memorytrap above, automatically saves the current state of the machine and passes control to a pre-specified control routine by changing the PSW to the values specified in E[1]. Key to understand whether or not it is possible to virtualize a given piece of hardware is to divide the instructions into groups. In particular, privileged instructions are those that do not trap when the processor is in supervisor mode, but do trap (a privileged instruction trap) when in user mode. Privileged instructions are independent of the virtualization process. They are merely characteristics of the machine which may be determined from reading the principles of operation. Note, however, that the way we have defined privileged instructions requires them to trap. Merely NOPing the instruction without trapping is insufficient. The latter case should not be called a privileged instruction; maybe “user mode NOP” would be more accurate. Sensitive instructions may be either control sensitive, or behaviour sensitive. Control sensitive instructions are those that affect or can affect control over system resources – in our simplified model the only such resource is memory. A control sensitive instruction attempts to change the amount of resource (memory) available, or change processor mode, without going through a memory trap. A behaviour sensitive instruction is one whereby the effect of its execution is dependent on the value of the relocation bounds register (location in real memory) or processor mode. An instruction that is not sensitive is innocuous. A virtual machine monitor is a control program comprising a dispatcher, an allocator, and a set of interpreters, one per privileged instruction. The location of the control program (dispatcher) is placed in the program counter at E[1], it directs execution to the allocator or interpreters as needed. The allocator decides what system resources are to be provided (e.g. keeping the VMM and VM memory separate). The allocator will be invoked by the dispatcher whenever an attempted execution of a privileged instruction in a virtual machine environment occurs which would have the effect of changing the machine resources associated with that environment. Attempting to reset the R (relocation-bounds) register is the primary example in our skeletal model. If the processor were to be treated as a resource, a halt would be another. The job of the interpreters is to simulate the instruction that trapped. A virtual machine monitor [that satisfies the three properties of efficiency, resource control, and equivalence] may be constructed if the set of sensitive instructions for that computer is a subset of the privileged instructions. The proof of this statement is given in the paper and the appendices – it rests on showing a one-one homomorphism f between real machine states and virtual machine states, and that if the real machine halts in state S, then the virtual machine halts in state f(S). The final step is an existence argument (i.e. there is at least one way) to build the privileged instruction interpreters – using a lookup table. Furthermore, recursive virtualization (a VM that runs a copy of itself under the VMM) is possible if (a) a VMM can be constructed for the hardware as above, and (b) the VMM does not have any timing dependencies. The theorem provides a fairly simple condition sufficient to guarantee virtualizability, assuming, of course, that the requisite features of “conventional third generation machines” are present. However, those features which have been assumed are fairly standard ones, so the relationship between the sets of sensitive and privileged instructions is the only constraint. It is a very modest one, easy to check. Further, it is also a simple matter for hardware designers to use as a design requirement.", "pdf_url": "https://www.princeton.edu/~rblee/ELE572Papers/Fall04Readings/secureOS/popek_virtualizable.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/formal-requirements-for-virtualizable-third-generation-architectures.json"}
{"id": "75962230", "bin": "1400_1500", "summary_sentences": ["The paradigms of programming Floyd, CACM 1979  (Also available in  )  A couple of weeks ago we looked at Dan Bernstein’s very topical “ thoughts on security after ten years of qmail 1.0 .” From the general reaction I can tell that lots of you enjoyed reading that paper, but in the discussions that I saw, no-one was picking up on what I see as the real underlying secret to Bernstein’s success and progression as a software engineer.", "(Perhaps because it is one level of indirection away from the main topic of security in that paper).", "Here is my favourite extract again:  For many years I have been systematically identifying error-prone programming habits — by reviewing the literature, analyzing other people’s mistakes, and analyzing my own mistakes — and redesigning my programming environment to eliminate those habits.", "In today’s paper choice we’ll be looking at some other ways of systematically improving your skills over time (along with quite a few other gems).", "In 1978 Professor Robert Floyd was presented with the ACM Turing Award for “helping to found the following important subfields of computer science: the theory of parsing, the semantics of programming languages, automatic program verification, automatic program synthesis, and analysis of algorithms.” Not a bad list!", "“The paradigms of programming” is his acceptance speech.", "Today I want to talk about the paradigms of programming, how they affect our success as designers of computer programs, how they should be taught, and how they should be embodied in our programming languages.", "Dominant at the time was the idea of structured programming (whose ideas are still very much with us today of course).", "The notion of starting with a top-down, stepwise refinement of the problem, and then building upwards from the primitives of the underlying machine to ‘meet in the middle’ with a set of more abstract modules and functions to be used by the top-down design.", "See e.g. ‘ Program development by stepwise refinement ’, and ‘ On the criteria to be used in decomposing systems into modules ’.", "The structured programming paradigm is useful, says Floyd, but it’s not the only one.", "Programming paradigms are at the heart of this paper – and a reasonable interpretation of what Floyd means by paradigm here is, I think, ‘a strategy or tactic for solving a class of problems.’ That sounds a bit like a design pattern when I say it that way, but the examples Floyd gives us are at a slightly more fundamental level than those the phrase ‘design patterns’ conjures in my mind.", "Far more powerful than how many languages you know (in terms of syntax), is how many paradigms you are fluent with.", "I believe that the current state of the art of computer programming reflects inadequacies in our stock of paradigms, and in the way our programming languages support, or fail to support, the paradigms of their user communities.", "Computer science quickly breaks down into communities each with its own languages and dominant paradigms.", "The problem of falling into one of these and not escaping is that it becomes hard to see the fundamentals afresh and discover new approaches.", "Quoting from Kuhn in ‘The Structure of Scientific Revolutions’ :  The study of paradigms, including many that are far more specialized than those named illustratively above, is what mainly prepares the student for membership in the particular scientific community with which he will later practice.", "Because he there joins men who learned the bases of their field from the same concrete models, his subsequent practice will seldom evoke overt disagreement over fundamentals.", "John Cocke invented the dynamic programming paradigm to solve a problem with the efficient parsing of context-free languages.", "Floyd discovered recursive coroutines as a structure while building hierarchical top-down parsers.", "John Cocke’s experience and mine illustrate the likelihood that continued advance in programming will require the continuing invention, elaboration, and communication of new paradigms.", "On developing as a programmer  So much for the advancement of the field, what about developing your own skills?", "If the advancement of the general art of programming requires the continuing invention and elaboration of paradigms, advancement of the art of the individual programming requires that he expand his repertory of paradigms.", "Here’s the technique that Floyd used to expand his own capabilities.", "After solving a challenging problem, I solve it again from scratch, retracing only the insight of the earlier solution.", "I repeat this until the solution is as clean and direct as I can hope for.", "Then I look for a general rule for attacking similar problems, that would have led me to approach the given problem in the most efficient way the first time.", "Often, such a rule is of permanent value.", "It can be hard to gain exposure to new paradigms from within your own immediate environment, because it’s likely your colleagues are all working within the same local paradigm set — witness job advertisements that specify the desired programming language (“The rules of Fortran can be learned within a few hours; the associated paradigms take much longer,  both to learn and unlearn.”).", "Floyd writes of an eye-opening experience of visiting MIT and seeing the power of Lisp first-hand (as someone grown up more in the tradition of Algol-like languages).", "… my message to the serious programmer is to spend a part of your working day examining and refining your own methods.", "Even though programmers are always struggling to meet some future or past deadline, methodological abstraction is a wise long term investment.", "On designing (and evaluating) programming languages  Everyone wants to design a new programming language.", "Bah!", "Floyd doesn’t find much satisfaction in the incremental extensions to existing languages (example: adding variant records to Pascal).", "Instead, it’s far more important to look at the paradigms a language supports.", "I believe that the continued advance of programming as a craft requires the development and dissemination of languages which support the major paradigms of their user’s communities.", "The design of a language should be preceded by enumeration of those paradigms, including a study of the deficiencies in programming caused by discouragement of unsupported paradigms… If there is ever a science of programming language design, it will probably consist largely of matching languages to the design methods they support.", "It’s not just the programming language itself of course, “the entire environment in which we program, diagnostic systems, files systems, editors, and all, can be analyzed as supporting or failing to support the spectrum of methods for design of programs.”  To persuade me of the merit of your language, you must show me how to construct programs in it.", "I don’t want to discourage the design of new languages; I want to encourage the language designer to become a serious student of the details of the design process.", "On teaching programming  We have an unfortunate obsession with form over content (Floyd is speaking in 1978 remember, not a lot has changed in the intervening 40 years!).", "You can feel Floyd’s heart sink in the following exchange:  If I ask another professor what he teaches in the introductory programming course, whether he answers proudly “Pascal” or diffidently “FORTRAN,” I know that he is teaching a grammar, a set of semantic rules, and some finished algorithms, leaving the students to discover, on their own, some process of design.", "We would do better to explicitly teach a set of systematic methods for all levels of program design.", "Students trained this way “have a large head start over those conventionally taught.”  To the teacher of programming… I say: identify the paradigms you use, as fully as you can, then teach them explicitly.", "They will serve your students when Fortran has replaced Latin and Sanskrit as the archetypal dead language.", "How many paradigms do you have in your toolbox?"], "summary_text": "The paradigms of programming Floyd, CACM 1979  (Also available in  )  A couple of weeks ago we looked at Dan Bernstein’s very topical “ thoughts on security after ten years of qmail 1.0 .” From the general reaction I can tell that lots of you enjoyed reading that paper, but in the discussions that I saw, no-one was picking up on what I see as the real underlying secret to Bernstein’s success and progression as a software engineer. (Perhaps because it is one level of indirection away from the main topic of security in that paper). Here is my favourite extract again:  For many years I have been systematically identifying error-prone programming habits — by reviewing the literature, analyzing other people’s mistakes, and analyzing my own mistakes — and redesigning my programming environment to eliminate those habits. In today’s paper choice we’ll be looking at some other ways of systematically improving your skills over time (along with quite a few other gems). In 1978 Professor Robert Floyd was presented with the ACM Turing Award for “helping to found the following important subfields of computer science: the theory of parsing, the semantics of programming languages, automatic program verification, automatic program synthesis, and analysis of algorithms.” Not a bad list! “The paradigms of programming” is his acceptance speech. Today I want to talk about the paradigms of programming, how they affect our success as designers of computer programs, how they should be taught, and how they should be embodied in our programming languages. Dominant at the time was the idea of structured programming (whose ideas are still very much with us today of course). The notion of starting with a top-down, stepwise refinement of the problem, and then building upwards from the primitives of the underlying machine to ‘meet in the middle’ with a set of more abstract modules and functions to be used by the top-down design. See e.g. ‘ Program development by stepwise refinement ’, and ‘ On the criteria to be used in decomposing systems into modules ’. The structured programming paradigm is useful, says Floyd, but it’s not the only one. Programming paradigms are at the heart of this paper – and a reasonable interpretation of what Floyd means by paradigm here is, I think, ‘a strategy or tactic for solving a class of problems.’ That sounds a bit like a design pattern when I say it that way, but the examples Floyd gives us are at a slightly more fundamental level than those the phrase ‘design patterns’ conjures in my mind. Far more powerful than how many languages you know (in terms of syntax), is how many paradigms you are fluent with. I believe that the current state of the art of computer programming reflects inadequacies in our stock of paradigms, and in the way our programming languages support, or fail to support, the paradigms of their user communities. Computer science quickly breaks down into communities each with its own languages and dominant paradigms. The problem of falling into one of these and not escaping is that it becomes hard to see the fundamentals afresh and discover new approaches. Quoting from Kuhn in ‘The Structure of Scientific Revolutions’ :  The study of paradigms, including many that are far more specialized than those named illustratively above, is what mainly prepares the student for membership in the particular scientific community with which he will later practice. Because he there joins men who learned the bases of their field from the same concrete models, his subsequent practice will seldom evoke overt disagreement over fundamentals. John Cocke invented the dynamic programming paradigm to solve a problem with the efficient parsing of context-free languages. Floyd discovered recursive coroutines as a structure while building hierarchical top-down parsers. John Cocke’s experience and mine illustrate the likelihood that continued advance in programming will require the continuing invention, elaboration, and communication of new paradigms. On developing as a programmer  So much for the advancement of the field, what about developing your own skills? If the advancement of the general art of programming requires the continuing invention and elaboration of paradigms, advancement of the art of the individual programming requires that he expand his repertory of paradigms. Here’s the technique that Floyd used to expand his own capabilities. After solving a challenging problem, I solve it again from scratch, retracing only the insight of the earlier solution. I repeat this until the solution is as clean and direct as I can hope for. Then I look for a general rule for attacking similar problems, that would have led me to approach the given problem in the most efficient way the first time. Often, such a rule is of permanent value. It can be hard to gain exposure to new paradigms from within your own immediate environment, because it’s likely your colleagues are all working within the same local paradigm set — witness job advertisements that specify the desired programming language (“The rules of Fortran can be learned within a few hours; the associated paradigms take much longer,  both to learn and unlearn.”). Floyd writes of an eye-opening experience of visiting MIT and seeing the power of Lisp first-hand (as someone grown up more in the tradition of Algol-like languages). … my message to the serious programmer is to spend a part of your working day examining and refining your own methods. Even though programmers are always struggling to meet some future or past deadline, methodological abstraction is a wise long term investment. On designing (and evaluating) programming languages  Everyone wants to design a new programming language. Bah! Floyd doesn’t find much satisfaction in the incremental extensions to existing languages (example: adding variant records to Pascal). Instead, it’s far more important to look at the paradigms a language supports. I believe that the continued advance of programming as a craft requires the development and dissemination of languages which support the major paradigms of their user’s communities. The design of a language should be preceded by enumeration of those paradigms, including a study of the deficiencies in programming caused by discouragement of unsupported paradigms… If there is ever a science of programming language design, it will probably consist largely of matching languages to the design methods they support. It’s not just the programming language itself of course, “the entire environment in which we program, diagnostic systems, files systems, editors, and all, can be analyzed as supporting or failing to support the spectrum of methods for design of programs.”  To persuade me of the merit of your language, you must show me how to construct programs in it. I don’t want to discourage the design of new languages; I want to encourage the language designer to become a serious student of the details of the design process. On teaching programming  We have an unfortunate obsession with form over content (Floyd is speaking in 1978 remember, not a lot has changed in the intervening 40 years!). You can feel Floyd’s heart sink in the following exchange:  If I ask another professor what he teaches in the introductory programming course, whether he answers proudly “Pascal” or diffidently “FORTRAN,” I know that he is teaching a grammar, a set of semantic rules, and some finished algorithms, leaving the students to discover, on their own, some process of design. We would do better to explicitly teach a set of systematic methods for all levels of program design. Students trained this way “have a large head start over those conventionally taught.”  To the teacher of programming… I say: identify the paradigms you use, as fully as you can, then teach them explicitly. They will serve your students when Fortran has replaced Latin and Sanskrit as the archetypal dead language. How many paradigms do you have in your toolbox?", "pdf_url": "https://pdfs.semanticscholar.org/a57d/cde5113855aec888b2a4e1fdd6e3956ce2e6.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/the-paradigms-of-programming.json"}
{"id": "8984509", "bin": "1400_1500", "summary_sentences": ["Uncertainty propagation in data processing systems Manousakis et al., SoCC’18  When I’m writing an edition of The Morning Paper, I often imagine a conversation with a hypothetical reader sat in a coffee shop somewhere at the start of their day.", "There are three levels of takeaway from today’s paper choice:  If you’re downing a quick espresso, then it’s good to know that uncertainty can creep into our data in lots of different ways, and if you compute with those uncertain values as if they were precise, errors can compound quickly leading to incorrect results or false confidence.", "If you’re savouring a cortado, then you might also want to dip into the techniques we can use to propagate uncertainty through a computation.", "If you’re lingering over a latte, then the UP (Uncertainty Propagation) framework additionally shows how to integrate these techniques into a dataflow framework.", "We implement this framework in a system called UP-MapReduce, and use it to modify ten applications, including AI/ML, image processing, and trend analysis applications to process uncertain data.", "Our evaluation shows that UP-MapReduce propagates uncertainties with high accuracy and, in many cases, low performance overheads.", "Are you sure?", "Uncertainty can arise from a number of different sources including probabilistic modelling, machine learning, approximate computing, imprecise sensor data, and such like.", "For many applications, uncertain data should be represented as probability distributions or estimated values with error bounds rather than exact values.", "Failure to properly account for this uncertainty may lead to incorrect results.", "For example, Bornholt et al. have shown that computing speeds from recorded GPS positions can lead to absurd values (e.g., walking speeds above 30mph) when ignoring uncertainties in the recordings.", "If you have a dataflow system with computation based on a DAG, then uncertainty in upstream data values needs to flow through the computation.", "For example, consider a simple 2-node DAG where an approximate query is used to produce an approximate count of the number of customers in different age groups (e.g., using BlinkDB ), and then we take a weighted average of those groups.", "The second node will by default produce a single value, but in reality it should result in a distribution.", "There may be meaningful parts of that distribution where the outcome would be disadvantageous (for example), but the probability of this is completely lost when reporting a single value.", "Uncertainty propagation  Our method offers, to the best of our knowledge, the only known computationally tractable (and as our evaluation will show, potentially with low overheads) large-scale uncertainty propagation.", "Consider a function  , where  is an arbitrary function without side-effects representing the computation at a node in a dataflow,  is a set of random variables representing inputs with uncertainties, and  is a set of random variables representing outputs with uncertainties.", "Depending on the nature of  , we can use different statistical methods to approximate the mean and variance of each variable in the output.", "When  is a continuous differentiable function we can use first-order Differential Analysis:  The general strategy is to compute  by approximating  using its first-order Taylor series at the expected value of  .", "This approximation is accurate if  is roughly linear around the support (in other words, neighborhood) of  …  When there are multiple inputs and multiple outputs, the calculation also needs to take into account the covariances between the outputs.", "When  is a semi-continuous function we have two possibilities.", "If the support of each input mostly or entirely falls within a continuous differentiable part of the function then we can use Differential Analysis (DA) as before.", "If it spans a discontinuity then we have to use Monte Carlo simulation.", "For example, consider the function  when  , and  otherwise.", "If each input is greater than  then we can use DA.", "We use Monte Carlo simulation to approximate  for functions  that do not meet (or the developers do not know whether they meet) the requirements for DA.", "is evaluated on  randomly drawn samples of the input, and the outputs are used as an approximation of  .", "To generate accurate samples, one must know the joint density of  and pay the heavy computational cost of any rejection-sampling algorithm.", "Unfortunately that cost grows exponentially with an increasing size of  and thus we resort to two approximations:  Given input distributions, generate samples accordingly and ignore covariances  In the absence of full distributional information, assume that each input is normally distributed with the same mean and covariance matrix as the unknown distribution.", "(This approximation works because the mean and variance estimation of Y depends solely on the mean and variance of  ).", "Uncertainty propagation in dataflows  As stated earlier, in a dataflow graph we need to perform uncertainty propagation at all nodes downstream of uncertain data.", "For Monte Carlo simulation-based uncertainty propagation (UP-MC) we can just treat a node as a black box, dynamically generate samples from the input set, and compute the mean and variance for each output using empirically derived distributions (or assume normal distributions in the absence of this information).", "The implementation of Differential Analysis (henceforth called UP-DA) is more challenging.", "Specifically, when a DAG node produces multiple outputs, we view it as being implemented by multiple sub-functions, each producing one of the outputs… input covariances can require additional data flow to be added to the DAG for computing output variances and covariances.", "If the programmer can provide a partial derivative function, then using this often gives better performance than resorting to numerical differentiation.", "Observe that we might make a saving early in the dataflow by introducing uncertainty (e.g. by computing an approximate result), but then we have to pay more later for the resulting uncertainty propagation.", "The evaluation explores this trade-off.", "UP-MapReduce is an implementation of the above ideas in the in MapReduce.", "The UP-MapReduce extension includes three Mapper and three Reducer classes that implement UP-MC, UP-DA for continuous functions, and UP-DA for semi-continuous functions.", "The extension also introduce the uncertain type PV (Probabilistic Value) which contains one or more random variables, each described by a mean, a variance-covariance matrix, and possibly an entire empirical distribution.", "The UP-DA Continuous Reducer class for example provides an abstract derivative method that a developer can implement to provide a closed-form derivative function.", "Uncertainty propagation in practice  We have built a toolbox of common operations (e.g., sum) and modified ten common data processing applications using UP-MapReduce to process uncertain data.", "Baselines for the evaluation are established by running a large Monte Carlo experiment over a precise version of each application.", "When input errors are small (e.g. below 3%) then UP-MapReduce estimates means with very low error.", "The following figure shows the relative errors and execution times for the three variants of UP-MC as compared to the baseline.", "Enlarge  For six of the applications UP-MapReduce is highly accurate, but when input errors are significant (e.g. eig, svd) its estimated relative errors can deviate noticeably from baseline values.", "The best performance is obtained when using closed-form (user provided) derivatives.", "tsocial and latency are both multi-stage approximate workflows.", "The following chart shows the execution times and maximum relative errors for sampling rates from 0.1% to 100% (precise).", "For tsocial, a sampling rate of 80% or less is required before the overheads of uncertainty propagation are outweighed by the sampling benefits.", "Experimentation with ten common data analytic applications revealed that UP-MapReduce is highly accurate in many cases, while its performance overheads are very low— an average of 6% performance degradation— when closed-form derivatives are provided.", "When numerical differentiation or Monte Carlo simulation must be used, overheads can become much more significant as input size increases.", "Fortunately, the impact of these overheads on overall execution time can be reduced by allocating additional computational resources."], "summary_text": "Uncertainty propagation in data processing systems Manousakis et al., SoCC’18  When I’m writing an edition of The Morning Paper, I often imagine a conversation with a hypothetical reader sat in a coffee shop somewhere at the start of their day. There are three levels of takeaway from today’s paper choice:  If you’re downing a quick espresso, then it’s good to know that uncertainty can creep into our data in lots of different ways, and if you compute with those uncertain values as if they were precise, errors can compound quickly leading to incorrect results or false confidence. If you’re savouring a cortado, then you might also want to dip into the techniques we can use to propagate uncertainty through a computation. If you’re lingering over a latte, then the UP (Uncertainty Propagation) framework additionally shows how to integrate these techniques into a dataflow framework. We implement this framework in a system called UP-MapReduce, and use it to modify ten applications, including AI/ML, image processing, and trend analysis applications to process uncertain data. Our evaluation shows that UP-MapReduce propagates uncertainties with high accuracy and, in many cases, low performance overheads. Are you sure? Uncertainty can arise from a number of different sources including probabilistic modelling, machine learning, approximate computing, imprecise sensor data, and such like. For many applications, uncertain data should be represented as probability distributions or estimated values with error bounds rather than exact values. Failure to properly account for this uncertainty may lead to incorrect results. For example, Bornholt et al. have shown that computing speeds from recorded GPS positions can lead to absurd values (e.g., walking speeds above 30mph) when ignoring uncertainties in the recordings. If you have a dataflow system with computation based on a DAG, then uncertainty in upstream data values needs to flow through the computation. For example, consider a simple 2-node DAG where an approximate query is used to produce an approximate count of the number of customers in different age groups (e.g., using BlinkDB ), and then we take a weighted average of those groups. The second node will by default produce a single value, but in reality it should result in a distribution. There may be meaningful parts of that distribution where the outcome would be disadvantageous (for example), but the probability of this is completely lost when reporting a single value. Uncertainty propagation  Our method offers, to the best of our knowledge, the only known computationally tractable (and as our evaluation will show, potentially with low overheads) large-scale uncertainty propagation. Consider a function  , where  is an arbitrary function without side-effects representing the computation at a node in a dataflow,  is a set of random variables representing inputs with uncertainties, and  is a set of random variables representing outputs with uncertainties. Depending on the nature of  , we can use different statistical methods to approximate the mean and variance of each variable in the output. When  is a continuous differentiable function we can use first-order Differential Analysis:  The general strategy is to compute  by approximating  using its first-order Taylor series at the expected value of  . This approximation is accurate if  is roughly linear around the support (in other words, neighborhood) of  …  When there are multiple inputs and multiple outputs, the calculation also needs to take into account the covariances between the outputs. When  is a semi-continuous function we have two possibilities. If the support of each input mostly or entirely falls within a continuous differentiable part of the function then we can use Differential Analysis (DA) as before. If it spans a discontinuity then we have to use Monte Carlo simulation. For example, consider the function  when  , and  otherwise. If each input is greater than  then we can use DA. We use Monte Carlo simulation to approximate  for functions  that do not meet (or the developers do not know whether they meet) the requirements for DA. is evaluated on  randomly drawn samples of the input, and the outputs are used as an approximation of  . To generate accurate samples, one must know the joint density of  and pay the heavy computational cost of any rejection-sampling algorithm. Unfortunately that cost grows exponentially with an increasing size of  and thus we resort to two approximations:  Given input distributions, generate samples accordingly and ignore covariances  In the absence of full distributional information, assume that each input is normally distributed with the same mean and covariance matrix as the unknown distribution. (This approximation works because the mean and variance estimation of Y depends solely on the mean and variance of  ). Uncertainty propagation in dataflows  As stated earlier, in a dataflow graph we need to perform uncertainty propagation at all nodes downstream of uncertain data. For Monte Carlo simulation-based uncertainty propagation (UP-MC) we can just treat a node as a black box, dynamically generate samples from the input set, and compute the mean and variance for each output using empirically derived distributions (or assume normal distributions in the absence of this information). The implementation of Differential Analysis (henceforth called UP-DA) is more challenging. Specifically, when a DAG node produces multiple outputs, we view it as being implemented by multiple sub-functions, each producing one of the outputs… input covariances can require additional data flow to be added to the DAG for computing output variances and covariances. If the programmer can provide a partial derivative function, then using this often gives better performance than resorting to numerical differentiation. Observe that we might make a saving early in the dataflow by introducing uncertainty (e.g. by computing an approximate result), but then we have to pay more later for the resulting uncertainty propagation. The evaluation explores this trade-off. UP-MapReduce is an implementation of the above ideas in the in MapReduce. The UP-MapReduce extension includes three Mapper and three Reducer classes that implement UP-MC, UP-DA for continuous functions, and UP-DA for semi-continuous functions. The extension also introduce the uncertain type PV (Probabilistic Value) which contains one or more random variables, each described by a mean, a variance-covariance matrix, and possibly an entire empirical distribution. The UP-DA Continuous Reducer class for example provides an abstract derivative method that a developer can implement to provide a closed-form derivative function. Uncertainty propagation in practice  We have built a toolbox of common operations (e.g., sum) and modified ten common data processing applications using UP-MapReduce to process uncertain data. Baselines for the evaluation are established by running a large Monte Carlo experiment over a precise version of each application. When input errors are small (e.g. below 3%) then UP-MapReduce estimates means with very low error. The following figure shows the relative errors and execution times for the three variants of UP-MC as compared to the baseline. Enlarge  For six of the applications UP-MapReduce is highly accurate, but when input errors are significant (e.g. eig, svd) its estimated relative errors can deviate noticeably from baseline values. The best performance is obtained when using closed-form (user provided) derivatives. tsocial and latency are both multi-stage approximate workflows. The following chart shows the execution times and maximum relative errors for sampling rates from 0.1% to 100% (precise). For tsocial, a sampling rate of 80% or less is required before the overheads of uncertainty propagation are outweighed by the sampling benefits. Experimentation with ten common data analytic applications revealed that UP-MapReduce is highly accurate in many cases, while its performance overheads are very low— an average of 6% performance degradation— when closed-form derivatives are provided. When numerical differentiation or Monte Carlo simulation must be used, overheads can become much more significant as input size increases. Fortunately, the impact of these overheads on overall execution time can be reduced by allocating additional computational resources.", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3267809.3267833?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/uncertainty-propagation-in-data-processing-systems.json"}
{"id": "85505359", "bin": "1400_1500", "summary_sentences": ["RPCValet: NI-driven tail-aware balancing of µs-scale RPCs Daglis et al., ASPLOS’19  Last week we learned about the [increased tail-latency sensitivity of microservices based applications with high RPC fan-outs.", "Seer uses estimates of queue depths to mitigate latency spikes on the order of 10-100ms, in conjunction with a cluster manager.", "Today’s paper choice, RPCValet, operates at latencies 3 orders of magnitude lower, targeting reduction in tail latency for services that themselves have service times on the order of a small number of µs (e.g., the average service time for memcached is approximately 2µs).", "The net result of rapid advancements in the networking world is that inter-tier communications latency will approach the fundamental lower bound of speed-of-light propagation in the foreseeable future.", "The focus of optimization hence will completely shift to efficiently handling RPCs at the endpoints as soon as they are delivered from the network.", "Furthermore, the evaluation shows that “RPCValet leaves no significant room for improvement” when compared against the theoretical ideal (it comes within 3-15%).", "So what we have here is a glimpse of the limits for low-latency RPCs under load.", "When it’s no longer physically possible to go meaningfully faster, further application-level performance gains will have to come from eliminating RPCs altogether.", "RPCValet balances incoming RPC requests among the multiple cores of a server.", "Consider for example a Redis server maintaining a sorted array in memory…  … an RPC to add a new entry may incur multiple TLB misses, stalling the core for a few µs while new translations are installed.", "While this core is stalled on the TLB miss(es), it is best to dispatch RPCs to other available cores on the server.", "In theory, how fast could we go?", "Consider a 16-core server handling 16 requests.", "We could put anywhere from 1 to 16 queues in front of those cores.", "At one extreme we have a ‘16 x 1’ architecture with 16 queues each with one associated processing unit.", "At the other extreme is a ‘1 x 16’ architecture with one shared queue serving all 16 processing units.", "Or we could have e.g. a ‘4 x 4’ with 4 queues each serving 4 units, and so on…  If you model that out with Poisson arrivals and variety of different service time distributions (fixed, uniform, exponential, and generalised extreme value, GEV) what you’ll find is that the ‘1×16’ architecture performs the best, and the ‘16×1’ architecture performs the worst.", "1 x 16 significantly outperforms 16 x 1.", "16 x 1’s inability to assign requests to idle cores results in higher tail latencies and a peak throughput 25-73% lower than 1 x 16 under a tail latency SLO at 10x the mean service time… The theoretical results suggest that systems should implement a queueing configuration that is as close as possible to a single queue (1 x 16) configuration.", "In practice those models failed to account for a significant source of overhead: synchronizing across multiple cores on the single queue.", "When you take synchronisation into account, a dedicated queue per core starts to look like a good idea again.", "NICs supporting Receive-Side Scaling (RSS) can push messages into each core’s queue, but while RSS can help to achieve load distribution, it can’t truly achieving load balancing.", "Any resulting load imbalance after applying these [RSS] rules must be handled by system software, introducing unacceptable latency for the most latency-sensitive RPCs with µs-scale service times.", "Introducing RPCValet  RPCValet uses a push-based model, while also taking into account the current loading of the cores.", "It’s designed for “emerging architectures featuring fully integrated NIs and hardware-terminated transport protocols.”.", "The key hardware feature is that the network interface has direct access to the server’s memory hierarchy, eliminating round trips over e.g. PCIe.", "Servers register a part of their DRAM into a partitioned global address space (PGAS), where every server can read and write memory in RDMA fashion.", "(We’re not given any details on system reconfiguration etc.).", "An integrated NI can, with proper hardware support, monitor each core’s state and steer RPCs to the least loaded cores.", "Such monitoring is implausible without NI integration, as the latency of transferring load information over an I/O bus (e.g. ~ 1.5 µs for a 3-hop posted PCIe transaction) would mean that the NI will make delayed—hence sub-optimal, or even wrong— decisions until the information arrives.", "With N participants in the system, RPCValet first writes every incoming message into a single PGAS-resident message buffer of NxS slots.", "Then it notifies the selected core to process the request.", "Message arrival and memory location are thus decoupled from the assignment of a core for processing.", "We have the synchronization-free zero-copy behaviour of a partitioned multi-queue architecture, together with the load-balancing flexibility of a single queue system.", "In the implementation each node maintains a send and a receive buffer.", "Send buffers contain a valid bit indicating whether they are currently being used, and a pointer to a buffer in local memory containing the payload.", "Receive buffers on the other hand have slots that are size to accommodate message payloads directly.", "Overall, the messaging mechanism’s memory footprint is 32 x N x S + (max_msg_size + 64) x N x S bytes.", "That is, a few tens of MB at most.", "The implementation uses a simple scheme to estimate core loads.", "RPCValet simply keeps track of the number of outstanding send requests assigned to each core.", "Allowing only one inflight request per core corresponds to a true single-system queue behaviour, but introduces a small inefficiency waiting for the notification of completed request processing.", "A practical compromise is to allow two outstanding requests per core.", "This results in marginal performance gains over the single request design for ultra-fast RPCs with service times of a few nanoseconds.", "All IN backends independently handle incoming network packets and access memory directly, but they hand off to a single NI dispatcher (over the on-chip interconnect) that is statically assigned to dispatch requests to cores for servicing.", "… for modern server processor core counts, the required dispatch throughput should be easily sustainable by a single centralized hardware unit, while the additional latency due to the indirection from any NI backend to the NI dispatcher is negligible (just a few ns).", "Evaluation  The evaluation is conducted using against three different service implementations: a synthetic service that emulates different service time distributions; the HERD key-value store, and the Masstree data store.", "An SLO of less than or equal to 10x the mean service time is assumed in each case, and configurations are evaluated in terms of throughput under SLO.", "Compared to a software implementation, which requires a synchronisation primitive, the hardware implementation delivers 2.3x-2.7x higher throughput under SLO.", "The following plots show the performance of different queueing arrangements under the three workloads, with the ‘1×16’ arrangement that RPCValet simulates performing the best as expected.", "( Enlarge )  Compared to a theoretically optimal 1 x 16 model, RPCValet gets within 3-15% (depending on the service time distribution).", "“We attribute the gap between the implementation and the model to contention that emerges under high load in the implemented systems, which is not captured by the model.”  At the end of the day though:  RPCValet leaves no significant room for improvement; neither centralizing dispatch nor maintaining private request queues per core introduces performance concerns.", "Throughput under tight tail latency goals is improved by up to 1.4x, and tail latency before saturation is reduced by up to 4x."], "summary_text": "RPCValet: NI-driven tail-aware balancing of µs-scale RPCs Daglis et al., ASPLOS’19  Last week we learned about the [increased tail-latency sensitivity of microservices based applications with high RPC fan-outs. Seer uses estimates of queue depths to mitigate latency spikes on the order of 10-100ms, in conjunction with a cluster manager. Today’s paper choice, RPCValet, operates at latencies 3 orders of magnitude lower, targeting reduction in tail latency for services that themselves have service times on the order of a small number of µs (e.g., the average service time for memcached is approximately 2µs). The net result of rapid advancements in the networking world is that inter-tier communications latency will approach the fundamental lower bound of speed-of-light propagation in the foreseeable future. The focus of optimization hence will completely shift to efficiently handling RPCs at the endpoints as soon as they are delivered from the network. Furthermore, the evaluation shows that “RPCValet leaves no significant room for improvement” when compared against the theoretical ideal (it comes within 3-15%). So what we have here is a glimpse of the limits for low-latency RPCs under load. When it’s no longer physically possible to go meaningfully faster, further application-level performance gains will have to come from eliminating RPCs altogether. RPCValet balances incoming RPC requests among the multiple cores of a server. Consider for example a Redis server maintaining a sorted array in memory…  … an RPC to add a new entry may incur multiple TLB misses, stalling the core for a few µs while new translations are installed. While this core is stalled on the TLB miss(es), it is best to dispatch RPCs to other available cores on the server. In theory, how fast could we go? Consider a 16-core server handling 16 requests. We could put anywhere from 1 to 16 queues in front of those cores. At one extreme we have a ‘16 x 1’ architecture with 16 queues each with one associated processing unit. At the other extreme is a ‘1 x 16’ architecture with one shared queue serving all 16 processing units. Or we could have e.g. a ‘4 x 4’ with 4 queues each serving 4 units, and so on…  If you model that out with Poisson arrivals and variety of different service time distributions (fixed, uniform, exponential, and generalised extreme value, GEV) what you’ll find is that the ‘1×16’ architecture performs the best, and the ‘16×1’ architecture performs the worst. 1 x 16 significantly outperforms 16 x 1. 16 x 1’s inability to assign requests to idle cores results in higher tail latencies and a peak throughput 25-73% lower than 1 x 16 under a tail latency SLO at 10x the mean service time… The theoretical results suggest that systems should implement a queueing configuration that is as close as possible to a single queue (1 x 16) configuration. In practice those models failed to account for a significant source of overhead: synchronizing across multiple cores on the single queue. When you take synchronisation into account, a dedicated queue per core starts to look like a good idea again. NICs supporting Receive-Side Scaling (RSS) can push messages into each core’s queue, but while RSS can help to achieve load distribution, it can’t truly achieving load balancing. Any resulting load imbalance after applying these [RSS] rules must be handled by system software, introducing unacceptable latency for the most latency-sensitive RPCs with µs-scale service times. Introducing RPCValet  RPCValet uses a push-based model, while also taking into account the current loading of the cores. It’s designed for “emerging architectures featuring fully integrated NIs and hardware-terminated transport protocols.”. The key hardware feature is that the network interface has direct access to the server’s memory hierarchy, eliminating round trips over e.g. PCIe. Servers register a part of their DRAM into a partitioned global address space (PGAS), where every server can read and write memory in RDMA fashion. (We’re not given any details on system reconfiguration etc.). An integrated NI can, with proper hardware support, monitor each core’s state and steer RPCs to the least loaded cores. Such monitoring is implausible without NI integration, as the latency of transferring load information over an I/O bus (e.g. ~ 1.5 µs for a 3-hop posted PCIe transaction) would mean that the NI will make delayed—hence sub-optimal, or even wrong— decisions until the information arrives. With N participants in the system, RPCValet first writes every incoming message into a single PGAS-resident message buffer of NxS slots. Then it notifies the selected core to process the request. Message arrival and memory location are thus decoupled from the assignment of a core for processing. We have the synchronization-free zero-copy behaviour of a partitioned multi-queue architecture, together with the load-balancing flexibility of a single queue system. In the implementation each node maintains a send and a receive buffer. Send buffers contain a valid bit indicating whether they are currently being used, and a pointer to a buffer in local memory containing the payload. Receive buffers on the other hand have slots that are size to accommodate message payloads directly. Overall, the messaging mechanism’s memory footprint is 32 x N x S + (max_msg_size + 64) x N x S bytes. That is, a few tens of MB at most. The implementation uses a simple scheme to estimate core loads. RPCValet simply keeps track of the number of outstanding send requests assigned to each core. Allowing only one inflight request per core corresponds to a true single-system queue behaviour, but introduces a small inefficiency waiting for the notification of completed request processing. A practical compromise is to allow two outstanding requests per core. This results in marginal performance gains over the single request design for ultra-fast RPCs with service times of a few nanoseconds. All IN backends independently handle incoming network packets and access memory directly, but they hand off to a single NI dispatcher (over the on-chip interconnect) that is statically assigned to dispatch requests to cores for servicing. … for modern server processor core counts, the required dispatch throughput should be easily sustainable by a single centralized hardware unit, while the additional latency due to the indirection from any NI backend to the NI dispatcher is negligible (just a few ns). Evaluation  The evaluation is conducted using against three different service implementations: a synthetic service that emulates different service time distributions; the HERD key-value store, and the Masstree data store. An SLO of less than or equal to 10x the mean service time is assumed in each case, and configurations are evaluated in terms of throughput under SLO. Compared to a software implementation, which requires a synchronisation primitive, the hardware implementation delivers 2.3x-2.7x higher throughput under SLO. The following plots show the performance of different queueing arrangements under the three workloads, with the ‘1×16’ arrangement that RPCValet simulates performing the best as expected. ( Enlarge )  Compared to a theoretically optimal 1 x 16 model, RPCValet gets within 3-15% (depending on the service time distribution). “We attribute the gap between the implementation and the model to contention that emerges under high load in the implemented systems, which is not captured by the model.”  At the end of the day though:  RPCValet leaves no significant room for improvement; neither centralizing dispatch nor maintaining private request queues per core introduces performance concerns. Throughput under tight tail latency goals is improved by up to 1.4x, and tail latency before saturation is reduced by up to 4x.", "pdf_url": "https://www.cc.gatech.edu/~adaglis3/files/papers/RPCValet_asplos19.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/rpcvalet.json"}
{"id": "4536127", "bin": "1400_1500", "summary_sentences": ["Popularity prediction of Facebook videos for higher quality streaming Tang et al., USENIX ATC’17  Suppose I could grant you access to a clairvoyance service, which could make one class of predictions about your business for you with perfect accuracy.", "What would you want to know, and what difference would knowing that make to your business?", "(For example, in the VC world you’d want to know which companies are going to make it big — that’s a hard one!).", "In many cases though, although perfect clairvoyance isn’t achievable, with some care and attention to data collection and modelling, you can get predictions that are useful.", "Today’s paper looks at the problem of predicting the popularity of videos on Facebook.", "Why does that matter?", "As we saw yesterday, videos can be encoded at multiple different bitrates.", "Having a broader choice of bitrates means a better overall experience for clients across a range of bandwidths, at the expense of more resources consumed on the server side in encoding.", "In addition, Facebook’s QuickFire engine can produce versions of a video with the same quality but approximately 20% smaller than the standard encoding.", "It uses up to 20x the computation to do so though!", "Since video popularity on Facebook follows a power law, accurate identification of e.g. the top 1% of videos would cover 83% of the total video watch time, and allow us to expend server side effort where it will have the most impact.", "The challenge in exploiting this insight is in scalably and accurately predicting the videos that will become popular.", "State of the art popularity prediction algorithms are accurate but do not scale to handle the Facebook video workload… Simple heuristics that exploit information from the social network scale, but are not accurate.", "Facebook’s solution to this problem is CHESS: a Constant History, Exponential kernels, and Social Signals popularity prediction algorithm.", "Impressively, CHESS requires only four machines to provide popularity prediction for all of Facebook’s videos.", "Compared to the heuristic currently used in production, CHESS improves the watch time coverage of QuickFire by 8%-30% using the same CPU resources for re-encoding.", "To cover 80% of watch time, CHESS reduces the overhead from 54% to 17%.", "CHESS aligns with another theme of our times too.", "Whereas previous work on video popularity considered popularity on a daily (batch) basis, CHESS is able to make predictions quickly (on the order of minutes).", "This matters because if prediction takes longer than the interval between when a video is uploaded and when it peaks then much of the watch time will already be in the past by the time we figure out it would have been worth optimising.", "So in summary we have scalable near-real time predictions to guide business decisions.", "Although CHESS has been applied here in a video context, the fundamental approach should generalize to predict popularity in other settings too.", "Let’s jump straight into looking at the details of the CHESS prediction algorithm.", "How the CHESS prediction algorithm works  A common theme in popularity prediction is exploiting past access patterns.", "The state of the art approaches do so by modeling the behavior as a self-exciting process that predicts future accesses based on all past accesses.", "A past access at time  is assumed to provide some influence on future popularity at time  , as modeled by a kernel function  .", "Such self-exciting processes are based on the sum of the influence off all past requests from the current time to infinity.", "Power-law based kernels provide high accuracy predictions but require each new prediction to compute over all past accesses.", "Which in turn means linear growth of storage and computation with respect to past accesses.", "At the heart of CHESS is a new exponentially decaying kernel:  where  represents a time window modelling how far into the future the influence of past requests extends.", "Such a kernel allows us to simplify the computation of a new prediction to only require the last prediction,  , and its timestamp,  , which drastically reduces the space and time overhead.", "Such an exponentially decayed watch time (EDWT) kernel is efficiently computable, but a weaker predictor than self-exciting processes with more powerful kernels.", "We overcome this limitation of EDWTs with the second key insight in the CHESS design: combining many weak, but readily computable, signals through a learning framework achieves high accuracy while remaining efficient.", "The learning framework is a neural network, and it’s refreshingly simple!", "The authors use a 2-layer network with 100 hidden nodes.", "Compared to linear models, using such a network reduces prediction error by 40%, but there was no further gain in moving to more complex network models.", "The inputs to the network are a combination of static and dynamic features.", "The static (stateless) features do not change dramatically during the life-cycle of a video – for example, the number of friends of the video owner, as well as things like the length of the video.", "The dynamic (stateful) features do change, and therefore we need state to track them.", "This include things such as the number of comments, likes, shares, saves, and so on.", "To keep the state constant per video, four exponential kernels with different time windows are used: 1, 4, 16, and 64 hours.", "Values for some of the features can vary over a very wide scale: from 10-10^8 depending on video popularity.", "Although linear scaling, in the form of standardization, is the commonly used method in statistical learning, we find that logarithmic scaling, i.e.,  , delivers much better performance for our workload.", "Logarithmic scaling improves the coverage ratio of QuickFire by as much as 6% over linear scaling.", "To generate training examples the authors use an example queue.", "The current state of a video is appended to the queue when it is accessed, and while it remains in the queue the watch time and feature values are tracked.", "After a prediction horizon amount of time has expired, the video is evicted from the queue again.", "A prediction horizon of six days is empirically found to give a good tradeoff between high accuracy and low overhead.", "To avoid the example queue being flooded by data points from the most popular videos, an additional constraint is that an example for a video is only admitted to the queue if at least D = 2 hours of time has elapsed singe the last example of the same video.", "Putting it all together: the CHESS video prediction service  The CHESS video prediction service (CHESS-VPS) uses 8 workers distributed across 4 machines to generate predictions on the full Facebook workload.", "Facebook video accesses are logged to Scribe, and from there they are streamed to CHESS-VPS and sharded based on video ID into eight shards.", "Each worker queries TAO for the additional features that are needed.", "Queries are batched, and the results are cached for 10 minutes to reduce load.", "Workers maintain tables with their most recent predictions for the top 10 million most popular videos in its shard.", "Thus across the 8 shards there are 80 million videos that make up actively accessed video working set.", "Every ten minutes each worker scans its table and sorts videos based on their predictions.", "An aggregator in each machine collects the top 1 million videos from the workers on that machine, and broadcasts these to all aggregators and waits for their predictions.", "When all the predictions have been received these are merged and sorted.", "These predictions are then used to answer queries for the next ten minutes.", "Each video has 12 stateless features and 7 stateful features.", "These features, associated metadata, and current popularity prediction add up to a storage overhead of ~250 bytes per video.", "Thus, all 80 million videos use ~20GB RAM in total to maintain.", "This results in a total memory overhead of ~44GB RAM from models and metadata, or only ~11GB RAM per machine."], "summary_text": "Popularity prediction of Facebook videos for higher quality streaming Tang et al., USENIX ATC’17  Suppose I could grant you access to a clairvoyance service, which could make one class of predictions about your business for you with perfect accuracy. What would you want to know, and what difference would knowing that make to your business? (For example, in the VC world you’d want to know which companies are going to make it big — that’s a hard one!). In many cases though, although perfect clairvoyance isn’t achievable, with some care and attention to data collection and modelling, you can get predictions that are useful. Today’s paper looks at the problem of predicting the popularity of videos on Facebook. Why does that matter? As we saw yesterday, videos can be encoded at multiple different bitrates. Having a broader choice of bitrates means a better overall experience for clients across a range of bandwidths, at the expense of more resources consumed on the server side in encoding. In addition, Facebook’s QuickFire engine can produce versions of a video with the same quality but approximately 20% smaller than the standard encoding. It uses up to 20x the computation to do so though! Since video popularity on Facebook follows a power law, accurate identification of e.g. the top 1% of videos would cover 83% of the total video watch time, and allow us to expend server side effort where it will have the most impact. The challenge in exploiting this insight is in scalably and accurately predicting the videos that will become popular. State of the art popularity prediction algorithms are accurate but do not scale to handle the Facebook video workload… Simple heuristics that exploit information from the social network scale, but are not accurate. Facebook’s solution to this problem is CHESS: a Constant History, Exponential kernels, and Social Signals popularity prediction algorithm. Impressively, CHESS requires only four machines to provide popularity prediction for all of Facebook’s videos. Compared to the heuristic currently used in production, CHESS improves the watch time coverage of QuickFire by 8%-30% using the same CPU resources for re-encoding. To cover 80% of watch time, CHESS reduces the overhead from 54% to 17%. CHESS aligns with another theme of our times too. Whereas previous work on video popularity considered popularity on a daily (batch) basis, CHESS is able to make predictions quickly (on the order of minutes). This matters because if prediction takes longer than the interval between when a video is uploaded and when it peaks then much of the watch time will already be in the past by the time we figure out it would have been worth optimising. So in summary we have scalable near-real time predictions to guide business decisions. Although CHESS has been applied here in a video context, the fundamental approach should generalize to predict popularity in other settings too. Let’s jump straight into looking at the details of the CHESS prediction algorithm. How the CHESS prediction algorithm works  A common theme in popularity prediction is exploiting past access patterns. The state of the art approaches do so by modeling the behavior as a self-exciting process that predicts future accesses based on all past accesses. A past access at time  is assumed to provide some influence on future popularity at time  , as modeled by a kernel function  . Such self-exciting processes are based on the sum of the influence off all past requests from the current time to infinity. Power-law based kernels provide high accuracy predictions but require each new prediction to compute over all past accesses. Which in turn means linear growth of storage and computation with respect to past accesses. At the heart of CHESS is a new exponentially decaying kernel:  where  represents a time window modelling how far into the future the influence of past requests extends. Such a kernel allows us to simplify the computation of a new prediction to only require the last prediction,  , and its timestamp,  , which drastically reduces the space and time overhead. Such an exponentially decayed watch time (EDWT) kernel is efficiently computable, but a weaker predictor than self-exciting processes with more powerful kernels. We overcome this limitation of EDWTs with the second key insight in the CHESS design: combining many weak, but readily computable, signals through a learning framework achieves high accuracy while remaining efficient. The learning framework is a neural network, and it’s refreshingly simple! The authors use a 2-layer network with 100 hidden nodes. Compared to linear models, using such a network reduces prediction error by 40%, but there was no further gain in moving to more complex network models. The inputs to the network are a combination of static and dynamic features. The static (stateless) features do not change dramatically during the life-cycle of a video – for example, the number of friends of the video owner, as well as things like the length of the video. The dynamic (stateful) features do change, and therefore we need state to track them. This include things such as the number of comments, likes, shares, saves, and so on. To keep the state constant per video, four exponential kernels with different time windows are used: 1, 4, 16, and 64 hours. Values for some of the features can vary over a very wide scale: from 10-10^8 depending on video popularity. Although linear scaling, in the form of standardization, is the commonly used method in statistical learning, we find that logarithmic scaling, i.e.,  , delivers much better performance for our workload. Logarithmic scaling improves the coverage ratio of QuickFire by as much as 6% over linear scaling. To generate training examples the authors use an example queue. The current state of a video is appended to the queue when it is accessed, and while it remains in the queue the watch time and feature values are tracked. After a prediction horizon amount of time has expired, the video is evicted from the queue again. A prediction horizon of six days is empirically found to give a good tradeoff between high accuracy and low overhead. To avoid the example queue being flooded by data points from the most popular videos, an additional constraint is that an example for a video is only admitted to the queue if at least D = 2 hours of time has elapsed singe the last example of the same video. Putting it all together: the CHESS video prediction service  The CHESS video prediction service (CHESS-VPS) uses 8 workers distributed across 4 machines to generate predictions on the full Facebook workload. Facebook video accesses are logged to Scribe, and from there they are streamed to CHESS-VPS and sharded based on video ID into eight shards. Each worker queries TAO for the additional features that are needed. Queries are batched, and the results are cached for 10 minutes to reduce load. Workers maintain tables with their most recent predictions for the top 10 million most popular videos in its shard. Thus across the 8 shards there are 80 million videos that make up actively accessed video working set. Every ten minutes each worker scans its table and sorts videos based on their predictions. An aggregator in each machine collects the top 1 million videos from the workers on that machine, and broadcasts these to all aggregators and waits for their predictions. When all the predictions have been received these are merged and sorted. These predictions are then used to answer queries for the next ten minutes. Each video has 12 stateless features and 7 stateful features. These features, associated metadata, and current popularity prediction add up to a storage overhead of ~250 bytes per video. Thus, all 80 million videos use ~20GB RAM in total to maintain. This results in a total memory overhead of ~44GB RAM from models and metadata, or only ~11GB RAM per machine.", "pdf_url": "https://www.usenix.org/system/files/conference/atc17/atc17-tang.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/popularity-predictions-of-facebook-videos-for-higher-quality-streaming.json"}
{"id": "93025332", "bin": "1400_1500", "summary_sentences": ["Use of Formal Methods at Amazon Web Services – Newcombe et al 2014  Leslie Lamport recently gave a talk at the React conference on the specification language TLA.", "I wasn’t there to hear the talk, but I was intrigued enough to dig in and find out a little more.", "Especially since I have some experience with Z and CSP.", "My journey led me to today’s paper choice, a wonderful find and something that I hope spreads further into accepted industry practice.", "Something a lot of people believe…  In industry, formal methods have a reputation of requiring a huge amount of training and effort to verify a tiny piece of relatively straightforward code, so the return on investment is only justified in safety- critical domains such as medical systems and avionics.", "That isn’t necessarily so:  Our experience with TLA+ has shown that perception to be quite wrong.", "So far we have used TLA+ on 10 large complex real-world systems.", "In every case TLA+ has added significant value, either finding subtle bugs that we are sure we would not have found by other means, or giving us enough understanding and confidence to make aggressive performance optimizations without sacrificing correctness.", "We now have 7 teams using TLA+, with encouragement from senior management and technical leadership.", "Engineers from entry level to Principal have been able to learn TLA+ from scratch and get useful results in 2 to 3 weeks, in some cases just in their personal time on weekends and evenings, and without help or training.", "The AWS teams have been using formal specification since 2011, and the paper tells the story of how this came to be and how their use has spread.", "It’s highly readable so I encourage you to check it out.", "I will focus on the rationale and benefits for this short summary.", "Building distributed systems is hard  S3 is just one of tens of AWS services that store and process data that our customers have entrusted to us.", "To safeguard that data, the core of each service relies on fault-tolerant distributed algorithms for replication, consistency, concurrency control, auto-scaling, load balancing, and other coordination tasks.", "When it comes to building real production systems, there’s more to it than just coding up algorithms from the literature:  There are many such algorithms in the literature, but combining them into a cohesive system is a major challenge, as the algorithms must usually be modified in order to interact properly in a real-world system.", "In addition, we have found it necessary to invent algorithms of our own.", "We work hard to avoid unnecessary complexity, but the essential complexity of the task remains high.", "How can you gain confidence that your system is correct?", "AWS used multiple verification methods (short of formal methods), but still weren’t finding all the subtle bugs that may lurk.", "before launching such a service, we need to reach extremely high confidence that the core of the system is correct.", "We have found that the standard verification techniques in industry are necessary but not sufficient.", "We use deep design reviews, code reviews, static code analysis, stress testing, fault-injection testing, and many other techniques, but we still find that subtle bugs can hide in complex concurrent fault-tolerant systems.", "Some of the most difficult to detect bugs are not in the code, but in the design…  some of the more subtle, dangerous bugs turn out to be errors in design; the code faithfully implements the intended design, but the design fails to correctly handle a particular ‘rare’ scenario.", "Finding a precise design language  In order to find subtle bugs in a system design, it is necessary to have a precise description of that design.", "There are at least two major benefits to writing a precise design; the author is forced to think more clearly, which helps eliminate ‘plausible hand-waving’, and tools can be applied to check for errors in the design, even while it is being written.", "AWS needed a precise design language, but also a pragmatic one:  As our designs are unavoidably complex, we needed a highly expressive language, far above the level of code, but with precise semantics.", "That expressivity must cover real-world concurrency and fault-tolerance.", "And, as we wish to build services quickly, we wanted a language that is simple to learn and apply, avoiding esoteric concepts.", "We also very much wanted an existing ecosystem of tools.", "They ‘found what we were looking for in TLA+, a formal specification language.’  Benefits  TLA+ helped AWS find subtle bugs in S3, DynamoDB, EBS, and an internal distributed lock manager, as well as verifying several optimizations.", "The effects go deeper than just finding bugs though.", "TLA+ has been helping us shift to a better way of designing systems.", "The process begins by stating clearly ‘what needs to go right?’ by defining correctness properties.", "Safety properties define what the system is allowed to do, and liveness properties define what the system must eventually do.", "The benefits are not just in the initial system design, but over the lifetime of the system.", "We have found that writing a formal specification pays several dividends over the lifetime of the system.", "All production services at Amazon are under constant development, even those released years ago; we add new features that customers have requested, we re-design components to handle massive increases in scale, and we improve performance by removing bottlenecks.", "Many of these changes are complex, and they must be made to the running system with no downtime.", "Our first priority is always to avoid causing bugs in a production system, so we often need to answer the question, “is this change safe?” We have found that a major benefit of having a precise, testable model of the core system is that we can rapidly verify that even deep changes are safe, or learn that they are unsafe without doing any harm.", "In several cases we have prevented subtle, serious bugs from reaching production.", "In other cases we have been able to make innovative performance optimizations – e.g. removing or narrowing locks, or weakening constraints on message ordering – which we would not have dared to do without having model checked those changes.", "Spreading the word  Success breeds success, but it’s best not to scare people off too early:  …we have found that software engineers more readily grasp the concept and practical value of TLA+ if we dub it: exhaustively testable pseudo-code.", "We initially avoid the words ‘formal’, ‘verification’, and ‘proof’, due to the widespread view that formal methods are impractical.", "Formal methods seem to be becoming an important part of the AWS processes:  At AWS, formal methods have been a big success.", "They have helped us prevent subtle, serious bugs from reaching production, bugs that we would not have found via any other technique.", "They have helped us to make aggressive optimizations to complex algorithms without sacrificing quality.", "So far, seven teams have used TLA+, and all have found high value in doing so.", "At the time of writing, more teams are starting to use TLA+.", "We believe that use of TLA+ will accelerate both time-to-market and quality of these projects.", "Executive management is now proactively encouraging teams to write TLA+ specs for new features and other significant design changes.", "In annual planning, managers are now allocating engineering time to use TLA+.", "Summary  It’s great to see the value of formal methods being recognised once more.", "This paper is also a good insight into the distributed systems design methodology at AWS.", "It adds another dimension to AWS’s challenge to the rest of the industry: keep up if you can!"], "summary_text": "Use of Formal Methods at Amazon Web Services – Newcombe et al 2014  Leslie Lamport recently gave a talk at the React conference on the specification language TLA. I wasn’t there to hear the talk, but I was intrigued enough to dig in and find out a little more. Especially since I have some experience with Z and CSP. My journey led me to today’s paper choice, a wonderful find and something that I hope spreads further into accepted industry practice. Something a lot of people believe…  In industry, formal methods have a reputation of requiring a huge amount of training and effort to verify a tiny piece of relatively straightforward code, so the return on investment is only justified in safety- critical domains such as medical systems and avionics. That isn’t necessarily so:  Our experience with TLA+ has shown that perception to be quite wrong. So far we have used TLA+ on 10 large complex real-world systems. In every case TLA+ has added significant value, either finding subtle bugs that we are sure we would not have found by other means, or giving us enough understanding and confidence to make aggressive performance optimizations without sacrificing correctness. We now have 7 teams using TLA+, with encouragement from senior management and technical leadership. Engineers from entry level to Principal have been able to learn TLA+ from scratch and get useful results in 2 to 3 weeks, in some cases just in their personal time on weekends and evenings, and without help or training. The AWS teams have been using formal specification since 2011, and the paper tells the story of how this came to be and how their use has spread. It’s highly readable so I encourage you to check it out. I will focus on the rationale and benefits for this short summary. Building distributed systems is hard  S3 is just one of tens of AWS services that store and process data that our customers have entrusted to us. To safeguard that data, the core of each service relies on fault-tolerant distributed algorithms for replication, consistency, concurrency control, auto-scaling, load balancing, and other coordination tasks. When it comes to building real production systems, there’s more to it than just coding up algorithms from the literature:  There are many such algorithms in the literature, but combining them into a cohesive system is a major challenge, as the algorithms must usually be modified in order to interact properly in a real-world system. In addition, we have found it necessary to invent algorithms of our own. We work hard to avoid unnecessary complexity, but the essential complexity of the task remains high. How can you gain confidence that your system is correct? AWS used multiple verification methods (short of formal methods), but still weren’t finding all the subtle bugs that may lurk. before launching such a service, we need to reach extremely high confidence that the core of the system is correct. We have found that the standard verification techniques in industry are necessary but not sufficient. We use deep design reviews, code reviews, static code analysis, stress testing, fault-injection testing, and many other techniques, but we still find that subtle bugs can hide in complex concurrent fault-tolerant systems. Some of the most difficult to detect bugs are not in the code, but in the design…  some of the more subtle, dangerous bugs turn out to be errors in design; the code faithfully implements the intended design, but the design fails to correctly handle a particular ‘rare’ scenario. Finding a precise design language  In order to find subtle bugs in a system design, it is necessary to have a precise description of that design. There are at least two major benefits to writing a precise design; the author is forced to think more clearly, which helps eliminate ‘plausible hand-waving’, and tools can be applied to check for errors in the design, even while it is being written. AWS needed a precise design language, but also a pragmatic one:  As our designs are unavoidably complex, we needed a highly expressive language, far above the level of code, but with precise semantics. That expressivity must cover real-world concurrency and fault-tolerance. And, as we wish to build services quickly, we wanted a language that is simple to learn and apply, avoiding esoteric concepts. We also very much wanted an existing ecosystem of tools. They ‘found what we were looking for in TLA+, a formal specification language.’  Benefits  TLA+ helped AWS find subtle bugs in S3, DynamoDB, EBS, and an internal distributed lock manager, as well as verifying several optimizations. The effects go deeper than just finding bugs though. TLA+ has been helping us shift to a better way of designing systems. The process begins by stating clearly ‘what needs to go right?’ by defining correctness properties. Safety properties define what the system is allowed to do, and liveness properties define what the system must eventually do. The benefits are not just in the initial system design, but over the lifetime of the system. We have found that writing a formal specification pays several dividends over the lifetime of the system. All production services at Amazon are under constant development, even those released years ago; we add new features that customers have requested, we re-design components to handle massive increases in scale, and we improve performance by removing bottlenecks. Many of these changes are complex, and they must be made to the running system with no downtime. Our first priority is always to avoid causing bugs in a production system, so we often need to answer the question, “is this change safe?” We have found that a major benefit of having a precise, testable model of the core system is that we can rapidly verify that even deep changes are safe, or learn that they are unsafe without doing any harm. In several cases we have prevented subtle, serious bugs from reaching production. In other cases we have been able to make innovative performance optimizations – e.g. removing or narrowing locks, or weakening constraints on message ordering – which we would not have dared to do without having model checked those changes. Spreading the word  Success breeds success, but it’s best not to scare people off too early:  …we have found that software engineers more readily grasp the concept and practical value of TLA+ if we dub it: exhaustively testable pseudo-code. We initially avoid the words ‘formal’, ‘verification’, and ‘proof’, due to the widespread view that formal methods are impractical. Formal methods seem to be becoming an important part of the AWS processes:  At AWS, formal methods have been a big success. They have helped us prevent subtle, serious bugs from reaching production, bugs that we would not have found via any other technique. They have helped us to make aggressive optimizations to complex algorithms without sacrificing quality. So far, seven teams have used TLA+, and all have found high value in doing so. At the time of writing, more teams are starting to use TLA+. We believe that use of TLA+ will accelerate both time-to-market and quality of these projects. Executive management is now proactively encouraging teams to write TLA+ specs for new features and other significant design changes. In annual planning, managers are now allocating engineering time to use TLA+. Summary  It’s great to see the value of formal methods being recognised once more. This paper is also a good insight into the distributed systems design methodology at AWS. It adds another dimension to AWS’s challenge to the rest of the industry: keep up if you can!", "pdf_url": "http://research.microsoft.com/en-us/um/people/lamport/tla/formal-methods-amazon.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/use-of-formal-methods-at-amazon-web-services.json"}
{"id": "52464539", "bin": "1400_1500", "summary_sentences": ["Chimera: Large-Scale Classification Using Machine Learning, Rules, and Crowdsourcing – Sun et al. 2014 (WalmartLabs)  Large-scale classification, where we need to classify hundreds of thousands or millions of items into thousands of classes, is becoming increasingly common in this age of Big Data… So far, however, very little has been published on how large-scale classification has been carried out in practice, even though there are many interesting questions about such cases.", "Today’s paper is a case study on large-scale classification of products at Walmart.", "The requirement is to classify 10M+ products into 5000+ categories based on fairly minimal product descriptions.", "Oh, and new products turn up all the time, and the set of categories is continuously evolving.", "Many learning solutions assume that we can take a random sample from the universe of items, manually label the sample to create training data, then train a classifier.", "At this scale, however, we do not even know the universe of items, as product descriptions keep “trickling in”, a few tens of thousands or hundreds of thousands at a time… concept drift becomes common (e.g., the notion “computer cables” keeps drifting because new types of computer cables keep appearing).", "Walmart vendors submit product descriptions for their new products continuously.", "The way in which they fill-in the product detail information varies wildly from vendor to vendor, and asking them to be more thorough or detailed doesn’t really work.", "So the only field that classification really can rely on is ‘Title’.", "Creating training data manually is time-consuming – a good analyst can classify about 100 items a day.", "(Matches to the correct category can take some research – where do you put a ‘Dynomax Exhaust 17656 Thrush Welded Muffler’ when you have 150 automotive categories and none of them are ‘Mufflers’?).", "Given the rate of 100 product items per day per analyst, it would take 200 days for a team of 5 analysts to manually classify 100,000 items.", "Thus, asking analysts to manually classify incoming product items is clearly not a practical solution.", "To label just 200 items per product category as training data would require labeling 1M product items.", "Outsourcing (in-place of using in-house analysts) is prohibitively expensive – at $10/hour it would cost Walmart $770K to get 1 million items classified, “an unacceptable cost to us.” Crowdsourcing (E.g. Mechanical Turk) doesn’t work very well because the classification task is too complex compared to the preferred micro tasks on those platforms that take a few tens of seconds to answer yes or no.", "If you can’t get enough training data, perhaps a rules-based approach will work instead?", "“But writing rules to cover all 5000+ product types is a very slow and daunting process.", "In fact, we did not find it to be scalable.”  Since none of the approaches (manual classification, machine learning, and rules) can solve the problem in isolation, Walmarts Chimera system uses all three in combination:  Chimera uses a combination of machine learning, hand-crafted rules, developers, analysts, and crowd workers to form a solution that continuously improves over time, and that keeps precision high while trying to improve recall.", "Chimera is initialised using a basic set of training data and hand-crafted rules supplied by analysts.", "The learning-based classifiers are trained using this initial data,  and the system then proceeds to iterate as follows:  Given a set of incoming product items, classify them, then use crowdsourcing to continuously evaluate the results and flag cases judged incorrect by the crowd.", "The analysts examine the flagged cases, and fix them by writing new rules, relabeling certain items, and alerting the developers.", "The newly created rules and the relabeled items are incorporated into the system, and the developers may fine tune the underlying automatic algorithm.", "For those items that the system refuses to classify (due to a lack of confidence), the analysts examine them, then create hand-crafted rules as well as training data (i.e., labeled items) to target those cases.", "The newly created rules and training data are incorporated into the system, and it is run again over the product items.", "Through this loop, the system continuously improves its performance.", "The big picture looks like this:  A new product item is first examined by the GateKeeper.", "If the GateKeeper can trivially classify it with high confidence (for example, it’s an exact match to training data) then the result is immediately sent to the results processing phase (for sampling & possible evaluation).", "Otherwise the item is passed to three different classification systems in parallel:  A rules-based system that uses white-list and black-list regexs on product title text.", "(Yes, the thought of thousands of regexs sounds a handful, but each regex is quite simple based on the examples shown.", "For example: ‘wedding bands?", "-> rings).", "An attribute and value-based classifier that examines attributes and uses a rules-based approach to classify based on them.", "For example, if the product has an ISBN, classify it as a book.", "A machine learning ensemble using naive-Bayes, K-Nearest Neighbours, and Perceptron classifiers.", "Given an item, all classifiers will make predictions (each prediction is a set of product types optionally with weights).", "The Voting Master and the Filter combine these predictions into a final prediction.", "The pair (product item, final predicted product type) is then added to the result set.", "Why is there a need for a filter step after the voting master?", "Once the voting master has produced a combined ranked list of product types, the filter applies a set of rules (created by the analysts) to exert a final control over the output types.", "Note that analysts can already control the rule-based classifiers.", "But without the filter, they do not have a way to control the output of the learning-based classifier as well as the voting master, and this can produce undesired cases.", "For example, learning-based classifiers may keep misclassifying “necklace pendant” as of type “necklace” (because we have many items of type “necklace” in the training data that do contain the word “necklace”).", "As a result, the voting master may produce “necklace” as the output type for “necklace pendant”.", "The analysts can easily address this case by adding a rule such as “pendant → NOT necklace” to the filter.", "Items that could not be classified with sufficient confidence are passed to the in-house analysts.", "Items that have been classified with sufficient confidence are randomly sampled to be sent to crowdsourced workers for category verification.", "If the crowd indicates an item was wrongly classified it is sent to the analysts who then create rules to correctly classify it in the future.", "As of March 2014, the system had been stable for about 6 months and classified about 2.5M items from marketplace vendors.", "Overall we managed to classify more than 90% of them with 92% precision.", "Chimera has also been applied to 14M items from walmart.com.", "Overall it classified 93% of them with 93% precision.", "The authors summarise six main lessons learned while building Chimera:  Things break down at large scale  Both learning and hand-crafted rules are critical  Crowdsourcing is critical, but must be closely monitored  Crowdsourcing must be coupled with in-house analysts and developers  Outsourcing does not work at a very large scale  Hybrid human-machine systems are here to stay…  While academia has only recently started exploring such systems, largely in the context of crowdsourcing, they have been used for years in industry, as evidenced by Chimera and other systems that we know.", "Such systems use not just crowd workers, but also analysts, and developers, and treat them as “first-class citizens”.", "They have been quite successful, and deserve much more attention and closer studies."], "summary_text": "Chimera: Large-Scale Classification Using Machine Learning, Rules, and Crowdsourcing – Sun et al. 2014 (WalmartLabs)  Large-scale classification, where we need to classify hundreds of thousands or millions of items into thousands of classes, is becoming increasingly common in this age of Big Data… So far, however, very little has been published on how large-scale classification has been carried out in practice, even though there are many interesting questions about such cases. Today’s paper is a case study on large-scale classification of products at Walmart. The requirement is to classify 10M+ products into 5000+ categories based on fairly minimal product descriptions. Oh, and new products turn up all the time, and the set of categories is continuously evolving. Many learning solutions assume that we can take a random sample from the universe of items, manually label the sample to create training data, then train a classifier. At this scale, however, we do not even know the universe of items, as product descriptions keep “trickling in”, a few tens of thousands or hundreds of thousands at a time… concept drift becomes common (e.g., the notion “computer cables” keeps drifting because new types of computer cables keep appearing). Walmart vendors submit product descriptions for their new products continuously. The way in which they fill-in the product detail information varies wildly from vendor to vendor, and asking them to be more thorough or detailed doesn’t really work. So the only field that classification really can rely on is ‘Title’. Creating training data manually is time-consuming – a good analyst can classify about 100 items a day. (Matches to the correct category can take some research – where do you put a ‘Dynomax Exhaust 17656 Thrush Welded Muffler’ when you have 150 automotive categories and none of them are ‘Mufflers’?). Given the rate of 100 product items per day per analyst, it would take 200 days for a team of 5 analysts to manually classify 100,000 items. Thus, asking analysts to manually classify incoming product items is clearly not a practical solution. To label just 200 items per product category as training data would require labeling 1M product items. Outsourcing (in-place of using in-house analysts) is prohibitively expensive – at $10/hour it would cost Walmart $770K to get 1 million items classified, “an unacceptable cost to us.” Crowdsourcing (E.g. Mechanical Turk) doesn’t work very well because the classification task is too complex compared to the preferred micro tasks on those platforms that take a few tens of seconds to answer yes or no. If you can’t get enough training data, perhaps a rules-based approach will work instead? “But writing rules to cover all 5000+ product types is a very slow and daunting process. In fact, we did not find it to be scalable.”  Since none of the approaches (manual classification, machine learning, and rules) can solve the problem in isolation, Walmarts Chimera system uses all three in combination:  Chimera uses a combination of machine learning, hand-crafted rules, developers, analysts, and crowd workers to form a solution that continuously improves over time, and that keeps precision high while trying to improve recall. Chimera is initialised using a basic set of training data and hand-crafted rules supplied by analysts. The learning-based classifiers are trained using this initial data,  and the system then proceeds to iterate as follows:  Given a set of incoming product items, classify them, then use crowdsourcing to continuously evaluate the results and flag cases judged incorrect by the crowd. The analysts examine the flagged cases, and fix them by writing new rules, relabeling certain items, and alerting the developers. The newly created rules and the relabeled items are incorporated into the system, and the developers may fine tune the underlying automatic algorithm. For those items that the system refuses to classify (due to a lack of confidence), the analysts examine them, then create hand-crafted rules as well as training data (i.e., labeled items) to target those cases. The newly created rules and training data are incorporated into the system, and it is run again over the product items. Through this loop, the system continuously improves its performance. The big picture looks like this:  A new product item is first examined by the GateKeeper. If the GateKeeper can trivially classify it with high confidence (for example, it’s an exact match to training data) then the result is immediately sent to the results processing phase (for sampling & possible evaluation). Otherwise the item is passed to three different classification systems in parallel:  A rules-based system that uses white-list and black-list regexs on product title text. (Yes, the thought of thousands of regexs sounds a handful, but each regex is quite simple based on the examples shown. For example: ‘wedding bands? -> rings). An attribute and value-based classifier that examines attributes and uses a rules-based approach to classify based on them. For example, if the product has an ISBN, classify it as a book. A machine learning ensemble using naive-Bayes, K-Nearest Neighbours, and Perceptron classifiers. Given an item, all classifiers will make predictions (each prediction is a set of product types optionally with weights). The Voting Master and the Filter combine these predictions into a final prediction. The pair (product item, final predicted product type) is then added to the result set. Why is there a need for a filter step after the voting master? Once the voting master has produced a combined ranked list of product types, the filter applies a set of rules (created by the analysts) to exert a final control over the output types. Note that analysts can already control the rule-based classifiers. But without the filter, they do not have a way to control the output of the learning-based classifier as well as the voting master, and this can produce undesired cases. For example, learning-based classifiers may keep misclassifying “necklace pendant” as of type “necklace” (because we have many items of type “necklace” in the training data that do contain the word “necklace”). As a result, the voting master may produce “necklace” as the output type for “necklace pendant”. The analysts can easily address this case by adding a rule such as “pendant → NOT necklace” to the filter. Items that could not be classified with sufficient confidence are passed to the in-house analysts. Items that have been classified with sufficient confidence are randomly sampled to be sent to crowdsourced workers for category verification. If the crowd indicates an item was wrongly classified it is sent to the analysts who then create rules to correctly classify it in the future. As of March 2014, the system had been stable for about 6 months and classified about 2.5M items from marketplace vendors. Overall we managed to classify more than 90% of them with 92% precision. Chimera has also been applied to 14M items from walmart.com. Overall it classified 93% of them with 93% precision. The authors summarise six main lessons learned while building Chimera:  Things break down at large scale  Both learning and hand-crafted rules are critical  Crowdsourcing is critical, but must be closely monitored  Crowdsourcing must be coupled with in-house analysts and developers  Outsourcing does not work at a very large scale  Hybrid human-machine systems are here to stay…  While academia has only recently started exploring such systems, largely in the context of crowdsourcing, they have been used for years in industry, as evidenced by Chimera and other systems that we know. Such systems use not just crowd workers, but also analysts, and developers, and treat them as “first-class citizens”. They have been quite successful, and deserve much more attention and closer studies.", "pdf_url": "http://pages.cs.wisc.edu/~anhai/papers/chimera-vldb14.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/chimera.json"}
{"id": "68340528", "bin": "1400_1500", "summary_sentences": ["Beyond news contents: the role of social context for fake news detection Shu et al., WSDM’19  Today we’re looking at a more general fake news problem: detecting fake news that is being spread on a social network.", "Forgetting the computer science angle for a minute, it seems intuitive to me that some important factors here might be:  what is being said (the content of the news), and perhaps how it is being said (although fake news can be deliberately written to mislead users by mimicking true news)  where it was published (the credibility / authority of the source publication).", "For example, something in the Financial Times is more likely to be true than something in The Onion!", "who is spreading the news (the credibility of the user accounts retweeting it for example – are they bots??)", "Therefore I’m a little surprised to read in the introduction that:  The majority of existing detection algorithms focus on finding clues from the news content, which are generally not effective because fake news is often intentionally written to mislead users by mimicking true news.", "(The related work section does however discuss several works that include social context.).", "So instead of just looking at the content, we should also look at the social context: the publishers and the users spreading the information!", "The fake news detection system developed in this paper, TriFN considers tri-relationships between news pieces, publishers, and social network users.", "… we are to our best knowledge the first to classify fake news by learning the effective news features through the tri-relationship embedding among publishers, news contents, and social engagements.", "And guess what, considering publishers and users does indeed turn out to improve fake news detection!", "Inputs  We have  publishers,  social network users, and  news articles.", "Using a vocabulary of t words, we can compute an  bag-of-word feature matrix.", "For the m users, we can have an m x m adjacency matrix  , where  is 1 if i and j are friends, and 0 otherwise.", "We also know which users have shared which news pieces, this is encoded in a matrix  .", "The matrix  similarly encodes which publishers have published which news pieces.", "For some publishers, we can know their partisan bias.", "In this work, bias ratings from mediabiasfactcheck.com are used, taking just the ‘Left-Bias’, ‘Least-Bias’ (neutral) and ‘Right-Bias’ values (ignoring the intermediate left-center and right-center values) and encoding these as -1, 0, and 1 respectively in a publisher partisan label vector,  .", "Not every publisher will have a bias rating available.", "We’d like to put ‘-’ in the entry for that publisher in  but since we can’t do that, the separate vector  encodes whether or not we have a bias rating available for publisher p.  There’s one last thing at our disposal: a labelled dataset for news articles telling us whether they are fake or not.", "(Here we have just the news article content, not the social context).", "The Tri-relationship embedding framework  TriFN takes all of those inputs and combines them with a fake news binary classifier.", "Given lots of users and lots of news articles, we can expect some of the raw inputs to be pretty big, so the authors make heavy use of dimensionality reduction using non-negative matrix factorisation to learn latent space embeddings (more on that in a minute!)", "TriFN combines:  A news content embedding  A user embedding  A user-news interaction embedding  A publisher-news interaction embedding, and  The prediction made by a linear classifier trained on the labelled fake news dataset  Pictorially it looks like this (with apologies for the poor resolution, which is an artefact of the original):  News content embedding  Let’s take a closer look at non-negative matrix factorisation (NMF) to see how this works to reduce dimensionality.", "Remember the bag-of-words sketch for news articles?", "That’s an n x t matrix where n is the number of news articles and t is the number of words in the vocabulary.", "NMF tries to learn a latent embedding that captures the information in the matrix in a much smaller space.", "In the general form NMF seeks to factor a (non-negative) matrix M into the product of two (non-negative) matrices W and H (or D and V as used in this paper).", "How does that help us?", "We can pick some dimension d (controlling the size of the latent space) and break down the  matrix into a d-dimension representation of news articles  , and a d-dimension representation of words in the vocabulary,  .", "That means that  has shape  and so  ends up with the desired shape  .", "Once we’ve learned a good representation of news articles,  we can use those as the news content embeddings within TriFN.", "We’d like to get  as close to  as we can, and at the same time keep  and  ‘sensible’ to avoid over-fitting.", "We can do that with a regularisation term.", "So the overall optimisation problem looks like this:  User embedding  For the user embedding there’s a similar application of NMF, but in this case we’re splitting the adjacency matrix  into a user latent matrix  , and a user correlation matrix  .", "So in this case we’re using NMF to learn  which has shape mxd .", "dxd .", "dxm, resulting in the desired mxm shape.", "There’s also a user-user relation matrix  which  controls the contribution of  .", "The basic idea is that any given user will only share a small fraction of news articles, so a positive case (having shared an article) should have more weight than a negative case (not having shared).", "User-news interaction embedding  For the user-news interaction embedding we want to capture the relationship between user features and the labels of news items.", "The intuition is that users with low credibility are more likely to spread fake news.", "So how do we get user credibility?", "Following ‘ Measuring user credibility in social media ’ the authors base this on similarity to other users.", "First users are clustered into groups such that members of the same cluster all tend to share the same news stories.", "Then each cluster is given a credibility score based on its relative size.", "Users take on the credibility score of the cluster they belong to.", "It all seems rather vulnerable to the creation of large numbers of fake bot accounts that collaborate to spread fake news if you ask me.", "Nevertheless, assuming we have reliable credibility scores then we want to set things up such that the latent features of high-credibility users are close to true news, and the latent features of low-credibility users are close to fake news.", "Publisher-news embeddings  Recall we have the matrix  encoding which publishers have published which news pieces.", "Let  be the normalised version of the same.", "We want to find  , a weighting matrix mapping news publisher’s latent features to the corresponding partisan label vector  .", "It looks like this:  Semi-supervised linear classifier  Using the labelled data available, we also learn a weighting matrix  mapping news latent features to fake news labels.", "Putting it all together  The overall objective becomes to find matrices  using a weighted combination of each of the above embedding formulae, and a regularisation term combining all of the learned matrices.", "It looks like this:  and it’s trained like this:  Evaluation  TriFN is evaluated against several state of the art fake news detection methods using the FakeNewsNet BuzzFeed and PolitiFact datasets.", "It gives the best performance on both of them:  ( Enlarge )"], "summary_text": "Beyond news contents: the role of social context for fake news detection Shu et al., WSDM’19  Today we’re looking at a more general fake news problem: detecting fake news that is being spread on a social network. Forgetting the computer science angle for a minute, it seems intuitive to me that some important factors here might be:  what is being said (the content of the news), and perhaps how it is being said (although fake news can be deliberately written to mislead users by mimicking true news)  where it was published (the credibility / authority of the source publication). For example, something in the Financial Times is more likely to be true than something in The Onion! who is spreading the news (the credibility of the user accounts retweeting it for example – are they bots??) Therefore I’m a little surprised to read in the introduction that:  The majority of existing detection algorithms focus on finding clues from the news content, which are generally not effective because fake news is often intentionally written to mislead users by mimicking true news. (The related work section does however discuss several works that include social context.). So instead of just looking at the content, we should also look at the social context: the publishers and the users spreading the information! The fake news detection system developed in this paper, TriFN considers tri-relationships between news pieces, publishers, and social network users. … we are to our best knowledge the first to classify fake news by learning the effective news features through the tri-relationship embedding among publishers, news contents, and social engagements. And guess what, considering publishers and users does indeed turn out to improve fake news detection! Inputs  We have  publishers,  social network users, and  news articles. Using a vocabulary of t words, we can compute an  bag-of-word feature matrix. For the m users, we can have an m x m adjacency matrix  , where  is 1 if i and j are friends, and 0 otherwise. We also know which users have shared which news pieces, this is encoded in a matrix  . The matrix  similarly encodes which publishers have published which news pieces. For some publishers, we can know their partisan bias. In this work, bias ratings from mediabiasfactcheck.com are used, taking just the ‘Left-Bias’, ‘Least-Bias’ (neutral) and ‘Right-Bias’ values (ignoring the intermediate left-center and right-center values) and encoding these as -1, 0, and 1 respectively in a publisher partisan label vector,  . Not every publisher will have a bias rating available. We’d like to put ‘-’ in the entry for that publisher in  but since we can’t do that, the separate vector  encodes whether or not we have a bias rating available for publisher p.  There’s one last thing at our disposal: a labelled dataset for news articles telling us whether they are fake or not. (Here we have just the news article content, not the social context). The Tri-relationship embedding framework  TriFN takes all of those inputs and combines them with a fake news binary classifier. Given lots of users and lots of news articles, we can expect some of the raw inputs to be pretty big, so the authors make heavy use of dimensionality reduction using non-negative matrix factorisation to learn latent space embeddings (more on that in a minute!) TriFN combines:  A news content embedding  A user embedding  A user-news interaction embedding  A publisher-news interaction embedding, and  The prediction made by a linear classifier trained on the labelled fake news dataset  Pictorially it looks like this (with apologies for the poor resolution, which is an artefact of the original):  News content embedding  Let’s take a closer look at non-negative matrix factorisation (NMF) to see how this works to reduce dimensionality. Remember the bag-of-words sketch for news articles? That’s an n x t matrix where n is the number of news articles and t is the number of words in the vocabulary. NMF tries to learn a latent embedding that captures the information in the matrix in a much smaller space. In the general form NMF seeks to factor a (non-negative) matrix M into the product of two (non-negative) matrices W and H (or D and V as used in this paper). How does that help us? We can pick some dimension d (controlling the size of the latent space) and break down the  matrix into a d-dimension representation of news articles  , and a d-dimension representation of words in the vocabulary,  . That means that  has shape  and so  ends up with the desired shape  . Once we’ve learned a good representation of news articles,  we can use those as the news content embeddings within TriFN. We’d like to get  as close to  as we can, and at the same time keep  and  ‘sensible’ to avoid over-fitting. We can do that with a regularisation term. So the overall optimisation problem looks like this:  User embedding  For the user embedding there’s a similar application of NMF, but in this case we’re splitting the adjacency matrix  into a user latent matrix  , and a user correlation matrix  . So in this case we’re using NMF to learn  which has shape mxd . dxd . dxm, resulting in the desired mxm shape. There’s also a user-user relation matrix  which  controls the contribution of  . The basic idea is that any given user will only share a small fraction of news articles, so a positive case (having shared an article) should have more weight than a negative case (not having shared). User-news interaction embedding  For the user-news interaction embedding we want to capture the relationship between user features and the labels of news items. The intuition is that users with low credibility are more likely to spread fake news. So how do we get user credibility? Following ‘ Measuring user credibility in social media ’ the authors base this on similarity to other users. First users are clustered into groups such that members of the same cluster all tend to share the same news stories. Then each cluster is given a credibility score based on its relative size. Users take on the credibility score of the cluster they belong to. It all seems rather vulnerable to the creation of large numbers of fake bot accounts that collaborate to spread fake news if you ask me. Nevertheless, assuming we have reliable credibility scores then we want to set things up such that the latent features of high-credibility users are close to true news, and the latent features of low-credibility users are close to fake news. Publisher-news embeddings  Recall we have the matrix  encoding which publishers have published which news pieces. Let  be the normalised version of the same. We want to find  , a weighting matrix mapping news publisher’s latent features to the corresponding partisan label vector  . It looks like this:  Semi-supervised linear classifier  Using the labelled data available, we also learn a weighting matrix  mapping news latent features to fake news labels. Putting it all together  The overall objective becomes to find matrices  using a weighted combination of each of the above embedding formulae, and a regularisation term combining all of the learned matrices. It looks like this:  and it’s trained like this:  Evaluation  TriFN is evaluated against several state of the art fake news detection methods using the FakeNewsNet BuzzFeed and PolitiFact datasets. It gives the best performance on both of them:  ( Enlarge )", "pdf_url": "http://www.public.asu.edu/~skai2/files/wsdm_2019_fake_news.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/beyond-news-contents-the-role-of-social-context-for-fake-news-detection.json"}
{"id": "92615357", "bin": "1400_1500", "summary_sentences": ["Watching for software inefficiencies with Witch Wen et al., ASPLOS’18  (The link above is to the ACM Digital Library, if you don’t have membership you should still be able to access the paper pdf by following the link from The Morning Paper blog post directly.)", "Inefficiencies abound in complex, layered software.", "These inefficiencies can arise during design (poor choice of algorithm), implementation, or translation (e.g., compiler optimisations or lack thereof).", "At the level of the hardware, inefficiencies involving the memory subsystem are some of the most costly…  Repeated initialization, register spill and restore on hot paths, lack of inlining hot functions, missed optimization opportunities due to aliasing, computing and storing already computed or sparingly changing values, and contention and false sharing (in multi-threaded codes), are some of the common prodigal uses of the memory subsystem.", "Coarse grained profilers (e.g., gprof) have comparatively little overhead and can detect hotspots, but fail to distinguish between efficient and inefficient resource usage.", "Fine-grained profilers (e.g. DeadSpy) can detect inefficiencies, but typically introduce high overheads (10-80x slowdown and 6-100x extra memory).", "These high overheads prevent such tools from being widely used.", "Witch is a fine-grained inefficiency detection framework with low overhead, a trick it pulls off by sampling hardware PMUs (performance monitoring units) to gather its data.", "Our key observation is that an important class of inefficiency detection schemes, explored previously via fine-grained profilers, requires monitoring consecutive accesses to the same memory location.", "For example, detecting repeated initialization— a dead write— requires monitoring store after store without an intervening load to the same location.", "Witch is implemented as part of the open source HPCToolkit performance analysis tools suite.", "It comprises the base framework, on top of which various detection tools can be implemented.", "Three such tools are described in the paper:  DeadCraft detects dead stores, a store followed by another store to the same address with on intervening load  SilentCraft detects stores that update a location with a value already present at the location (i.e., useless stores).", "LoadCraft detects loads followed by another load from the same location where the value remains unchanged between the two loads.", "Not every instance of these situations is a problem of course, but they can be very useful in pointing developers in the right direction.", "Let’s look at an example applying SilentCraft to the Caffe deep learning framework.", "SilentCraft reports that 25% of all memory stores in a key loop of the pooling and normalisation layers are redundant.", "This is a clue to look at the code and see what is going on.", "Investigation reveals that a large portion of elements in top_diff are zeroes, effectively leading to execution of bottom_diff[index] += 0 in line 8.", "Adding a zero check for the value in top_diff can eliminate a division, an addition, and a memory store.", "This change speeds up the pooling layer by 1.16x, and the normalization layers by 1.34x.", "Using a near-zero check (less than 1e-7) gives a 2% accuracy loss, but yields 1.16x and 2.23x speedups for pooling and normalization respectively.", "(A 6% speedup over the program as a whole).", "The NWChem computational chemistry package provides an illustration of the benefits of detecting dead stores with DeadCraft.", "DeadCraft reveals that 60% of memory stores are dead,  with 94% of those dead stores due to one store pair in the call of the dfill function.", "More than 200K calls to dfill are made, writing 500GB of data that is never used.", "Analysis revealed that the size of the work2 array was larger than necessary, and that the zero initialisation (the cause of all those stores) was also unnecessary.", "Eliminating it lead to a 1.43x speedup.", "For a LoadCraft example we can turn to GNU Binutils-2.27.", "LoadCraft identifies 96% of the loads in this program as loading the same value from the same location.", "The culprit is a linear scan over addresses in a function table.", "Replacing the linked list with a sorted array and using a binary search instead sped up execution by 10x.", "We’ve seen how Witch can help programmers focus their attention on interesting parts of the codebase.", "Now let’s take a look at how it works under the covers…  PMU Sampling  Hardware performance monitoring units in CPUs offer a programmable way to count hardware events such as loads and stores, and hardware debug registers can trap execution when the PC reaches an address, or an instruction accesses a designated address (a watchpoint).", "Linux offers a standard interface to program and sample PMUs.", "PMU samples that include the effective address accessed in a sample provide the knowledge of the addresses accessed in an execution.", "Given this effective address, a hardware debug register allows us to keep an eye on (watch) a location and recognize what the program subsequently does to such a location.", "The following figure shows how all the pieces fit together in the context of dead store detection (the DeadCraft client):  Intervening accesses  We can only monitor a small number of locations at a time (e.g., four hardware debug registers), so reservoir sampling allows us to monitor a subset of previously seen addresses without any bias.", "Two accesses to the same memory address, separated by many PMU samples in the intervening time, present an issue.", "Once all the watchpoints are in use, a simple ‘replace the oldest watchpoint’ scheme will most likely not detect e.g. dead stores, separated by such a distance.", "Monitoring a new sample may help detect a new, previously unseen problem whereas continuing to monitor an old, already-armed address may help detect a problem separated by many intervening operations.", "We should detect both.", "The solution uses reservoir sampling and relies on multiple unbiased samples taken over a repetitive execution to capture both scenarios.", "Since only counts of previous samples are maintained the technique needs only O(1) memory.", "In practice, the “blindspot window” (number of consecutive unmonitored PMU samples) for many applications is very short – e.g., less than 0.02% of the total samples in the SPEC CPU2006 benchmarks.", "Proportional attribution  Consider the following code fragment:  There are many dead stores in the i-loop (line 3) due to the overwriting j-loop (line 11).", "However, only a few watchpoints survive between these two loops because of all the watchpoints consumed in the middle loop (lines 7-8).", "Without correcting for this sampling imbalance, a disproportionately high dead write count will be recorded for the line pairs (7,8) and (8,7) compared to the rest.", "We solve this problem with a context-sensitive approximation.", "The code behaviour is typically the same in a calling context; hence, an observation made by monitoring an address accessed in a calling context can approximately represent other unmonitored samples occurring in the same calling context.", "If in a sequence of N samples occurring in a calling context C, only one sample is monitored through a debug register, we scale the observation made for the monitored sample by N to approximated the behaviour of the remaining N-1 unmonitored samples taken at C.  Accuracy and overheads  The following charts show the accuracy of DeadCraft, SilentCraft, and LoadCraft at different sampling rates, as compared to ground truth exhaustive monitoring.", "Clearly, the sampling rate, when chosen with some care, does not significantly affect the results.", "Unsurprisingly, sampling also has much lower overheads than exhaustive monitoring, as shown in this table.", "( Enlarge )  Perhaps more interesting is the overhead compared to normal (i.e., non-instrumented) execution, which is typically less than 5%:"], "summary_text": "Watching for software inefficiencies with Witch Wen et al., ASPLOS’18  (The link above is to the ACM Digital Library, if you don’t have membership you should still be able to access the paper pdf by following the link from The Morning Paper blog post directly.) Inefficiencies abound in complex, layered software. These inefficiencies can arise during design (poor choice of algorithm), implementation, or translation (e.g., compiler optimisations or lack thereof). At the level of the hardware, inefficiencies involving the memory subsystem are some of the most costly…  Repeated initialization, register spill and restore on hot paths, lack of inlining hot functions, missed optimization opportunities due to aliasing, computing and storing already computed or sparingly changing values, and contention and false sharing (in multi-threaded codes), are some of the common prodigal uses of the memory subsystem. Coarse grained profilers (e.g., gprof) have comparatively little overhead and can detect hotspots, but fail to distinguish between efficient and inefficient resource usage. Fine-grained profilers (e.g. DeadSpy) can detect inefficiencies, but typically introduce high overheads (10-80x slowdown and 6-100x extra memory). These high overheads prevent such tools from being widely used. Witch is a fine-grained inefficiency detection framework with low overhead, a trick it pulls off by sampling hardware PMUs (performance monitoring units) to gather its data. Our key observation is that an important class of inefficiency detection schemes, explored previously via fine-grained profilers, requires monitoring consecutive accesses to the same memory location. For example, detecting repeated initialization— a dead write— requires monitoring store after store without an intervening load to the same location. Witch is implemented as part of the open source HPCToolkit performance analysis tools suite. It comprises the base framework, on top of which various detection tools can be implemented. Three such tools are described in the paper:  DeadCraft detects dead stores, a store followed by another store to the same address with on intervening load  SilentCraft detects stores that update a location with a value already present at the location (i.e., useless stores). LoadCraft detects loads followed by another load from the same location where the value remains unchanged between the two loads. Not every instance of these situations is a problem of course, but they can be very useful in pointing developers in the right direction. Let’s look at an example applying SilentCraft to the Caffe deep learning framework. SilentCraft reports that 25% of all memory stores in a key loop of the pooling and normalisation layers are redundant. This is a clue to look at the code and see what is going on. Investigation reveals that a large portion of elements in top_diff are zeroes, effectively leading to execution of bottom_diff[index] += 0 in line 8. Adding a zero check for the value in top_diff can eliminate a division, an addition, and a memory store. This change speeds up the pooling layer by 1.16x, and the normalization layers by 1.34x. Using a near-zero check (less than 1e-7) gives a 2% accuracy loss, but yields 1.16x and 2.23x speedups for pooling and normalization respectively. (A 6% speedup over the program as a whole). The NWChem computational chemistry package provides an illustration of the benefits of detecting dead stores with DeadCraft. DeadCraft reveals that 60% of memory stores are dead,  with 94% of those dead stores due to one store pair in the call of the dfill function. More than 200K calls to dfill are made, writing 500GB of data that is never used. Analysis revealed that the size of the work2 array was larger than necessary, and that the zero initialisation (the cause of all those stores) was also unnecessary. Eliminating it lead to a 1.43x speedup. For a LoadCraft example we can turn to GNU Binutils-2.27. LoadCraft identifies 96% of the loads in this program as loading the same value from the same location. The culprit is a linear scan over addresses in a function table. Replacing the linked list with a sorted array and using a binary search instead sped up execution by 10x. We’ve seen how Witch can help programmers focus their attention on interesting parts of the codebase. Now let’s take a look at how it works under the covers…  PMU Sampling  Hardware performance monitoring units in CPUs offer a programmable way to count hardware events such as loads and stores, and hardware debug registers can trap execution when the PC reaches an address, or an instruction accesses a designated address (a watchpoint). Linux offers a standard interface to program and sample PMUs. PMU samples that include the effective address accessed in a sample provide the knowledge of the addresses accessed in an execution. Given this effective address, a hardware debug register allows us to keep an eye on (watch) a location and recognize what the program subsequently does to such a location. The following figure shows how all the pieces fit together in the context of dead store detection (the DeadCraft client):  Intervening accesses  We can only monitor a small number of locations at a time (e.g., four hardware debug registers), so reservoir sampling allows us to monitor a subset of previously seen addresses without any bias. Two accesses to the same memory address, separated by many PMU samples in the intervening time, present an issue. Once all the watchpoints are in use, a simple ‘replace the oldest watchpoint’ scheme will most likely not detect e.g. dead stores, separated by such a distance. Monitoring a new sample may help detect a new, previously unseen problem whereas continuing to monitor an old, already-armed address may help detect a problem separated by many intervening operations. We should detect both. The solution uses reservoir sampling and relies on multiple unbiased samples taken over a repetitive execution to capture both scenarios. Since only counts of previous samples are maintained the technique needs only O(1) memory. In practice, the “blindspot window” (number of consecutive unmonitored PMU samples) for many applications is very short – e.g., less than 0.02% of the total samples in the SPEC CPU2006 benchmarks. Proportional attribution  Consider the following code fragment:  There are many dead stores in the i-loop (line 3) due to the overwriting j-loop (line 11). However, only a few watchpoints survive between these two loops because of all the watchpoints consumed in the middle loop (lines 7-8). Without correcting for this sampling imbalance, a disproportionately high dead write count will be recorded for the line pairs (7,8) and (8,7) compared to the rest. We solve this problem with a context-sensitive approximation. The code behaviour is typically the same in a calling context; hence, an observation made by monitoring an address accessed in a calling context can approximately represent other unmonitored samples occurring in the same calling context. If in a sequence of N samples occurring in a calling context C, only one sample is monitored through a debug register, we scale the observation made for the monitored sample by N to approximated the behaviour of the remaining N-1 unmonitored samples taken at C.  Accuracy and overheads  The following charts show the accuracy of DeadCraft, SilentCraft, and LoadCraft at different sampling rates, as compared to ground truth exhaustive monitoring. Clearly, the sampling rate, when chosen with some care, does not significantly affect the results. Unsurprisingly, sampling also has much lower overheads than exhaustive monitoring, as shown in this table. ( Enlarge )  Perhaps more interesting is the overhead compared to normal (i.e., non-instrumented) execution, which is typically less than 5%:", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3173162.3177159?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/watching-for-software-inefficiencies-with-witch.json"}
{"id": "263861", "bin": "1400_1500", "summary_sentences": ["TensorFlow: A system for large-scale machine learning Abadi et al. (Google Brain) OSDI 2016  This is my last paper review for 2016!", "The Morning Paper will be taking a two week break for the holidays, resuming again on the 2nd January.", "Sometime inbetween I’ll do a short retrospective on the year.", "It seems fitting to finish the year with a software system that was released as OSS just over a year ago and has since gathered a lot of mindshare and attention: Google’s TensorFlow.", "A large number of groups at Google have deployed TensorFlow in production, and TensorFlow is helping our research colleagues to make new new advances in machine learning.", "Since we released TensorFlow as open-source software, more than 14,000 people have forked the source code repository, the binary distribution has been downloaded over one million times, and dozens of machine learning models that use TensorFlow have been published.", "TensorFlow essentials  A tensor is simply a multi-dimensional array of primitive types.", "A machine learning system in TensorFlow is represented by a dataflow graph with operators and state at the nodes in the graph, and tensors flowing on the edges between them.", "This explicit representation of the computation and the communication between stages makes it easy to partition computation across devices, and to execute independent computations in parallel.", "At the system level, all tensors are treated as dense.", "If you want to model a sparse tensor therefore, you need to encode it somehow at the application level.", "One option is to encode the data into variable-length string elements in a dense tensor; another option is to use a tuple of dense tensors, the first carrying coordinates, and the second the (non-zero) values at those coordinates.", "Operations take one or more tensors as input and produce one or more tensors as output.", "The attributes of an operation at compile-time determine both the expected types and the arity of inputs and outputs.", "Operations may contain mutable state that is read and/or written each time it executes.", "The special Variable operation simply owns a mutable buffer that may be used to store the shared parameters of a model as it is trained.", "In this way, parameters may be contained within the dataflow itself, rather than being ‘outside’ of the system in a parameter server.", "A second type of stateful operator is a queue operator.", "Queues support more advanced forms of coordination.", "The simplest queue is FIFOQueue, which owns an internal queue of tensors, and allows concurrent access in first-in first-out order.", "Other types of queues dequeue tensors in random and priority orders, which ensure that input data are sampled appropriately.", "Advanced machine learning algorithms may contain conditional and iterative control flow (e.g., RNNs).", "Given that expressing everything in the dataflow graph is a fundamental tenet of TensorFlow, this control flow also needs to be expressed in the graph.", "TensorFlow borrows Switch and Merge operations from traditional dynamic dataflow architectures to implement conditionals.", "Enter, Exit, and Next Iteration operators are used to support looping.", "The execution of iterations can overlap, and TensorFlow can also partition conditional branches and loop bodies across multiple devices and processes.", "The partitioning step adds logic to coordinate the start and termination of each iteration on each device, and to decide the termination of the loop.", "It’s the job of the TensorFlow runtime to place operations on devices within task processes.", "TensorFlow supports CPUs, GPUs, and Google’s own custom ASIC TPUs – Tensor Processing Units.", "TPUs give an order of magnitude improvement in performance-per-watt compared to the alternative state-of-the-art.", "The placement algorithm computes a feasible set of devices for each operation, calculates the set of operations that must be colocated, and selects a satisfying device for each colocation group.", "An operation may have multiple kernels registered for it, with specialized implementations for particular devices or data types.", "When submitting a graph to the TensorFlow runtime, the user can specify zero or more edges to feed input tensors into the dataflow, and one or more edges to fetch output tensors from.", "The distributed master prunes the graph to support just what is needed for the given inputs and outputs, partitions it into subgraphs for each participating device, and caches them for reuse in subsequent steps.", "Since the master sees the overall computation for a step, it applies standard optimizations such as common subexpression elimination and constant folding; pruning is a form of dead code elimination.", "It then coordinates execution of the optimized subgraphs across a set of tasks.", "For transfers between task processes, TensorFlow can take advantage of multiple protocols including gRPC over TCP, and RDMA over converged Ethernet (RoCE).", "TensorFlow differs from standard batch dataflow systems in that:  the model supports multiple concurrent executions on overlapping subgraphs of the overall graph  Individual vertices may have mutable state that can be shared between different executions of the graph  Naiad with its Differential dataflow support seems to come close to many of the general dataflow requirements of TensorFlow (without having the specialized operators etc.", "for ML).", "Since Amazon have just blessed MXNet as their deep learning system of choice , it’s interesting to see what the TensorFlow authors have to say about it:  MXNet is perhaps the closest system in design to TensorFlow.", "It uses a dataflow graph to represent the computation at each worker, and uses a parameter server to scale training across multiple machines.", "The MXNet parameter server exports a key-value store interface that supports aggregating updates sent from multiple devices in each worker, and using an arbitrary user-provided function to combine incoming updates with the current value.", "The MXNet key-value store interface does not currently allow sparse gradient updates within a single value, which are crucial for the distributed training of large models, and adding this feature would require modifications to the core system.", "History and design rationale  TensorFlow is a successor to a previous Google system called DistBelief which used a parameter server architecture.", "One of its key goals was to provide much more flexibility to users and hence support rapid experimentation with new algorithms etc..  Making everything part of a dataflow makes it easier for users to compose novel layers using just a high-level scripting interface.", "Having state in the dataflow graph enables experimentation with different update rules.", "Having global information about the computation enables optimization of the execution phase – for example, TensorFlow achieves high GPU utilization by using the graph’s dependency structure to issue a sequence of kernels to the GPU without waiting for intermediate results.", "Allowing operations to have multiple kernels enables exploitation of special-purpose accelerators when they are available.", "This enable a TensorFlow program, for example, to be deployed to a cluster of GPUs for training, a cluster of TPUs for serving, and a cellphone for mobile inference.", "Where next?", "TensorFlow is a work in progress.", "Its flexible dataflow representation enables power users to achieve excellent performance, but we have not yet determined default policies that work well for all users.", "Further research on automatic optimization should bridge this gap.", "On the system level, we are actively developing algorithms for automatic placement, kernel fusion, memory management, and scheduling.", "Fault-tolerance today is supported by user-level checkpointing operations (Save and Restore).", "A typical configuration connects each Variable in a task to the same Save operation to maximize I/O bandwidth to a distributed file system.", "While the current implementations of mutable state and fault tolerance suffice for applications with weak consistency requirements, we expect that some TensorFlow applications will require stronger consistency, and we are investigating how to build such policies at user-level.", "TensorFlow was originally designed to support asynchronous training , but new research suggests in some configurations synchnronous training may be faster to get to a certain quality level than asynchronous training, thus the team have begun experimenting with synchronous methods.", "Finally, some users have begun to chafe at the limitations of a static dataflow graph, especially for algorithms like deep reinforcement learning.", "Therefore, we face the intriguing problem of providing a system that transparently and efficiently uses distributed resources, even when the structure of the computation unfolds dynamically."], "summary_text": "TensorFlow: A system for large-scale machine learning Abadi et al. (Google Brain) OSDI 2016  This is my last paper review for 2016! The Morning Paper will be taking a two week break for the holidays, resuming again on the 2nd January. Sometime inbetween I’ll do a short retrospective on the year. It seems fitting to finish the year with a software system that was released as OSS just over a year ago and has since gathered a lot of mindshare and attention: Google’s TensorFlow. A large number of groups at Google have deployed TensorFlow in production, and TensorFlow is helping our research colleagues to make new new advances in machine learning. Since we released TensorFlow as open-source software, more than 14,000 people have forked the source code repository, the binary distribution has been downloaded over one million times, and dozens of machine learning models that use TensorFlow have been published. TensorFlow essentials  A tensor is simply a multi-dimensional array of primitive types. A machine learning system in TensorFlow is represented by a dataflow graph with operators and state at the nodes in the graph, and tensors flowing on the edges between them. This explicit representation of the computation and the communication between stages makes it easy to partition computation across devices, and to execute independent computations in parallel. At the system level, all tensors are treated as dense. If you want to model a sparse tensor therefore, you need to encode it somehow at the application level. One option is to encode the data into variable-length string elements in a dense tensor; another option is to use a tuple of dense tensors, the first carrying coordinates, and the second the (non-zero) values at those coordinates. Operations take one or more tensors as input and produce one or more tensors as output. The attributes of an operation at compile-time determine both the expected types and the arity of inputs and outputs. Operations may contain mutable state that is read and/or written each time it executes. The special Variable operation simply owns a mutable buffer that may be used to store the shared parameters of a model as it is trained. In this way, parameters may be contained within the dataflow itself, rather than being ‘outside’ of the system in a parameter server. A second type of stateful operator is a queue operator. Queues support more advanced forms of coordination. The simplest queue is FIFOQueue, which owns an internal queue of tensors, and allows concurrent access in first-in first-out order. Other types of queues dequeue tensors in random and priority orders, which ensure that input data are sampled appropriately. Advanced machine learning algorithms may contain conditional and iterative control flow (e.g., RNNs). Given that expressing everything in the dataflow graph is a fundamental tenet of TensorFlow, this control flow also needs to be expressed in the graph. TensorFlow borrows Switch and Merge operations from traditional dynamic dataflow architectures to implement conditionals. Enter, Exit, and Next Iteration operators are used to support looping. The execution of iterations can overlap, and TensorFlow can also partition conditional branches and loop bodies across multiple devices and processes. The partitioning step adds logic to coordinate the start and termination of each iteration on each device, and to decide the termination of the loop. It’s the job of the TensorFlow runtime to place operations on devices within task processes. TensorFlow supports CPUs, GPUs, and Google’s own custom ASIC TPUs – Tensor Processing Units. TPUs give an order of magnitude improvement in performance-per-watt compared to the alternative state-of-the-art. The placement algorithm computes a feasible set of devices for each operation, calculates the set of operations that must be colocated, and selects a satisfying device for each colocation group. An operation may have multiple kernels registered for it, with specialized implementations for particular devices or data types. When submitting a graph to the TensorFlow runtime, the user can specify zero or more edges to feed input tensors into the dataflow, and one or more edges to fetch output tensors from. The distributed master prunes the graph to support just what is needed for the given inputs and outputs, partitions it into subgraphs for each participating device, and caches them for reuse in subsequent steps. Since the master sees the overall computation for a step, it applies standard optimizations such as common subexpression elimination and constant folding; pruning is a form of dead code elimination. It then coordinates execution of the optimized subgraphs across a set of tasks. For transfers between task processes, TensorFlow can take advantage of multiple protocols including gRPC over TCP, and RDMA over converged Ethernet (RoCE). TensorFlow differs from standard batch dataflow systems in that:  the model supports multiple concurrent executions on overlapping subgraphs of the overall graph  Individual vertices may have mutable state that can be shared between different executions of the graph  Naiad with its Differential dataflow support seems to come close to many of the general dataflow requirements of TensorFlow (without having the specialized operators etc. for ML). Since Amazon have just blessed MXNet as their deep learning system of choice , it’s interesting to see what the TensorFlow authors have to say about it:  MXNet is perhaps the closest system in design to TensorFlow. It uses a dataflow graph to represent the computation at each worker, and uses a parameter server to scale training across multiple machines. The MXNet parameter server exports a key-value store interface that supports aggregating updates sent from multiple devices in each worker, and using an arbitrary user-provided function to combine incoming updates with the current value. The MXNet key-value store interface does not currently allow sparse gradient updates within a single value, which are crucial for the distributed training of large models, and adding this feature would require modifications to the core system. History and design rationale  TensorFlow is a successor to a previous Google system called DistBelief which used a parameter server architecture. One of its key goals was to provide much more flexibility to users and hence support rapid experimentation with new algorithms etc..  Making everything part of a dataflow makes it easier for users to compose novel layers using just a high-level scripting interface. Having state in the dataflow graph enables experimentation with different update rules. Having global information about the computation enables optimization of the execution phase – for example, TensorFlow achieves high GPU utilization by using the graph’s dependency structure to issue a sequence of kernels to the GPU without waiting for intermediate results. Allowing operations to have multiple kernels enables exploitation of special-purpose accelerators when they are available. This enable a TensorFlow program, for example, to be deployed to a cluster of GPUs for training, a cluster of TPUs for serving, and a cellphone for mobile inference. Where next? TensorFlow is a work in progress. Its flexible dataflow representation enables power users to achieve excellent performance, but we have not yet determined default policies that work well for all users. Further research on automatic optimization should bridge this gap. On the system level, we are actively developing algorithms for automatic placement, kernel fusion, memory management, and scheduling. Fault-tolerance today is supported by user-level checkpointing operations (Save and Restore). A typical configuration connects each Variable in a task to the same Save operation to maximize I/O bandwidth to a distributed file system. While the current implementations of mutable state and fault tolerance suffice for applications with weak consistency requirements, we expect that some TensorFlow applications will require stronger consistency, and we are investigating how to build such policies at user-level. TensorFlow was originally designed to support asynchronous training , but new research suggests in some configurations synchnronous training may be faster to get to a certain quality level than asynchronous training, thus the team have begun experimenting with synchronous methods. Finally, some users have begun to chafe at the limitations of a static dataflow graph, especially for algorithms like deep reinforcement learning. Therefore, we face the intriguing problem of providing a system that transparently and efficiently uses distributed resources, even when the structure of the computation unfolds dynamically.", "pdf_url": "https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/tensorflow-a-system-for-large-scale-machine-learning.json"}
{"id": "23942242", "bin": "1400_1500", "summary_sentences": ["Measuring the tendency of CNNs to learn surface statistical regularities Jo et al., arXiv’17  With thanks to Cris Conde for bringing this paper to my attention.", "We’ve looked at quite a few adversarial attacks on deep learning systems in previous editions of The Morning Paper.", "I find them fascinating for what they reveal about the current limits of our understanding.", "…humans are able to correctly classify the adversarial image with relative ease, whereas the CNNs predict the wrong label, usually with very high confidence.", "The sensitivity of high performance CNNs to adversarial examples casts serious doubt that these networks are actually learning high level abstract concepts.", "This begs the following question: How can a network that is not learning high level abstract concepts manage to generalize so well?", "In this paper,  Jo and Bengio conduct a series of careful experiments to try and discover what’s going on.", "The initial hypothesis runs like this:  There are really only two ways we could be seeing the strong generalisation performance that we do.", "Either (a) the networks are learning high level concepts, or (b) there may be a number of superficial cues in images that are shared across training and test datasets, and the networks are learning these instead.", "We have reason to doubt scenario (a) because the success of adversarial images  We have some reasons to believe scenario (b) given results in the computer vision literature that show a strong statistical relationship between image statistics and visual understanding.", "These suggest that computer vision algorithms may “…lean heavily on background features to perform categorization.” (For example, cars tend to be on roads).", "If scenario (b) is true, then we ought to see a drop in generalisation performance if we manipulate the image statistics in such a way that they are appreciably different, and yet humans can still easily recognise the target object.", "When the training and the test set share similar image statistics, it is wholly possible for a machine learning model to learn superficial cues and generalize well, albeit in a very narrow sense as they are highly dependent on the image statistics.", "Adversarial examples would be destroying the superficial cues.", "We believe that this is precisely how deep CNNs can attain record breaking generalization performance on all sorts of natural image tasks, and yet can be so sensitive to adversarial perturbations.", "To gather evidence in favour of this hypothesis, we need to find a perturbation function over a dataset such that:  Object recognisability is preserved from the perspective of a human  The perturbed images exhibit qualitatively different image statistics.", "When trained on either (but not both!)", "of the original or perturbed images, and then tested against both the original and perturbed images, we see a non-trivial generalisation gap between the generalisation capability on the test images with similar statistics, and the the generalisation capability on the test images with different statistics.", "Conditions (1) and (2) together guarantee that the original and perturbed datasets share the same high level abstractions but exhibit different superficial cues.", "Fourier filtering to the rescue  While natural images are known to exhibit a huge variance in the raw pixel space, it has been shown that the Fourier image statistics of natural images obey a power law decay… An immediate takeaway is that natural images tend to have the bulk of their Fourier spectrum concentrated in the low to medium range frequencies.", "Due to this power law concentration of energy in the frequency space, it is possible to perform certain types of Fourier filtering and preserve much of the perceptual content of an image.", "The authors experiment with two different kinds of Fourier filters: a low-frequency filter that uses a radial mask in the Fourier domain to set higher frequency modes to zero, and a random filter that uses a uniformly random mask to set a Fourier mode to zero with probability p.  Here are some example images from the SVHN dataset (top row), and the results of applying the radial and random masking filters respectively:  And this is the same thing, but for the CIFAR-10 dataset:  The filters do introduce artifacts into the images, but these don’t really interfere with human perception, and tend to occur in the background of the image.", "Running the experiment  For both the CIFAR-10 and SVHN datasets the authors use some established high-performance CNN architectures (Preact ResNet).", "A model is trained on one of the unfiltered, radial mask, or random mask datasets, and then tested across all three test groups (unfiltered, radial, random).", "This enables us to measure the test gap or generalisation gap as the maximum difference in accuracy across the test sets.", "For the SVHN source dataset, the three figures below show the generalisation results after training on (a) unfiltered, (b) random, and (c) radial.", "And here’s the same thing for CIFAR-10:  With SVHN the largest generalisation gap (7.89%) occurs when training on randomly filtered data, and testing on the radially filtered dataset.", "Training on the radially filtered dataset actually turned out to improve generalisation performance on the unfiltered test set by 1.5%.", "The authors put this down to a regularisation effect.", "Changing the network depth seemed to have little impact on the generalisation gap.", "That is, there is no evidence of an ability to more successfully learn higher level abstractions when we add more data.", "With CIFAR-10 the generalisation gaps are bigger: 28% when trained on randomly filtered data tested on radially filtered.", "Changing the depth has little impact in this case too.", "Discussion  In addition to still being recognisable to the human eye (a subjective measure), we know also know that networks trained on the filtered datasets actually generalised quite well to the unfiltered test set (off by 1-2% of the best unfiltered accuracy).", "This provides further evidence that the choice of Fourier filtering schemes is producing datasets that are perceptually not too far off from the original unfiltered dataset.", "We see that deep CNNs trained on a unfiltered natural image dataset exhibit a tendency to latch onto the image statistics of the training set, yielding a non-trivial generalization gap…  When training on all three training sets (unfiltered, radial, and random) there is an improvement in the generalisation gap.", "“However, we cast doubt on the notion that this sort of data augmentation scheme is sufficient to learn higher level semantic features in the dataset.", "Rather it is far more likely that the CNNs are learning a superficial robustness to the varying image statistics.”  Overall, the data seems to support the initial hypothesis: “the current incarnation of deep neural networks have a tendency to learn surface statistical regularities as opposed to high level abstractions.”  What are we to do?", "I’m reminded of the brightly coloured word books we teach our children with – often with a single strong cartoon-like image of an object, on a plain white background, and the label (word) underneath.", "By seeing real world objects with movement (either of our own point of view, or because they have independent motion) we are also forced to more strongly separate object from background.", "The authors cite six pieces of work which they believe represent promising new directions:  “ Independently controllable factors ” and “ Reinforcement learning with unsupervised auxiliary tasks ” aim to learn good disentangled feature representations by combining unsupervised and reinforcement learning.", "“ SCAN: learning abstract hierarchical compositional visual concepts ” uses a variational setup.", "“ Discovering objects and their relations from untangled scene representations ” aims to learn abstract relations between objects in natural scene images  “ The consciousness prior ” moves away from making predictions in the perceptual space, and instead operates in the higher-order abstract space."], "summary_text": "Measuring the tendency of CNNs to learn surface statistical regularities Jo et al., arXiv’17  With thanks to Cris Conde for bringing this paper to my attention. We’ve looked at quite a few adversarial attacks on deep learning systems in previous editions of The Morning Paper. I find them fascinating for what they reveal about the current limits of our understanding. …humans are able to correctly classify the adversarial image with relative ease, whereas the CNNs predict the wrong label, usually with very high confidence. The sensitivity of high performance CNNs to adversarial examples casts serious doubt that these networks are actually learning high level abstract concepts. This begs the following question: How can a network that is not learning high level abstract concepts manage to generalize so well? In this paper,  Jo and Bengio conduct a series of careful experiments to try and discover what’s going on. The initial hypothesis runs like this:  There are really only two ways we could be seeing the strong generalisation performance that we do. Either (a) the networks are learning high level concepts, or (b) there may be a number of superficial cues in images that are shared across training and test datasets, and the networks are learning these instead. We have reason to doubt scenario (a) because the success of adversarial images  We have some reasons to believe scenario (b) given results in the computer vision literature that show a strong statistical relationship between image statistics and visual understanding. These suggest that computer vision algorithms may “…lean heavily on background features to perform categorization.” (For example, cars tend to be on roads). If scenario (b) is true, then we ought to see a drop in generalisation performance if we manipulate the image statistics in such a way that they are appreciably different, and yet humans can still easily recognise the target object. When the training and the test set share similar image statistics, it is wholly possible for a machine learning model to learn superficial cues and generalize well, albeit in a very narrow sense as they are highly dependent on the image statistics. Adversarial examples would be destroying the superficial cues. We believe that this is precisely how deep CNNs can attain record breaking generalization performance on all sorts of natural image tasks, and yet can be so sensitive to adversarial perturbations. To gather evidence in favour of this hypothesis, we need to find a perturbation function over a dataset such that:  Object recognisability is preserved from the perspective of a human  The perturbed images exhibit qualitatively different image statistics. When trained on either (but not both!) of the original or perturbed images, and then tested against both the original and perturbed images, we see a non-trivial generalisation gap between the generalisation capability on the test images with similar statistics, and the the generalisation capability on the test images with different statistics. Conditions (1) and (2) together guarantee that the original and perturbed datasets share the same high level abstractions but exhibit different superficial cues. Fourier filtering to the rescue  While natural images are known to exhibit a huge variance in the raw pixel space, it has been shown that the Fourier image statistics of natural images obey a power law decay… An immediate takeaway is that natural images tend to have the bulk of their Fourier spectrum concentrated in the low to medium range frequencies. Due to this power law concentration of energy in the frequency space, it is possible to perform certain types of Fourier filtering and preserve much of the perceptual content of an image. The authors experiment with two different kinds of Fourier filters: a low-frequency filter that uses a radial mask in the Fourier domain to set higher frequency modes to zero, and a random filter that uses a uniformly random mask to set a Fourier mode to zero with probability p.  Here are some example images from the SVHN dataset (top row), and the results of applying the radial and random masking filters respectively:  And this is the same thing, but for the CIFAR-10 dataset:  The filters do introduce artifacts into the images, but these don’t really interfere with human perception, and tend to occur in the background of the image. Running the experiment  For both the CIFAR-10 and SVHN datasets the authors use some established high-performance CNN architectures (Preact ResNet). A model is trained on one of the unfiltered, radial mask, or random mask datasets, and then tested across all three test groups (unfiltered, radial, random). This enables us to measure the test gap or generalisation gap as the maximum difference in accuracy across the test sets. For the SVHN source dataset, the three figures below show the generalisation results after training on (a) unfiltered, (b) random, and (c) radial. And here’s the same thing for CIFAR-10:  With SVHN the largest generalisation gap (7.89%) occurs when training on randomly filtered data, and testing on the radially filtered dataset. Training on the radially filtered dataset actually turned out to improve generalisation performance on the unfiltered test set by 1.5%. The authors put this down to a regularisation effect. Changing the network depth seemed to have little impact on the generalisation gap. That is, there is no evidence of an ability to more successfully learn higher level abstractions when we add more data. With CIFAR-10 the generalisation gaps are bigger: 28% when trained on randomly filtered data tested on radially filtered. Changing the depth has little impact in this case too. Discussion  In addition to still being recognisable to the human eye (a subjective measure), we know also know that networks trained on the filtered datasets actually generalised quite well to the unfiltered test set (off by 1-2% of the best unfiltered accuracy). This provides further evidence that the choice of Fourier filtering schemes is producing datasets that are perceptually not too far off from the original unfiltered dataset. We see that deep CNNs trained on a unfiltered natural image dataset exhibit a tendency to latch onto the image statistics of the training set, yielding a non-trivial generalization gap…  When training on all three training sets (unfiltered, radial, and random) there is an improvement in the generalisation gap. “However, we cast doubt on the notion that this sort of data augmentation scheme is sufficient to learn higher level semantic features in the dataset. Rather it is far more likely that the CNNs are learning a superficial robustness to the varying image statistics.”  Overall, the data seems to support the initial hypothesis: “the current incarnation of deep neural networks have a tendency to learn surface statistical regularities as opposed to high level abstractions.”  What are we to do? I’m reminded of the brightly coloured word books we teach our children with – often with a single strong cartoon-like image of an object, on a plain white background, and the label (word) underneath. By seeing real world objects with movement (either of our own point of view, or because they have independent motion) we are also forced to more strongly separate object from background. The authors cite six pieces of work which they believe represent promising new directions:  “ Independently controllable factors ” and “ Reinforcement learning with unsupervised auxiliary tasks ” aim to learn good disentangled feature representations by combining unsupervised and reinforcement learning. “ SCAN: learning abstract hierarchical compositional visual concepts ” uses a variational setup. “ Discovering objects and their relations from untangled scene representations ” aims to learn abstract relations between objects in natural scene images  “ The consciousness prior ” moves away from making predictions in the perceptual space, and instead operates in the higher-order abstract space.", "pdf_url": "https://arxiv.org/pdf/1711.11561", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/measuring-the-tendency-of-cnns-to-learn-surface-statistical-regularities.json"}
{"id": "91200961", "bin": "1400_1500", "summary_sentences": ["What  Current object detectors follow either a two-stage or single-stage approach.", "Two-stage: Accurate, but rather slow as they use two different networks to predict RoIs and then classify them.", "Single-stage  Almost as accurate as two-stage detectors nowadays and faster than them.", "Single-stage detectors place a dense grid of anchor boxes on the image and classify for each of them whether they match an object in the image.", "There has to be a large number of anchor boxes for different object sizes (and also possible more when the detectors works on multiple scales).", "This increases memory demands, decreases performance and introduces difficult hyperparameter choices (number of anchors boxes and their sizes).", "They develop an object detector that is single-stage and free of anchor boxes.", "Their system predicts bounding boxes by localizing the top left and bottom right corner of an object.", "They argue that predicting corners is easier than the anchor-based approach.", "There are essentially exactly two points to predict per bounding box, while there can be many more valid anchors per object (that are than fine-tuned via regression to fit the object).", "They also propose a corner pooling layer that complements their technique.", "Pools along the horizontal/vertical axis.", "This is useful for objects where no object parts are locally in the corners (e.g. imagine a frontal-view on a human with stretched out arms).", "They argue that this pooling layer encodes prior knowledge of the prediction task.", "How  Corner Detection  They predict for each object class two heatmaps: One for top-left corners and one for bottom-right corners.", "The ground truth heatmaps contain positive values at the locations of corners.", "Gaussian Ground Truth  They argue that corners that are slightly off are still good hits reaching high IoUs.", "They set the desired target IoU to t=0.7 and derive from that per corner pair k a radius r_k describing how far the corners can be off to still fulfill t.  They place on each ground truth location an unnormalized 2D gaussian e^(-(x^2 + y^2)/2sigma^2), where sigma=r_k/3.", "Corner Position Loss  They use a variant of Focal Loss, applied once for the top-left corner heatmaps and once for bottom-right corners.", "where  y_cij is ground truth corner heatmap for class c at location y=i and x=j.", "p_cij is the predicted heatmap.", "alpha=2, beta=4 are focal loss hyperparameters.", "N is the number of objects in one image.", "Offsets  Predicted heatmaps are often downsampled compared to the ground truth heatmaps.", "That can lead to predicted locations being slightly off compared to true locations.", "They compensate for that by letting the network learn that offset (per spatial location).", "The ground truth for the offset value is:  where n is the downsampling factor.", "They apply a smooth L1 loss to the offset predictions.", "They apply the loss only at the ground truth corner locations.", "Grouping Corners  Top-left and bottom-right corners have to be matched in order to create a full bounding box.", "They do this by predicting embedding vectors for each top-left and bottom-right corner.", "They train these to be similar for corners belonging to the same objects.", "They train these to be different for corners belonging to different objects.", "For (1) they use a pull loss and for (2) a push loss:  where  e_t_k is the embedding of the top left corner of object k.  e_b_k is analogously is the embedding of the bottom right corner.", "e_k is the average of e_t_k and e_b_k.", "Delta=1.", "Note that there is no ground truth here, because only the distances matter.", "They apply these losses only at the ground truth corner locations.", "Corner Pooling  In many cases, there is no local evidence for an object around its top-left or bottom-right corners.", "But there is evidence around its top and left sides (analogously bottom and right sides).", "Visualization of the problem:  They compensate for that by max-pooling along the sides of each potential object.", "E.g. for the top-left corner they would max-pool along the corner's row and to the right, as well as along the corner's column und to the bottom.", "They sum both of these results.", "Visualization:  Architecture  They use stacked hourglass networks as their backbone (with minor modifications, e.g. striding instead of max-pooling).", "They place a corner pooling layer in residual fashion on top of the backbone.", "They then apply three branches:  Corner heatmaps branch (predicts 2*C channels for C classes).", "Embeddings branch (predicts ?", "*C channels).", "Offsets branch (predicts 2*C channels).", "Visualization:  Bounding box extraction  To get bounding boxes out of their corner predictions, they first apply non-maximum suppression to their corner heatmaps via a 3x3 max pooling layer.", "Then they extract the top 100 top-left and bottom-right corners over all classes.", "They shift them by the predicted offsets.", "They pair per class the top-left and bottom-right corners with the most similar embeddings, rejecting anything with an L1 distance above 0.5.", "To these bounding box candidates they apply soft-NMS to remove strongly overlapping bounding boxes.", "Results  Loss weightings: They weight their corner heatmaps loss with 1.0, the offset loss also with 1.0 and the pull and push losses for the embeddings with 0.1 each.", "They train on 10 PASCAL Titan X.", "For inference they zero-pad images to the desired input size (511x511) and feed the padded image as well as its horizontally flipped version through the network.", "They need about 244ms per image for inference.", "They train and test on COCO.", "Corner Loss  Corner Pooling is essential for the performance of the network.", "It improves AP by about 2 percentage points.", "It is especially important for large objects (+3.7 AP), not for small objects (+0.1 AP).", "Location Penalty (via gaussians)  They investigate whether it is necessary to reduce the location penalty in the corner location heatmaps by using gaussians.", "They compare using  no penalty reduction (just set corner locations to 1, everything else to 0),  placing gaussians with a fixed radius of 2.5 and  placing gaussians with object-dependent radii.", "Option (3) performs best, (1) worst.", "(2) is in between the two options, usually half-way from (1) to (3).", "They observe increases of AP between 5 and 6 percentage points when using (3) as opposed to (1).", "They difference is more pronounced for large objects (about 12 points) as opposed to small objects (2.3 points).", "Importance of each branch  They evaluate which branch (corner location heatmaps, offsets, embeddings) has most influence on the AP.", "They replace the predicted corner location heatmaps with ground truth heatmaps and increase AP by about 35 points (to 74.0%), suggesting that their corner heatmap prediction is the main bottleneck.", "They then add ground truth offsets and improve by 13.1 points (to 87.1%), suggesting that the offset prediction still has a significant impact on overall AP.", "This leaves 12.9 points for the other components (embedding prediction, bounding box extraction).", "Final results  They reach 42.1 AP in a multi-scale approach.", "(I guess they feed the images in at multiple scales?", "Not really explained.", "In previous chapters they write specifically they don't use multi-scale feature maps, but only the final feature map.)", "They beat the best multi-scale competitor (RefineDet512) by 0.3 points.", "They reach 40.5 AP in a single-scale approach.", "They beat the best single-scale competitor (RetinaNet800) by 1.4 points.", "Example predictions and extracted bounding boxes (each left: top-left corner, each right: bottom-right):"], "summary_text": "What  Current object detectors follow either a two-stage or single-stage approach. Two-stage: Accurate, but rather slow as they use two different networks to predict RoIs and then classify them. Single-stage  Almost as accurate as two-stage detectors nowadays and faster than them. Single-stage detectors place a dense grid of anchor boxes on the image and classify for each of them whether they match an object in the image. There has to be a large number of anchor boxes for different object sizes (and also possible more when the detectors works on multiple scales). This increases memory demands, decreases performance and introduces difficult hyperparameter choices (number of anchors boxes and their sizes). They develop an object detector that is single-stage and free of anchor boxes. Their system predicts bounding boxes by localizing the top left and bottom right corner of an object. They argue that predicting corners is easier than the anchor-based approach. There are essentially exactly two points to predict per bounding box, while there can be many more valid anchors per object (that are than fine-tuned via regression to fit the object). They also propose a corner pooling layer that complements their technique. Pools along the horizontal/vertical axis. This is useful for objects where no object parts are locally in the corners (e.g. imagine a frontal-view on a human with stretched out arms). They argue that this pooling layer encodes prior knowledge of the prediction task. How  Corner Detection  They predict for each object class two heatmaps: One for top-left corners and one for bottom-right corners. The ground truth heatmaps contain positive values at the locations of corners. Gaussian Ground Truth  They argue that corners that are slightly off are still good hits reaching high IoUs. They set the desired target IoU to t=0.7 and derive from that per corner pair k a radius r_k describing how far the corners can be off to still fulfill t.  They place on each ground truth location an unnormalized 2D gaussian e^(-(x^2 + y^2)/2sigma^2), where sigma=r_k/3. Corner Position Loss  They use a variant of Focal Loss, applied once for the top-left corner heatmaps and once for bottom-right corners. where  y_cij is ground truth corner heatmap for class c at location y=i and x=j. p_cij is the predicted heatmap. alpha=2, beta=4 are focal loss hyperparameters. N is the number of objects in one image. Offsets  Predicted heatmaps are often downsampled compared to the ground truth heatmaps. That can lead to predicted locations being slightly off compared to true locations. They compensate for that by letting the network learn that offset (per spatial location). The ground truth for the offset value is:  where n is the downsampling factor. They apply a smooth L1 loss to the offset predictions. They apply the loss only at the ground truth corner locations. Grouping Corners  Top-left and bottom-right corners have to be matched in order to create a full bounding box. They do this by predicting embedding vectors for each top-left and bottom-right corner. They train these to be similar for corners belonging to the same objects. They train these to be different for corners belonging to different objects. For (1) they use a pull loss and for (2) a push loss:  where  e_t_k is the embedding of the top left corner of object k.  e_b_k is analogously is the embedding of the bottom right corner. e_k is the average of e_t_k and e_b_k. Delta=1. Note that there is no ground truth here, because only the distances matter. They apply these losses only at the ground truth corner locations. Corner Pooling  In many cases, there is no local evidence for an object around its top-left or bottom-right corners. But there is evidence around its top and left sides (analogously bottom and right sides). Visualization of the problem:  They compensate for that by max-pooling along the sides of each potential object. E.g. for the top-left corner they would max-pool along the corner's row and to the right, as well as along the corner's column und to the bottom. They sum both of these results. Visualization:  Architecture  They use stacked hourglass networks as their backbone (with minor modifications, e.g. striding instead of max-pooling). They place a corner pooling layer in residual fashion on top of the backbone. They then apply three branches:  Corner heatmaps branch (predicts 2*C channels for C classes). Embeddings branch (predicts ? *C channels). Offsets branch (predicts 2*C channels). Visualization:  Bounding box extraction  To get bounding boxes out of their corner predictions, they first apply non-maximum suppression to their corner heatmaps via a 3x3 max pooling layer. Then they extract the top 100 top-left and bottom-right corners over all classes. They shift them by the predicted offsets. They pair per class the top-left and bottom-right corners with the most similar embeddings, rejecting anything with an L1 distance above 0.5. To these bounding box candidates they apply soft-NMS to remove strongly overlapping bounding boxes. Results  Loss weightings: They weight their corner heatmaps loss with 1.0, the offset loss also with 1.0 and the pull and push losses for the embeddings with 0.1 each. They train on 10 PASCAL Titan X. For inference they zero-pad images to the desired input size (511x511) and feed the padded image as well as its horizontally flipped version through the network. They need about 244ms per image for inference. They train and test on COCO. Corner Loss  Corner Pooling is essential for the performance of the network. It improves AP by about 2 percentage points. It is especially important for large objects (+3.7 AP), not for small objects (+0.1 AP). Location Penalty (via gaussians)  They investigate whether it is necessary to reduce the location penalty in the corner location heatmaps by using gaussians. They compare using  no penalty reduction (just set corner locations to 1, everything else to 0),  placing gaussians with a fixed radius of 2.5 and  placing gaussians with object-dependent radii. Option (3) performs best, (1) worst. (2) is in between the two options, usually half-way from (1) to (3). They observe increases of AP between 5 and 6 percentage points when using (3) as opposed to (1). They difference is more pronounced for large objects (about 12 points) as opposed to small objects (2.3 points). Importance of each branch  They evaluate which branch (corner location heatmaps, offsets, embeddings) has most influence on the AP. They replace the predicted corner location heatmaps with ground truth heatmaps and increase AP by about 35 points (to 74.0%), suggesting that their corner heatmap prediction is the main bottleneck. They then add ground truth offsets and improve by 13.1 points (to 87.1%), suggesting that the offset prediction still has a significant impact on overall AP. This leaves 12.9 points for the other components (embedding prediction, bounding box extraction). Final results  They reach 42.1 AP in a multi-scale approach. (I guess they feed the images in at multiple scales? Not really explained. In previous chapters they write specifically they don't use multi-scale feature maps, but only the final feature map.) They beat the best multi-scale competitor (RefineDet512) by 0.3 points. They reach 40.5 AP in a single-scale approach. They beat the best single-scale competitor (RetinaNet800) by 1.4 points. Example predictions and extracted bounding boxes (each left: top-left corner, each right: bottom-right):", "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Hei_Law_CornerNet_Detecting_Objects_ECCV_2018_paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/cornernet.json"}
{"id": "87411149", "bin": "1400_1500", "summary_sentences": ["Scalable Atomic Visibility with RAMP Transactions – Bailis et al. 2014  RAMP transactions came up last week as part of the secret sauce in Coordination avoidance in database systems that contributed to a 25x improvement on the TPC-C benchmark.", "So what exactly are RAMP transactions and why might we need them?", "As soon as you partition your database across multiple servers, things start to get interesting.", "We’d like to maintain atomic isolation – either all of a transaction’s effects are visible or none are – for transactions that span partitions…  The status quo for these multi-partition atomic transactions provides an uncomfortable choice between algorithms that are fast but deliver inconsistent results and algorithms that deliver consistent results but are often slow and unavailable under failure.", "A lot of implemented systems have chosen to go with the fast-and-furious option resulting in incorrect behaviour for cases where atomic visibility matters.", "The RAMP (Read Atomic Multiple Partition) transaction models introduced in this paper show that you can have performance and scalability of transactions spanning multiple partitions with atomic visibility.", "…data stores like Bigtable, Dynamo, and many popular “NoSQL” and even some “NewSQL” stores do not provide transactional guarantees for multi-item operations.", "The designers of these Internet-scale, real-world systems have made a conscious decision to provide scalability at the expense of multi-partition transactional semantics.", "Our goal with RAMP transactions is to preserve this scalability but deliver correct, atomically visible behavior for the use cases we have described.", "Under evaluation, the RAMP algorithms did not degrade substantially under contention, and scaled linearly to over 7.1 million operations per second on 100 servers.", "Bad things that can happen when you don’t have atomic multi-partition isolation  Without atomic isolation foreign key constraints, secondary indexing, and materialized view maintenance can all break!", "Data models often represent bi-directional relationships as two distinct uni-directional relationships.", "“For example, in TAO, a user performing a ‘like’ action on a Facebook page produces updates to both the LIKES and LIKED_BY associations.”  These applications require foreign key maintenance and often, due to their unidirectional relationships, multi-entity update and access.", "Without atomic isolation broken bi-directional relationships, and dangling or incorrect references can surface.", "With data partitioned across servers by primary key, access by secondary attributes becomes more challenging.", "There are two dominant strategies for distributed secondary indexing.", "First, the local secondary index approach co-locates secondary indexes and primary data, so each server contains a secondary index that only references (and indexes) data stored on its server.", "This allows easy, single-server updates but requires contacting every partition for secondary attribute lookups (write-one, read-all), compromising scalability for read-heavy workloads.", "Alternatively, the global secondary index approach locates secondary indexes (which may be partitioned, but by a secondary attribute) separately from primary data.", "This alternative allows fast secondary lookups (read-one) but requires multi-partition update (at least write-two)  Real-world services tend to use either local secondary indexing (non-scalable but correct), or non-atomic (scalable but incorrect) global indexes.", "In the latter cases queries involving the secondary attributes can return records that shouldn’t match, and omit ones that should.", "Without atomic isolation, materialized views can diverge from the base data.", "For example, a count may become inaccurate.", "With RAMP transactions, base data and views can be updated atomically.", "The physical maintenance of a view depends on its specification, but RAMP transactions provide appropriate concurrency control primitives for ensuring that changes are delivered to the materialized view partition.", "For select-project views, a simple solution is to treat the view as a separate table and perform maintenance as needed: new rows can be inserted/deleted according to the specification, and, if necessary, the view can be (re-)computed on demand (i.e., lazy view maintenance).", "For more complex views, such as counters, users can execute RAMP transactions over specialized data structures such as the CRDT G-Counter.", "Scalability Requirements  Consider databases that are partitioned over multiple servers.", "Each item has a single logical copy stored on one of those partitions, which one can be calculated using the item itself (e.g. primary key).", "In order to achieve scalability the author’s identify two key properties that must be preserved: synchronization independence, and partition independence.", "Synchronization independence ensures that one client’s transactions cannot cause another client’s to block and that, if a client can contact the partition responsible for each item in its transaction, the transaction will eventually commit (or abort of its own volition).", "(Also known as transactional availability).", "Partition independence ensures that, in order to execute a transaction, a client never has to contact partitions that its transaction does not access.", "Thus, a partition failure only affects transactions that access items contained on the partition.", "This also reduces load on servers not directly involved in a transaction’s execution.", "In the distributed systems literature, partition independence for replicated data is called replica availability or genuine partial replication.", "A third constraint is that the metadata required to achieve synchronization and partition independence is not too large: “there are many potential solutions for providing atomic visibility that rely on storing prohibitive amounts of state.”  The RAMP transaction algorithms  You may be wondering why I keep referring to algorithms (plural).", "This is because the authors actually define three RAMP variants: RAMP-Fast, RAMP-Small, and RAMP-Hybrid.", "These trade-off between performance and the amount of metadata that needs to be kept.", "At a high level, RAMP transactions allow reads and writes to proceed concurrently.", "This provides excellent performance but, in turn, introduces a race condition: one transaction might only read a subset of another transaction’s writes, violating RA (i.e., fractured reads might occur).", "Instead of preventing this race (hampering scalability), RAMP readers autonomously detect the race (using metadata attached to each data item) and fetch any missing, in-flight writes from their respective partitions.", "To make sure that readers never have to block for writes to arrive at a partition, writers use a two-phase (atomic commitment) protocol that ensures that once a write is visible to readers on one partition, any other writes in the transaction are present on and, if appropriately identified by version, readable from their respective partitions.", "RAMP-Fast stores metadata in the form of write sets (thus the overhead is linear in transaction size), and has one RTT for reads in the best case (two in the worst case).", "RAMP-Small uses constant size metadata (it only stores the transaction timestamp) but always requires two RTT for reads.", "RAMP-Hybrid takes the same write set information as RAMP-Fast, but encodes it in a Bloom filter.", "With no false positives from the filter, Ramp-Hybrid would therefore behave as RAMP-Fast.", "And with all false positives, it behaves as RAMP-Small.", "All of the variants require two RTTs/transaction for writes.", "The two-phase atomic commitment protocol used by RAMP ensures readers never block waiting for writes to arrive.", "It is known that every atomic commitment protocol may block during failures.", "Blocked writes instead act as “resource leaks” on partitions: partitions will retain prepared versions indefinitely unless action is taken.", "To “free” these leaks, RAMP servers can use the Cooperative Termination Protocol (CTP).", "CTP can always complete the transaction except when every partition has performed PREPARE but no partition has performed COMMIT… Compared to alternatives (e.g. replicating clients), we have found CTP to be both lightweight and effective.", "There is of course much more detail in the full paper, which I encourage you to go on and read.", "Section 6 on Related Work contains a nice short summary of isolation guarantees in the wild.", "“In recent years, many ‘NoSQL’ designs have avoided cross-partition transactions entirely, effectively providing Read Uncommitted isolation…”"], "summary_text": "Scalable Atomic Visibility with RAMP Transactions – Bailis et al. 2014  RAMP transactions came up last week as part of the secret sauce in Coordination avoidance in database systems that contributed to a 25x improvement on the TPC-C benchmark. So what exactly are RAMP transactions and why might we need them? As soon as you partition your database across multiple servers, things start to get interesting. We’d like to maintain atomic isolation – either all of a transaction’s effects are visible or none are – for transactions that span partitions…  The status quo for these multi-partition atomic transactions provides an uncomfortable choice between algorithms that are fast but deliver inconsistent results and algorithms that deliver consistent results but are often slow and unavailable under failure. A lot of implemented systems have chosen to go with the fast-and-furious option resulting in incorrect behaviour for cases where atomic visibility matters. The RAMP (Read Atomic Multiple Partition) transaction models introduced in this paper show that you can have performance and scalability of transactions spanning multiple partitions with atomic visibility. …data stores like Bigtable, Dynamo, and many popular “NoSQL” and even some “NewSQL” stores do not provide transactional guarantees for multi-item operations. The designers of these Internet-scale, real-world systems have made a conscious decision to provide scalability at the expense of multi-partition transactional semantics. Our goal with RAMP transactions is to preserve this scalability but deliver correct, atomically visible behavior for the use cases we have described. Under evaluation, the RAMP algorithms did not degrade substantially under contention, and scaled linearly to over 7.1 million operations per second on 100 servers. Bad things that can happen when you don’t have atomic multi-partition isolation  Without atomic isolation foreign key constraints, secondary indexing, and materialized view maintenance can all break! Data models often represent bi-directional relationships as two distinct uni-directional relationships. “For example, in TAO, a user performing a ‘like’ action on a Facebook page produces updates to both the LIKES and LIKED_BY associations.”  These applications require foreign key maintenance and often, due to their unidirectional relationships, multi-entity update and access. Without atomic isolation broken bi-directional relationships, and dangling or incorrect references can surface. With data partitioned across servers by primary key, access by secondary attributes becomes more challenging. There are two dominant strategies for distributed secondary indexing. First, the local secondary index approach co-locates secondary indexes and primary data, so each server contains a secondary index that only references (and indexes) data stored on its server. This allows easy, single-server updates but requires contacting every partition for secondary attribute lookups (write-one, read-all), compromising scalability for read-heavy workloads. Alternatively, the global secondary index approach locates secondary indexes (which may be partitioned, but by a secondary attribute) separately from primary data. This alternative allows fast secondary lookups (read-one) but requires multi-partition update (at least write-two)  Real-world services tend to use either local secondary indexing (non-scalable but correct), or non-atomic (scalable but incorrect) global indexes. In the latter cases queries involving the secondary attributes can return records that shouldn’t match, and omit ones that should. Without atomic isolation, materialized views can diverge from the base data. For example, a count may become inaccurate. With RAMP transactions, base data and views can be updated atomically. The physical maintenance of a view depends on its specification, but RAMP transactions provide appropriate concurrency control primitives for ensuring that changes are delivered to the materialized view partition. For select-project views, a simple solution is to treat the view as a separate table and perform maintenance as needed: new rows can be inserted/deleted according to the specification, and, if necessary, the view can be (re-)computed on demand (i.e., lazy view maintenance). For more complex views, such as counters, users can execute RAMP transactions over specialized data structures such as the CRDT G-Counter. Scalability Requirements  Consider databases that are partitioned over multiple servers. Each item has a single logical copy stored on one of those partitions, which one can be calculated using the item itself (e.g. primary key). In order to achieve scalability the author’s identify two key properties that must be preserved: synchronization independence, and partition independence. Synchronization independence ensures that one client’s transactions cannot cause another client’s to block and that, if a client can contact the partition responsible for each item in its transaction, the transaction will eventually commit (or abort of its own volition). (Also known as transactional availability). Partition independence ensures that, in order to execute a transaction, a client never has to contact partitions that its transaction does not access. Thus, a partition failure only affects transactions that access items contained on the partition. This also reduces load on servers not directly involved in a transaction’s execution. In the distributed systems literature, partition independence for replicated data is called replica availability or genuine partial replication. A third constraint is that the metadata required to achieve synchronization and partition independence is not too large: “there are many potential solutions for providing atomic visibility that rely on storing prohibitive amounts of state.”  The RAMP transaction algorithms  You may be wondering why I keep referring to algorithms (plural). This is because the authors actually define three RAMP variants: RAMP-Fast, RAMP-Small, and RAMP-Hybrid. These trade-off between performance and the amount of metadata that needs to be kept. At a high level, RAMP transactions allow reads and writes to proceed concurrently. This provides excellent performance but, in turn, introduces a race condition: one transaction might only read a subset of another transaction’s writes, violating RA (i.e., fractured reads might occur). Instead of preventing this race (hampering scalability), RAMP readers autonomously detect the race (using metadata attached to each data item) and fetch any missing, in-flight writes from their respective partitions. To make sure that readers never have to block for writes to arrive at a partition, writers use a two-phase (atomic commitment) protocol that ensures that once a write is visible to readers on one partition, any other writes in the transaction are present on and, if appropriately identified by version, readable from their respective partitions. RAMP-Fast stores metadata in the form of write sets (thus the overhead is linear in transaction size), and has one RTT for reads in the best case (two in the worst case). RAMP-Small uses constant size metadata (it only stores the transaction timestamp) but always requires two RTT for reads. RAMP-Hybrid takes the same write set information as RAMP-Fast, but encodes it in a Bloom filter. With no false positives from the filter, Ramp-Hybrid would therefore behave as RAMP-Fast. And with all false positives, it behaves as RAMP-Small. All of the variants require two RTTs/transaction for writes. The two-phase atomic commitment protocol used by RAMP ensures readers never block waiting for writes to arrive. It is known that every atomic commitment protocol may block during failures. Blocked writes instead act as “resource leaks” on partitions: partitions will retain prepared versions indefinitely unless action is taken. To “free” these leaks, RAMP servers can use the Cooperative Termination Protocol (CTP). CTP can always complete the transaction except when every partition has performed PREPARE but no partition has performed COMMIT… Compared to alternatives (e.g. replicating clients), we have found CTP to be both lightweight and effective. There is of course much more detail in the full paper, which I encourage you to go on and read. Section 6 on Related Work contains a nice short summary of isolation guarantees in the wild. “In recent years, many ‘NoSQL’ designs have avoided cross-partition transactions entirely, effectively providing Read Uncommitted isolation…”", "pdf_url": "http://www.bailis.org/papers/ramp-sigmod2014.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/scalable-atomic-visibility-with-ramp-transactions.json"}
{"id": "23675750", "bin": "1400_1500", "summary_sentences": ["Towards a theory of software development expertise Baltes et al., ESEC/FSE’18  This is the last paper we’ll be looking at this year, so I’ve chosen something a little more reflective to leave you with (The Morning Paper will return on Monday 7th January, 2019).", "The question Baltes and Diehl tackle is this: “How do you get better as a software developer?” What does expert performance look like?", "We present a first conceptual theory of software development expertise that is grounded in data from a mixed-methods survey with 335 software developers and in literature on expertise and expert performance….", "[the theory] describes central properties of software development expertise and important factors influencing its formation.", "In essence, ask a bunch of practitioners what they think, use a disciplined coding scheme to interpret the answers (a “grounded theory”), and then layer in what we know about expertise and expert performance in general.", "The end result is a “conceptual theory” that shows the various contributors to expert performance and the relationships between them.", "“Software Development” in the current work is synonymous with “programming.”  To make the paper come alive you need to engage with it a little: Does the theory developed by the authors make sense to you?", "What’s missing?", "How would you weight the various factors?", "How could you apply this on a personal level in 2019?", "How could this be applied in your team or organisation to raise the collective level of expertise next year?", "Software developers can use our results to see which properties are distinctive for experts in their field, and which behaviors may lead to becoming a better software developer….", "Employers can learn what typical reasons for demotivation among their employees are, and how they can build a work environment supporting the self-improvement of their staff.", "A grounded theory  The first phase involved sending a questionnaire to users active on both GitHub and StackOverflow between Jan 2014 and October 2015.", "The questionnaire was sent to 1,000 individuals, and received 122 responses.", "( Enlarge )  The grounded theory (GT) coding exercise was then used to generate a theory from the qualitative data:  … the process of coding assigns “summative, salient, essence-capturing” words or phrases to portions of the unstructured data.", "Those codes are iteratively and continuously compared, aggregrated, and structured into higher levels of abstractions, the categories and the concepts.", "This iterative process is called constant comparison.", "(Aside: it strikes me that the body of work on grounded theory development might be very interesting to study from the perspective of domain-driven design and the building of a ubiquitous language.)", "After much distillation, the model comes out looking like this:  The grounded theory describes software development expertise as a combination of a certain quantity and quality of knowledge and experience, both general and for a particular language.", "The work context, behavior, character traits, and skills influence the formation of expertise, which can be observed when experts write well-structured, readable, and maintainable source code.", "You’ll know an expert programmer by the quality of the code that they write.", "Experts have good communication skills, both sharing their own knowledge and soliciting input from others.", "They are self-aware, understanding the kinds of mistakes they can make, and reflective.", "They are also fast (but not at the expense of quality).", "Experience should be measured not just on its quantity (i.e., number of years in the role), but on its quality.", "For example, working on a variety of different code bases, shipping significant amounts of code to production, and working on shared code bases.", "The knowledge of an expert is T-shaped with depth in the programming language and domain at hand, and a broad knowledge of algorithms, data structures, and programming paradigms.", "A preliminary conceptual theory  The next phase was to take the grounded theory and embed it within the existing literature on expertise and expert performance, for which the main resource used was ‘ The Cambridge Handbook of Expertise and Expert Performance ’.", "This handbook is the first, and to the best of our knowledge most comprehensive, book summarizing scientific knowledge on expertise and expert performance.", "The result of this process is a preliminary conceptual theory that looks like this:  Acquiring expertise is not exclusively a cognitive matter, personality and motivation influence behaviours that may or may not lead to improvements of expertise.", "The work context, including team members, managers, and customers, can also influence the behaviour of a developer, and this can also vary according to the type of task being undertaken.", "Reaching true experts levels requires deliberate practice combined with monitoring, feedback, and self-reflection.", "Deliberate practice  Having more experience with a task does not automatically lead to better performance.", "Research has shown that once an acceptable level of performance has been attained, additional “common” experience has only a negligible effect, in many domains the performance even decreases over time.", "The length of experience has been found to be only a weak correlate of job performance after the first two years.", "Deliberate practice is required to become an expert: prolonged efforts to improve performance while continuously increasing the difficulty and centrality of development tasks.", "…studies have shown that deliberate practice is necessary but not sufficient to achieve high levels of expert performance— individual differences also play an important role.", "Monitoring, feedback, and self-reflection  Deliberate practice requires a way of monitoring performance, which could be e.g. from a teacher, coach, mentor, or peer: “the more channels of accurate and helpful feedback we have access to, the better we are likely to perform.“.", "Monitoring and self-reflection also influence motivation and consequently behaviour.", "The full conceptual theory  For the third and final phase the authors sampled two additional programmer populations, active Java developers, and very experienced developers, with the goal of further elaborating and refining the categories and relationships in the theory.", "The final resulting model looks like this:  ( Enlarge )  The most frequently cited tasks that an expert should be good at were designing software architecture, writing source code, and analysing and understanding requirements.", "Within the software architecture task, understanding modularisation and decomposition were frequently mentioned.", "In terms of personality traits, experts should be open minded and curious, be team players, and pay attention to detail.", "Patience and self-reflection were also cited.", "In terms of general skills, “problem solving” came top of the list under which analytical thinking, logical thinking, and abstraction/decomposition all feature.", "Another important skill is being to assess trade-offs.", "Mentors should be guiding, patient, and open-minded.", "Participants were most motivated by mentors that posed challenging tasks.", "To facilitate continuous development of their employee’s software development skills, (employees suggested that) employers should:  Encourage learning (e.g. training courses, conference attendance, and access to a good analog or digital library)  Encourage experimentation (e.g.", "through side projects and by building a work environment that is open to new ideas and technologies)  Improve information exchange between development teams, departments, and even companies.", "E.g. lunch and learn sessions, rotation between teams, pairing, mentoring, and code reviews.", "Grant freedom (primarily in the form of less time pressure) to allow developers to invest in learning new technologies or skills.", "In contrast, non-challenging or routine tasks result in demotivation.", "Other causes of performance decline over time are lack of a clear vision or direction, absence of reward for quality work, stress in the work environment, and bad management or team structure.", "Your turn  How will you ensure that in 2019 you grow your expertise, and not simply add another year of (the same or similar) ‘experience’ ?", "See you in January!", "Thanks, Adrian."], "summary_text": "Towards a theory of software development expertise Baltes et al., ESEC/FSE’18  This is the last paper we’ll be looking at this year, so I’ve chosen something a little more reflective to leave you with (The Morning Paper will return on Monday 7th January, 2019). The question Baltes and Diehl tackle is this: “How do you get better as a software developer?” What does expert performance look like? We present a first conceptual theory of software development expertise that is grounded in data from a mixed-methods survey with 335 software developers and in literature on expertise and expert performance…. [the theory] describes central properties of software development expertise and important factors influencing its formation. In essence, ask a bunch of practitioners what they think, use a disciplined coding scheme to interpret the answers (a “grounded theory”), and then layer in what we know about expertise and expert performance in general. The end result is a “conceptual theory” that shows the various contributors to expert performance and the relationships between them. “Software Development” in the current work is synonymous with “programming.”  To make the paper come alive you need to engage with it a little: Does the theory developed by the authors make sense to you? What’s missing? How would you weight the various factors? How could you apply this on a personal level in 2019? How could this be applied in your team or organisation to raise the collective level of expertise next year? Software developers can use our results to see which properties are distinctive for experts in their field, and which behaviors may lead to becoming a better software developer…. Employers can learn what typical reasons for demotivation among their employees are, and how they can build a work environment supporting the self-improvement of their staff. A grounded theory  The first phase involved sending a questionnaire to users active on both GitHub and StackOverflow between Jan 2014 and October 2015. The questionnaire was sent to 1,000 individuals, and received 122 responses. ( Enlarge )  The grounded theory (GT) coding exercise was then used to generate a theory from the qualitative data:  … the process of coding assigns “summative, salient, essence-capturing” words or phrases to portions of the unstructured data. Those codes are iteratively and continuously compared, aggregrated, and structured into higher levels of abstractions, the categories and the concepts. This iterative process is called constant comparison. (Aside: it strikes me that the body of work on grounded theory development might be very interesting to study from the perspective of domain-driven design and the building of a ubiquitous language.) After much distillation, the model comes out looking like this:  The grounded theory describes software development expertise as a combination of a certain quantity and quality of knowledge and experience, both general and for a particular language. The work context, behavior, character traits, and skills influence the formation of expertise, which can be observed when experts write well-structured, readable, and maintainable source code. You’ll know an expert programmer by the quality of the code that they write. Experts have good communication skills, both sharing their own knowledge and soliciting input from others. They are self-aware, understanding the kinds of mistakes they can make, and reflective. They are also fast (but not at the expense of quality). Experience should be measured not just on its quantity (i.e., number of years in the role), but on its quality. For example, working on a variety of different code bases, shipping significant amounts of code to production, and working on shared code bases. The knowledge of an expert is T-shaped with depth in the programming language and domain at hand, and a broad knowledge of algorithms, data structures, and programming paradigms. A preliminary conceptual theory  The next phase was to take the grounded theory and embed it within the existing literature on expertise and expert performance, for which the main resource used was ‘ The Cambridge Handbook of Expertise and Expert Performance ’. This handbook is the first, and to the best of our knowledge most comprehensive, book summarizing scientific knowledge on expertise and expert performance. The result of this process is a preliminary conceptual theory that looks like this:  Acquiring expertise is not exclusively a cognitive matter, personality and motivation influence behaviours that may or may not lead to improvements of expertise. The work context, including team members, managers, and customers, can also influence the behaviour of a developer, and this can also vary according to the type of task being undertaken. Reaching true experts levels requires deliberate practice combined with monitoring, feedback, and self-reflection. Deliberate practice  Having more experience with a task does not automatically lead to better performance. Research has shown that once an acceptable level of performance has been attained, additional “common” experience has only a negligible effect, in many domains the performance even decreases over time. The length of experience has been found to be only a weak correlate of job performance after the first two years. Deliberate practice is required to become an expert: prolonged efforts to improve performance while continuously increasing the difficulty and centrality of development tasks. …studies have shown that deliberate practice is necessary but not sufficient to achieve high levels of expert performance— individual differences also play an important role. Monitoring, feedback, and self-reflection  Deliberate practice requires a way of monitoring performance, which could be e.g. from a teacher, coach, mentor, or peer: “the more channels of accurate and helpful feedback we have access to, the better we are likely to perform.“. Monitoring and self-reflection also influence motivation and consequently behaviour. The full conceptual theory  For the third and final phase the authors sampled two additional programmer populations, active Java developers, and very experienced developers, with the goal of further elaborating and refining the categories and relationships in the theory. The final resulting model looks like this:  ( Enlarge )  The most frequently cited tasks that an expert should be good at were designing software architecture, writing source code, and analysing and understanding requirements. Within the software architecture task, understanding modularisation and decomposition were frequently mentioned. In terms of personality traits, experts should be open minded and curious, be team players, and pay attention to detail. Patience and self-reflection were also cited. In terms of general skills, “problem solving” came top of the list under which analytical thinking, logical thinking, and abstraction/decomposition all feature. Another important skill is being to assess trade-offs. Mentors should be guiding, patient, and open-minded. Participants were most motivated by mentors that posed challenging tasks. To facilitate continuous development of their employee’s software development skills, (employees suggested that) employers should:  Encourage learning (e.g. training courses, conference attendance, and access to a good analog or digital library)  Encourage experimentation (e.g. through side projects and by building a work environment that is open to new ideas and technologies)  Improve information exchange between development teams, departments, and even companies. E.g. lunch and learn sessions, rotation between teams, pairing, mentoring, and code reviews. Grant freedom (primarily in the form of less time pressure) to allow developers to invest in learning new technologies or skills. In contrast, non-challenging or routine tasks result in demotivation. Other causes of performance decline over time are lack of a clear vision or direction, absence of reward for quality work, stress in the work environment, and bad management or team structure. Your turn  How will you ensure that in 2019 you grow your expertise, and not simply add another year of (the same or similar) ‘experience’ ? See you in January! Thanks, Adrian.", "pdf_url": "https://arxiv.org/pdf/1807.06087", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/towards-a-theory-of-software-development-expertise.json"}
{"id": "23313716", "bin": "1400_1500", "summary_sentences": ["Keeping master green at scale Ananthanarayanan et al., EuroSys’19  This paper provides a fascinating look at a key part of Uber’s software delivery machine.", "With a monorepo, and many thousands of engineers concurrently committing changes, keeping the build green, and keeping commit-to-live latencies low, is a major challenge.", "This paper introduces a change management system called SubmitQueue that is responsible for continuous integration of changes into the mainline at scale while always keeping the mainline green.", "The challenge: build fails at scale  Each individual submitted change will have passed all local tests, but when you put large numbers of concurrent changes together conflicts can still happen.", "Finding out what’s gone wrong is a tedious and error-prone task often requiring human intervention.", "Meanwhile, new features are blocked from rolling out.", "So the goal is to keep it green:  …the monorepo mainline needs to remain green at all times.", "A mainline is called green if all build steps (e.g., compilation, unit tests, UI tests) can successfully execute for every commit point in the history.", "Keeping the mainline green allows developers to (i) instantly release new features from any commit point in the mainline, (ii) roll back to any previously committed change, and not necessarily to the last working version, and (iii) always develop against the most recent and healthy version of the monorepo.", "Here’s 9 months of data for the Uber iOS and Android repos, showing the probability of conflicts as the number of concurrent changes increases:  At ‘only’ 16 concurrent and potentially conflicting changes, there’s a 40% chance of a problem.", "Thus, “despite all efforts to minimize mainline breakages, it is very likely that the mainline experiences daily breakages due to the sheer volume of everyday code changes committed to a big monorepo.”  And that’s exactly what Uber saw.", "Here’s a one week view of the iOS mainline prior to the introduction of SubmitQueue.", "The mainline was green only 52% of the time.", "(Since the introduction of SubmitQueue over a year ago, mainlines have remained green at all times).", "To keep the mainline green we need to totally order changes and only apply patches to mainline HEAD if all build steps succeed.", "The simplest solution to keep the mainline green is to enqueue every change that gets submitted to the system.", "A change at the head of the queue gets committed into the mainline if its build steps succeed.", "For instance, the rustproject uses this technique to ensure that the mainline remains healthy all the time.", "This approach does not scale as the number of changes grows.", "For instance, with a thousand changes per day, where each change takes 30 minutes to pass all build steps, the turnaround time of the last enqueued change will be over 20 days.", "20 day turnarounds clearly is not going to lead to a high performing organisation!", "One possible solution to reduce the latency is batching changes, but then we’re back at the problem of conflicts and complex manual resolution if we’re not careful.", "Another tactic is optimistic execution – given enough compute we can start builds in parallel on, with the assumption that all pending changes submitted will succeed.", "This approach suffers from high failure rates and turnaround times still though as failure of a change can abort many optimistically executing builds.", "SubmitQueue  Uber’s solution to these challenges is SubmitQueue.", "SubmitQueue guarantees an always green mainline by providing the illusion of a single queue where every change gets enqueued, performs all its build steps, and ultimately gets merged with the mainline branch if all build steps succeed.", "Developers create changes, which pending a review process are packaged into a revision.", "Revisions are submitted to the SubmitQueue for integration into the monorepo.", "SubmitQueue’s planner engine orchestrates executions of ending changes.", "In order to scale to thousands of changes per day while ensuring serializability, the planner engine speculates on outcomes of pending changes using a speculation engine, and executes their corresponding builds in parallel by using a build controller.", "The planner periodically asks the speculation engine for the builds most likely to succeed.", "The speculation engine in turn uses a probabilistic model to compute the likelihood of a given build passing.", "At each epoch the planner schedules execution of the selected builds and stops execution of any currently running builds not included in the new schedule.", "Once it is safe to do so, the planner commits change patches to the monorepo.", "When distributing work among worker nodes, the planner tries to ensure a uniform distribution.", "To this end, it keep a history of build steps performed together with their average build durations.", "The key challenge is to determine which set of builds we need to run in parallel, in order to improve turnaround time and throughput, while ensuring an always green mainline.", "To this end, the speculation engine builds a binary decision tree, called a speculation tree, annotated with prediction probabilities for each edge.", "The model selects builds based on their predicted value – which is a combination of likelihood of success and change priority (e.g. , security patches may have higher values).", "In the current implementation, all builds are given the same priority (benefit) value.", "When we include independent changes in the mix, the speculation tree can become a speculation graph.", "This enables independent changes to be committed in parallel.", "To determine independence, we need to know if changes conflict with each other.", "In order to build a conflict graph among pending changes, the conflict analyzer relies on the build system.", "A build system partitions the code into smaller entities called targets… Roughly speaking, two changes conflict if they both affect a common set of build targets.", "Every build target is associated with a unique target hash that represents its current state (a bit like a Merkle tree, this is the result of combining the hashes of all the inputs to the build of that target).", "Predicting success  We trained our success prediction models in a supervised manner using logistic regression.", "We selected historical changes that went through SubmitQueue along with their final results for this purpose.", "We then extracted around 100 handpicked features.", "The trained model achieved 97% accuracy.", "The features with the highest positive correlation scores were:  The number of successful speculations so far  Revision and revert test plans included as part of the submission  The number of initial tests that succeeded before submitting a change  The strongest negative correlations were with the number of failed speculations, and the number of times changes were submitted to a revision.", "We also note that while developer features such as the developer name had high predictive power, the correlation varied based on different developers.", "Evaluation  Taken in isolation, an iOS or Android build at Uber takes around 30-60 minutes:  When considering concurrent changes, and given an Oracle able to make perfect predictions, the turnaround times for builds looks like this:  (Each plot line shows a different number of changes per hour coming into the system).", "With n changes per hour, and n worker nodes available, SubmitQueue can achieve a turnaround time with 1.2x of the Oracle.", "Future work  The current version of SubmitQueue respects the order in which changes are submitted to the system.", "Thus small changes can be backed up behind larger ones.", "Future work will include re-ordering of non-independent changes to improve throughput.", "Another optimisation to be explored is batching independent changes expected to succeed together before running their build steps.", "This will enable Uber to make trade-offs between cost and turnaround time."], "summary_text": "Keeping master green at scale Ananthanarayanan et al., EuroSys’19  This paper provides a fascinating look at a key part of Uber’s software delivery machine. With a monorepo, and many thousands of engineers concurrently committing changes, keeping the build green, and keeping commit-to-live latencies low, is a major challenge. This paper introduces a change management system called SubmitQueue that is responsible for continuous integration of changes into the mainline at scale while always keeping the mainline green. The challenge: build fails at scale  Each individual submitted change will have passed all local tests, but when you put large numbers of concurrent changes together conflicts can still happen. Finding out what’s gone wrong is a tedious and error-prone task often requiring human intervention. Meanwhile, new features are blocked from rolling out. So the goal is to keep it green:  …the monorepo mainline needs to remain green at all times. A mainline is called green if all build steps (e.g., compilation, unit tests, UI tests) can successfully execute for every commit point in the history. Keeping the mainline green allows developers to (i) instantly release new features from any commit point in the mainline, (ii) roll back to any previously committed change, and not necessarily to the last working version, and (iii) always develop against the most recent and healthy version of the monorepo. Here’s 9 months of data for the Uber iOS and Android repos, showing the probability of conflicts as the number of concurrent changes increases:  At ‘only’ 16 concurrent and potentially conflicting changes, there’s a 40% chance of a problem. Thus, “despite all efforts to minimize mainline breakages, it is very likely that the mainline experiences daily breakages due to the sheer volume of everyday code changes committed to a big monorepo.”  And that’s exactly what Uber saw. Here’s a one week view of the iOS mainline prior to the introduction of SubmitQueue. The mainline was green only 52% of the time. (Since the introduction of SubmitQueue over a year ago, mainlines have remained green at all times). To keep the mainline green we need to totally order changes and only apply patches to mainline HEAD if all build steps succeed. The simplest solution to keep the mainline green is to enqueue every change that gets submitted to the system. A change at the head of the queue gets committed into the mainline if its build steps succeed. For instance, the rustproject uses this technique to ensure that the mainline remains healthy all the time. This approach does not scale as the number of changes grows. For instance, with a thousand changes per day, where each change takes 30 minutes to pass all build steps, the turnaround time of the last enqueued change will be over 20 days. 20 day turnarounds clearly is not going to lead to a high performing organisation! One possible solution to reduce the latency is batching changes, but then we’re back at the problem of conflicts and complex manual resolution if we’re not careful. Another tactic is optimistic execution – given enough compute we can start builds in parallel on, with the assumption that all pending changes submitted will succeed. This approach suffers from high failure rates and turnaround times still though as failure of a change can abort many optimistically executing builds. SubmitQueue  Uber’s solution to these challenges is SubmitQueue. SubmitQueue guarantees an always green mainline by providing the illusion of a single queue where every change gets enqueued, performs all its build steps, and ultimately gets merged with the mainline branch if all build steps succeed. Developers create changes, which pending a review process are packaged into a revision. Revisions are submitted to the SubmitQueue for integration into the monorepo. SubmitQueue’s planner engine orchestrates executions of ending changes. In order to scale to thousands of changes per day while ensuring serializability, the planner engine speculates on outcomes of pending changes using a speculation engine, and executes their corresponding builds in parallel by using a build controller. The planner periodically asks the speculation engine for the builds most likely to succeed. The speculation engine in turn uses a probabilistic model to compute the likelihood of a given build passing. At each epoch the planner schedules execution of the selected builds and stops execution of any currently running builds not included in the new schedule. Once it is safe to do so, the planner commits change patches to the monorepo. When distributing work among worker nodes, the planner tries to ensure a uniform distribution. To this end, it keep a history of build steps performed together with their average build durations. The key challenge is to determine which set of builds we need to run in parallel, in order to improve turnaround time and throughput, while ensuring an always green mainline. To this end, the speculation engine builds a binary decision tree, called a speculation tree, annotated with prediction probabilities for each edge. The model selects builds based on their predicted value – which is a combination of likelihood of success and change priority (e.g. , security patches may have higher values). In the current implementation, all builds are given the same priority (benefit) value. When we include independent changes in the mix, the speculation tree can become a speculation graph. This enables independent changes to be committed in parallel. To determine independence, we need to know if changes conflict with each other. In order to build a conflict graph among pending changes, the conflict analyzer relies on the build system. A build system partitions the code into smaller entities called targets… Roughly speaking, two changes conflict if they both affect a common set of build targets. Every build target is associated with a unique target hash that represents its current state (a bit like a Merkle tree, this is the result of combining the hashes of all the inputs to the build of that target). Predicting success  We trained our success prediction models in a supervised manner using logistic regression. We selected historical changes that went through SubmitQueue along with their final results for this purpose. We then extracted around 100 handpicked features. The trained model achieved 97% accuracy. The features with the highest positive correlation scores were:  The number of successful speculations so far  Revision and revert test plans included as part of the submission  The number of initial tests that succeeded before submitting a change  The strongest negative correlations were with the number of failed speculations, and the number of times changes were submitted to a revision. We also note that while developer features such as the developer name had high predictive power, the correlation varied based on different developers. Evaluation  Taken in isolation, an iOS or Android build at Uber takes around 30-60 minutes:  When considering concurrent changes, and given an Oracle able to make perfect predictions, the turnaround times for builds looks like this:  (Each plot line shows a different number of changes per hour coming into the system). With n changes per hour, and n worker nodes available, SubmitQueue can achieve a turnaround time with 1.2x of the Oracle. Future work  The current version of SubmitQueue respects the order in which changes are submitted to the system. Thus small changes can be backed up behind larger ones. Future work will include re-ordering of non-independent changes to improve throughput. Another optimisation to be explored is batching independent changes expected to succeed together before running their build steps. This will enable Uber to make trade-offs between cost and turnaround time.", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3302424.3303970?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/keeping-master-green-at-scale.json"}
{"id": "74156567", "bin": "1400_1500", "summary_sentences": ["Columnstore and B+ tree – are hybrid physical designs important?", "Dziedzic et al., SIGMOD’18  Earlier this week we looked at the design of column stores and their advantages for analytic workloads.", "What should you do though if you have a mixed workload including transaction processing, decision support, and operational analytics?", "Microsoft SQL Server supports hybrid physical design combining both column store and B+ tree indexes in the same database.", "It is generally understood that columnstores are crucial to achieving high performance for analytic queries and that B+ tree indexes are key to supporting transactional workloads efficiently.", "However, it is not well understood whether hybrid physical designs – both columnstore and B+ tree indices on the same database and potentially the same table – are important for any of the above workloads.", "Through a series of benchmarks the authors show that hybrid physical designs can result in more than an order of magnitude lower execution costs for many workloads when compared to alternatives using B+ tree-only or columnstore-only.", "The Database Engine Tuning Advisor (DTA) for SQL Server is extended to analyze and recommend the appropriate indices for a given workload.", "Support for columnstore indices and the new DTA functionality was released in January 2017 as part of the Community Technology Preview release for Microsoft SQL Server 2017.", "Physical design options in SQL Server  RDBMSs have supported B+ trees and heap files for several decades.", "With the advent of columnstores, which significantly outperform B+ trees for data analysis workloads, many commercial RDBMS vendors have added support for columnstore indexes (CSI)…  In SQL Server columnstores are treated as indexes.", "They can be primary (the main storage for all columns of the table) or secondary (e.g., just a subset of columns).", "You can have any combination of primary and secondary indexes on the same table.", "The primary can be a heap file, B+ tree, or a columnstore, and secondaries can be B+ trees or columnstore.", "At most one columnstore index is supported per table though.", "SQL Server columnstores support vectorised operations and compression using run-length encoding and dictionary encoding.", "Within a columnstore, column data is held in column segments, each with data for 100K-1M rows.", "Inserts are handled using delta stores implemented as B+ trees.", "Secondary columnstores, optimised for operational analytics, used a B+ tree delete buffer for logical deletion of rows.", "This is periodically compressed into a delete bitmap storing the physical identifiers of the deleted rows.", "Primary columnstores don’t support delete buffers and work directly with delete bitmaps instead.", "Microbenchmarks  The authors conducted a series of microbenchmarks as follows:  scans with single predicates with varying selectivity to study the trade-off between the range scan of a B+ tree vs a columnstore scan  sort and group-by queries to study the benefit of the sort order supported by B+ trees (columnstores in SQL Server are not sorted).", "update statements with varying numbers of updated rows to analyze the cost of updating the different index types  mixed workloads with different combinations of reads and updates  The key findings are summarised in the table below.", "In a nutshell, B+ tree indexes are suitable for short range scans where the index allows efficient point and short range lookups.", "B+ trees are also the cheapest to update.", "On the other hand, primary CSIs are most suitable for large scans and bulk updates typical in data warehousing and analysis workloads.", "Secondary CSIs can provide significant speed-up for operational analytics on the same database where the OLTP application generating the data also runs.", "The basic workload axes can be combined in a variety of ways where a mix of the basic physical design axes are needed for optimal performance.", "Well, surprise!", "B+ trees are good for OLTP, and columstores are good for analytics.", "Where things get interesting though, is when we combine the two for certain workloads….", "Recommending hybrid designs  The Database Engine Tuning Advisor (DTA) recommends B+ tree indexes (primary and/or secondary), materialized views, and partitioning.", "We extended DTA to analyze the combined space of B+ tree and columnstore indexes.", "By analyzing the workload, DTA is now capable of recommending B+ tree indexes only, columnstore indexes only, or a combination.", "The high level architecture of DTA is shown in the figure below.", "DTA begins with a local per-query analysis stage called candidate selection, which determines the optimal set ofd indexes for each query.", "Then it proceeds to global analysis, starting out by considering the potential to merge indexes on the same table.", "Once this is done DTA selects a final set of indexes to minimise the total cost of the workload subject to specified constraints.", "DTA uses a cost-base search, which means it needs to estimate the costs using some indexes it hasn’t actually built yet.", "The “what-if” API is used to simulate such hypothetical indexes.", "During the candidate selection phase DTA considers columnstore indexes on the tables referenced in the query.", "Given the constraint in SQL Server of only one columnstore index per table DTA chooses to include all columns with data types suitable for columnstore indexes.", "(And if the table includes a column with an unsupported data type, then a primary columnstore index is ruled out).", "During merging there’s not much extra that can be done – only one columstore index is supported per table, and we can’t merge a columnstore index with a B+ tree index.", "During the final selection phase it is necessary to estimated per-column sizes for cost estimation.", "For hypothetical indexes, we need to do this without actually building the index.", "DTA uses block-level sampling coupled with techniques to estimate the impact of compression.", "The effectiveness of run-length encoding depends on the number of runs in the column and the length of each run… SQL Server uses a greedy strategy that picks the next column to sort by based on the column with the fewest runs; we mimic this approach in our [estimation] technique.", "The GEE estimator is used to estimate the number of distinct values for a set of columns.", "Evaluation  The paper closes with an evaluation of both industry-standard benchmarks and real-world customer workloads to see how well the hybrid physical designs suggested by DTA improve query performance.", "Read-only workloads are based on the TPC-DS benchmark and five real customer workloads.", "For mixed workloads, the CH benchmark (an extension of TPC-C) is used.", "Figure 9 below shows the results for the read-only workloads.", "In each chart the blue bars show the speed-up obtained by the hybrid design vs CSI-only, and the green bars show the speed-up versus a B+ tree only design.", "For example, on TPC-DS, 46 queries were sped-up by a factor of 1.2x when comparing the hybrid design to a CSI only design.", "( Enlarge )  … hybrid leverages the best of columnstores and B+ tree across several workloads.", "For each workload, there are several queries for which a hybrid physical design results in more than an order of magnitude improvement in execution cost.", "In some cases, the improvement is 2-3 orders of magnitude.", "An example of a TCP-DC query that really benefits from the hybrid design is query #54.", "It references several large fact tables and as well as many dimension tables.", "The predicates on the dimension tables are selective enough that B+ trees have a significant advantage.", "Other tables have columnstore indexes.", "A similar pattern emerges with the workload of customer four where the optimiser uses an index seek on the fact table(s) followed by a scan of the columnstore on the dimensions, joining the tables with a hash join.", "Here are the results for the CH workload:  The hybrid design significantly speeds up the H (analytic) queries, while resulting in a moderate slow-down for the C (OLTP) queries – mostly the write transactions NewOrder and Payment."], "summary_text": "Columnstore and B+ tree – are hybrid physical designs important? Dziedzic et al., SIGMOD’18  Earlier this week we looked at the design of column stores and their advantages for analytic workloads. What should you do though if you have a mixed workload including transaction processing, decision support, and operational analytics? Microsoft SQL Server supports hybrid physical design combining both column store and B+ tree indexes in the same database. It is generally understood that columnstores are crucial to achieving high performance for analytic queries and that B+ tree indexes are key to supporting transactional workloads efficiently. However, it is not well understood whether hybrid physical designs – both columnstore and B+ tree indices on the same database and potentially the same table – are important for any of the above workloads. Through a series of benchmarks the authors show that hybrid physical designs can result in more than an order of magnitude lower execution costs for many workloads when compared to alternatives using B+ tree-only or columnstore-only. The Database Engine Tuning Advisor (DTA) for SQL Server is extended to analyze and recommend the appropriate indices for a given workload. Support for columnstore indices and the new DTA functionality was released in January 2017 as part of the Community Technology Preview release for Microsoft SQL Server 2017. Physical design options in SQL Server  RDBMSs have supported B+ trees and heap files for several decades. With the advent of columnstores, which significantly outperform B+ trees for data analysis workloads, many commercial RDBMS vendors have added support for columnstore indexes (CSI)…  In SQL Server columnstores are treated as indexes. They can be primary (the main storage for all columns of the table) or secondary (e.g., just a subset of columns). You can have any combination of primary and secondary indexes on the same table. The primary can be a heap file, B+ tree, or a columnstore, and secondaries can be B+ trees or columnstore. At most one columnstore index is supported per table though. SQL Server columnstores support vectorised operations and compression using run-length encoding and dictionary encoding. Within a columnstore, column data is held in column segments, each with data for 100K-1M rows. Inserts are handled using delta stores implemented as B+ trees. Secondary columnstores, optimised for operational analytics, used a B+ tree delete buffer for logical deletion of rows. This is periodically compressed into a delete bitmap storing the physical identifiers of the deleted rows. Primary columnstores don’t support delete buffers and work directly with delete bitmaps instead. Microbenchmarks  The authors conducted a series of microbenchmarks as follows:  scans with single predicates with varying selectivity to study the trade-off between the range scan of a B+ tree vs a columnstore scan  sort and group-by queries to study the benefit of the sort order supported by B+ trees (columnstores in SQL Server are not sorted). update statements with varying numbers of updated rows to analyze the cost of updating the different index types  mixed workloads with different combinations of reads and updates  The key findings are summarised in the table below. In a nutshell, B+ tree indexes are suitable for short range scans where the index allows efficient point and short range lookups. B+ trees are also the cheapest to update. On the other hand, primary CSIs are most suitable for large scans and bulk updates typical in data warehousing and analysis workloads. Secondary CSIs can provide significant speed-up for operational analytics on the same database where the OLTP application generating the data also runs. The basic workload axes can be combined in a variety of ways where a mix of the basic physical design axes are needed for optimal performance. Well, surprise! B+ trees are good for OLTP, and columstores are good for analytics. Where things get interesting though, is when we combine the two for certain workloads…. Recommending hybrid designs  The Database Engine Tuning Advisor (DTA) recommends B+ tree indexes (primary and/or secondary), materialized views, and partitioning. We extended DTA to analyze the combined space of B+ tree and columnstore indexes. By analyzing the workload, DTA is now capable of recommending B+ tree indexes only, columnstore indexes only, or a combination. The high level architecture of DTA is shown in the figure below. DTA begins with a local per-query analysis stage called candidate selection, which determines the optimal set ofd indexes for each query. Then it proceeds to global analysis, starting out by considering the potential to merge indexes on the same table. Once this is done DTA selects a final set of indexes to minimise the total cost of the workload subject to specified constraints. DTA uses a cost-base search, which means it needs to estimate the costs using some indexes it hasn’t actually built yet. The “what-if” API is used to simulate such hypothetical indexes. During the candidate selection phase DTA considers columnstore indexes on the tables referenced in the query. Given the constraint in SQL Server of only one columnstore index per table DTA chooses to include all columns with data types suitable for columnstore indexes. (And if the table includes a column with an unsupported data type, then a primary columnstore index is ruled out). During merging there’s not much extra that can be done – only one columstore index is supported per table, and we can’t merge a columnstore index with a B+ tree index. During the final selection phase it is necessary to estimated per-column sizes for cost estimation. For hypothetical indexes, we need to do this without actually building the index. DTA uses block-level sampling coupled with techniques to estimate the impact of compression. The effectiveness of run-length encoding depends on the number of runs in the column and the length of each run… SQL Server uses a greedy strategy that picks the next column to sort by based on the column with the fewest runs; we mimic this approach in our [estimation] technique. The GEE estimator is used to estimate the number of distinct values for a set of columns. Evaluation  The paper closes with an evaluation of both industry-standard benchmarks and real-world customer workloads to see how well the hybrid physical designs suggested by DTA improve query performance. Read-only workloads are based on the TPC-DS benchmark and five real customer workloads. For mixed workloads, the CH benchmark (an extension of TPC-C) is used. Figure 9 below shows the results for the read-only workloads. In each chart the blue bars show the speed-up obtained by the hybrid design vs CSI-only, and the green bars show the speed-up versus a B+ tree only design. For example, on TPC-DS, 46 queries were sped-up by a factor of 1.2x when comparing the hybrid design to a CSI only design. ( Enlarge )  … hybrid leverages the best of columnstores and B+ tree across several workloads. For each workload, there are several queries for which a hybrid physical design results in more than an order of magnitude improvement in execution cost. In some cases, the improvement is 2-3 orders of magnitude. An example of a TCP-DC query that really benefits from the hybrid design is query #54. It references several large fact tables and as well as many dimension tables. The predicates on the dimension tables are selective enough that B+ trees have a significant advantage. Other tables have columnstore indexes. A similar pattern emerges with the workload of customer four where the optimiser uses an index seek on the fact table(s) followed by a scan of the columnstore on the dimensions, joining the tables with a hash join. Here are the results for the CH workload:  The hybrid design significantly speeds up the H (analytic) queries, while resulting in a moderate slow-down for the C (OLTP) queries – mostly the write transactions NewOrder and Payment.", "pdf_url": "https://adam-dziedzic.github.io/static/assets/papers/dziedzic-sigmod2018-recommend-hybrid-designs.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/columnstore-and-b-tree-are-hybrid-physical-designs-important.json"}
{"id": "29474820", "bin": "1500_1600", "summary_sentences": ["Occupy the cloud: distributed computing for the 99% Jonas et al., SoCC’17  ‘Occupy the cloud’ won the best vision paper award at the recent ACM Symposium on Cloud Computing event.", "In the spirit of a vision paper, you won’t find detailed implementation and evaluation  information here, but hopefully you’ll find something to make you think.", "If I had to summarise the argument, I think it’s this: speed of deployment and ease of use coupled with good enough runtime performance wins.", "DevOps are the new Kingmakers.", "Follow that line-of-thought, and the authors take you to an interesting proposition: serverless could be the centrepiece of a new distributed data processing platform.", "In this paper we argue that a serverless execution model with stateless functions can enable radically-simpler, fundamentally elastic, and more user-friendly distributed data processing systems.", "In this model, we have one simple primitive: users submit functions that are executed in a remote container; the functions are stateless as all the state for the function, including input, output is accessed from shared remote storage.", "At this point the alarm bells are probably ringing.", "How on earth can you make that even remotely efficient when all you have are stateless functions and hence you’ll be serializing and deserializing all over the place?", "Surprisingly, we find that the performance degradation from using such an approach is negligible for many workloads and thus, our simple primitive is in fact general enough to implement a number of higher-level data processing abstractions, including MapReduce and parameter servers.", "What users want  Many more casual users of data processing platforms (the 99% from the paper title) don’t want to spend hours understanding a complex stack of e.g. Spark, Scala, HDFS, Yarn and the JVM.", "Even the choice of instance types (70 on AWS at the time the paper was written), cluster sizes, regions and so on can be bewildering.", "… most software, especially in scientific and analytics applications, is not written by computer scientists, and it is many of these users who have been left out of the cloud revolution.", "What do these users want?", "To get vastly better performance than available on their laptop or workstation while taking minimal development time.", "For an interesting number of compute bound workloads, such as trying a large number of random initial seeds in a Monte Carlo simulations, or sweeping over a wide-range of hyperparameters in a machine learning setting (or even being guided through a set of input parameters by a system such as Vizier ), it’s a big step forward to parallelise across functions, without necessarily worrying about intra-function optimisations.", "Therefore, a simple function interface that captures sufficient local state, performs computation remotely, and returns the result is more than adequate.", "Even for data-bound workloads, many users would be served by a simpler version of existing map-reduce frameworks with outputs persisted on object storage.", "How simple can we make it?", "Many of the problems with current cloud computing abstractions stem from the fact that they are designed for a server-oriented resource model.", "Having servers as the unit of abstraction ties together multiple resources like memory, CPU and network bandwidth.", "Further servers are also often long running and hence require DevOps support for maintenance.", "Our proposal is to to instead use a serverless architecture with stateless functions as the unifying abstraction for data processing.", "Users can run arbitrary (data processing) functions without setting up and configuring servers/frameworks etc.", "To make this work, you need a low overhead execution runtime, a fast scheduler, and high performance remote storage.", "The architecture at the base level is very simple by design and just includes the basics needed to execute functions.", "More complex abstractions such as dataflow and BSP are implemented on top.", "What makes this more tractable than in the past is that the benefits of colocation (of code and data) are diminishing.", "For example, on AWS EC2 writing to remote storage is faster than storing data on a single local SSD (though not than multiple SSDs).", "The gap between network bandwidth and storage I/O bandwidth continues to narrow though.", "While the developer has no control of where a stateless function runs… the benefits of colocating computation and data – a major design goal for prior systems like Hadoop, Spark and Dryad – have diminished.", "The PyWren prototype  It will probably come as no surprise to you to hear that the prototype is built on top of AWS Lambda.", "PyWren exposes a map primitive from Python on top of Lambda.", "On first encounter there’s a slightly weird thing going on whereby instead of having the users deploy Lambda functions directly, PyWren has just a single common Lambda function.", "PyWren works by serializing the user written Python functions using cloudpickle and storing the results in S3 buckets.", "The single PyWren Lambda function then loads both the data to be processed and the function to process it from S3, with the result being serialized back to S3 at a pre-specified key.", "The stated reasons for doing it this way are:  to eliminate the majority of user overhead from deployment, packaging, and code versioning.", "to mitigate the high latency for function registration  to be able to execute functions that exceed Lambda’s code size limit  I would anticipate those advantages diminishing over time.", "PyWren’s map API mirrors the existing Python API for parallel processing, and so integrates easily with existing libraries for data processing and visualization.", "Calling map launches as many stateless functions as there are elements in the list that one is mapping over.", "The following benchmark results show the impact of using remote storage only and how that scales with worker counts.", "In our research group we have had students use PyWren for applications as diverse as computational imaging, scientific instrument design, solar physics, and object recognition.", "Working with heliphysicists at NASA’s Solar Dynamics Observatory, we have used PyWren for extracting relevant features across 16TB of solar imaging data for solar flare prediction.", "Working with applied physics colleagues, we have used PyWren to design novel types of microscope point-spread functions for 3d superresolution microscopy.", "This necessitates rapid and repeat evaluation of a complex physics-based optical model inside an inner-loop.", "Beyond simple functions: dataflow and BSP  The next step beyond a simple parallel map, is a parallel map with a single global reduce stage running on one machine.", "This suits a number of classical machine learning workloads, and can handle learning problems up to 2TB in size.", "BSP algorithms can be implemented by adding data shuffles across stages using high bandwidth remote storage.", "The authors use both PySpark and PyWren to run a word count program over a dataset of 83.68M product reviews split into 333 partitions.", "PyWren is 17% slower (about 14s), but this doesn’t count the 5-10 minutes needed to start the Spark cluster.", "The shuffle intensive Daytona sort benchmark is more challenging for PyWren.", "Finally using low-latency, high throughput key-value stores like Redis, RAMCloud, we can also implement parameter server style applications in PyWren.", "For example, we can implement HOGWILD!", "stochastic gradient descent by having each function compute the gradients based on the latest version of the shared model.", "Prototype limitations  You need to fit within the existing Lambda resource limits.", "Under current Lambda constraints this is enough to fill up the function’s 1.5GB of memory in around 40 seconds.", "Assuming the same time to write results back out you have up to 80 seconds of I/O time, and therefore about 220 seconds of compute.", "The pricing works out at about 2x that of on-demand instances, if you assume 100% utilisation of those instances.", "There is no opportunity to influence the scheduling  Debugging is harder, relying on AWS CloudWatch logs.", "Large shuffle intensive workloads are not easily supported  It takes about 20-30 seconds to launch a function in the absence of caching – setting up the Python runtime, downloading the code to run from S3 etc..", "Applications requiring access to specialized hardware (GPU, FPGA) aren’t supported in Lambda at the moment.", "Despite all of these, the list of applications that have been successfully deployed using PyWren (as we looked at earlier) is still quite impressive."], "summary_text": "Occupy the cloud: distributed computing for the 99% Jonas et al., SoCC’17  ‘Occupy the cloud’ won the best vision paper award at the recent ACM Symposium on Cloud Computing event. In the spirit of a vision paper, you won’t find detailed implementation and evaluation  information here, but hopefully you’ll find something to make you think. If I had to summarise the argument, I think it’s this: speed of deployment and ease of use coupled with good enough runtime performance wins. DevOps are the new Kingmakers. Follow that line-of-thought, and the authors take you to an interesting proposition: serverless could be the centrepiece of a new distributed data processing platform. In this paper we argue that a serverless execution model with stateless functions can enable radically-simpler, fundamentally elastic, and more user-friendly distributed data processing systems. In this model, we have one simple primitive: users submit functions that are executed in a remote container; the functions are stateless as all the state for the function, including input, output is accessed from shared remote storage. At this point the alarm bells are probably ringing. How on earth can you make that even remotely efficient when all you have are stateless functions and hence you’ll be serializing and deserializing all over the place? Surprisingly, we find that the performance degradation from using such an approach is negligible for many workloads and thus, our simple primitive is in fact general enough to implement a number of higher-level data processing abstractions, including MapReduce and parameter servers. What users want  Many more casual users of data processing platforms (the 99% from the paper title) don’t want to spend hours understanding a complex stack of e.g. Spark, Scala, HDFS, Yarn and the JVM. Even the choice of instance types (70 on AWS at the time the paper was written), cluster sizes, regions and so on can be bewildering. … most software, especially in scientific and analytics applications, is not written by computer scientists, and it is many of these users who have been left out of the cloud revolution. What do these users want? To get vastly better performance than available on their laptop or workstation while taking minimal development time. For an interesting number of compute bound workloads, such as trying a large number of random initial seeds in a Monte Carlo simulations, or sweeping over a wide-range of hyperparameters in a machine learning setting (or even being guided through a set of input parameters by a system such as Vizier ), it’s a big step forward to parallelise across functions, without necessarily worrying about intra-function optimisations. Therefore, a simple function interface that captures sufficient local state, performs computation remotely, and returns the result is more than adequate. Even for data-bound workloads, many users would be served by a simpler version of existing map-reduce frameworks with outputs persisted on object storage. How simple can we make it? Many of the problems with current cloud computing abstractions stem from the fact that they are designed for a server-oriented resource model. Having servers as the unit of abstraction ties together multiple resources like memory, CPU and network bandwidth. Further servers are also often long running and hence require DevOps support for maintenance. Our proposal is to to instead use a serverless architecture with stateless functions as the unifying abstraction for data processing. Users can run arbitrary (data processing) functions without setting up and configuring servers/frameworks etc. To make this work, you need a low overhead execution runtime, a fast scheduler, and high performance remote storage. The architecture at the base level is very simple by design and just includes the basics needed to execute functions. More complex abstractions such as dataflow and BSP are implemented on top. What makes this more tractable than in the past is that the benefits of colocation (of code and data) are diminishing. For example, on AWS EC2 writing to remote storage is faster than storing data on a single local SSD (though not than multiple SSDs). The gap between network bandwidth and storage I/O bandwidth continues to narrow though. While the developer has no control of where a stateless function runs… the benefits of colocating computation and data – a major design goal for prior systems like Hadoop, Spark and Dryad – have diminished. The PyWren prototype  It will probably come as no surprise to you to hear that the prototype is built on top of AWS Lambda. PyWren exposes a map primitive from Python on top of Lambda. On first encounter there’s a slightly weird thing going on whereby instead of having the users deploy Lambda functions directly, PyWren has just a single common Lambda function. PyWren works by serializing the user written Python functions using cloudpickle and storing the results in S3 buckets. The single PyWren Lambda function then loads both the data to be processed and the function to process it from S3, with the result being serialized back to S3 at a pre-specified key. The stated reasons for doing it this way are:  to eliminate the majority of user overhead from deployment, packaging, and code versioning. to mitigate the high latency for function registration  to be able to execute functions that exceed Lambda’s code size limit  I would anticipate those advantages diminishing over time. PyWren’s map API mirrors the existing Python API for parallel processing, and so integrates easily with existing libraries for data processing and visualization. Calling map launches as many stateless functions as there are elements in the list that one is mapping over. The following benchmark results show the impact of using remote storage only and how that scales with worker counts. In our research group we have had students use PyWren for applications as diverse as computational imaging, scientific instrument design, solar physics, and object recognition. Working with heliphysicists at NASA’s Solar Dynamics Observatory, we have used PyWren for extracting relevant features across 16TB of solar imaging data for solar flare prediction. Working with applied physics colleagues, we have used PyWren to design novel types of microscope point-spread functions for 3d superresolution microscopy. This necessitates rapid and repeat evaluation of a complex physics-based optical model inside an inner-loop. Beyond simple functions: dataflow and BSP  The next step beyond a simple parallel map, is a parallel map with a single global reduce stage running on one machine. This suits a number of classical machine learning workloads, and can handle learning problems up to 2TB in size. BSP algorithms can be implemented by adding data shuffles across stages using high bandwidth remote storage. The authors use both PySpark and PyWren to run a word count program over a dataset of 83.68M product reviews split into 333 partitions. PyWren is 17% slower (about 14s), but this doesn’t count the 5-10 minutes needed to start the Spark cluster. The shuffle intensive Daytona sort benchmark is more challenging for PyWren. Finally using low-latency, high throughput key-value stores like Redis, RAMCloud, we can also implement parameter server style applications in PyWren. For example, we can implement HOGWILD! stochastic gradient descent by having each function compute the gradients based on the latest version of the shared model. Prototype limitations  You need to fit within the existing Lambda resource limits. Under current Lambda constraints this is enough to fill up the function’s 1.5GB of memory in around 40 seconds. Assuming the same time to write results back out you have up to 80 seconds of I/O time, and therefore about 220 seconds of compute. The pricing works out at about 2x that of on-demand instances, if you assume 100% utilisation of those instances. There is no opportunity to influence the scheduling  Debugging is harder, relying on AWS CloudWatch logs. Large shuffle intensive workloads are not easily supported  It takes about 20-30 seconds to launch a function in the absence of caching – setting up the Python runtime, downloading the code to run from S3 etc.. Applications requiring access to specialized hardware (GPU, FPGA) aren’t supported in Lambda at the moment. Despite all of these, the list of applications that have been successfully deployed using PyWren (as we looked at earlier) is still quite impressive.", "pdf_url": "https://arxiv.org/pdf/1702.04024", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/occupy-the-cloud-distributed-computing-for-the-99.json"}
{"id": "66302705", "bin": "1500_1600", "summary_sentences": ["STTR: A system for tracking all vehicles all the time at the edge of the network Xu et al., DEBS’18  With apologies for only bringing you two paper write-ups this week: we moved house, which turns out to be not at all conducive to quiet study of research papers!", "Today’s smart camera surveillance systems are largely alert based, which gives two main modes of operation: either you know in advance the vehicles of interest so that you can detect them in real time, or you have to trawl through lots of camera footage post-facto (expensive and time-consuming).", "STTR is a system designed to track all of the vehicles all of the time, and store their trajectories for ever.", "I certainly have mixed feelings about the kinds of state surveillance and privacy invasions that enables (it’s trivial to link back to individuals given trajectories over time), but here we’ll just focus on the technology.", "Since the system is design with pluggable detection and matching algorithms, then given some calculations around volume it ought to be possible to use it to track objects other than vehicles.", "People for example?", "Assuming the availability of suitable detection and matching (figuring out if a vehicle is one you have seen before) algorithms, then the biggest challenge involved in tracking all of the vehicles all of the time is managing network bandwidth and storage.", "Storing and sending raw video clearly doesn’t work, so STTR simply stores vehicle trajectories (presumably some window, e.g. 24 hours, of video could also be accommodated) and processes video at local nodes.", "An edge/fog node can have multiple connected cameras.", "Processing sensor streams (especially cameras) at the edge of the network is advantageous for three reasons: (a) reducing the latency for processing the streams, (b) reducing the backhaul bandwidth needed to send raw sensor streams to the cloud; and (c) preserving privacy concerns for the collected sensor data.", "(I know what they mean by (c), even though the literal interpretation says the opposite of what the authors intended!).", "Even when we’re storing only trajectories and processing video locally, all trajectories for all vehicles for all time still sounds like a lot of storage, and worse, potentially unbounded.", "The first key insight is that there is a pragmatic bound if we make some assumptions about vehicle density and vehicle lifetime (e.g. age of vehicle or miles travelled).", "The second key insight is that any one node only needs to manage data for a finite ‘activity region’ (loosely, the area that it covers before other cameras take over).", "The finite size of the camera’s activity region gives us a very good property, because at any given time, the number of vehicles that are active under this camera has to be finite, for vehicles need to occupy space.", "Meanwhile, each vehicle’s life is also finite which indicates there exists trajectory upper bound for a given camera.", "Putting these facts together, we have the following simple idea – at any point in time, each camera stores the trajectory of vehicles that are active under its region.", "A back of the envelope calculation with 5 cameras every mile, up to 100 simultaneously active vehicles for a given camera, and a vehicle lifetime of 150,000 miles, gives a storage requirement of around 1.1GB per camera.", "Storing trajectories  The basic idea is to store the trajectory for a given vehicle at the node (camera) where it was last detected (active).", "Considering the following figure, the red car first seen at intersection  by camera  attached to fog node  begins with a trajectory  .", "When it moves through intersection  we append to its known trajectory and migrate the record to node  .", "When the vehicle then arrives at intersection  we migrate the record once more, this time to node  .", "The final trajectory record is  .", "It’s easy to spot that as vehicle trajectories get longer over time, this process will result in a lot of network traffic to migrate trajectories from node to node.", "So instead of full trajectory migration, trajectories are partitioned across nodes and only aggregated once a node comes under storage pressure.", "(The current paper doesn’t consider querying, so implications for query efficiency here are unaddressed).", "We end with a lazy version of the basic algorithm.", "Given the same vehicle movements as above the lazy trajectory aggregation might play out as follows:  At time  we create trajectory record (vertex)  at node  .", "At time  we create an a trajectory vertex at node  containing just the single step:  , and we also create an edge from the vertex at  to the newly created vertex at  (so we’re building up a distributed graph structure).", "This process repeats when the vehicle arrives at  , such that we end up with three linked trajectory portions.", "If node  later comes under storage pressure we can aggregate its portion of the trajectory into the node at  , ending up with an edge from  directly to  , and a trajectory vertex at  of  .", "Once a vehicle is assumed to have reached end-of-life, its trajectory can be migrated to cloud storage or similar.", "Network communication via forward and backward propagation  Information about detected vehicles is propagated through the network in order to build up the distributed graph and to reduce resource requirements at nodes.", "The primary mechanism is forward propagation.", "Once a vehicle has been detected by a camera, the vehicle’s signature (produced by an algorithm provided by domain experts) is sent to all cameras that are likely candidates for the vehicles to pass next.", "I infer that along with this message is sent the identifiers of all cameras in the propagation set.", "The cameras are then already primed to run the re-identification procedure when the vehicle is sighted.", "If a camera does detect a vehicle with a signature it received via forward propagation then it sends a sighting confirmation to all the other cameras in the propagation set, who can now drop it from their forward propagation local storage.", "Sometimes a camera might detect a vehicle that it has not been primed to expect via forward propagation.", "In this case, backward propagation is used to send a message to candidate upstream cameras given the direction of travel.", "If such a camera cannot immediately re-identify the detected object in the backward propagation message it can simply discard it.", "System architecture  The overall architecture of the STTR system looks like this:  Each fog node runs a collection of seven modules per camera (e.g., in a container).", "The detection module takes as input a raw video stream from the camera, and outputs a stream of detected object (signatures).", "The matching module implements the re-identification algorithm, looking for a match with a detected object in the candidate vehicle pool assembled from forward propagation messages  The camera map module maintains the geographical relationship between cameras  The forward and backward modules implement forward and backward propagation  The trajectory store manages trajectories as previously discussed and optimises network consumption  The policy module supports system configuration  The evaluation implementation is written in Python using ZeroMQ for communication between nodes and Redis as a persistent store.", "Evaluation  The evaluation is based on a simulation using the SUMO road traffic simulation package with a 10,000 second traffic flow emulation.", "Traffic is generated using a probability-based model.", "The fog computing topology is emulated using MaxiNet .", "The focus of the experiments is on understanding the storage and network bounds, and the impact of latency in forward and backward propagation.", "The short summary is that things pretty much work as you would expect.", "For example, if you have lower camera density then each camera uses more storage (since it is covering a larger area).", "The critical factor for enabling real-time surveillance turns out to be the speed of the re-identification algorithm.", "Plans for future work include:  Creation of efficient spatial and temporal index structures to support querying the network  Improving confidence in the calculated trajectories using a probabilistic approach to trajectory generation and maintenance  An on-campus deployment in conjunction with the campus police department"], "summary_text": "STTR: A system for tracking all vehicles all the time at the edge of the network Xu et al., DEBS’18  With apologies for only bringing you two paper write-ups this week: we moved house, which turns out to be not at all conducive to quiet study of research papers! Today’s smart camera surveillance systems are largely alert based, which gives two main modes of operation: either you know in advance the vehicles of interest so that you can detect them in real time, or you have to trawl through lots of camera footage post-facto (expensive and time-consuming). STTR is a system designed to track all of the vehicles all of the time, and store their trajectories for ever. I certainly have mixed feelings about the kinds of state surveillance and privacy invasions that enables (it’s trivial to link back to individuals given trajectories over time), but here we’ll just focus on the technology. Since the system is design with pluggable detection and matching algorithms, then given some calculations around volume it ought to be possible to use it to track objects other than vehicles. People for example? Assuming the availability of suitable detection and matching (figuring out if a vehicle is one you have seen before) algorithms, then the biggest challenge involved in tracking all of the vehicles all of the time is managing network bandwidth and storage. Storing and sending raw video clearly doesn’t work, so STTR simply stores vehicle trajectories (presumably some window, e.g. 24 hours, of video could also be accommodated) and processes video at local nodes. An edge/fog node can have multiple connected cameras. Processing sensor streams (especially cameras) at the edge of the network is advantageous for three reasons: (a) reducing the latency for processing the streams, (b) reducing the backhaul bandwidth needed to send raw sensor streams to the cloud; and (c) preserving privacy concerns for the collected sensor data. (I know what they mean by (c), even though the literal interpretation says the opposite of what the authors intended!). Even when we’re storing only trajectories and processing video locally, all trajectories for all vehicles for all time still sounds like a lot of storage, and worse, potentially unbounded. The first key insight is that there is a pragmatic bound if we make some assumptions about vehicle density and vehicle lifetime (e.g. age of vehicle or miles travelled). The second key insight is that any one node only needs to manage data for a finite ‘activity region’ (loosely, the area that it covers before other cameras take over). The finite size of the camera’s activity region gives us a very good property, because at any given time, the number of vehicles that are active under this camera has to be finite, for vehicles need to occupy space. Meanwhile, each vehicle’s life is also finite which indicates there exists trajectory upper bound for a given camera. Putting these facts together, we have the following simple idea – at any point in time, each camera stores the trajectory of vehicles that are active under its region. A back of the envelope calculation with 5 cameras every mile, up to 100 simultaneously active vehicles for a given camera, and a vehicle lifetime of 150,000 miles, gives a storage requirement of around 1.1GB per camera. Storing trajectories  The basic idea is to store the trajectory for a given vehicle at the node (camera) where it was last detected (active). Considering the following figure, the red car first seen at intersection  by camera  attached to fog node  begins with a trajectory  . When it moves through intersection  we append to its known trajectory and migrate the record to node  . When the vehicle then arrives at intersection  we migrate the record once more, this time to node  . The final trajectory record is  . It’s easy to spot that as vehicle trajectories get longer over time, this process will result in a lot of network traffic to migrate trajectories from node to node. So instead of full trajectory migration, trajectories are partitioned across nodes and only aggregated once a node comes under storage pressure. (The current paper doesn’t consider querying, so implications for query efficiency here are unaddressed). We end with a lazy version of the basic algorithm. Given the same vehicle movements as above the lazy trajectory aggregation might play out as follows:  At time  we create trajectory record (vertex)  at node  . At time  we create an a trajectory vertex at node  containing just the single step:  , and we also create an edge from the vertex at  to the newly created vertex at  (so we’re building up a distributed graph structure). This process repeats when the vehicle arrives at  , such that we end up with three linked trajectory portions. If node  later comes under storage pressure we can aggregate its portion of the trajectory into the node at  , ending up with an edge from  directly to  , and a trajectory vertex at  of  . Once a vehicle is assumed to have reached end-of-life, its trajectory can be migrated to cloud storage or similar. Network communication via forward and backward propagation  Information about detected vehicles is propagated through the network in order to build up the distributed graph and to reduce resource requirements at nodes. The primary mechanism is forward propagation. Once a vehicle has been detected by a camera, the vehicle’s signature (produced by an algorithm provided by domain experts) is sent to all cameras that are likely candidates for the vehicles to pass next. I infer that along with this message is sent the identifiers of all cameras in the propagation set. The cameras are then already primed to run the re-identification procedure when the vehicle is sighted. If a camera does detect a vehicle with a signature it received via forward propagation then it sends a sighting confirmation to all the other cameras in the propagation set, who can now drop it from their forward propagation local storage. Sometimes a camera might detect a vehicle that it has not been primed to expect via forward propagation. In this case, backward propagation is used to send a message to candidate upstream cameras given the direction of travel. If such a camera cannot immediately re-identify the detected object in the backward propagation message it can simply discard it. System architecture  The overall architecture of the STTR system looks like this:  Each fog node runs a collection of seven modules per camera (e.g., in a container). The detection module takes as input a raw video stream from the camera, and outputs a stream of detected object (signatures). The matching module implements the re-identification algorithm, looking for a match with a detected object in the candidate vehicle pool assembled from forward propagation messages  The camera map module maintains the geographical relationship between cameras  The forward and backward modules implement forward and backward propagation  The trajectory store manages trajectories as previously discussed and optimises network consumption  The policy module supports system configuration  The evaluation implementation is written in Python using ZeroMQ for communication between nodes and Redis as a persistent store. Evaluation  The evaluation is based on a simulation using the SUMO road traffic simulation package with a 10,000 second traffic flow emulation. Traffic is generated using a probability-based model. The fog computing topology is emulated using MaxiNet . The focus of the experiments is on understanding the storage and network bounds, and the impact of latency in forward and backward propagation. The short summary is that things pretty much work as you would expect. For example, if you have lower camera density then each camera uses more storage (since it is covering a larger area). The critical factor for enabling real-time surveillance turns out to be the speed of the re-identification algorithm. Plans for future work include:  Creation of efficient spatial and temporal index structures to support querying the network  Improving confidence in the calculated trajectories using a probabilistic approach to trajectory generation and maintenance  An on-campus deployment in conjunction with the campus police department", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3210284.3210291?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/sttr-a-system-for-tracking-all-vehicles-all-the-time-at-the-edge-of-the-network.json"}
{"id": "43018598", "bin": "1500_1600", "summary_sentences": ["Understanding real-world concurrency bugs in Go Tu, Liu et al., ASPLOS’19  The design of a programming (or data) model not only makes certain problems easier (or harder) to solve, but also makes certain classes of bugs easier (or harder) to create, detect, and subsequently fix.", "Today’s paper choice studies concurrency mechanisms in Go.", "Before we dive in, it might be interesting to pause for a moment and consider your own beliefs about Go, which may well include some of the following:  Go was explicitly designed to make concurrent programming easier and less error-prone  Go makes concurrent programming easier and less error-prone  Go programs make heavy use of message passing via channels, which is less error prone than shared memory synchronisation  Go programs have less concurrency bugs  Go’s built-in deadlock and data race detectors will catch any (most?)", "bugs you do let slip into your code  The first of those statements is true.", "For the remaining statements, you can use the data from this research to re-evaluate how strongly you want to hold those opinions…  We perform the first systematic study on concurrency bugs in real Go programs.", "We studied six popular Go software [projects] including Docker, Kubernetes, and gRPC.", "We analyzed 171 concurrency bugs in total, with more than half of them caused by non-traditional, Go-specific problems.", "Apart from root causes of these bugs, we also studied their fixes, performed experiments to reproduce them, and evaluated them with two publicly-available Go bug detectors.", "The six applications studied were Docker, Kubernetes, etcd, CockroachDB, gRPC, and BoltDB, so that’s a lot of important real-world Go-code right there.", "The analysis begins by studying how these applications actually make use of Go concurrency primitives, before going on to study concurrency related bugs from their issue trackers.", "These bugs are categorised on two main dimensions: the observed behaviour (blocking or non-blocking), and the type of concurrency primitive that is the cause (shared memory or message passing).", "Let’s begin with a very quick recap of the main concurrency mechanisms in Go.", "Concurrency in Go  A major design goal of Go is to improve traditional multi-threaded programming languages and make concurrent programming easier and less error-prone.", "For this purpose, Go centers its multi-threading design around two principles: 1) making threads (called goroutines) lightweight and easy to create and 2) using explicit messaging (via channels) to communicate across threads.", "Goroutines are lightweight user-level threads (‘green’ threads).", "A goroutine is created by adding the keyword go before a function call, including to an anonymous function.", "Local variables declared before an anonymous function are accessible within it and potentially shared.", "Channels are used to send data and states across goroutines, and may be buffered or unbuffered.", "When using unbuffered channels a goroutine will block on send (receive) until another goroutine is receiving (sending).", "The select statement allows a goroutine to wait on multiple channel operations, if more than one case is valid Go selects one at random.", "Go also has traditional synchronisation primitives including mutexes, condition variables, and atomic variables.", "How Go concurrency primitives are used in practice  The six applications make relatively heavy use of goroutines, especially with anonymous functions.", "An especially interesting comparison can be made in the case of gRPC, which has both a C implementation and a Go implementation.", "The following table shows the ratio of the number of goroutines created in gRPC-Go compared to gRPC-C, when processing the same number of requests.", "In this comparison goroutines tend to shorter lifetimes than the threads created in the C version, but are created much more frequently.", "This more frequent use of goroutines is to be expected as its something the Go language encourages.", "If we look at the use of concurrency primitives across the board in all of the applications, one more surprising finding is that shared memory synchronisation operations are still used more often than message passing:  The most frequently used message-passing primitive is chan, responsible for between 18.5% and 43% of all usages.", "So we have a situation in which traditional shared memory communication is still heavily used, in conjunction with significant amounts of message passing primitives.", "From a bug perspective that means we have all the exciting bug possibilities that shared memory communication affords, together with all of the bug possibilities message passing affords, and bugs caused be the interaction of these two styles!", "Go concurrency bugs  The authors searched the GitHub commit histories of the applications to find commits fixing concurrency bugs (3,211).", "From these 171 were randomly selected for study.", "Bugs are categorised into blocking bugs and non-blocking bugs.", "A blocking bug occurs when one or more goroutines are unintentionally stuck in their execution and cannot move forward.", "This definition is broader deadlocks and can include situations with no circular waits, but instead waits for resources that no other goroutines supply.", "85 of the bugs were blocking, and 86 were non- blocking.", "Bugs are also categorised in a second dimension according to whether they relate to shared memory protection (105 bugs) or message passing (66 bugs).", "Blocking bugs  Looking at the blocking bugs first of all, 42% of these are related to shared memory, and 58% are related to message passing.", "Recall as well that shared memory primitives are more widely used than message passing primitives.", "Contrary to the common belief that message passing is less error-prone, more blocking bugs in our studied Go applications are caused by wrong message passing than by wrong shared memory protection.", "The shared memory based bugs include all the usual suspects, with a few new twists due to Go’s implementation of RWMutex and Wait (see §5.1.1).", "For message-passing related bugs, many of these are due to missing sends or receives on channels, or closing channels.", "All blocking bugs caused by message passing are related to Go’s new message passing semantics like channel.", "They can be difficult to detect especially when message passing operations are used together with other synchronization primitives.", "Contrary to common belief, message passing can cause more blocking bugs than shared memory.", "Investigating the fixes for these bugs showed that once understood, the fixes are fairly simple, and the type of fix is correlated with the bug cause.", "This suggests that fully-automated or semi-automated tools for fixing blocking bugs in Go may be a promising direction.", "Go’s built-in deadlock detector was only able to detect two of the 21 blocking bugs reproduced in the study.", "Non-blocking bugs  More non-blocking bugs are caused by misuse of shared memory primitives than message passing.", "About half of these are caused by ‘traditional’ shared memory bug patterns.", "There are also several bugs caused by a lack of understanding of Go language features, especially the sharing of local variables declared before an anonymous function used in a goroutine, and the semantics of WaitGroups.", "New programming models and new libraries that Go introduced to ease multi-thread programming can themselves be the cause of more concurrency bugs.", "While message-passing based non-blocking bugs were comparatively less common, “the intricate design of message passing in a language can cause these bugs to be especially hard to find when combining with other language-specific features.”  Interestingly, programmers fixing bugs caused by misuse of shared memory primitives showed a preference for using message passing to do so.", "Go’s data race detector was able to detect half of the reproduced non-blocking bugs in the study.", "Wrapping up  Surprisingly, our study shows that it is as easy to make concurrency bugs with message passing as with shared memory, sometimes even more.", "Programmers have to have a clear understanding of:  goroutine creation with anonymous functions  the usage of buffered vs unbuffered channels  the non-determinism of waiting for multiple channel operations using select  correct use of the special library time  Although each of these features were designed to ease multi-threaded programming, in reality, it is difficult to write correct Go programs with them.", "I regret that I didn’t have space in this write-up to include the many illustrative code samples highlighting details of bugs.", "If you’re actively developing in Go, the full paper is well worth a read to study them."], "summary_text": "Understanding real-world concurrency bugs in Go Tu, Liu et al., ASPLOS’19  The design of a programming (or data) model not only makes certain problems easier (or harder) to solve, but also makes certain classes of bugs easier (or harder) to create, detect, and subsequently fix. Today’s paper choice studies concurrency mechanisms in Go. Before we dive in, it might be interesting to pause for a moment and consider your own beliefs about Go, which may well include some of the following:  Go was explicitly designed to make concurrent programming easier and less error-prone  Go makes concurrent programming easier and less error-prone  Go programs make heavy use of message passing via channels, which is less error prone than shared memory synchronisation  Go programs have less concurrency bugs  Go’s built-in deadlock and data race detectors will catch any (most?) bugs you do let slip into your code  The first of those statements is true. For the remaining statements, you can use the data from this research to re-evaluate how strongly you want to hold those opinions…  We perform the first systematic study on concurrency bugs in real Go programs. We studied six popular Go software [projects] including Docker, Kubernetes, and gRPC. We analyzed 171 concurrency bugs in total, with more than half of them caused by non-traditional, Go-specific problems. Apart from root causes of these bugs, we also studied their fixes, performed experiments to reproduce them, and evaluated them with two publicly-available Go bug detectors. The six applications studied were Docker, Kubernetes, etcd, CockroachDB, gRPC, and BoltDB, so that’s a lot of important real-world Go-code right there. The analysis begins by studying how these applications actually make use of Go concurrency primitives, before going on to study concurrency related bugs from their issue trackers. These bugs are categorised on two main dimensions: the observed behaviour (blocking or non-blocking), and the type of concurrency primitive that is the cause (shared memory or message passing). Let’s begin with a very quick recap of the main concurrency mechanisms in Go. Concurrency in Go  A major design goal of Go is to improve traditional multi-threaded programming languages and make concurrent programming easier and less error-prone. For this purpose, Go centers its multi-threading design around two principles: 1) making threads (called goroutines) lightweight and easy to create and 2) using explicit messaging (via channels) to communicate across threads. Goroutines are lightweight user-level threads (‘green’ threads). A goroutine is created by adding the keyword go before a function call, including to an anonymous function. Local variables declared before an anonymous function are accessible within it and potentially shared. Channels are used to send data and states across goroutines, and may be buffered or unbuffered. When using unbuffered channels a goroutine will block on send (receive) until another goroutine is receiving (sending). The select statement allows a goroutine to wait on multiple channel operations, if more than one case is valid Go selects one at random. Go also has traditional synchronisation primitives including mutexes, condition variables, and atomic variables. How Go concurrency primitives are used in practice  The six applications make relatively heavy use of goroutines, especially with anonymous functions. An especially interesting comparison can be made in the case of gRPC, which has both a C implementation and a Go implementation. The following table shows the ratio of the number of goroutines created in gRPC-Go compared to gRPC-C, when processing the same number of requests. In this comparison goroutines tend to shorter lifetimes than the threads created in the C version, but are created much more frequently. This more frequent use of goroutines is to be expected as its something the Go language encourages. If we look at the use of concurrency primitives across the board in all of the applications, one more surprising finding is that shared memory synchronisation operations are still used more often than message passing:  The most frequently used message-passing primitive is chan, responsible for between 18.5% and 43% of all usages. So we have a situation in which traditional shared memory communication is still heavily used, in conjunction with significant amounts of message passing primitives. From a bug perspective that means we have all the exciting bug possibilities that shared memory communication affords, together with all of the bug possibilities message passing affords, and bugs caused be the interaction of these two styles! Go concurrency bugs  The authors searched the GitHub commit histories of the applications to find commits fixing concurrency bugs (3,211). From these 171 were randomly selected for study. Bugs are categorised into blocking bugs and non-blocking bugs. A blocking bug occurs when one or more goroutines are unintentionally stuck in their execution and cannot move forward. This definition is broader deadlocks and can include situations with no circular waits, but instead waits for resources that no other goroutines supply. 85 of the bugs were blocking, and 86 were non- blocking. Bugs are also categorised in a second dimension according to whether they relate to shared memory protection (105 bugs) or message passing (66 bugs). Blocking bugs  Looking at the blocking bugs first of all, 42% of these are related to shared memory, and 58% are related to message passing. Recall as well that shared memory primitives are more widely used than message passing primitives. Contrary to the common belief that message passing is less error-prone, more blocking bugs in our studied Go applications are caused by wrong message passing than by wrong shared memory protection. The shared memory based bugs include all the usual suspects, with a few new twists due to Go’s implementation of RWMutex and Wait (see §5.1.1). For message-passing related bugs, many of these are due to missing sends or receives on channels, or closing channels. All blocking bugs caused by message passing are related to Go’s new message passing semantics like channel. They can be difficult to detect especially when message passing operations are used together with other synchronization primitives. Contrary to common belief, message passing can cause more blocking bugs than shared memory. Investigating the fixes for these bugs showed that once understood, the fixes are fairly simple, and the type of fix is correlated with the bug cause. This suggests that fully-automated or semi-automated tools for fixing blocking bugs in Go may be a promising direction. Go’s built-in deadlock detector was only able to detect two of the 21 blocking bugs reproduced in the study. Non-blocking bugs  More non-blocking bugs are caused by misuse of shared memory primitives than message passing. About half of these are caused by ‘traditional’ shared memory bug patterns. There are also several bugs caused by a lack of understanding of Go language features, especially the sharing of local variables declared before an anonymous function used in a goroutine, and the semantics of WaitGroups. New programming models and new libraries that Go introduced to ease multi-thread programming can themselves be the cause of more concurrency bugs. While message-passing based non-blocking bugs were comparatively less common, “the intricate design of message passing in a language can cause these bugs to be especially hard to find when combining with other language-specific features.”  Interestingly, programmers fixing bugs caused by misuse of shared memory primitives showed a preference for using message passing to do so. Go’s data race detector was able to detect half of the reproduced non-blocking bugs in the study. Wrapping up  Surprisingly, our study shows that it is as easy to make concurrency bugs with message passing as with shared memory, sometimes even more. Programmers have to have a clear understanding of:  goroutine creation with anonymous functions  the usage of buffered vs unbuffered channels  the non-determinism of waiting for multiple channel operations using select  correct use of the special library time  Although each of these features were designed to ease multi-threaded programming, in reality, it is difficult to write correct Go programs with them. I regret that I didn’t have space in this write-up to include the many illustrative code samples highlighting details of bugs. If you’re actively developing in Go, the full paper is well worth a read to study them.", "pdf_url": "https://songlh.github.io/paper/go-study.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/understanding-real-world-concurrency-bugs-in-go.json"}
{"id": "81780960", "bin": "1500_1600", "summary_sentences": ["Diamond: Automating data management and storage for wide-area, reactive applications Zhang et al., OSDI 2016  Diamond tackles the end-to-end problem of building reactive applications, defined here as those that update end-user visible state without requiring any explicit user action:  … today’s popular applications are reactive: they provide users with the illusion of continuous synchronization across their device without requiring them to explicitly save, reload, and exchange shared data.", "Such applications have widely distributed processes sharing data across mobile devices, desktops, and cloud services.", "They make concurrent updates, may stop or fail at any time, and can be connected by slow or unreliable links.", "Distributed storage systems on their own don’t provide a complete solution since programmers still need to synchronize updates between application processes and distributed storage.", "I didn’t see it in the related work section, but for me SwiftCloud is another very interesting system that extends transactions all the way to the application / mobile client (although it doesn’t emphasize the reactive nature in the same way as Diamond).", "Diamond aims to simplify development of such applications by combining a client-side library of reactive data types with a CRDT-aware data structure server (cf.", "Riak), push notifications, and client-side transaction support.", "A reactive data map (rmap) primitive lets applications create reactive data types and map them into the Diamond data management service  Reactive transactions automatically (re-)execute in response to shared data updates (push notifications)  A data-type aware form of optimistic concurrency control (DOCC) leverages the semantics of the supported data types to better cope with wide-area latencies (e.g., by concurrently committing transactions executing commutative operations).", "In case you were under any illusion that building such end-to-end reactive applications while maintaining safety was easy, §2 in a paper does a good job of explaining otherwise.", "I’m going to jump straight to the description of Diamond itself.", "System and data model  Diamond processes run on client device and on cloud servers.", "Every process is linked with a client-side library libdiamond, which provides access to the shared Diamond cloud.", "Front-end servers are scalable stateless nodes providing clients with access to Diamond’s back-end storage, saving them from having to authenticate with or track the partitioning scheme of the back-end servers.", "Back-end storage servers themselves use Viewstamped Replication for fault tolerance.", "Diamond supports reactive data types for fine-grained synchronization, efficient concurrency control, and persistence.", "As with popular data structure stores such as Redis and Riak, we found that simple data types are general enough to support a wide range of application and provide the necessary semantics to enable commutativity and avoid false sharing….", "In addition to primitive data types, Diamond support simple Conflict-Free Replicated Data Types (CRDTs) and collection types with efficient type-specific interfaces.", "A Diamond instance provides a set of tables, where a table is simply a key -> data type map.", "Diamond maps its data types into application memory space through an operation called rmap.", "“Applications call rmap with an application variable and a key to the Diamond record, giving them control over what data in their address space is shared and how it is organized.”  Transaction model and implementation  Diamond supports both traditional style (application initiated) read-write transactions, and reactive transactions.", "A reactive transaction is essentially a state-change callback that runs inside a transaction and is triggered by a push notification from the Diamond ‘cloud.’ Reactive transaction may read, but not write, reactive data items.", "Reactive transactions help application processes automatically propagate changes made to reactive data types.", "Each time a read-write transaction modifies an rmapped variable in a reactive transaction’s read set, the reactive transaction re-executes, propagating changes to derived local variables.", "As a result, reactive transactions provide a “live” view that gives the illusion of reactivity while maintaining an imperative programming style comfortable to application programmers.", "Further, because they read a consistent snapshot of rmapped data, reactive transactions avoid application-level bugs common to reactive programming models [48].", "Diamond’s transaction protocol is similar to Spanner’s , but uses DOCC instead of a locking mechanism, and commit timestamps from a timestamp service rather than TrueTime.", "When a read-write transaction commits, one or more reactive transactions may need to be executed:  The leader in the partition sends a publish request with the transaction’s commit timestamp to each front-end subscribed to the related record  For each publish, the front-end server looks up the reactive transactions that have the updated record in their read sets and checks if the commit timestamp is bigger than the last notification sent to that client.", "If so, the front-end server sends a notify request to the client with the commit timestamp and the reactive transaction id  The client logs the notification on receipt, updates its latest known timestamp, and re-executes the reactive transaction at the commit timestamp.", "How does the front-end server know which reactive transactions will have the updated record in their read-set?", "When a reactive transaction is first registered by a client, libdiamond executes it immediately with the most recent data and records the reads that the transaction makes.", "This also means that push notifications can be made efficient as they can contain the data the reactive transaction is most likely to request.", "The discussion affords the fact that the read-set of a reactive transaction may change over time.", "If a reactive transaction doesn’t have all of the data it wants to read pushed to it, it can always fetch it (and the read set will be updated for future go-rounds).", "What’s not clear is how the front-end server would know to notify a client at all for some data that changes but the client has never requested before.", "My best guess is that it’s up to the client to make sure it reads everything it might want to be notified about on that first run after registration.", "Reactive transactions in Diamond are similar to database triggers, events, and materialized views.", "They differ from these mechanisms because they modify local application state and execute application code rather than database queries that update storage state.", "Diamond’s design draws on Thialfi; however, Thialfi cannot efficiently support data push notifications without insight into the application’s access patterns.", "The Data-type (specific) Optimistic Concurrency Control mechanism (DOCC) uses fine-grained concurrency control based on the semantics of the different reactive data types.", "For example, allowing concurrent updates to different list elements.", "There’s an (implicit) assumption baked in here though that there are no integrity constraints that need to be maintained across such elements.", "DOCC also makes use of CRDT properties when these types are used.", "The Diamond guarantee (100% satisfaction or your money back ;) )  The Diamond guarantee is described as “ACID+R” and promises:  Atomicity – all or no updates to shared records in a r/w transaction succeed  Consistency – accesses in all transactions reflect a consistent view of shared records [Strongly consistent??]", "Isolation – accesses in all transactions reflect a global ordering of committed read-write transactions  Durability – updates to shared records in commited r/w transactions are never lost  Reactivity – accesses to modified records in registered reactive transactions will eventually re-execute.", "Diamond supports configurable isolation levels:  Evaluation  The team compared non-Diamond and Diamond-based versions of a number of reactive applications including ‘the 100 game,’ a chat room, a multiplayer scrabble game, and a twitter-clone.", "In addition to simplifying the design and programming of reactive apps, we found that Diamond facilitates the creation of general-purpose reactive libraries.", "As one example, Diamond transactions naturally lend themselves to managing UI elements.", "For instance, a check box usually maps a Boolean, re-draws a UI element in a reactive transaction, and writes to the Boolean in a read-write transaction when the user checks/unchecks the box.", "General findings when rewriting reactive apps to use Diamond include an elimination of bugs, and reduction in code size by around 30%.", "Performance evaluations show a low overhead in read-committed mode, through to a throughput reduction of about 50% when using strict serializability."], "summary_text": "Diamond: Automating data management and storage for wide-area, reactive applications Zhang et al., OSDI 2016  Diamond tackles the end-to-end problem of building reactive applications, defined here as those that update end-user visible state without requiring any explicit user action:  … today’s popular applications are reactive: they provide users with the illusion of continuous synchronization across their device without requiring them to explicitly save, reload, and exchange shared data. Such applications have widely distributed processes sharing data across mobile devices, desktops, and cloud services. They make concurrent updates, may stop or fail at any time, and can be connected by slow or unreliable links. Distributed storage systems on their own don’t provide a complete solution since programmers still need to synchronize updates between application processes and distributed storage. I didn’t see it in the related work section, but for me SwiftCloud is another very interesting system that extends transactions all the way to the application / mobile client (although it doesn’t emphasize the reactive nature in the same way as Diamond). Diamond aims to simplify development of such applications by combining a client-side library of reactive data types with a CRDT-aware data structure server (cf. Riak), push notifications, and client-side transaction support. A reactive data map (rmap) primitive lets applications create reactive data types and map them into the Diamond data management service  Reactive transactions automatically (re-)execute in response to shared data updates (push notifications)  A data-type aware form of optimistic concurrency control (DOCC) leverages the semantics of the supported data types to better cope with wide-area latencies (e.g., by concurrently committing transactions executing commutative operations). In case you were under any illusion that building such end-to-end reactive applications while maintaining safety was easy, §2 in a paper does a good job of explaining otherwise. I’m going to jump straight to the description of Diamond itself. System and data model  Diamond processes run on client device and on cloud servers. Every process is linked with a client-side library libdiamond, which provides access to the shared Diamond cloud. Front-end servers are scalable stateless nodes providing clients with access to Diamond’s back-end storage, saving them from having to authenticate with or track the partitioning scheme of the back-end servers. Back-end storage servers themselves use Viewstamped Replication for fault tolerance. Diamond supports reactive data types for fine-grained synchronization, efficient concurrency control, and persistence. As with popular data structure stores such as Redis and Riak, we found that simple data types are general enough to support a wide range of application and provide the necessary semantics to enable commutativity and avoid false sharing…. In addition to primitive data types, Diamond support simple Conflict-Free Replicated Data Types (CRDTs) and collection types with efficient type-specific interfaces. A Diamond instance provides a set of tables, where a table is simply a key -> data type map. Diamond maps its data types into application memory space through an operation called rmap. “Applications call rmap with an application variable and a key to the Diamond record, giving them control over what data in their address space is shared and how it is organized.”  Transaction model and implementation  Diamond supports both traditional style (application initiated) read-write transactions, and reactive transactions. A reactive transaction is essentially a state-change callback that runs inside a transaction and is triggered by a push notification from the Diamond ‘cloud.’ Reactive transaction may read, but not write, reactive data items. Reactive transactions help application processes automatically propagate changes made to reactive data types. Each time a read-write transaction modifies an rmapped variable in a reactive transaction’s read set, the reactive transaction re-executes, propagating changes to derived local variables. As a result, reactive transactions provide a “live” view that gives the illusion of reactivity while maintaining an imperative programming style comfortable to application programmers. Further, because they read a consistent snapshot of rmapped data, reactive transactions avoid application-level bugs common to reactive programming models [48]. Diamond’s transaction protocol is similar to Spanner’s , but uses DOCC instead of a locking mechanism, and commit timestamps from a timestamp service rather than TrueTime. When a read-write transaction commits, one or more reactive transactions may need to be executed:  The leader in the partition sends a publish request with the transaction’s commit timestamp to each front-end subscribed to the related record  For each publish, the front-end server looks up the reactive transactions that have the updated record in their read sets and checks if the commit timestamp is bigger than the last notification sent to that client. If so, the front-end server sends a notify request to the client with the commit timestamp and the reactive transaction id  The client logs the notification on receipt, updates its latest known timestamp, and re-executes the reactive transaction at the commit timestamp. How does the front-end server know which reactive transactions will have the updated record in their read-set? When a reactive transaction is first registered by a client, libdiamond executes it immediately with the most recent data and records the reads that the transaction makes. This also means that push notifications can be made efficient as they can contain the data the reactive transaction is most likely to request. The discussion affords the fact that the read-set of a reactive transaction may change over time. If a reactive transaction doesn’t have all of the data it wants to read pushed to it, it can always fetch it (and the read set will be updated for future go-rounds). What’s not clear is how the front-end server would know to notify a client at all for some data that changes but the client has never requested before. My best guess is that it’s up to the client to make sure it reads everything it might want to be notified about on that first run after registration. Reactive transactions in Diamond are similar to database triggers, events, and materialized views. They differ from these mechanisms because they modify local application state and execute application code rather than database queries that update storage state. Diamond’s design draws on Thialfi; however, Thialfi cannot efficiently support data push notifications without insight into the application’s access patterns. The Data-type (specific) Optimistic Concurrency Control mechanism (DOCC) uses fine-grained concurrency control based on the semantics of the different reactive data types. For example, allowing concurrent updates to different list elements. There’s an (implicit) assumption baked in here though that there are no integrity constraints that need to be maintained across such elements. DOCC also makes use of CRDT properties when these types are used. The Diamond guarantee (100% satisfaction or your money back ;) )  The Diamond guarantee is described as “ACID+R” and promises:  Atomicity – all or no updates to shared records in a r/w transaction succeed  Consistency – accesses in all transactions reflect a consistent view of shared records [Strongly consistent??] Isolation – accesses in all transactions reflect a global ordering of committed read-write transactions  Durability – updates to shared records in commited r/w transactions are never lost  Reactivity – accesses to modified records in registered reactive transactions will eventually re-execute. Diamond supports configurable isolation levels:  Evaluation  The team compared non-Diamond and Diamond-based versions of a number of reactive applications including ‘the 100 game,’ a chat room, a multiplayer scrabble game, and a twitter-clone. In addition to simplifying the design and programming of reactive apps, we found that Diamond facilitates the creation of general-purpose reactive libraries. As one example, Diamond transactions naturally lend themselves to managing UI elements. For instance, a check box usually maps a Boolean, re-draws a UI element in a reactive transaction, and writes to the Boolean in a read-write transaction when the user checks/unchecks the box. General findings when rewriting reactive apps to use Diamond include an elimination of bugs, and reduction in code size by around 30%. Performance evaluations show a low overhead in read-committed mode, through to a throughput reduction of about 50% when using strict serializability.", "pdf_url": "https://www.usenix.org/system/files/conference/osdi16/osdi16-zhang-irene.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/diamond-automating-data-management-and-storage-for-wide-area-reactive-applications.json"}
{"id": "8455503", "bin": "1500_1600", "summary_sentences": ["Polaris: Faster Page Loads Using Fine-Grained Dependency Tracking – Netravali et al. 2016  Yesterday we looked at Shandian which promised faster web page load times, but required a modified client-side browser.", "Today we’re sticking with the theme of reducing page load times with Polaris.", "Unlike Shandian, Polaris works with unmodified browsers, and in tests with content from 200 sites out of the top Alexa 500 it is able to reduce load times by 34% at the median, and 59% at the 95th percentile.", "To load a page, a browser must resolve the page’s dependency graph.", "The dependency graph captures “load-before” relationships between a page’s HTML, CSS, JavaScript, and image objects.", "Consider a browser parsing an HTML file that encounters a script tag.", "It has to halt the parsing and rendering of the page to download the linked .js file and evaluate it as this may alter the downstream HTML, or define JavaScript state required by later script files.", "Synchronously loading JavaScript files guarantees correctness, but this approach is often too cautious.", "For example, if first.js and second.js do not modify mutually observable state, the browser should be free to download and evaluate the files in whatever order maximizes the utilization of the network and the CPU.", "However, pages do not expose such fine-grained dependency information to browsers…  (Yes, “ tags can be marked with async or defer attributes, but by default they have neither.", "In the test corpus of 200 popular sites, this accounts for 98.3% of all scripts…).", "The Scout tool is used to load a page offline and produce a fine-grained dependency graph that is much more detailed than those produced by prior frameworks.", "“For 81% of the 200 real-world pages that we examined, our new graphs have different critical paths than those of graphs from prior work.”  Polaris is a dynamic client-side scheduler that uses the dependency graphs created by Scout to reduce page load times:  When a user makes a request for a Polaris-enabled page, the server returns a scheduler stub instead of the page’s original HTML.", "The scheduler stub includes the Polaris JavaScript library, the page’s fine-grained dependency graph (as generated by Scout), and the original HTML.", "The Polaris library uses the Scout graph, as well as dynamic observations about network conditions, to load objects in an order that reduces page load time.", "Scout  Given simply lexical dependency information, then:  A script tag might read CSS style properties from the DOM tree, so CSS evaluation must block JavaScript execution  A script tag might change downstream HTML, so when a browser encounters such a tag it must block or transfer HTML parsing to a speculative thread  Two script tags that are lexically adjacent might exhibit a read/write dependency on JavaScript state.", "Thus browsers must execute the script tags serially…  Scout finds out the true dependencies, not just the potential ones.", "It captures three types of data flows involving the JavaScript heap and the DOM state belonging to HTML and CSS:  Write/read dependencies: one object produces state that another object consumes.", "E.g. a global variable created by a.js and later read by b.js.", "Read/write dependencies: one object must read a piece of state before the value is updated by another object.", "For example, JavaScript code reading a DOM value before the value is changed by the HTML parser.", "“Any reordering of object evaluations must ensure value equivalence for DOM queries – regardless of when a JavaScript file is executed, its DOM queries must return the same results.”  Write/write dependencies: two objects update the same piece of state, and we must preserve the happens-before relationship.", "For example, CSS files update DOM state, changing the rules which govern a page’s visual presentation.", "The CSS specification states that if two files update the same rule, the last writer wins.", "…once we know the DOM dependencies and JavaScript heap dependencies for a script tag, the time at which the script can be evaluated is completely decoupled from the position of the script tag in the HTML – we merely have to ensure that we evaluate the script after its fine-grained dependencies are satisfied.", "Similarly, we can parse and render a piece of HTML at any time, as long as we ensure that we have blocked the evaluation of downstream objects in the dependency graph.", "The content of web pages is recorded using Mahimahi .", "Scout then rewrites each JavaScript and HTML file in the page adding instrumentation to log the fine-grained data flows across the JavaScript heap and the DOM.", "The page is then loaded in a regular browser and the log is used to generated the dependency graph.", "For a given page, a web server may generate a different dependency graph for different clients… The server-side logic must run Scout on each version of the dependency graph.", "We believe that this burden will be small in practice, since even customized versions of a page often share the same underlying graph structure (with different content in some of the nodes).", "An analysis of 200 sites from the Alexa top 500 showed that Scout finds 30% more edges at the median, and 118% more edges at the 95% percentile than existing dependency analysis tools (Klotski, WProf).", "Those additional edges have a dramatic impact on the characteristics of dependency graphs.", "For example, adding fine-grained dependencies alters the critical path length for 80.8% of the pages in our corpus.", "Polaris  Polaris is written completely in JavaScript and can be run on unmodified commodity browsers.", "It combines the dependency graph produced by Scout with observations about current network conditions to determine the dynamic critical path for a page.", "The dynamic critical path, i.e. the path which currently has the most unresolved objects, is influenced by the order and latency with which network fetches complete; importantly, the dynamic critical path may be different than the critical path in the static dependency graph.", "To load a page using Polaris, a web server is configured to respond to page requests with the Polaris scheduler stub HTML.", "This contains four components:  The scheduler itself, as inline JavaScript code  The Scout dependency graph, as a JavaScript variable inside the scheduler  DNS prefetch hints to indicate that the scheduler will be contacting certain hostnames in the near future.", "“DNS hints allow Polaris to pre-warm the DNS cache in the same way that the browser does during speculative HTML parsing.”  The page’s original HTML, broken into chunks by Scout as determined by Scout’s fine-grained dependency resolution.", "Across the 200 sites in test corpus, the schedule stub increased page size by 36.5KB (3%) at the median.", "Since modern browsers limit a page to at most six outstanding requests to a give origin, Polaris maintains per-origin priority queues.", "With the exception of the top-level HTML (which is included in the scheduler stub), each object in the dependency graph belongs to exactly one queue.", "Inside a queue, objects that are higher in the dependency tree receive a higher priority, since those objects prevent the evaluation of more downstream objects.", "At any given moment, the scheduler tries to fetch objects that reside in a dynamic critical path for the page load.", "However, if fetching the next object along a critical path would violate a per-origin network constraint, Polaris examines its queues, and fetches the highest priority object from an origin that has available request slots.", "How well does it work?", "… we demonstrate that Polaris can decrease page load times across a variety of web pages and network configurations: performance improves by 34% and 59% for the median and 95th percentile sites, respectively.", "Polaris’ benefits grow as network latencies increase, because higher RTTs increase the penalty for bad fetch schedules.", "Thus, Polaris is particularly valuable for clients with cellular or low-quality wired networks.", "However, even for networks with moderate RTTs, Polaris can often reduce load times by over 20%.", "A closer look at three sites, Apple, ESPN, and Weather.com shows the impact the dependency graph has on the benefits that Polaris can bring:  Apple’s home page has a flat dependency graph such that once the top-level HTML is loaded, all other objects can be fetched and evaluated in an arbitrary order.", "For low RTTs, this makes Polaris slower than the baseline.", "Weather.com has a much more complex dependency graph, which enables Polaris to beat the baseline handsomely.", "Polaris was also tested in conjunction with SPDY and found to be complementary: load times using Polaris over SPDY are 2.05%-4.03% faster than those with Polaris over HTTP/1.1."], "summary_text": "Polaris: Faster Page Loads Using Fine-Grained Dependency Tracking – Netravali et al. 2016  Yesterday we looked at Shandian which promised faster web page load times, but required a modified client-side browser. Today we’re sticking with the theme of reducing page load times with Polaris. Unlike Shandian, Polaris works with unmodified browsers, and in tests with content from 200 sites out of the top Alexa 500 it is able to reduce load times by 34% at the median, and 59% at the 95th percentile. To load a page, a browser must resolve the page’s dependency graph. The dependency graph captures “load-before” relationships between a page’s HTML, CSS, JavaScript, and image objects. Consider a browser parsing an HTML file that encounters a script tag. It has to halt the parsing and rendering of the page to download the linked .js file and evaluate it as this may alter the downstream HTML, or define JavaScript state required by later script files. Synchronously loading JavaScript files guarantees correctness, but this approach is often too cautious. For example, if first.js and second.js do not modify mutually observable state, the browser should be free to download and evaluate the files in whatever order maximizes the utilization of the network and the CPU. However, pages do not expose such fine-grained dependency information to browsers…  (Yes, “ tags can be marked with async or defer attributes, but by default they have neither. In the test corpus of 200 popular sites, this accounts for 98.3% of all scripts…). The Scout tool is used to load a page offline and produce a fine-grained dependency graph that is much more detailed than those produced by prior frameworks. “For 81% of the 200 real-world pages that we examined, our new graphs have different critical paths than those of graphs from prior work.”  Polaris is a dynamic client-side scheduler that uses the dependency graphs created by Scout to reduce page load times:  When a user makes a request for a Polaris-enabled page, the server returns a scheduler stub instead of the page’s original HTML. The scheduler stub includes the Polaris JavaScript library, the page’s fine-grained dependency graph (as generated by Scout), and the original HTML. The Polaris library uses the Scout graph, as well as dynamic observations about network conditions, to load objects in an order that reduces page load time. Scout  Given simply lexical dependency information, then:  A script tag might read CSS style properties from the DOM tree, so CSS evaluation must block JavaScript execution  A script tag might change downstream HTML, so when a browser encounters such a tag it must block or transfer HTML parsing to a speculative thread  Two script tags that are lexically adjacent might exhibit a read/write dependency on JavaScript state. Thus browsers must execute the script tags serially…  Scout finds out the true dependencies, not just the potential ones. It captures three types of data flows involving the JavaScript heap and the DOM state belonging to HTML and CSS:  Write/read dependencies: one object produces state that another object consumes. E.g. a global variable created by a.js and later read by b.js. Read/write dependencies: one object must read a piece of state before the value is updated by another object. For example, JavaScript code reading a DOM value before the value is changed by the HTML parser. “Any reordering of object evaluations must ensure value equivalence for DOM queries – regardless of when a JavaScript file is executed, its DOM queries must return the same results.”  Write/write dependencies: two objects update the same piece of state, and we must preserve the happens-before relationship. For example, CSS files update DOM state, changing the rules which govern a page’s visual presentation. The CSS specification states that if two files update the same rule, the last writer wins. …once we know the DOM dependencies and JavaScript heap dependencies for a script tag, the time at which the script can be evaluated is completely decoupled from the position of the script tag in the HTML – we merely have to ensure that we evaluate the script after its fine-grained dependencies are satisfied. Similarly, we can parse and render a piece of HTML at any time, as long as we ensure that we have blocked the evaluation of downstream objects in the dependency graph. The content of web pages is recorded using Mahimahi . Scout then rewrites each JavaScript and HTML file in the page adding instrumentation to log the fine-grained data flows across the JavaScript heap and the DOM. The page is then loaded in a regular browser and the log is used to generated the dependency graph. For a given page, a web server may generate a different dependency graph for different clients… The server-side logic must run Scout on each version of the dependency graph. We believe that this burden will be small in practice, since even customized versions of a page often share the same underlying graph structure (with different content in some of the nodes). An analysis of 200 sites from the Alexa top 500 showed that Scout finds 30% more edges at the median, and 118% more edges at the 95% percentile than existing dependency analysis tools (Klotski, WProf). Those additional edges have a dramatic impact on the characteristics of dependency graphs. For example, adding fine-grained dependencies alters the critical path length for 80.8% of the pages in our corpus. Polaris  Polaris is written completely in JavaScript and can be run on unmodified commodity browsers. It combines the dependency graph produced by Scout with observations about current network conditions to determine the dynamic critical path for a page. The dynamic critical path, i.e. the path which currently has the most unresolved objects, is influenced by the order and latency with which network fetches complete; importantly, the dynamic critical path may be different than the critical path in the static dependency graph. To load a page using Polaris, a web server is configured to respond to page requests with the Polaris scheduler stub HTML. This contains four components:  The scheduler itself, as inline JavaScript code  The Scout dependency graph, as a JavaScript variable inside the scheduler  DNS prefetch hints to indicate that the scheduler will be contacting certain hostnames in the near future. “DNS hints allow Polaris to pre-warm the DNS cache in the same way that the browser does during speculative HTML parsing.”  The page’s original HTML, broken into chunks by Scout as determined by Scout’s fine-grained dependency resolution. Across the 200 sites in test corpus, the schedule stub increased page size by 36.5KB (3%) at the median. Since modern browsers limit a page to at most six outstanding requests to a give origin, Polaris maintains per-origin priority queues. With the exception of the top-level HTML (which is included in the scheduler stub), each object in the dependency graph belongs to exactly one queue. Inside a queue, objects that are higher in the dependency tree receive a higher priority, since those objects prevent the evaluation of more downstream objects. At any given moment, the scheduler tries to fetch objects that reside in a dynamic critical path for the page load. However, if fetching the next object along a critical path would violate a per-origin network constraint, Polaris examines its queues, and fetches the highest priority object from an origin that has available request slots. How well does it work? … we demonstrate that Polaris can decrease page load times across a variety of web pages and network configurations: performance improves by 34% and 59% for the median and 95th percentile sites, respectively. Polaris’ benefits grow as network latencies increase, because higher RTTs increase the penalty for bad fetch schedules. Thus, Polaris is particularly valuable for clients with cellular or low-quality wired networks. However, even for networks with moderate RTTs, Polaris can often reduce load times by over 20%. A closer look at three sites, Apple, ESPN, and Weather.com shows the impact the dependency graph has on the benefits that Polaris can bring:  Apple’s home page has a flat dependency graph such that once the top-level HTML is loaded, all other objects can be fetched and evaluated in an arbitrary order. For low RTTs, this makes Polaris slower than the baseline. Weather.com has a much more complex dependency graph, which enables Polaris to beat the baseline handsomely. Polaris was also tested in conjunction with SPDY and found to be complementary: load times using Polaris over SPDY are 2.05%-4.03% faster than those with Polaris over HTTP/1.1.", "pdf_url": "http://web.mit.edu/ravinet/www/polaris_nsdi16.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/polaris-faster-page-loads-using-fine-grained-dependency-tracking.json"}
{"id": "45795840", "bin": "1500_1600", "summary_sentences": ["KV-Direct: High-performance in-memory key-value store with programmable NIC Li et al., SOSP’17  We’ve seen some pretty impressive in-memory datastores in past editions of The Morning Paper, including FaRM , RAMCloud , and DrTM .", "But nothing that compares with KV-Direct:  With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store.", "Check out the bottom line in this comparison table from the evaluation:  ( Enlarge )  In addition to sheer speed, you might also notice that KV-Direct is 3x more power efficient than other systems, and the first general purpose KVS system to achieve 1 million KV operations per watt on commodity servers.", "Since the server CPU can also be used to run other workloads at the same time, you can make a case for KV-Direct being as much as 10x more power efficient than CPU-based systems.", "What we’re seeing here is a glimpse of how large-scale systems software of the future may well be constructed.", "As the power ceiling puts a limit on multi-core scaling, people are now turning to domain-specific architectures for better performance.", "A first generation of key-value stores were built in a straightforward manner on top of traditional operating systems and TCP/IP stacks.", "More recently, as both the single core frequency scaling and multi-core architecture scaling are slowing down, a new research trend in distributed systems is to leverage Remote Direct Memory Access (RDMA) technology on NIC to reduce network processing cost.", "KV-Direct however, goes one step beyond.", "To support network virtualisation, more and more servers in data centers are now equipped with programmable NICS containing field-programmable gate arrays (FPGA).", "An embedded NIC chip connects to the network, and a PCIe connector attaches to the server.", "KV-Direct uses the FPGA in the NIC to implement key-value primitives directly.", "Like one-sided RDMA (Fig 1b below), KV-Direct bypasses the remote CPU.", "But it also extends the RDMA primitives from simple memory operations (READ and WRITE) to key-value operations (GET, PUT, DELETE and ATOMIC ops) — Fig 1c below.", "Compared with one-sided RDMA based systems, KV-Direct deals with the consistency and synchronization issues on the server-side, thus removing computation overhead in the client, and reducing network traffic.", "In addition, to support vector-based operations and reduce network traffic, KV-Direct also provides new vector primitives UPDATE, REDUCE, and FILTER, allowing users to define active messages and delegate certain computations to programmable NIC for efficiency.", "Design goals and challenges  Use cases for in-memory key-value stores have evolved beyond caching to things such as storing data indices, machine learning model parameters, nodes and edges in graph computing, and sequencers in distributed synchronisation.", "The role of the store shifts from object caching to a generic data structure store (c.f.", "Redis).", "This leads to the following design goals:  High batch throughput for small key-value pairs (e.g., model parameters, graph node neighbours).", "Predictable low-latency (e.g., for data-parallel computation,where tail latency matters)  High efficiency under write-intensive workloads (e.g., graph computations, and parameter servers)  Fast atomic operations  (e.g., for centralized schedulers, sequencers , counters and so on).", "Vector-type operations (for machine learning and graph computing workloads that often require operating on every element in a vector).", "The throughput constraint ends up being PCIe bandwidth:  In order to saturate the network with GET operations, the KVS on NIC must make full use of PCIe bandwidth and achieve close to one average memory access per GET.", "Getting to this level involves work on three fronts:  Minimising DMA (direct memory access) requests per KV operation.", "The two major components that drive random memory access are hash tables and memory allocation.", "Hiding PCIe latency while maintaining consistency, which entails pipelining requests.", "Care must be taken to respect causal dependencies here though.", "Balancing load between NIC DRAM and host memory.", "The NIC itself has a small amount of DRAM available, but it turns out not to be much faster than going over PCIe.", "So the trick turns out to be to use both in order to utilise the joint bandwidth.", "KV-Direct  KV-Direct enables remote direct key-value access.", "Clients send operation requests to the KVS server, and the programmable NIC processes requests and sends back results, bypassing the CPU.", "The following table shows the supported operations.", "The most interesting of course are the vector operations.", "KV-Direct supports two types of vector operations: sending a scalar to the NIC on the server, where the NIC applies the update to each element in the vector; and sending a vector to the server, where the NIC updates the original vector element-by-element.", "Furthermore, KV-Direct supports user-defined update functions as a generalisation to atomic operations.", "The update functions needs to be pre-registered and compiled to hardware logic before executing.", "When the user supplies an update function, the KV-Direct toolchain duplicates it several times to leverage FPGA parallelism and match computation with PCIe throughput, and then compiles it into reconfigurable hardware logic using a high-level synthesis (HLS) tool.", "These functions can be used for general stream processing on a vector value.", "The programmable NIC on the KVS server is reconfigured as a KV processor, which receives packets from the network, decodes vector operations, and buffers KV operations in a reservation station.", "The out-of-order engine then issues independent KV operations from the reservation station into the decoder.", "To minimise memory accesses, small KV pairs are stored inline in the hash table, while others are stored in dynamically allocated memory from a slab memory allocator.", "After a KV operation completes, the result is sent back to the out-of-order execution engine to find and execute matching KV operations in the reservation station.", "The reservation station is used to avoid dependencies between two KV operations leading to data hazards and a stalled pipeline.", "We borrow the concept of dynamic scheduling from computer architecture and implement a reservation station to track all in-flight KV operations and their execution context.", "To saturate PCIe, DRAM and the processing pipeline, up to 256 in-flight KV operations are needed.", "However, comparing 256 16-byte keys in parallel would take 40% of the logic resource of our FPGA.", "Instead, we store the KV operations in a small hash table in on-chip BRAM, indexed by the hash of the key.", "When a KV operation completes, the latest value is forwarded to the reservation station, where pending operations in the same hash slot are checked.", "Those with a matching key are executed immediately and removed from the station.", "Further design and implementation details can be found in sections 3 and 4 of the paper.", "Evaluation  The evaluation section contains a suite of microbenchmarks, followed by a system benchmark based on the YCSB workload.", "To simulate a skewed Zipf workload, skewness 0.99 was chosen.", "This is referred to as the long-tail workload in the figures.", "The testbed comprises eight servers with two 8-core CPUS per server,and one Arista switch.", "There is a total of 128 GiB of host memory per server.", "A programmable NIC is connected to the PCIe root complex of CPU 0, and its 40 Gbps Ethernet port is connected to the switch.", "The NIC has two PCIe Gen3 x8 links in a bifurcated Gen3 x16 physical connector.", "Here’s the overall throughput achieved by the system.", "The throughput of a KV-Direct NIC is on-par with a state-of-the-art KVS server with tens of CPU cores.", "Without network batching, the tail latency ranges from 3-9  s depending on KV size, operation type, and key distribution.", "Network batching adds less than 1  s latency, but significantly improves performance.", "It is possible to attach multiple NICs per server.", "With 10 KV-Direct NICs on a server, one billion KV ops/s is readily achievable on a commodity server.", "Each NIC owns a disjoin partition of the keys.", "Multiple NICs suffer the same load imbalance problem as a multi-core KVS implementation, but for a relatively small number of partitions (e.g. 10) the load imbalance is not too great – 1.5x of the average in the highest loaded NIC even for the long-tail highly skewed workload.", "KV-Direct throughput scales almost linearly with the number of NICS on a server.", "The last word:  After years of broken promises, FPGA-based reconfigurable hardware finally becomes widely available in main stream data centers.", "Many significant workloads will be scrutinized to see whether they can benefit from reconfigurable hardware, and we expect much more fruitful work in this general direction."], "summary_text": "KV-Direct: High-performance in-memory key-value store with programmable NIC Li et al., SOSP’17  We’ve seen some pretty impressive in-memory datastores in past editions of The Morning Paper, including FaRM , RAMCloud , and DrTM . But nothing that compares with KV-Direct:  With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store. Check out the bottom line in this comparison table from the evaluation:  ( Enlarge )  In addition to sheer speed, you might also notice that KV-Direct is 3x more power efficient than other systems, and the first general purpose KVS system to achieve 1 million KV operations per watt on commodity servers. Since the server CPU can also be used to run other workloads at the same time, you can make a case for KV-Direct being as much as 10x more power efficient than CPU-based systems. What we’re seeing here is a glimpse of how large-scale systems software of the future may well be constructed. As the power ceiling puts a limit on multi-core scaling, people are now turning to domain-specific architectures for better performance. A first generation of key-value stores were built in a straightforward manner on top of traditional operating systems and TCP/IP stacks. More recently, as both the single core frequency scaling and multi-core architecture scaling are slowing down, a new research trend in distributed systems is to leverage Remote Direct Memory Access (RDMA) technology on NIC to reduce network processing cost. KV-Direct however, goes one step beyond. To support network virtualisation, more and more servers in data centers are now equipped with programmable NICS containing field-programmable gate arrays (FPGA). An embedded NIC chip connects to the network, and a PCIe connector attaches to the server. KV-Direct uses the FPGA in the NIC to implement key-value primitives directly. Like one-sided RDMA (Fig 1b below), KV-Direct bypasses the remote CPU. But it also extends the RDMA primitives from simple memory operations (READ and WRITE) to key-value operations (GET, PUT, DELETE and ATOMIC ops) — Fig 1c below. Compared with one-sided RDMA based systems, KV-Direct deals with the consistency and synchronization issues on the server-side, thus removing computation overhead in the client, and reducing network traffic. In addition, to support vector-based operations and reduce network traffic, KV-Direct also provides new vector primitives UPDATE, REDUCE, and FILTER, allowing users to define active messages and delegate certain computations to programmable NIC for efficiency. Design goals and challenges  Use cases for in-memory key-value stores have evolved beyond caching to things such as storing data indices, machine learning model parameters, nodes and edges in graph computing, and sequencers in distributed synchronisation. The role of the store shifts from object caching to a generic data structure store (c.f. Redis). This leads to the following design goals:  High batch throughput for small key-value pairs (e.g., model parameters, graph node neighbours). Predictable low-latency (e.g., for data-parallel computation,where tail latency matters)  High efficiency under write-intensive workloads (e.g., graph computations, and parameter servers)  Fast atomic operations  (e.g., for centralized schedulers, sequencers , counters and so on). Vector-type operations (for machine learning and graph computing workloads that often require operating on every element in a vector). The throughput constraint ends up being PCIe bandwidth:  In order to saturate the network with GET operations, the KVS on NIC must make full use of PCIe bandwidth and achieve close to one average memory access per GET. Getting to this level involves work on three fronts:  Minimising DMA (direct memory access) requests per KV operation. The two major components that drive random memory access are hash tables and memory allocation. Hiding PCIe latency while maintaining consistency, which entails pipelining requests. Care must be taken to respect causal dependencies here though. Balancing load between NIC DRAM and host memory. The NIC itself has a small amount of DRAM available, but it turns out not to be much faster than going over PCIe. So the trick turns out to be to use both in order to utilise the joint bandwidth. KV-Direct  KV-Direct enables remote direct key-value access. Clients send operation requests to the KVS server, and the programmable NIC processes requests and sends back results, bypassing the CPU. The following table shows the supported operations. The most interesting of course are the vector operations. KV-Direct supports two types of vector operations: sending a scalar to the NIC on the server, where the NIC applies the update to each element in the vector; and sending a vector to the server, where the NIC updates the original vector element-by-element. Furthermore, KV-Direct supports user-defined update functions as a generalisation to atomic operations. The update functions needs to be pre-registered and compiled to hardware logic before executing. When the user supplies an update function, the KV-Direct toolchain duplicates it several times to leverage FPGA parallelism and match computation with PCIe throughput, and then compiles it into reconfigurable hardware logic using a high-level synthesis (HLS) tool. These functions can be used for general stream processing on a vector value. The programmable NIC on the KVS server is reconfigured as a KV processor, which receives packets from the network, decodes vector operations, and buffers KV operations in a reservation station. The out-of-order engine then issues independent KV operations from the reservation station into the decoder. To minimise memory accesses, small KV pairs are stored inline in the hash table, while others are stored in dynamically allocated memory from a slab memory allocator. After a KV operation completes, the result is sent back to the out-of-order execution engine to find and execute matching KV operations in the reservation station. The reservation station is used to avoid dependencies between two KV operations leading to data hazards and a stalled pipeline. We borrow the concept of dynamic scheduling from computer architecture and implement a reservation station to track all in-flight KV operations and their execution context. To saturate PCIe, DRAM and the processing pipeline, up to 256 in-flight KV operations are needed. However, comparing 256 16-byte keys in parallel would take 40% of the logic resource of our FPGA. Instead, we store the KV operations in a small hash table in on-chip BRAM, indexed by the hash of the key. When a KV operation completes, the latest value is forwarded to the reservation station, where pending operations in the same hash slot are checked. Those with a matching key are executed immediately and removed from the station. Further design and implementation details can be found in sections 3 and 4 of the paper. Evaluation  The evaluation section contains a suite of microbenchmarks, followed by a system benchmark based on the YCSB workload. To simulate a skewed Zipf workload, skewness 0.99 was chosen. This is referred to as the long-tail workload in the figures. The testbed comprises eight servers with two 8-core CPUS per server,and one Arista switch. There is a total of 128 GiB of host memory per server. A programmable NIC is connected to the PCIe root complex of CPU 0, and its 40 Gbps Ethernet port is connected to the switch. The NIC has two PCIe Gen3 x8 links in a bifurcated Gen3 x16 physical connector. Here’s the overall throughput achieved by the system. The throughput of a KV-Direct NIC is on-par with a state-of-the-art KVS server with tens of CPU cores. Without network batching, the tail latency ranges from 3-9  s depending on KV size, operation type, and key distribution. Network batching adds less than 1  s latency, but significantly improves performance. It is possible to attach multiple NICs per server. With 10 KV-Direct NICs on a server, one billion KV ops/s is readily achievable on a commodity server. Each NIC owns a disjoin partition of the keys. Multiple NICs suffer the same load imbalance problem as a multi-core KVS implementation, but for a relatively small number of partitions (e.g. 10) the load imbalance is not too great – 1.5x of the average in the highest loaded NIC even for the long-tail highly skewed workload. KV-Direct throughput scales almost linearly with the number of NICS on a server. The last word:  After years of broken promises, FPGA-based reconfigurable hardware finally becomes widely available in main stream data centers. Many significant workloads will be scrutinized to see whether they can benefit from reconfigurable hardware, and we expect much more fruitful work in this general direction.", "pdf_url": "https://lrita.github.io/images/blog/kv-direct.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/kv-direct-high-performance-in-memory-key-value-store-with-programmable-nic.json"}
{"id": "15084340", "bin": "1500_1600", "summary_sentences": ["How Much Up-Front?", "A Grounded Theory of Agile Architecture – Waterman et al. 2015  It’s time for something a little bit different, so this week I thought I’d bring you a selection of papers from the recently held  ICSE’15 conference (International Conference on Software Engineering).", "To kick things off, today’s choice looks at the question of how architecture fits into an agile process.", "Just how much effort should you spend up-front on architecture?", "And what happens if you defer architectural decisions to later?", "It’s all a bit of a conundrum…  Software architecture is the high-level structure and organisation of a software system.", "Because architecture defines system-level properties, it is difficult to change after development has started, causing a conflict with agile development’s central goal of better delivering value through responding to change.", "To maximise agility, agile developers often avoid or minimise architectural planning, because architecture planning is often seen as delivering little immediate value to the customer.", "Too little planning however may lead to an accidental architecture that has not been carefully thought through, and may lead to the team spending a lot of time fixing architecture problems and not enough time delivering functionality (value).", "A common strategy is to do ‘just enough’ architecture up-front.", "But how much is just enough?", "This paper identifies six forces that shape the answer to that question, leading to five different agile architecture strategies that can be used.", "The grounded theory reference in the paper title is a reference to the approach used to derive these forces and strategies.", "I found it interesting to read the explanation of  grounded theory in the paper, and encourage you to take  look if this piques your interest (section II C).", "The short version is that it’s an iterative bottom-up approach based on coding and clustering information given during structured interviews.", "The end result is a theory explaining how things relate to each other.", "This is in contrast to starting with a hypothesis and designing a test to prove or disprove it.", "We interviewed 44 participants in 37 interviews.", "Participants were gathered through industry contacts, agile interest groups and through direct contact with relevant organisations.", "Almost all participants were very experienced architects, senior developers, team leaders and development managers with at least six years’ experience (twenty years was not uncommon), and most were also very experienced in agile development.", "The six forces and five strategies that emerged from the work are summarised in the figure below.", "Six Forces that Influence the Approach to Agile Architecture  Requirements Instability due to incomplete or changing requirements favours deferring detailed requirements gathering, analysis, and architecture design in return for getting early feedback from the customer based on an initial implementation.", "Technical Risk through challenging or demanding architecturally significant requirements, through having many integration points with other systems, and by involving legacy systems, is a force in favour of more up-front architecture.", "Participants also identified integration points, or interfaces to external systems, as a major source of complexity in the systems being developed, particularly when the other systems are legacy or are built from different technologies.", "Integration with other systems require data and communications to be mapped between the systems, adding to the up-front effort to ensure integration is possible with the technologies being used.", "For example: “Today’s systems tend to be more interconnected – they have a lot more interfaces to external systems than older systems which are typically standalone.", "They have a lot higher level of complexity for the same sized system.” (P14, solutions architect)  Interestingly a similar argument can be made about the complexity introduced by microservices.", "Early Value : there is a need to start getting early value out of a system or product being built (not just feedback) before all functionality has been implemented.", "For example, to take advantage of a market opportunity or to use the revenue generated to fund further development.", "This force works against too much up-front architecture:  Teams that deliver early value must reduce the time to the first release by spending less time on up-front architecture design.", "They achieve this by reducing the planning horizon – how far ahead the team considers (high level) requirements for the purpose of architecture planning.", "Team Culture that is people-focused and collaborative is very important in fostering collaboration.", "“A team without a trusting people-focused and collaborative culture has to rely on documentation for communication and formal plans, and hence requires more up-front effort to guide development.”  Customer Agility is often required to match an agile approach to architecture.", "A customer must have an agile culture that is similar to the team’s culture, whether the team is in-house or an ISV (independent software vendor), for the team to be truly agile.", "A highly-agile team will not fit in well with a heavyweight process-oriented organisation that prefers planning and formal communication.", "Experience enables pattern recognition that can short-circuit the amount of work needed to be done up-front.", "While generally important for all software development methods, experience is more important in agile development because the tacit knowledge and implicit decision-making that come with experience supports agile development’s reduced process and documentation, and reduces the up-front effort: “You implement certain patterns without thinking […] you’ve done this kind of pattern for solving this kind of a problem, without even thinking that this is the way that you are going.” (P16b, head of engineering)  Five Strategies for Implementing Agile Architecture  In response to these forces there are five strategies that a team may choose from…  Respond to change  A team’s ability to use S1 (RESPOND TO CHANGE) is directly related to how agile it is.", "S1 increases the architecture’s agility by increasing its modifiability and its tolerance of change, and allows the team to ensure the architecture continuously represents the best solution to the problem as it evolves.", "We are given five tactics for responding to change: keep designs simple, prove the architecture with code iteratively, use good design practices, delay-decision making, and plan for options.", "The first and last of these are in tension, which becomes more obvious when we juxtapose the descriptions in the paper:  Keeping designs simple means only designing for what is immediately required: no gold plating and no designing for what might be required or for what can be deferred… Planning for options means building in generality and avoiding making decisions that are unnecessarily constrained and which may close off possible future requirements without significant refactoring.", "Address Risk  S2 (ADDRESS RISK) reduces the impact of risk before it causes problems, and is usually done up-front, particularly for risk relating to system-wide decisions (for example, risk in selecting the technology stack or top-level styles).", "Using S2, a team designs the architecture in sufficient detail that it is comfortable that it is actually possible to build the system with the required Architecturally Significant Requirements with a satisfactory level of risk.", "Emergent Architecture is likely to be used when developing a minimum viable product in response to the Early Value force.", "The team makes only the minimal architectural decisions up-front such as selecting the technology stack and the highest level architectural styles and patterns.", "Big Design Up-Front  S4 (BIG DESIGN UP-FRONT) requires that the team acquires a full set of requirements and completes a full architecture design before development starts.", "There are no emergent design decisions, although the architecture may evolve during development.", "S4 is undesirable in agile development because it reduces the architecture’s ability to use S1 (RESPOND TO CHANGE) by increasing the time to the first opportunity for feedback, increasing the chance that decisions will need to be changed later, and increasing the chance of over-engineering.", "While S4 may be considered the case of addressing risk (S2) taken to the extreme, in reality the use of S4 is driven primarily by an absence of CUSTOMER AGILITY (F5) rather than the presence of TECHNICAL RISK (F2).", "Using Frameworks and Template Architectures using software frameworks and the reference architectures from their creators provides the benefit of standard solutions to standard problems."], "summary_text": "How Much Up-Front? A Grounded Theory of Agile Architecture – Waterman et al. 2015  It’s time for something a little bit different, so this week I thought I’d bring you a selection of papers from the recently held  ICSE’15 conference (International Conference on Software Engineering). To kick things off, today’s choice looks at the question of how architecture fits into an agile process. Just how much effort should you spend up-front on architecture? And what happens if you defer architectural decisions to later? It’s all a bit of a conundrum…  Software architecture is the high-level structure and organisation of a software system. Because architecture defines system-level properties, it is difficult to change after development has started, causing a conflict with agile development’s central goal of better delivering value through responding to change. To maximise agility, agile developers often avoid or minimise architectural planning, because architecture planning is often seen as delivering little immediate value to the customer. Too little planning however may lead to an accidental architecture that has not been carefully thought through, and may lead to the team spending a lot of time fixing architecture problems and not enough time delivering functionality (value). A common strategy is to do ‘just enough’ architecture up-front. But how much is just enough? This paper identifies six forces that shape the answer to that question, leading to five different agile architecture strategies that can be used. The grounded theory reference in the paper title is a reference to the approach used to derive these forces and strategies. I found it interesting to read the explanation of  grounded theory in the paper, and encourage you to take  look if this piques your interest (section II C). The short version is that it’s an iterative bottom-up approach based on coding and clustering information given during structured interviews. The end result is a theory explaining how things relate to each other. This is in contrast to starting with a hypothesis and designing a test to prove or disprove it. We interviewed 44 participants in 37 interviews. Participants were gathered through industry contacts, agile interest groups and through direct contact with relevant organisations. Almost all participants were very experienced architects, senior developers, team leaders and development managers with at least six years’ experience (twenty years was not uncommon), and most were also very experienced in agile development. The six forces and five strategies that emerged from the work are summarised in the figure below. Six Forces that Influence the Approach to Agile Architecture  Requirements Instability due to incomplete or changing requirements favours deferring detailed requirements gathering, analysis, and architecture design in return for getting early feedback from the customer based on an initial implementation. Technical Risk through challenging or demanding architecturally significant requirements, through having many integration points with other systems, and by involving legacy systems, is a force in favour of more up-front architecture. Participants also identified integration points, or interfaces to external systems, as a major source of complexity in the systems being developed, particularly when the other systems are legacy or are built from different technologies. Integration with other systems require data and communications to be mapped between the systems, adding to the up-front effort to ensure integration is possible with the technologies being used. For example: “Today’s systems tend to be more interconnected – they have a lot more interfaces to external systems than older systems which are typically standalone. They have a lot higher level of complexity for the same sized system.” (P14, solutions architect)  Interestingly a similar argument can be made about the complexity introduced by microservices. Early Value : there is a need to start getting early value out of a system or product being built (not just feedback) before all functionality has been implemented. For example, to take advantage of a market opportunity or to use the revenue generated to fund further development. This force works against too much up-front architecture:  Teams that deliver early value must reduce the time to the first release by spending less time on up-front architecture design. They achieve this by reducing the planning horizon – how far ahead the team considers (high level) requirements for the purpose of architecture planning. Team Culture that is people-focused and collaborative is very important in fostering collaboration. “A team without a trusting people-focused and collaborative culture has to rely on documentation for communication and formal plans, and hence requires more up-front effort to guide development.”  Customer Agility is often required to match an agile approach to architecture. A customer must have an agile culture that is similar to the team’s culture, whether the team is in-house or an ISV (independent software vendor), for the team to be truly agile. A highly-agile team will not fit in well with a heavyweight process-oriented organisation that prefers planning and formal communication. Experience enables pattern recognition that can short-circuit the amount of work needed to be done up-front. While generally important for all software development methods, experience is more important in agile development because the tacit knowledge and implicit decision-making that come with experience supports agile development’s reduced process and documentation, and reduces the up-front effort: “You implement certain patterns without thinking […] you’ve done this kind of pattern for solving this kind of a problem, without even thinking that this is the way that you are going.” (P16b, head of engineering)  Five Strategies for Implementing Agile Architecture  In response to these forces there are five strategies that a team may choose from…  Respond to change  A team’s ability to use S1 (RESPOND TO CHANGE) is directly related to how agile it is. S1 increases the architecture’s agility by increasing its modifiability and its tolerance of change, and allows the team to ensure the architecture continuously represents the best solution to the problem as it evolves. We are given five tactics for responding to change: keep designs simple, prove the architecture with code iteratively, use good design practices, delay-decision making, and plan for options. The first and last of these are in tension, which becomes more obvious when we juxtapose the descriptions in the paper:  Keeping designs simple means only designing for what is immediately required: no gold plating and no designing for what might be required or for what can be deferred… Planning for options means building in generality and avoiding making decisions that are unnecessarily constrained and which may close off possible future requirements without significant refactoring. Address Risk  S2 (ADDRESS RISK) reduces the impact of risk before it causes problems, and is usually done up-front, particularly for risk relating to system-wide decisions (for example, risk in selecting the technology stack or top-level styles). Using S2, a team designs the architecture in sufficient detail that it is comfortable that it is actually possible to build the system with the required Architecturally Significant Requirements with a satisfactory level of risk. Emergent Architecture is likely to be used when developing a minimum viable product in response to the Early Value force. The team makes only the minimal architectural decisions up-front such as selecting the technology stack and the highest level architectural styles and patterns. Big Design Up-Front  S4 (BIG DESIGN UP-FRONT) requires that the team acquires a full set of requirements and completes a full architecture design before development starts. There are no emergent design decisions, although the architecture may evolve during development. S4 is undesirable in agile development because it reduces the architecture’s ability to use S1 (RESPOND TO CHANGE) by increasing the time to the first opportunity for feedback, increasing the chance that decisions will need to be changed later, and increasing the chance of over-engineering. While S4 may be considered the case of addressing risk (S2) taken to the extreme, in reality the use of S4 is driven primarily by an absence of CUSTOMER AGILITY (F5) rather than the presence of TECHNICAL RISK (F2). Using Frameworks and Template Architectures using software frameworks and the reference architectures from their creators provides the benefit of standard solutions to standard problems.", "pdf_url": "https://ecs.victoria.ac.nz/foswiki/pub/Main/TechnicalReportSeries/ECSTR15-01.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/how-much-up-front-a-grounded-theory-of-agile-architecture.json"}
{"id": "10587146", "bin": "1500_1600", "summary_sentences": ["Statiscal foundations of virtual democracy Kahng et al., ICML’19  This is another paper on the theme of combining information and making decisions in the face of noise and uncertainty – but the setting is quite different to those we’ve been looking at recently.", "Consider a food bank that receives donations of food and distributes it to those in need.", "The goal is to implement an automated decision making system such that when a food donation is received, the system outputs the organisation (e.g. housing authority or food pantry) that should receive it.", "We could hard code a set of rules, but what should they be?", "And who gets to decide?", "A democratic solution to this would be to give each of the stakeholders a vote on every decision.", "In the food bank setting, identified classes of stakeholders include the donors, the recipients, the volunteers (who pick up food from the donor and deliver it to the recipient), and employees.", "Their votes encode their own preferences and biases, perhaps in a way that even the voters themselves couldn’t neatly codify in a set of explicit rules.", "It’s not really practical to have an actual vote with all stakeholders participating every time a food donation is made though!", "One of the most basic ideas underlying democracy is that complicated decisions can be made by asking a group of people to vote on the alternatives at hand.", "As a decision-making framework, this paradigm is versatile, because people can express a sensible opinion about a wide range of issues.", "One of its seemingly inherent shortcomings, though, is that voters must take the time to cast a vote— hopefully an informed one— every time a new dilemma arises.", "The big idea behind virtual democracy is that we learn the voting preferences of each stakeholder, essentially creating an agent which is able to vote in their place, a virtual voter.", "Then when we need to make a decision we ask those virtual voters to cast their votes (in the form of a preference ranking).", "The central question in this paper is this: given a set of preference rankings, how should we combine them to produce an actual decision?", "The procedure for doing this is known as the voting rule.", "… the choice of voting rule can have a major impact on the efficacy of the system.", "In fact, the question of which voting rule to employ is one of the central questions in computational social choice.", "It’s one thing to come up with a voting rule that works well when we have the actual true preference rankings of all of the stakeholders.", "In a virtual democracy setting though, where we have learned approximations to those preference rankings, a highly desirable feature of a voting rule is that it is robust to noise.", "I.e., we want a voting rule whereby…  … the output on the true preferences is likely to coincide with the output on noisy estimates thereof.", "Learning preferences  To learn voter preferences, voters are asked to make a set of pairwise comparisons (about 100) between alternatives.", "I.e., given this donation, should it be sent to recipient A or recipient B?", "Each alternative is presented as a set of pre-determined features.", "In the case of the food bank question voters are given information about the type of donation, and seven additional features such as distance between the donor and recipient, and when the recipient last received a donation.", "At the end of this process, the training data is used to learn a model of the preferences of the voter.", "This model is then used to predict the voter’s preference ranking over many hundreds of recipients for a given donation.", "The Mallows model  To be able to compare the efficacy of various voting rules, we’re going to need a way to compare how good their outputs are.", "The Kendall tau (KT) distance between two rankings (permutations) of a set is defined as the number of pairs of alternatives on which the rankings disagree.", "By disagree we mean that given a pair  one ranks  ahead of  , and the other ranks  ahead of  .", "For example, the KT distance between  and  is 2.", "The Mallows (1957) model was originally designed for use in situations where there is true ranking of the alternatives, and assigns a probability that a given voter is associated with a given alternative ranking.", "The probability decreases exponentially with the number of pairs of alternatives on which the true and alternative ranking disagree, i.e., their KT distance.", "A Mallows model is parameterised by a  parameter  .", "Our technical approach relies on the observation that the classic Mallows (1957).", "model is an unusually good fit with our problem.", "In the problem at hand, instead of a single true ranking, each voter has their own true ranking.", "When validating a learned  model, the test for accuracy is done using pairwise comparisons, just like in Mallows.", "Given an observed prediction accuracy  , we can relate this accuracy to an underlying Mallows model through a parameter  , where pairwise comparisons are drawn from within the top  ranked items in the true ranking.", "(See §3 in the paper).", "Voting rules and the Borda count  The next piece of the puzzle is the selection of a voting rule to combine rankings and produce a final decision.", "The main result in the paper concerns the Borda count voting rule.", "Borda count is a positional scoring rule.", "Positional scoring rules give a score vector that assigns points to each position in a ranking.", "E.g. 5 points for being ranked first, 3 points for being ranked second, and so on.", "The score of an alternative is the sum of its ranking points across all of the voters.", "The alternative with the biggest score wins (break ties via random selection).", "The Borda count uses a very straightforward score vector: if there are  alternatives in the ranking, the score vector is defined as  .", "The heart of the paper is §4, where drawing on the properties of the Mallows model, it’s relationship to the predicted accuracy, and the Borda count rule, the authors show the Borda count is surprisingly robust to noise.", "I’m going to happily skip over the proofs here and leave you to follow up on those if you’re interested!", "… it is intuitive that the separation in Borda scores has to depend on  , but it is encouraging (and, to us, surprising) that his dependence is almost linear… the theorem implies that our noisy Borda ranking is highly unlikely to make mistakes on pairs of alternatives whose average score difference is linear in  .", "Other rules  So far so good, but what about other voting rules?", "Are they also robust to noise or is there something special about the Borda count?", "The main alternative to positional scoring rules are pairwise-majority consistent (PMC) rules, of which there are many examples (e.g., the ranked pairs method).", "The key result in §5 of the paper is that all rules in this class are not robust to noise.", "It is instructive to contrast our positive result, Theorem 1, with this negative result.", "On a very high level, the former result asserts that “if Borda count says that the gaps between alternatives are signiﬁcant, then the alternatives will not ﬂip under Borda count,” whereas the latter says “even if a PMC rule says that the gaps between alternatives are very signiﬁcant, some alternatives are likely to ﬂip under that rule.”  Borda count FTW  So there you have it: if you need to robustly combine noisy rankings of alternatives to make a decision, use the Borda count!", "Our theoretical and empirical results identify Borda count as an especially attractive voting rule for virtual democracy, from a statistical viewpoint.", "Another important feature of the Borda count rule is that the decisions it takes can be easily explained.", "An explanation consists of two elements: first the average position in the predicted preferences of each of the stakeholder groups, and second the features that were most important in achieving that ranking position (possible since alternatives are presented as vectors of features)."], "summary_text": "Statiscal foundations of virtual democracy Kahng et al., ICML’19  This is another paper on the theme of combining information and making decisions in the face of noise and uncertainty – but the setting is quite different to those we’ve been looking at recently. Consider a food bank that receives donations of food and distributes it to those in need. The goal is to implement an automated decision making system such that when a food donation is received, the system outputs the organisation (e.g. housing authority or food pantry) that should receive it. We could hard code a set of rules, but what should they be? And who gets to decide? A democratic solution to this would be to give each of the stakeholders a vote on every decision. In the food bank setting, identified classes of stakeholders include the donors, the recipients, the volunteers (who pick up food from the donor and deliver it to the recipient), and employees. Their votes encode their own preferences and biases, perhaps in a way that even the voters themselves couldn’t neatly codify in a set of explicit rules. It’s not really practical to have an actual vote with all stakeholders participating every time a food donation is made though! One of the most basic ideas underlying democracy is that complicated decisions can be made by asking a group of people to vote on the alternatives at hand. As a decision-making framework, this paradigm is versatile, because people can express a sensible opinion about a wide range of issues. One of its seemingly inherent shortcomings, though, is that voters must take the time to cast a vote— hopefully an informed one— every time a new dilemma arises. The big idea behind virtual democracy is that we learn the voting preferences of each stakeholder, essentially creating an agent which is able to vote in their place, a virtual voter. Then when we need to make a decision we ask those virtual voters to cast their votes (in the form of a preference ranking). The central question in this paper is this: given a set of preference rankings, how should we combine them to produce an actual decision? The procedure for doing this is known as the voting rule. … the choice of voting rule can have a major impact on the efficacy of the system. In fact, the question of which voting rule to employ is one of the central questions in computational social choice. It’s one thing to come up with a voting rule that works well when we have the actual true preference rankings of all of the stakeholders. In a virtual democracy setting though, where we have learned approximations to those preference rankings, a highly desirable feature of a voting rule is that it is robust to noise. I.e., we want a voting rule whereby…  … the output on the true preferences is likely to coincide with the output on noisy estimates thereof. Learning preferences  To learn voter preferences, voters are asked to make a set of pairwise comparisons (about 100) between alternatives. I.e., given this donation, should it be sent to recipient A or recipient B? Each alternative is presented as a set of pre-determined features. In the case of the food bank question voters are given information about the type of donation, and seven additional features such as distance between the donor and recipient, and when the recipient last received a donation. At the end of this process, the training data is used to learn a model of the preferences of the voter. This model is then used to predict the voter’s preference ranking over many hundreds of recipients for a given donation. The Mallows model  To be able to compare the efficacy of various voting rules, we’re going to need a way to compare how good their outputs are. The Kendall tau (KT) distance between two rankings (permutations) of a set is defined as the number of pairs of alternatives on which the rankings disagree. By disagree we mean that given a pair  one ranks  ahead of  , and the other ranks  ahead of  . For example, the KT distance between  and  is 2. The Mallows (1957) model was originally designed for use in situations where there is true ranking of the alternatives, and assigns a probability that a given voter is associated with a given alternative ranking. The probability decreases exponentially with the number of pairs of alternatives on which the true and alternative ranking disagree, i.e., their KT distance. A Mallows model is parameterised by a  parameter  . Our technical approach relies on the observation that the classic Mallows (1957). model is an unusually good fit with our problem. In the problem at hand, instead of a single true ranking, each voter has their own true ranking. When validating a learned  model, the test for accuracy is done using pairwise comparisons, just like in Mallows. Given an observed prediction accuracy  , we can relate this accuracy to an underlying Mallows model through a parameter  , where pairwise comparisons are drawn from within the top  ranked items in the true ranking. (See §3 in the paper). Voting rules and the Borda count  The next piece of the puzzle is the selection of a voting rule to combine rankings and produce a final decision. The main result in the paper concerns the Borda count voting rule. Borda count is a positional scoring rule. Positional scoring rules give a score vector that assigns points to each position in a ranking. E.g. 5 points for being ranked first, 3 points for being ranked second, and so on. The score of an alternative is the sum of its ranking points across all of the voters. The alternative with the biggest score wins (break ties via random selection). The Borda count uses a very straightforward score vector: if there are  alternatives in the ranking, the score vector is defined as  . The heart of the paper is §4, where drawing on the properties of the Mallows model, it’s relationship to the predicted accuracy, and the Borda count rule, the authors show the Borda count is surprisingly robust to noise. I’m going to happily skip over the proofs here and leave you to follow up on those if you’re interested! … it is intuitive that the separation in Borda scores has to depend on  , but it is encouraging (and, to us, surprising) that his dependence is almost linear… the theorem implies that our noisy Borda ranking is highly unlikely to make mistakes on pairs of alternatives whose average score difference is linear in  . Other rules  So far so good, but what about other voting rules? Are they also robust to noise or is there something special about the Borda count? The main alternative to positional scoring rules are pairwise-majority consistent (PMC) rules, of which there are many examples (e.g., the ranked pairs method). The key result in §5 of the paper is that all rules in this class are not robust to noise. It is instructive to contrast our positive result, Theorem 1, with this negative result. On a very high level, the former result asserts that “if Borda count says that the gaps between alternatives are signiﬁcant, then the alternatives will not ﬂip under Borda count,” whereas the latter says “even if a PMC rule says that the gaps between alternatives are very signiﬁcant, some alternatives are likely to ﬂip under that rule.”  Borda count FTW  So there you have it: if you need to robustly combine noisy rankings of alternatives to make a decision, use the Borda count! Our theoretical and empirical results identify Borda count as an especially attractive voting rule for virtual democracy, from a statistical viewpoint. Another important feature of the Borda count rule is that the decisions it takes can be easily explained. An explanation consists of two elements: first the average position in the predicted preferences of each of the stakeholder groups, and second the features that were most important in achieving that ranking position (possible since alternatives are presented as vectors of features).", "pdf_url": "http://proceedings.mlr.press/v97/kahng19a/kahng19a.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/statistical-foundations-of-virtual-democracy.json"}
{"id": "17539393", "bin": "1500_1600", "summary_sentences": ["Why neurons have thousands of synapses, a theory of sequence memory in neocortex Hawkins & Ahmad, Front.", "Neural Circuits 2016  It all began with a fascinating lunchtime conversation with Martin Thompson (@mjpt777), who mentioned to me a thought-provoking video he’d seen online from Jeff Hawkins regarding models of behaviour in the brain.", "A few days later I watched the video, then I bought the book “ On Intelligence ” and devoured it in one sitting.", "That book was written in 2004, and I was curious to learn of developments in the theory since.", "So we’re starting this week with a selection of three papers bringing the story up to date.", "There’s also a link to the Turing Test that we finished up with last week.", "In the book, Hawkins stresses the central role of prediction in how our brain works, and ultimately therefore in what it means to be thinking:  Prediction is not just one of the things your brain does.", "It is the primary function of the neocortex, and the foundation of intelligence.", "The cortex is an organ of prediction.", "If we want to understand what intelligence is, what creativity is, how your brain works, and how to build intelligent machines, we must understand the nature of these predictions and how the cortex makes them.", "Even behavior is best understood as a by-product of prediction.", "And:  We can now see where Alan Turing went wrong, prediction, not behavior, is the proof of intelligence….", "the Turing Test, by equating intelligence with human behavior, limited our vision of what is possible.", "Today’s paper choice looks at neurons in the brain, and the question of why they have so many excitatory synapses (thousands of them).", "Many of these aren’t modelled in the neural networks of machine learning.", "What do they do?", "And with that understanding, what would it look like to extend our neural network models to include a similar capability?", "A neuron has a soma (cell body), an axon, and a network of dendrites .", "Synapses are the structures that pass signals between neurons.", "Synapses close to the cell body are called proximal.", "Those farther away are called distal.", "The activation of a distal synapse doesn’t seem to have much effect at the soma, so what are they for?", "If several distal synapses close to each other are activated together (an active dendrite), then a local spike can depolarise the soma.", "A slightly depolarized cell fires earlier than it would otherwise if it subsequently receives sufficient feedforward input.", "By firing earlier it inhibits neighboring cells, creating highly sparse patterns of activity for correctly predicted inputs.", "Neurons as pattern recognisers  It takes 8-20 close together synapses to activate at the same time, which then combine in a non-linear fashion, to trigger a dendritic spike.", "Thus, a small set of neighboring synapses acts as a pattern detector.", "It follows that the thousands of synapses on a cell’s dendrites act as a set of independent pattern detectors.", "The detection of any of these patterns causes an NDMA spike and subsequent depolarisation at the soma.", "When relatively few neurons are active relative to the population, then such pattern recognition is robust.", "That is, the chances of a false match become very low.", "By forming more synapses than the the minimum needed to trigger a spike, recognition also becomes robust to noise and variation.", "A single dendritic segment can contain several hundred synapses.", "If we assume an average of 20 synapses are allocated to recognize each pattern, and that a neuron has 6000 synapses, then a cell would have the ability to recognize approximately 300 different patterns.", "This is a rough approximation, but makes evident that a neuron with active dendrites can learn to reliably recognize hundreds of patterns within a large population of cells.", "Neurons as predictors  Neurons receive input from different sources segregated on different parts of the dendritic tree.", "There are typical several hundred proximal synapses which receive feedforward input and have a relatively large effect at the soma, and define the basic receptive field response of the neuron.", "The basal synapses receive contextual input from nearly cells in the same cortical region, and apical synapses receive feedback input.", "Spikes due to basal synapses activating depolarize the soma, but not enough to generate somatic action potential.", "We propose that this sub-threshold depolarization is an important state of the cell.", "It represents a prediction that the cell will become active shortly and plays an important role in network behavior.", "Apical synapses have a similar effect, and are used to establish a top-down expectation – which can be thought of as another form of prediction.", "Learning sequences of patterns  Because all tissue in the neocortex consists of neurons with active dendrites and thousands of synapses, it suggests there are common network principles underlying everything the neocortex does.", "This leads to the question, what network property is so fundamental that it is a necessary component of sensory inference, prediction, language, and motor planning?", "We propose that the most fundamental operation of all neocortical tissue is learning and recalling sequences of patterns…  The neocortex is divided into cellular layers.", "Within layers we find mini-columns of cells.", "When an input is unexpected (i.e., it has not been predicted by pattern matching at the synapses leading to a spike and depolarisation) then all the cells in a column become active.", "This is the situation we find in (B) below when the sequences ‘ABCD’ and ‘XBCY’ have not yet been learned.", "( Enlarge )  After learning the sequences though, depolarised cells from the prediction of what should come next will fire first as they are ‘primed’, and this firing will inhibit the other cells nearby.", "( (C) in the figure above).", "Thus, a predicted input will lead to a very sparse pattern of cell activation that is unique to a particular element, at a particular location, in a particular sequence.", "As feedforward input arrives it activates cells, while basal input is generating predictions.", "So long as the next input matches the current prediction, the sequence continues.", "The network may make multiple simultaneous predictions without confusion.", "If an input matches any of the predictions it will result in the correct highly sparse representation.", "The apical dendrites connect neurons across layers 2, 3 and 5 in the neocortex.", "Their role seems to be to alert to deviations from expected sequences, as illustrated below.", "( Enlarge )  The HTM model neuron  The authors model the biology of the brain in software with Hierarchical Temporal Memory (HTM) neurons.", "We model a cell’s dendrites as a set of threshold coincidence detectors; each with its own synapses.", "If the number of active synapses on a dendrite/coincidence detector exceeds a threshold the cell detects a pattern.", "The coincidence detectors are in three groups corresponding to the proximal, basal, and apical dendrites of a pyramidal cell.", "Networks built out of HTM neurons use continuous on-line learning with learning rules that are local to each neuron and no global objective function.", "For each dendritic segment a set of ‘potential’ synapses between the segment and other cells in the network is maintained.", "A scalar value called ‘permanence’ models the growth of the synapse.", "Permanence values close to zero indicate that although potential to grow a synapse exists, one has not started growing yet.", "Values close to one represent a large fully-formed synapse.", "The permanence value is incremented and decremented using a Hebbian-like rule .", "If the permanence value exceeds a threshold, such as 0.3, then the weight of the synapse is 1, if the permanence value is at or below the threshold, then the weight of the synapse is 0… Using a scalar permanence value enables on-line learning in the presence of noise.", "The figure below shows a network being fed a mixture of random elements and repeated sequences.", "The maximum possible average prediction accuracy based on the input data set is 50%, and this is only possible using higher-order representations.", "In (A) below the sequences in the data stream were changed after 3000 elements, and the network relearns the new sequences.", "In (B) varying proportions of the neurons are disabled after the network reaches a stable state, and the performance of the network can be seen to recover as it relearns using the remaining neurons.", "( Enlarge )  We’ll look at HTM networks in more detail tomorrow."], "summary_text": "Why neurons have thousands of synapses, a theory of sequence memory in neocortex Hawkins & Ahmad, Front. Neural Circuits 2016  It all began with a fascinating lunchtime conversation with Martin Thompson (@mjpt777), who mentioned to me a thought-provoking video he’d seen online from Jeff Hawkins regarding models of behaviour in the brain. A few days later I watched the video, then I bought the book “ On Intelligence ” and devoured it in one sitting. That book was written in 2004, and I was curious to learn of developments in the theory since. So we’re starting this week with a selection of three papers bringing the story up to date. There’s also a link to the Turing Test that we finished up with last week. In the book, Hawkins stresses the central role of prediction in how our brain works, and ultimately therefore in what it means to be thinking:  Prediction is not just one of the things your brain does. It is the primary function of the neocortex, and the foundation of intelligence. The cortex is an organ of prediction. If we want to understand what intelligence is, what creativity is, how your brain works, and how to build intelligent machines, we must understand the nature of these predictions and how the cortex makes them. Even behavior is best understood as a by-product of prediction. And:  We can now see where Alan Turing went wrong, prediction, not behavior, is the proof of intelligence…. the Turing Test, by equating intelligence with human behavior, limited our vision of what is possible. Today’s paper choice looks at neurons in the brain, and the question of why they have so many excitatory synapses (thousands of them). Many of these aren’t modelled in the neural networks of machine learning. What do they do? And with that understanding, what would it look like to extend our neural network models to include a similar capability? A neuron has a soma (cell body), an axon, and a network of dendrites . Synapses are the structures that pass signals between neurons. Synapses close to the cell body are called proximal. Those farther away are called distal. The activation of a distal synapse doesn’t seem to have much effect at the soma, so what are they for? If several distal synapses close to each other are activated together (an active dendrite), then a local spike can depolarise the soma. A slightly depolarized cell fires earlier than it would otherwise if it subsequently receives sufficient feedforward input. By firing earlier it inhibits neighboring cells, creating highly sparse patterns of activity for correctly predicted inputs. Neurons as pattern recognisers  It takes 8-20 close together synapses to activate at the same time, which then combine in a non-linear fashion, to trigger a dendritic spike. Thus, a small set of neighboring synapses acts as a pattern detector. It follows that the thousands of synapses on a cell’s dendrites act as a set of independent pattern detectors. The detection of any of these patterns causes an NDMA spike and subsequent depolarisation at the soma. When relatively few neurons are active relative to the population, then such pattern recognition is robust. That is, the chances of a false match become very low. By forming more synapses than the the minimum needed to trigger a spike, recognition also becomes robust to noise and variation. A single dendritic segment can contain several hundred synapses. If we assume an average of 20 synapses are allocated to recognize each pattern, and that a neuron has 6000 synapses, then a cell would have the ability to recognize approximately 300 different patterns. This is a rough approximation, but makes evident that a neuron with active dendrites can learn to reliably recognize hundreds of patterns within a large population of cells. Neurons as predictors  Neurons receive input from different sources segregated on different parts of the dendritic tree. There are typical several hundred proximal synapses which receive feedforward input and have a relatively large effect at the soma, and define the basic receptive field response of the neuron. The basal synapses receive contextual input from nearly cells in the same cortical region, and apical synapses receive feedback input. Spikes due to basal synapses activating depolarize the soma, but not enough to generate somatic action potential. We propose that this sub-threshold depolarization is an important state of the cell. It represents a prediction that the cell will become active shortly and plays an important role in network behavior. Apical synapses have a similar effect, and are used to establish a top-down expectation – which can be thought of as another form of prediction. Learning sequences of patterns  Because all tissue in the neocortex consists of neurons with active dendrites and thousands of synapses, it suggests there are common network principles underlying everything the neocortex does. This leads to the question, what network property is so fundamental that it is a necessary component of sensory inference, prediction, language, and motor planning? We propose that the most fundamental operation of all neocortical tissue is learning and recalling sequences of patterns…  The neocortex is divided into cellular layers. Within layers we find mini-columns of cells. When an input is unexpected (i.e., it has not been predicted by pattern matching at the synapses leading to a spike and depolarisation) then all the cells in a column become active. This is the situation we find in (B) below when the sequences ‘ABCD’ and ‘XBCY’ have not yet been learned. ( Enlarge )  After learning the sequences though, depolarised cells from the prediction of what should come next will fire first as they are ‘primed’, and this firing will inhibit the other cells nearby. ( (C) in the figure above). Thus, a predicted input will lead to a very sparse pattern of cell activation that is unique to a particular element, at a particular location, in a particular sequence. As feedforward input arrives it activates cells, while basal input is generating predictions. So long as the next input matches the current prediction, the sequence continues. The network may make multiple simultaneous predictions without confusion. If an input matches any of the predictions it will result in the correct highly sparse representation. The apical dendrites connect neurons across layers 2, 3 and 5 in the neocortex. Their role seems to be to alert to deviations from expected sequences, as illustrated below. ( Enlarge )  The HTM model neuron  The authors model the biology of the brain in software with Hierarchical Temporal Memory (HTM) neurons. We model a cell’s dendrites as a set of threshold coincidence detectors; each with its own synapses. If the number of active synapses on a dendrite/coincidence detector exceeds a threshold the cell detects a pattern. The coincidence detectors are in three groups corresponding to the proximal, basal, and apical dendrites of a pyramidal cell. Networks built out of HTM neurons use continuous on-line learning with learning rules that are local to each neuron and no global objective function. For each dendritic segment a set of ‘potential’ synapses between the segment and other cells in the network is maintained. A scalar value called ‘permanence’ models the growth of the synapse. Permanence values close to zero indicate that although potential to grow a synapse exists, one has not started growing yet. Values close to one represent a large fully-formed synapse. The permanence value is incremented and decremented using a Hebbian-like rule . If the permanence value exceeds a threshold, such as 0.3, then the weight of the synapse is 1, if the permanence value is at or below the threshold, then the weight of the synapse is 0… Using a scalar permanence value enables on-line learning in the presence of noise. The figure below shows a network being fed a mixture of random elements and repeated sequences. The maximum possible average prediction accuracy based on the input data set is 50%, and this is only possible using higher-order representations. In (A) below the sequences in the data stream were changed after 3000 elements, and the network relearns the new sequences. In (B) varying proportions of the neurons are disabled after the network reaches a stable state, and the performance of the network can be seen to recover as it relearns using the remaining neurons. ( Enlarge )  We’ll look at HTM networks in more detail tomorrow.", "pdf_url": "https://www.frontiersin.org/articles/10.3389/fncir.2016.00023/pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/why-neurons-have-thousands-of-synapses-a-theory-of-sequence-memory-in-neocortex.json"}
{"id": "97291551", "bin": "1500_1600", "summary_sentences": ["Learning networking by reproducing research results Yan & McKeown et al., SIGCOMM’17  Students taking Stanford’s Advanced Topics in Networking class have to select a networking research paper and reproduce a result from it as part of a three-week pair project.", "At the end of the process, they publish their findings on the course’s public Reproducing Network Research blog.", "It’s well worth having a look around the blog: the students manage to achieve a lot in only three weeks!", "In the last five years, 200 students have reproduced results from 40 papers.", "In ‘Learning networking by reproducing research results’ the authors explain how this reproduction project came to be part of the course, what happens when students try to reproduce research, and the many benefits the students get from the experience.", "It’s a wonderful and inspiring idea that I’m sure could be applied more broadly too.", "We began the project as a means of teaching both engineering rigor and critical thinking, qualities that are necessary for careers in networking research and industry.", "We have observed that reproducing research can simultaneously be a tool for education and a means for students to contribute to the networking community.", "Why a reproducibility project?", "In high-school science students repeat well-known experiments in the lab, and it is widely agreed that this gives them a much deeper understanding of the underlying concepts.", "Our main goal for adapting this scientific approach to our networking class is for students to obtain a detailed, in-depth understanding of a significant paper, its key ideas, and its key results.", "Over and above just getting to the results, the act of recreation itself proves to be valuable and rewarding – perhaps more so even than whether or not identical results are actually achieved in the end.", "In fact, we find that students learn a huge amount when their experiments yield different results from the original research: they must figure out where the discrepancies lie and discern if there are unstated assumptions or inaccuracies in their own results or the published results.", "This is a fascinating and educational experience, and often a good lesson in diplomacy.", "Studying a paper in order to reproduce it leads to much deeper reflection on, and understanding of the paper.", "You could think of it as the logical fourth pass of Keshav’s three-pass approach in ‘ How to Read a Paper .’ It also helps to instil in future researchers the principle that their results should be reproducible by others whenever possible.", "Publishing their own reproducible results on the course blog encourages the whole community to do likewise.", "How the projects are run  Students work in pairs and have three weeks out of a ten-week course to complete the assignment.", "ONE: The first step is to choose a central figure or table from a research paper of interest.", "To get the students started, we provide a list of suggested conferences and research publications that we think make good examples, and we encourage students to choose more recent works, or ones that have not yet been attempted by students in previous course offerings.", "At Stanford, we have had students successfully reproduce results ranging from widely cited papers such as Hedera and DCTCP to traditional papers like RED, to cutting-edge, as-yet unpublished work like SPDY.", "Here are the most popular chosen papers:  TWO: The students then need to figure out their strategy for trying to reproduce the results, with use of either the Mininet or Mahimahi emulators encouraged.", "THREE: The next step, which I’m sure is where part of the magic happens, is that the students are helped to contact the paper authors.", "Opening up this communication channel between students and researchers has two main benefits: the first is for the student, who now has a primary source to contact regarding the tools, setup, workload and use-cases of the given experiment or research tool.", "The second is for the researcher, who is now aware that his or her work is being analyzed critically… the researcher will have additional feedback on the benefits, caveats, and persistence of his or her findings.", "FOUR: Then of course the students work with the instructors, peers, and teaching assistants to recreate the research.", "FIVE: A public blog is posted, which must contain all the code and workload in order for someone else to easily repeat experiments too.", "(And many do come to the blog for that purpose).", "SIX: The results in every blog post are verified using peer validation – every student group is required to replicate the results of another student group.", "“This reproduction effort is required to be an easy two-step process: (1) download and install any code, and (2) click ‘run.’ All code must be available in public code repositories.”  Most projects are successful, although a few are not.", "Sometimes students can recreate the original work almost perfectly (subject to scaling or computational resource limits).", "Other times students find discrepancies between their own results and those in the paper, and investigate why.", "For example, one group found that the settings used for ECMP had a material impact on results, but the configuration used in the original testbed was not available.", "Sometimes other changes in the environment (such as TCP implementations moving on since the original date of publication of the paper) hamper recreation, and other times of course the students end up being a little bit too ambitious in their undertakings!", "The teaching staff try to coach the students to help them avoid this latter mistake.", "Benefits for participants and the research community  My favourite piece of the whole process is the way that it tears down a mental barrier that might otherwise hold back students from full engagement in the research community:  An unexpected outcome of this project is an increased role of students in the networking research community.", "While designing and running the experiments, students had to interact with the original authors, new researchers who came across our course blog, and even developers of the emulators or simulators.", "We believe the benefits of these interactions go both ways; the networking community at large can also benefit from these student research reproduction projects.", "Students themselves found multiple benefits from the course:  “I specifically like the level of familiarity I got with the paper.", "There’s a level you can only get by reproducing or implementing it.”  It introduced them to cutting-edge research.", "For example, one pair were inspired to reproduce QJump .", "It gave them confidence to use research results in their own work.", "For example, one of the students working on the QJump project went on to implement a similar scheduler in her own research, “which is something that I wouldn’t have done if [I had just read] the paper.”  It made them realise that papers could be understood, “even by students who have taken only two courses in networking, and results can be reproduced in part.”  They felt good about publishing the blog posts as a contribution in their own right.", "“[From an educational standpoint,] blog posts are easier to read than papers.", "If there is one cool idea from a paper that you can reproduce and put into a blog post, I think that could be very valuable.”  The practical skills gained from the reproduction experiments proved useful when students went on to work in industry.", "The last word  We have found that the experience is rewarding and interesting for the students, and it gives them a chance to interact with researchers.", "In addition, we have learned that documenting the results of these reproduction studies is an essential resource for both future students and the research community at large.", "We hope that the materials presented in this editorial inspire you to consider offering similar projects in your graduate networking courses too.", "I wonder where else this approach might be applied.", "For example, in distributed systems students might try to model an algorithm using TLA+ (as Murat Demirbas teaches them to do in his course: see ‘ My experience with using TLA+ in distributed systems classes ’), and in programming languages papers critical proofs could be reconstructed in Coq."], "summary_text": "Learning networking by reproducing research results Yan & McKeown et al., SIGCOMM’17  Students taking Stanford’s Advanced Topics in Networking class have to select a networking research paper and reproduce a result from it as part of a three-week pair project. At the end of the process, they publish their findings on the course’s public Reproducing Network Research blog. It’s well worth having a look around the blog: the students manage to achieve a lot in only three weeks! In the last five years, 200 students have reproduced results from 40 papers. In ‘Learning networking by reproducing research results’ the authors explain how this reproduction project came to be part of the course, what happens when students try to reproduce research, and the many benefits the students get from the experience. It’s a wonderful and inspiring idea that I’m sure could be applied more broadly too. We began the project as a means of teaching both engineering rigor and critical thinking, qualities that are necessary for careers in networking research and industry. We have observed that reproducing research can simultaneously be a tool for education and a means for students to contribute to the networking community. Why a reproducibility project? In high-school science students repeat well-known experiments in the lab, and it is widely agreed that this gives them a much deeper understanding of the underlying concepts. Our main goal for adapting this scientific approach to our networking class is for students to obtain a detailed, in-depth understanding of a significant paper, its key ideas, and its key results. Over and above just getting to the results, the act of recreation itself proves to be valuable and rewarding – perhaps more so even than whether or not identical results are actually achieved in the end. In fact, we find that students learn a huge amount when their experiments yield different results from the original research: they must figure out where the discrepancies lie and discern if there are unstated assumptions or inaccuracies in their own results or the published results. This is a fascinating and educational experience, and often a good lesson in diplomacy. Studying a paper in order to reproduce it leads to much deeper reflection on, and understanding of the paper. You could think of it as the logical fourth pass of Keshav’s three-pass approach in ‘ How to Read a Paper .’ It also helps to instil in future researchers the principle that their results should be reproducible by others whenever possible. Publishing their own reproducible results on the course blog encourages the whole community to do likewise. How the projects are run  Students work in pairs and have three weeks out of a ten-week course to complete the assignment. ONE: The first step is to choose a central figure or table from a research paper of interest. To get the students started, we provide a list of suggested conferences and research publications that we think make good examples, and we encourage students to choose more recent works, or ones that have not yet been attempted by students in previous course offerings. At Stanford, we have had students successfully reproduce results ranging from widely cited papers such as Hedera and DCTCP to traditional papers like RED, to cutting-edge, as-yet unpublished work like SPDY. Here are the most popular chosen papers:  TWO: The students then need to figure out their strategy for trying to reproduce the results, with use of either the Mininet or Mahimahi emulators encouraged. THREE: The next step, which I’m sure is where part of the magic happens, is that the students are helped to contact the paper authors. Opening up this communication channel between students and researchers has two main benefits: the first is for the student, who now has a primary source to contact regarding the tools, setup, workload and use-cases of the given experiment or research tool. The second is for the researcher, who is now aware that his or her work is being analyzed critically… the researcher will have additional feedback on the benefits, caveats, and persistence of his or her findings. FOUR: Then of course the students work with the instructors, peers, and teaching assistants to recreate the research. FIVE: A public blog is posted, which must contain all the code and workload in order for someone else to easily repeat experiments too. (And many do come to the blog for that purpose). SIX: The results in every blog post are verified using peer validation – every student group is required to replicate the results of another student group. “This reproduction effort is required to be an easy two-step process: (1) download and install any code, and (2) click ‘run.’ All code must be available in public code repositories.”  Most projects are successful, although a few are not. Sometimes students can recreate the original work almost perfectly (subject to scaling or computational resource limits). Other times students find discrepancies between their own results and those in the paper, and investigate why. For example, one group found that the settings used for ECMP had a material impact on results, but the configuration used in the original testbed was not available. Sometimes other changes in the environment (such as TCP implementations moving on since the original date of publication of the paper) hamper recreation, and other times of course the students end up being a little bit too ambitious in their undertakings! The teaching staff try to coach the students to help them avoid this latter mistake. Benefits for participants and the research community  My favourite piece of the whole process is the way that it tears down a mental barrier that might otherwise hold back students from full engagement in the research community:  An unexpected outcome of this project is an increased role of students in the networking research community. While designing and running the experiments, students had to interact with the original authors, new researchers who came across our course blog, and even developers of the emulators or simulators. We believe the benefits of these interactions go both ways; the networking community at large can also benefit from these student research reproduction projects. Students themselves found multiple benefits from the course:  “I specifically like the level of familiarity I got with the paper. There’s a level you can only get by reproducing or implementing it.”  It introduced them to cutting-edge research. For example, one pair were inspired to reproduce QJump . It gave them confidence to use research results in their own work. For example, one of the students working on the QJump project went on to implement a similar scheduler in her own research, “which is something that I wouldn’t have done if [I had just read] the paper.”  It made them realise that papers could be understood, “even by students who have taken only two courses in networking, and results can be reproduced in part.”  They felt good about publishing the blog posts as a contribution in their own right. “[From an educational standpoint,] blog posts are easier to read than papers. If there is one cool idea from a paper that you can reproduce and put into a blog post, I think that could be very valuable.”  The practical skills gained from the reproduction experiments proved useful when students went on to work in industry. The last word  We have found that the experience is rewarding and interesting for the students, and it gives them a chance to interact with researchers. In addition, we have learned that documenting the results of these reproduction studies is an essential resource for both future students and the research community at large. We hope that the materials presented in this editorial inspire you to consider offering similar projects in your graduate networking courses too. I wonder where else this approach might be applied. For example, in distributed systems students might try to model an algorithm using TLA+ (as Murat Demirbas teaches them to do in his course: see ‘ My experience with using TLA+ in distributed systems classes ’), and in programming languages papers critical proofs could be reconstructed in Coq.", "pdf_url": "https://ccronline.sigcomm.org/wp-content/uploads/2017/05/acmdl17-97.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/learning-networking-by-reproducing-research-results.json"}
{"id": "60253398", "bin": "1500_1600", "summary_sentences": ["Read-Log-Update: A Lightweight Synchronization Mechanism for Concurrent Programming – Matveev et al. 2015  An important paradigm in concurrent data structure scalability is to support read-only traversals: sequences of reads that execute without any synchronization (and hence require no memory fences and generate no contention).", "The gain from such unsynchronized traversals is significant because they account for a large fraction of operations in many data structures and applications.", "The popular read-copy-update (RCU) mechanism of McKenney and Slingwine provides scalability by enabling this paradigm.", "It allows read-only traversals to proceed concurrently with updates by creating a copy of the data structure being modified.", "Readers access the unmodified data structure while updaters modify the copy.", "The key to RCU is that once modifications are complete, they are installed using a single pointer modification in a way that does not interfere with ongoing readers.", "RCU has been supported in the Linux kernel since 2002, and the User-space RCU library is available for user-space applications.", "In this paper Matveev et al. take a big step forward beyond RCU with the introduction of  an extension to the base RCU ideas they call Read-Log-Update (RLU).", "RLU provides better performance and concurrency than RCU (it allows multiple simultaneous writers for example), but I think even more importantly it has a much simpler programming model meaning it can be applied to many more scenarios.", "For example, the RCU-based doubly-linked list in the Linux kernel only allows threads to traverse the list in the forward direction because of the limitations of RCU, whereas the RLU based alternative can easily support traversal in both directions.", "I’m going to focus here on the core RLU algorithm.", "The paper also contains an extensive evaluation section which is a fascinating read, and has the by-product of reminding me just how big our field is and of the huge body of knowledge built up in data structures and algorithms.", "I collected a good number of additions to my ‘known unknowns’ list reading through it!", "For example: “We then apply the more advanced RLU scheme with the deferral mechanism of synchronize calls to the state-of-the-art RCU-based Citrus tree, an enhancement of the Bonsai tree of Clements et al. The Citrus tree uses both RCU and fine-grained locks to deliver the best performing search tree to date…”  Let’s dive into how RLU works:  RLU provides support for multiple object updates in a single operation by combining the quiescence mechanism of RCU with a global clock and per thread object-level logs.", "The global clock and per-thread object logs will shortly be explained.", "Just in case the quiescence mechanism of RCU isn’t front-and-centre in your mind right now, here’s a quick refresher:  When [RCU readers] are not inside a critical section, readers are said to be in a quiescent state.", "A period of time during which every thread goes through at least one quiescent state is called a grace period.", "The key principle of RCU is that, if an updater removes an element from an RCU-protected shared data structure and waits for a grace period, there can be no reader still accessing that element.", "It is therefore safe to dispose of it.", "Waiting for a grace period can be achieved by calling synchronize rcu().", "The global clock is a logical clock (counter).", "All operations read the global clock value before they start, and  use this clock value to determine what version of a shared object they should read.", "A write thread first makes a copy of object to be updated in its  write-log, and then modifies the object in the log.", "This mechanism hides updates from concurrent reads.", "To avoid conflicts with concurrent writes, each object is also locked before it is duplicated and modified.", "Then, to commit the new object copies, a write operation increments the global clock, which effectively splits operations into two sets: (1) old operations that started before the clock increment, and (2) new operations that start after the clock increment.", "The first set of operations will read the old object copies while the second set will read the new object copies of this writer.", "Therefore, in the next step, the writer waits for old operations to finish by executing the RCU-style quiescence loop, while new operations “steal” new object copies of this writer by accessing the per thread write-log of this writer.", "After the completion of old operations, no other operation may access the old object memory locations, so the writer can safely write back the new objects from the writer-log into the memory, overwriting the old objects.", "It can then release the locks.", "Figure 3 from the paper provides a series of worked examples which is probably the best way to quickly grasp the idea.", "In each of the diagrams that follow, swim-lanes represent threads (T1, T2, T3) and time progress downwards.", "The global clock and fine-grained object locks are represented in the ‘memory’ column.", "Full pseudo-code for the algorithm is given in section 3.5 of the paper.", "Concurrent reads, no writers  In the example above, threads T1 and T2 both perform reads.", "They begin by copying the value of the global clock (22) to their local clocks.", "T1 reads O1, and T2 reads O1 and O2.", "None of these objects are locked, so reads proceed concurrently from memory.", "Concurrent reads with uncommitted writes  T2 now wants to update objects O2 and O3.", "Each of these objects is logged (copied into T2’s write-log) and modified in the log.", "T2 locks these objects using the RLU shared locks (memory column) before logging them.", "T1 tries to read O2, after T2 has locked it.", "T1 sees that T2 holds the lock, and compares its local clock (22) with the value of T2’s write clock (currently at &inf; since T2 has not committed its writes yet).", "From this comparison we see that this read has not started after a write clock increment (T1’s clock is < T2’s), and so T1 should not try to read O2 from T2’s log (‘steal’ it), but instead should read the object directly from memory.", "Committing writes  T2 begins the process of committing its writes.", "It increments the clock value and stores this first in its write-clock, and secondly in the global clock (the order matters).", "T2 now has to wait for a quiescent point in order to be able to install its updates – it must wait for ‘old’ operations to finish, those with local clock values < its own.", "In this worked example, that means waiting for T1 to finish.", "T3 has also started at this point, but has a local clock value of 23 (&geq; T2’s), so T2 does not need to wait for it.", "When T1 completes, T2 can safely write back the objects in its log to memory and release the locks.", "T3 meanwhile wants to read O2 before T2 has been able to install its updates.", "It compares its local clock value (23) with the write-clock value of T2 (since T2 has the lock at this point), and since its local clock is &geq; T2’s write-clock, it ‘steals’ the value from T2s write-log.", "An optimisation – deferring synchronisation  RLU synchronize deferral works as follows.", "On commit, instead of incrementing the global clock and executing RLU synchronize, the RLU writer simply saves the current write-log and generates a new log for the next writer.", "In this way, RLU writers execute without blocking on RLU synchronize calls, while aggregating write-logs and locks of objects being modified.", "The RLU synchronize call is actually only necessary when a writer tries to lock an object that is already locked.", "Therefore, only in this case, the writer sends a “sync request” to the conflicting thread to force it to release its locks, by making the thread increment the global clock, execute RLU synchronize, write back, and unlock.", "This significantly reduces the amount of RLU synchronize calls, as well as contention on the global clock.", "The aggregration of write-logs and deferring of the global clock update also defers the stealing process to a later time, allowing more reads from memory."], "summary_text": "Read-Log-Update: A Lightweight Synchronization Mechanism for Concurrent Programming – Matveev et al. 2015  An important paradigm in concurrent data structure scalability is to support read-only traversals: sequences of reads that execute without any synchronization (and hence require no memory fences and generate no contention). The gain from such unsynchronized traversals is significant because they account for a large fraction of operations in many data structures and applications. The popular read-copy-update (RCU) mechanism of McKenney and Slingwine provides scalability by enabling this paradigm. It allows read-only traversals to proceed concurrently with updates by creating a copy of the data structure being modified. Readers access the unmodified data structure while updaters modify the copy. The key to RCU is that once modifications are complete, they are installed using a single pointer modification in a way that does not interfere with ongoing readers. RCU has been supported in the Linux kernel since 2002, and the User-space RCU library is available for user-space applications. In this paper Matveev et al. take a big step forward beyond RCU with the introduction of  an extension to the base RCU ideas they call Read-Log-Update (RLU). RLU provides better performance and concurrency than RCU (it allows multiple simultaneous writers for example), but I think even more importantly it has a much simpler programming model meaning it can be applied to many more scenarios. For example, the RCU-based doubly-linked list in the Linux kernel only allows threads to traverse the list in the forward direction because of the limitations of RCU, whereas the RLU based alternative can easily support traversal in both directions. I’m going to focus here on the core RLU algorithm. The paper also contains an extensive evaluation section which is a fascinating read, and has the by-product of reminding me just how big our field is and of the huge body of knowledge built up in data structures and algorithms. I collected a good number of additions to my ‘known unknowns’ list reading through it! For example: “We then apply the more advanced RLU scheme with the deferral mechanism of synchronize calls to the state-of-the-art RCU-based Citrus tree, an enhancement of the Bonsai tree of Clements et al. The Citrus tree uses both RCU and fine-grained locks to deliver the best performing search tree to date…”  Let’s dive into how RLU works:  RLU provides support for multiple object updates in a single operation by combining the quiescence mechanism of RCU with a global clock and per thread object-level logs. The global clock and per-thread object logs will shortly be explained. Just in case the quiescence mechanism of RCU isn’t front-and-centre in your mind right now, here’s a quick refresher:  When [RCU readers] are not inside a critical section, readers are said to be in a quiescent state. A period of time during which every thread goes through at least one quiescent state is called a grace period. The key principle of RCU is that, if an updater removes an element from an RCU-protected shared data structure and waits for a grace period, there can be no reader still accessing that element. It is therefore safe to dispose of it. Waiting for a grace period can be achieved by calling synchronize rcu(). The global clock is a logical clock (counter). All operations read the global clock value before they start, and  use this clock value to determine what version of a shared object they should read. A write thread first makes a copy of object to be updated in its  write-log, and then modifies the object in the log. This mechanism hides updates from concurrent reads. To avoid conflicts with concurrent writes, each object is also locked before it is duplicated and modified. Then, to commit the new object copies, a write operation increments the global clock, which effectively splits operations into two sets: (1) old operations that started before the clock increment, and (2) new operations that start after the clock increment. The first set of operations will read the old object copies while the second set will read the new object copies of this writer. Therefore, in the next step, the writer waits for old operations to finish by executing the RCU-style quiescence loop, while new operations “steal” new object copies of this writer by accessing the per thread write-log of this writer. After the completion of old operations, no other operation may access the old object memory locations, so the writer can safely write back the new objects from the writer-log into the memory, overwriting the old objects. It can then release the locks. Figure 3 from the paper provides a series of worked examples which is probably the best way to quickly grasp the idea. In each of the diagrams that follow, swim-lanes represent threads (T1, T2, T3) and time progress downwards. The global clock and fine-grained object locks are represented in the ‘memory’ column. Full pseudo-code for the algorithm is given in section 3.5 of the paper. Concurrent reads, no writers  In the example above, threads T1 and T2 both perform reads. They begin by copying the value of the global clock (22) to their local clocks. T1 reads O1, and T2 reads O1 and O2. None of these objects are locked, so reads proceed concurrently from memory. Concurrent reads with uncommitted writes  T2 now wants to update objects O2 and O3. Each of these objects is logged (copied into T2’s write-log) and modified in the log. T2 locks these objects using the RLU shared locks (memory column) before logging them. T1 tries to read O2, after T2 has locked it. T1 sees that T2 holds the lock, and compares its local clock (22) with the value of T2’s write clock (currently at &inf; since T2 has not committed its writes yet). From this comparison we see that this read has not started after a write clock increment (T1’s clock is < T2’s), and so T1 should not try to read O2 from T2’s log (‘steal’ it), but instead should read the object directly from memory. Committing writes  T2 begins the process of committing its writes. It increments the clock value and stores this first in its write-clock, and secondly in the global clock (the order matters). T2 now has to wait for a quiescent point in order to be able to install its updates – it must wait for ‘old’ operations to finish, those with local clock values < its own. In this worked example, that means waiting for T1 to finish. T3 has also started at this point, but has a local clock value of 23 (&geq; T2’s), so T2 does not need to wait for it. When T1 completes, T2 can safely write back the objects in its log to memory and release the locks. T3 meanwhile wants to read O2 before T2 has been able to install its updates. It compares its local clock value (23) with the write-clock value of T2 (since T2 has the lock at this point), and since its local clock is &geq; T2’s write-clock, it ‘steals’ the value from T2s write-log. An optimisation – deferring synchronisation  RLU synchronize deferral works as follows. On commit, instead of incrementing the global clock and executing RLU synchronize, the RLU writer simply saves the current write-log and generates a new log for the next writer. In this way, RLU writers execute without blocking on RLU synchronize calls, while aggregating write-logs and locks of objects being modified. The RLU synchronize call is actually only necessary when a writer tries to lock an object that is already locked. Therefore, only in this case, the writer sends a “sync request” to the conflicting thread to force it to release its locks, by making the thread increment the global clock, execute RLU synchronize, write back, and unlock. This significantly reduces the amount of RLU synchronize calls, as well as contention on the global clock. The aggregration of write-logs and deferring of the global clock update also defers the stealing process to a later time, allowing more reads from memory.", "pdf_url": "http://sigops.org/sosp/sosp15/current/2015-Monterey/printable/077-matveev.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/read-log-update-a-lightweight-synchronization-mechanism-for-concurrent-programming.json"}
{"id": "5225266", "bin": "1500_1600", "summary_sentences": ["Time evolving graph processing at scale Iyer et al., GRADES 2016  Here’s a new (June 2016) paper from the distinguished AMPlab group at Berkeley that really gave me cause to reflect.", "The work addresses the problem of performing graph computations on graphs that are constantly changing (because updates flow in, such as a new follower in a social graph).", "Many graphs of interest have this property of constantly evolving.", "In part, that’s what makes them interesting.", "You could always take a snapshot of e.g. the graph as it was at the end of the previous day and compute on that, but some applications need more up to date results (e.g.", "detecting traffic hotspots in cellular networks), and many applications would benefit from real-time results.", "GraphTau is a solution to this problem, implemented on top of GraphX which is in turn implemented on top of Spark’s RDDs.", "It’s a convergence of stream processing and graph processing.", "I’m seeing a lot of streaming recently, and a lot of convergence.", "That topic probably warrants a separate post.", "In the meantime…  Graph-structured data is on the rise, in size, complexity and the dynamism they exhibit.", "From social networks to telecommunication networks, applications that generate graph-structured data are ubiquitous… the dynamic nature of these datasets gives them a unique characteristic – the graph-structure underlying the data evolves over time.", "Unbounded, real-time data is fast becoming the norm , and thus it is important to process these time-evolving graph-structured datasets efficiently.", "(Aside: applications generating graph-structured data certainly are ubiquitous – pretty much any relational database has graph structure the minute you introduce foreign keys.", "It’s applications generating graph-structured data and that require extensive traversals or graph-specific computations that we’re really interested in here).", "For time-evolving graph-structured datasets, the authors identify three core requirements:  The ability to execute iterative graph algorithms in real-time  The ability to combine graph-structured data with unstructured and tabular data  The ability to run analytics over windows of input data  While some specialized systems for evolving-graph processing exist, these do not support the second and third requirements.", "GraphTau is “the first time-evolving graph processing system, to our knowledge, built on a general purpose dataflow framework.” GraphTau is built on top of GraphX, which maintains graphs internally as a pair of RDDs: a vertex collection and an edge collection.", "(Note that Apache Flink has Gelly , which builds graph processing on top of a streaming dataflow core, but does not support iterative processing over evolving graphs to the best of my knowledge.)", "The main idea in GraphTau is to treat time-evolving graphs as a series of consistent graph snapshots, and dynamic graph computations as a series of deterministic batch computations on discrete time intervals.", "A graph snapshot is simply a regular graph, stored as two RDDs, the vertex RDD and the edge RDD.", "We define GraphStream as a sequence of immutable, partitioned datasets (graphs represented as two RDDs) that can be acted on by deterministic operators.", "User programs manipulate GraphStreams to produce new GraphStreams, as well as intermediate state in the form of RDDs or graphs.", "A DeltaRDD is an RDD whose elements are updates that need to be applied to a graph.", "A stream of such updates is called a DeltaDStream.", "GraphStreams can be built from a DeltaDStream or directly from a vertex DStream and an edge DStream.", "There are two supported computational models, called pause-shift-resume and online rectification.", "Pause-shift-resume  Some classes of graph algorithms can cope with the graph being modified while the algorithm is still converging.", "For example, if a graph changes during an evaluation of PageRank you’ll still get an answer, which studies have shown will be within a reasonable error to the actual answer you’d get if you started the algorithm again from scratch with the now current graph.", "Under these conditions, the pause-shift-resume (PSR) model is appropriate.", "In this model, GraphTau starts running a graph algorithm as soon as the first snapshot of a graph is available.", "Upon the availability of a new snapshot, it pauses the computation on the current graph, shifts the algorithm specific data to the new snapshot, and resumes the computation on the new graph.", "Online rectification  Algorithms such as connected-components will produce incorrect results under the PSR model (consider an edge or vertex that is removed during processing).", "For such algorithms, GraphTau proposes the online rectification model.", "In this model, GraphTau rectifies the errors caused by the underlying graph modificationts in an online fashion using minimal state.", "In the connected component example, it is necessary for every vertex to keep track of its component id over time.", "The approach works for any algorithm based on label propagation, at the expense of needing to keep algorithm-specific state.", "The question of time  GraphStream splits time into non-overlapping intervals, and stores all the inputs received during these intervals in batches (worker nodes are synchronized using NTP).", "Such intervals are based on receive time, there is also an option to process based on external timestamps (event time) which requires either the introduction of limited slack time to wait for late events, or application specific code to correct for late records.", "Each interval’s updates reflects all of the input received until then.", "This is despite the fact that the DeltaRDD and its updated graph snapshot are distributed across nodes.", "As long as we process the whole batch consistently (e.g. ordered by timestamps), we will get a consistent snapshot.", "This makes distributed state much easier to reason about and is the same as “exactly once” processing of the graph updates even with faults or stragglers.", "GraphStream inherits its recovery mechanisms from GraphX and its RDDs: parallel recovery of lost state and speculative execution.", "Programming with GraphTau  The GraphStream interface supports transform, merge, streamingBSP, and forEachGraph operations as well an updateLocalState operator to allow for event processing and state tracking.", "mergeByWindow merges all graphs from a sliding window of past time intervals into one graph  forEachGraph applies a function to each graph generated from the GraphStream  transformWith combines two graph streams with various join and cogroup operators.", "the streamingBSP operator supports differential computation  This [streamingBSP] operator enables efficient implementation of a large class of incremental algorithms on time-evolving graphs.", "We signal the availability of the new graph snapshot using a variable in the driver program.", "In each iteration of Pregel , we check whether a new graph is available.", "If so, we do not proceed to the next iteration on the current graph.", "Instead, we resume computation on the new graph reusing the result, where only vertices in the new active set continue message passing.", "The new active set is a function of the old active set and the changes between the new graph and the old graph.", "For a large class of algorithms (e.g. incremental PageRank), the new active set includes vertices from the old set, any new vertices, and vertices with edge additions and deletions.", "Here’s what the Page Rank example looks like:  Even on a simple six-node graph where one edge is added after 10 iterations, this saves 13/34 iterations overall.", "Here’s another example GraphTau program, showing the ability to unify data and graph stream processing.", "This example computes top users in terms of triangle counts from a Twitter attention graph.", "A DStream ds is created from the external Twitter feed, and then a GraphStream is built from it.", "Triangle count is applied to each graph snapshot, and then we compute the number of times a user is a top user over a sliding window of ten seconds, outputting results every second.", "Preliminary evaluation shows that GraphTau’s performances matches or exceeds that of specialized systems on a streaming connected components task based on a cellular dataset.", "The last word…  In this paper, we presented GraphTau, a time-evolving graph processing system built on a data flow framework that addresses this demand.", "GraphTau represents time-evolving graphs as a series of consistent graph snapshots.", "On these snapshots, GraphTau enables two computational model, the Pause-Shift-Resume model and the Online Rectification model which allows the application of differential computation on a wide variety of graph algorithms.", "These techniques enable GraphTau to achieve significant performance improvements."], "summary_text": "Time evolving graph processing at scale Iyer et al., GRADES 2016  Here’s a new (June 2016) paper from the distinguished AMPlab group at Berkeley that really gave me cause to reflect. The work addresses the problem of performing graph computations on graphs that are constantly changing (because updates flow in, such as a new follower in a social graph). Many graphs of interest have this property of constantly evolving. In part, that’s what makes them interesting. You could always take a snapshot of e.g. the graph as it was at the end of the previous day and compute on that, but some applications need more up to date results (e.g. detecting traffic hotspots in cellular networks), and many applications would benefit from real-time results. GraphTau is a solution to this problem, implemented on top of GraphX which is in turn implemented on top of Spark’s RDDs. It’s a convergence of stream processing and graph processing. I’m seeing a lot of streaming recently, and a lot of convergence. That topic probably warrants a separate post. In the meantime…  Graph-structured data is on the rise, in size, complexity and the dynamism they exhibit. From social networks to telecommunication networks, applications that generate graph-structured data are ubiquitous… the dynamic nature of these datasets gives them a unique characteristic – the graph-structure underlying the data evolves over time. Unbounded, real-time data is fast becoming the norm , and thus it is important to process these time-evolving graph-structured datasets efficiently. (Aside: applications generating graph-structured data certainly are ubiquitous – pretty much any relational database has graph structure the minute you introduce foreign keys. It’s applications generating graph-structured data and that require extensive traversals or graph-specific computations that we’re really interested in here). For time-evolving graph-structured datasets, the authors identify three core requirements:  The ability to execute iterative graph algorithms in real-time  The ability to combine graph-structured data with unstructured and tabular data  The ability to run analytics over windows of input data  While some specialized systems for evolving-graph processing exist, these do not support the second and third requirements. GraphTau is “the first time-evolving graph processing system, to our knowledge, built on a general purpose dataflow framework.” GraphTau is built on top of GraphX, which maintains graphs internally as a pair of RDDs: a vertex collection and an edge collection. (Note that Apache Flink has Gelly , which builds graph processing on top of a streaming dataflow core, but does not support iterative processing over evolving graphs to the best of my knowledge.) The main idea in GraphTau is to treat time-evolving graphs as a series of consistent graph snapshots, and dynamic graph computations as a series of deterministic batch computations on discrete time intervals. A graph snapshot is simply a regular graph, stored as two RDDs, the vertex RDD and the edge RDD. We define GraphStream as a sequence of immutable, partitioned datasets (graphs represented as two RDDs) that can be acted on by deterministic operators. User programs manipulate GraphStreams to produce new GraphStreams, as well as intermediate state in the form of RDDs or graphs. A DeltaRDD is an RDD whose elements are updates that need to be applied to a graph. A stream of such updates is called a DeltaDStream. GraphStreams can be built from a DeltaDStream or directly from a vertex DStream and an edge DStream. There are two supported computational models, called pause-shift-resume and online rectification. Pause-shift-resume  Some classes of graph algorithms can cope with the graph being modified while the algorithm is still converging. For example, if a graph changes during an evaluation of PageRank you’ll still get an answer, which studies have shown will be within a reasonable error to the actual answer you’d get if you started the algorithm again from scratch with the now current graph. Under these conditions, the pause-shift-resume (PSR) model is appropriate. In this model, GraphTau starts running a graph algorithm as soon as the first snapshot of a graph is available. Upon the availability of a new snapshot, it pauses the computation on the current graph, shifts the algorithm specific data to the new snapshot, and resumes the computation on the new graph. Online rectification  Algorithms such as connected-components will produce incorrect results under the PSR model (consider an edge or vertex that is removed during processing). For such algorithms, GraphTau proposes the online rectification model. In this model, GraphTau rectifies the errors caused by the underlying graph modificationts in an online fashion using minimal state. In the connected component example, it is necessary for every vertex to keep track of its component id over time. The approach works for any algorithm based on label propagation, at the expense of needing to keep algorithm-specific state. The question of time  GraphStream splits time into non-overlapping intervals, and stores all the inputs received during these intervals in batches (worker nodes are synchronized using NTP). Such intervals are based on receive time, there is also an option to process based on external timestamps (event time) which requires either the introduction of limited slack time to wait for late events, or application specific code to correct for late records. Each interval’s updates reflects all of the input received until then. This is despite the fact that the DeltaRDD and its updated graph snapshot are distributed across nodes. As long as we process the whole batch consistently (e.g. ordered by timestamps), we will get a consistent snapshot. This makes distributed state much easier to reason about and is the same as “exactly once” processing of the graph updates even with faults or stragglers. GraphStream inherits its recovery mechanisms from GraphX and its RDDs: parallel recovery of lost state and speculative execution. Programming with GraphTau  The GraphStream interface supports transform, merge, streamingBSP, and forEachGraph operations as well an updateLocalState operator to allow for event processing and state tracking. mergeByWindow merges all graphs from a sliding window of past time intervals into one graph  forEachGraph applies a function to each graph generated from the GraphStream  transformWith combines two graph streams with various join and cogroup operators. the streamingBSP operator supports differential computation  This [streamingBSP] operator enables efficient implementation of a large class of incremental algorithms on time-evolving graphs. We signal the availability of the new graph snapshot using a variable in the driver program. In each iteration of Pregel , we check whether a new graph is available. If so, we do not proceed to the next iteration on the current graph. Instead, we resume computation on the new graph reusing the result, where only vertices in the new active set continue message passing. The new active set is a function of the old active set and the changes between the new graph and the old graph. For a large class of algorithms (e.g. incremental PageRank), the new active set includes vertices from the old set, any new vertices, and vertices with edge additions and deletions. Here’s what the Page Rank example looks like:  Even on a simple six-node graph where one edge is added after 10 iterations, this saves 13/34 iterations overall. Here’s another example GraphTau program, showing the ability to unify data and graph stream processing. This example computes top users in terms of triangle counts from a Twitter attention graph. A DStream ds is created from the external Twitter feed, and then a GraphStream is built from it. Triangle count is applied to each graph snapshot, and then we compute the number of times a user is a top user over a sliding window of ten seconds, outputting results every second. Preliminary evaluation shows that GraphTau’s performances matches or exceeds that of specialized systems on a streaming connected components task based on a cellular dataset. The last word…  In this paper, we presented GraphTau, a time-evolving graph processing system built on a data flow framework that addresses this demand. GraphTau represents time-evolving graphs as a series of consistent graph snapshots. On these snapshots, GraphTau enables two computational model, the Pause-Shift-Resume model and the Online Rectification model which allows the application of differential computation on a wide variety of graph algorithms. These techniques enable GraphTau to achieve significant performance improvements.", "pdf_url": "http://www.cs.columbia.edu/~lierranli/publications/GraphTau-GRADES2016.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/time-evolving-graph-processing-at-scale.json"}
{"id": "50388820", "bin": "1500_1600", "summary_sentences": ["RobinHood: tail latency aware caching – dynamic reallocation from cache-rich to cache-poor Berger et al., OSDI’18  It’s time to rethink everything you thought you knew about caching!", "My mental model goes something like this: we have a set of items that probably follow a power-law of popularity.", "We have a certain finite cache capacity, and we use it to cache the most frequently requested items, speeding up request processing.", "Now, there’s a long tail of less frequently requested items, and if we request one of these that’s not in the cache the request is going to take longer (higher latency).", "But it makes no sense whatsoever to try and improve the latency for these requests by ‘shifting our cache to the right.’  Hence the received wisdom that unless the full working set fits entirely in the cache, then a caching layer doesn’t address tail latency.", "So far we’ve been talking about one uniform cache.", "But in a typical web application one incoming request might fan out to many back-end service requests processed in parallel.", "The OneRF page rendering framework at Microsoft (which serves msn.com, microsoft.com and xbox.com among others) relies on more than 20 backend systems for example.", "The cache is shared across these back-end requests, either with a static allocation per back-end that has been empirically tuned, or perhaps with dynamic allocation so that more popular back-ends get a bigger share of the cache.", "The thing about this common pattern is that we need to wait for all of these back-end requests to complete before returning to the user.", "So improving the average latency of these requests doesn’t help us one little bit.", "Since each request must wait for all of its queries to complete, the overall request latency is defined to be the latency of the request’s slowest query.", "Even if almost all backends have low tail latencies, the tail latency of the maximum of several queries could be high.", "(See ‘ The Tail at Scale ’).", "The user can easily see P99 latency or greater.", "Techniques to mitigate tail latencies include making redundant requests, clever use of scheduling, auto-scaling and capacity provisioning, and approximate computing.", "Robin Hood takes a different (complementary) approach: use the cache to improve tail latency!", "Robin Hood doesn’t necessarily allocate caching resources to the most popular back-ends, instead, it allocates caching resources to the backends (currently) responsible for the highest tail latency.", "…RobinHood dynamically allocates cache space to those backends responsible for high request tail latency (cache-poor) backends, while stealing space from backends that do not affect the request tail latency (cache-rich backends).", "In doing so, Robin Hood makes compromises that may seem counter-intuitive (e.g., significantly increasing the tail latencies of certain backends).", "If you’re still not yet a believer that caching can help with tail latencies, the evaluation results should do the trick.", "RobinHood is evaluated with production traces from a 50-server cluster with 20 different backend systems.", "It’s able to address tail latency even when working sets are much larger than the cache size.", "In the presence of load spikes, RobinHood meets a 150ms P99 goal 99.7% of the time, whereas the next best policy meets this goal only 70% of the time.", "Look at that beautiful blue line!", "When RobinHood allocates extra cache space to a backend experience high tail latency, the hit ratio for that backend typically improves.", "We get a double benefit:  Since backend query latency is highly variable in practice, decreasing the number of queries to a backend will decrease the number of high-latency queries observed, improving the P99 request latency.", "The backend system will see fewer requests.", "As we’ve studied before on The Morning Paper , small reductions in resource congestion can have an outsized impact on backend latency once a system has started degrading.", "Caching challenges  Why can’t we just figure out which backends contribute the most to tail latency and just statically assign more cache space to them?", "Because the latencies of different backends tends to vary wildly over time: they are complex distributed systems in their own right.", "The backends are often shared across several customers too (either within the company, or perhaps you’re calling an external service).", "So the changing demands from other consumers can impact the latency you see.", "Most existing cache systems implicitly assume that latency is balanced.", "They focus on optimizing cache-centric metrics (e.g., hit ratio), which can be a poor representation of overall performance if latency is imbalanced.", "Query latency is not correlated with query popularity, but instead reflects a more holistic state of the backed system at some point in time.", "An analysis of OneRF traces over a 24 hour period shows that the seventh most queried backend receives only about 0.06x as many queries as the most queried backend, but has 3x the query latency.", "Yet shared caching systems inherently favour backends with higher query rates (they have more shots at getting something in the cache).", "The RobinHood caching system  RobinHood operates in 5 second time windows, repeatedly taxing every backend by reclaiming 1% of its cache space and redistributing the wealth to cache-poor backends.", "Within each window RobinHood tracks the latency of each request, and chooses a small interval (P98.5 to P99.5) around P99 to focus on, since the goal is to minimise the P99 latency.", "For each request that falls within this interval, RobinHood tracks the ID of the backend corresponding to the slowest query in the request.", "At the end of the window RobinHood calculates the request blocking count (RBC) of each backend – the number of times it was responsible for the slowest query.", "Backends with a high RBC are frequently the bottleneck in slow requests.", "RobinHood thus considers a backend’s RBC as a measure of how cache-poor it is, and distributes the pooled tax to each backend in proportion to its RBC.", "RBC neatly encapsulates the dual considerations of how likely a backend is to have high latency, and how many times that backend is queried during request processing.", "Since some backends are slow to make use of the additional cache space (e.g., if their hit rations are already high).", "RobinHood monitors the gap between the allocated and used cache capacity for each backend, and temporarily ignores the RBC of any backend with more than a 30% gap.", "When load balancing across a set of servers RobinHood makes allocation decisions locally on each server.", "To avoid divergence of cache allocations over time, RobinHood controllers exchange RBC data.", "With a time window of 5 seconds, RobinHood caches converge to the average allocation within about 30 minutes.", "The RobinHood implementation uses off-the-shelf memcached instances to form the caching layer in each application server.", "A lightweight cache controller at each node implements the RobinHood algorithm and issues resize commands to the local cache partitions.", "A centralised RBC server is used for exchange of RBC information.", "RBC components store only soft state (aggregated RBC for the last one million requests, in a ring buffer), so can quickly recover after a crash or restart.", "Key evaluation results  The RobinHood evaluation is based on detailed statistics of production traffic in the OneRF system for several days in 2018.", "The dataset describes queries to more than 40 distinct backend systems.", "RobinHood is compared against the existing OneRF policy, the policy from Facebook’s TAO , and three research systems Cliffhanger , FAIR, and LAMA.", "Here are the key results:  RobinHood brings SLO violations down to 0.3%, compared to 30% SLO violations under the next best policy.", "For quickly increasing backend load imbalances, RobinHood maintains SLO violations below 1.5%, compared to 38% SLO violations under the next best policy.", "Under simultaneous latency spikes, RobinHood maintains less than 5% SLO violations, while other policies do significantly worse.", "Compared to the maximum allocation for each backend under RobinHood, even a perfectly clairvoyant static allocation would need 73% more cache space.", "RobinHood introduces negligible overhead on network, CPU, and memory usage.", "Our evaluation shows that RobinHood can reduce SLO violations from 30% to 0.3% for highly variable workloads such an OneRF.", "RobinHood is also lightweight, scalable, and can be deployed on top of an off-the-shelf software stack… RobinHood shows that, contrary to popular belief, a properly designed caching layer can be used to reduce higher percentiles of request latency."], "summary_text": "RobinHood: tail latency aware caching – dynamic reallocation from cache-rich to cache-poor Berger et al., OSDI’18  It’s time to rethink everything you thought you knew about caching! My mental model goes something like this: we have a set of items that probably follow a power-law of popularity. We have a certain finite cache capacity, and we use it to cache the most frequently requested items, speeding up request processing. Now, there’s a long tail of less frequently requested items, and if we request one of these that’s not in the cache the request is going to take longer (higher latency). But it makes no sense whatsoever to try and improve the latency for these requests by ‘shifting our cache to the right.’  Hence the received wisdom that unless the full working set fits entirely in the cache, then a caching layer doesn’t address tail latency. So far we’ve been talking about one uniform cache. But in a typical web application one incoming request might fan out to many back-end service requests processed in parallel. The OneRF page rendering framework at Microsoft (which serves msn.com, microsoft.com and xbox.com among others) relies on more than 20 backend systems for example. The cache is shared across these back-end requests, either with a static allocation per back-end that has been empirically tuned, or perhaps with dynamic allocation so that more popular back-ends get a bigger share of the cache. The thing about this common pattern is that we need to wait for all of these back-end requests to complete before returning to the user. So improving the average latency of these requests doesn’t help us one little bit. Since each request must wait for all of its queries to complete, the overall request latency is defined to be the latency of the request’s slowest query. Even if almost all backends have low tail latencies, the tail latency of the maximum of several queries could be high. (See ‘ The Tail at Scale ’). The user can easily see P99 latency or greater. Techniques to mitigate tail latencies include making redundant requests, clever use of scheduling, auto-scaling and capacity provisioning, and approximate computing. Robin Hood takes a different (complementary) approach: use the cache to improve tail latency! Robin Hood doesn’t necessarily allocate caching resources to the most popular back-ends, instead, it allocates caching resources to the backends (currently) responsible for the highest tail latency. …RobinHood dynamically allocates cache space to those backends responsible for high request tail latency (cache-poor) backends, while stealing space from backends that do not affect the request tail latency (cache-rich backends). In doing so, Robin Hood makes compromises that may seem counter-intuitive (e.g., significantly increasing the tail latencies of certain backends). If you’re still not yet a believer that caching can help with tail latencies, the evaluation results should do the trick. RobinHood is evaluated with production traces from a 50-server cluster with 20 different backend systems. It’s able to address tail latency even when working sets are much larger than the cache size. In the presence of load spikes, RobinHood meets a 150ms P99 goal 99.7% of the time, whereas the next best policy meets this goal only 70% of the time. Look at that beautiful blue line! When RobinHood allocates extra cache space to a backend experience high tail latency, the hit ratio for that backend typically improves. We get a double benefit:  Since backend query latency is highly variable in practice, decreasing the number of queries to a backend will decrease the number of high-latency queries observed, improving the P99 request latency. The backend system will see fewer requests. As we’ve studied before on The Morning Paper , small reductions in resource congestion can have an outsized impact on backend latency once a system has started degrading. Caching challenges  Why can’t we just figure out which backends contribute the most to tail latency and just statically assign more cache space to them? Because the latencies of different backends tends to vary wildly over time: they are complex distributed systems in their own right. The backends are often shared across several customers too (either within the company, or perhaps you’re calling an external service). So the changing demands from other consumers can impact the latency you see. Most existing cache systems implicitly assume that latency is balanced. They focus on optimizing cache-centric metrics (e.g., hit ratio), which can be a poor representation of overall performance if latency is imbalanced. Query latency is not correlated with query popularity, but instead reflects a more holistic state of the backed system at some point in time. An analysis of OneRF traces over a 24 hour period shows that the seventh most queried backend receives only about 0.06x as many queries as the most queried backend, but has 3x the query latency. Yet shared caching systems inherently favour backends with higher query rates (they have more shots at getting something in the cache). The RobinHood caching system  RobinHood operates in 5 second time windows, repeatedly taxing every backend by reclaiming 1% of its cache space and redistributing the wealth to cache-poor backends. Within each window RobinHood tracks the latency of each request, and chooses a small interval (P98.5 to P99.5) around P99 to focus on, since the goal is to minimise the P99 latency. For each request that falls within this interval, RobinHood tracks the ID of the backend corresponding to the slowest query in the request. At the end of the window RobinHood calculates the request blocking count (RBC) of each backend – the number of times it was responsible for the slowest query. Backends with a high RBC are frequently the bottleneck in slow requests. RobinHood thus considers a backend’s RBC as a measure of how cache-poor it is, and distributes the pooled tax to each backend in proportion to its RBC. RBC neatly encapsulates the dual considerations of how likely a backend is to have high latency, and how many times that backend is queried during request processing. Since some backends are slow to make use of the additional cache space (e.g., if their hit rations are already high). RobinHood monitors the gap between the allocated and used cache capacity for each backend, and temporarily ignores the RBC of any backend with more than a 30% gap. When load balancing across a set of servers RobinHood makes allocation decisions locally on each server. To avoid divergence of cache allocations over time, RobinHood controllers exchange RBC data. With a time window of 5 seconds, RobinHood caches converge to the average allocation within about 30 minutes. The RobinHood implementation uses off-the-shelf memcached instances to form the caching layer in each application server. A lightweight cache controller at each node implements the RobinHood algorithm and issues resize commands to the local cache partitions. A centralised RBC server is used for exchange of RBC information. RBC components store only soft state (aggregated RBC for the last one million requests, in a ring buffer), so can quickly recover after a crash or restart. Key evaluation results  The RobinHood evaluation is based on detailed statistics of production traffic in the OneRF system for several days in 2018. The dataset describes queries to more than 40 distinct backend systems. RobinHood is compared against the existing OneRF policy, the policy from Facebook’s TAO , and three research systems Cliffhanger , FAIR, and LAMA. Here are the key results:  RobinHood brings SLO violations down to 0.3%, compared to 30% SLO violations under the next best policy. For quickly increasing backend load imbalances, RobinHood maintains SLO violations below 1.5%, compared to 38% SLO violations under the next best policy. Under simultaneous latency spikes, RobinHood maintains less than 5% SLO violations, while other policies do significantly worse. Compared to the maximum allocation for each backend under RobinHood, even a perfectly clairvoyant static allocation would need 73% more cache space. RobinHood introduces negligible overhead on network, CPU, and memory usage. Our evaluation shows that RobinHood can reduce SLO violations from 30% to 0.3% for highly variable workloads such an OneRF. RobinHood is also lightweight, scalable, and can be deployed on top of an off-the-shelf software stack… RobinHood shows that, contrary to popular belief, a properly designed caching layer can be used to reduce higher percentiles of request latency.", "pdf_url": "https://www.usenix.org/system/files/osdi18-berger.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/robinhood-tail-latency-aware-caching-dynamic-reallocation-from-cache-rich-to-cache-poor.json"}
{"id": "14819310", "bin": "1500_1600", "summary_sentences": ["Barrier-enabled IO stack for flash storage Won et al., FAST’18  The performance of Flash storage has benefited greatly from concurrency and parallelism – for example, multi-channel controllers, large caches, and deep command queues.", "At the same time, the time to program an individual Flash cell has stayed fairly static (and even become slightly worse in some cases).", "That’s ok until an application needs to guarantee ordering and/or durability of writes.", "Enforcing a storage order is achieved by an extremely expensive approach: dispatching the following request only after the block associated with preceding request has been completely transferred to the storage device and made durable.", "We call this mechanism Transfer-and-Flush.", "With transfer-and-flush, the parallelism and concurrency benefits are lost.", "As a consequence, on a single channel mobile storage SSD for smartphones, ordered writes offer only 20% of the performance (IOPS) of unordered writes.", "For a 32-channel Flash array, this ratio decreases to just 1%.", "In today’s paper (which also won a best-paper award at FAST’18), the authors introduce a Barrier-enabled IO stack, which can enforce storage order between requests without the filesystem having to wait for a full transfer-and-flush.", "The headline performance gains certainly make you sit up and take notice:  SQLite performance increases by 270% and 75%, in server and in smartphone, respectively.", "In server storage BarrierFS brings performance gains of 43x and 73x in MySQL and SQLite respectively when compared to EXT4.", "There’s some important fine-print in the headline numbers – the biggest gains come when ordering is preserved, but we don’t wait for durability.", "The baseline being compared against offers both.", "Getting your ducks in order  The modern IO stack is said to be orderless…  Consider a set of writes issued in some order (issue order) by a file system:  The IO scheduler may reorder and coalesce IO requests according so some scheduling principle (e.g., CFQ), before placing them onto a dispatch queue to be dispatched to the storage device (dispatch order).", "The storage controller receives the incoming commands and places them in its command queue.", "It is free to schedule commands for transfer as it sees fit.", "Errors, time-outs, and retries further impact the eventual ordering (transfer completion order).", "The overall persistence order is governed not by the order in which data blocks are made durable, but by the order in which the associated mapping table entries are updated.", "The two may not coincide.", "The long standing assumption (a holdover from the physical characteristics of rotating disks) is that the host cannot control the persistence order.", "The constraint that the host cannot control the persist order is a fundamental limitation in modern IO stack design.", "To guarantee ordering in the face of this, an expensive transfer-on-flush mechanism is used.", "If a needs to be ordered before b, then after dispatching a to the storage device, the caller needs to wait for a to be serviced (the DMA transfer to complete) — wait-on-transfer.", "Then the caller can issue a flush command and wait for its completion (wait-on-flush).", "Only once the flush has returned can the caller dispatch b.", "When ext4 (in its default Ordered mode) commits a journal transaction it uses two write requests: one for writing a coalesced chunk of journal descriptor block and log blocks (JD), and one for writing the commit block (JC).", "JD needs to be made durable before JC.", "Across transactions, ext4 also has to ensure that transactions are made durable in order.", "If either of these two ordering constraints are violated, the file system may recover incorrectly in the case of an unexpected failure.", "Order-preserving block devices with barriers  The ‘cache barrier,’ or ‘barrier’ for short, command is defined in the standard command set for mobile Flash storage.", "With the barrier command, the host can control the persist order without explicitly invoking cache flush.", "When the storage controller receives the barrier command, it guarantees that the data blocks transferred before the barrier command become durable after the ones that follow the barrier command do.", "The authors implement the barrier command concept using a new attribute REQ_BARRIER that can be set on a regular request object (to avoid the overhead of dispatching a dedicated command).", "For crash recovery they use a simple LFS style scheme.", "The actual implementation of the barrier command is not the main focus of the paper, it’s how the barrier command is used and the benefits it brings that the authors focus on.", "(“Developing a barrier-enabled SSD controller is an engineering exercise…”).", "With the barrier command in hand, it is possible to implement order-preserving dispatch:  Order-preserving dispatch is a fundamental innovation in this work.", "In order-preserving dispatch, the block device layer dispatches the following command immediately after it dispatches the preceding one and yet the host can ensure that that the two commands are serviced in order.", "We refer to this mechanism as wait-on-dispatch.", "Wait-on-dispatch eliminates the wait-on-transfer overhead.", "Using a barrier write request (i.e., a write request with both ORDERED and BARRIER attributes set), the existing command priority of the SCSI interface ensures that the barrier write is serviced only after the existing requests in the command queue are serviced and before any of the commands following the barrier write are serviced.", "The ordered priority command has rarely been used in existing block device implementations.", "This is because when the host cannot control the persist order, enforcing a transfer order with an ordered priority command barely carries any meaning from the perspective of ensuring the storage order.", "With the emergence of the barrier write, the ordered priority plays an essential role in making the entire IO stack an order-preserving one.", "Epoch-based scheduling (with epochs delineated by barrier writes)  ensures that:  the partial order between epochs is honoured  within an epoch, requests can be freely scheduled  orderless requests can be scheduled across epochs  Barrier-enabled filesystem (BFS)  The barrier-enabled IO stack adds two primitives, fbarrier() and fdatabarrier(), which synchronise the same set of blocks as fsync() and fdatasync() respectively, but return without ensuring the associated blocks become durable.", "I.e., they guarantee ordering but not durability.", "By interleaving write class with fdatabarrier an application can ensure that data blocks preceding the data barrier call are made durable ahead of data blocks that follow it.", "The authors modified ext4 to make it barrier enabled.", "Exploiting the order-preserving nature of the underlying block device, we physically separate the control plane activity (dispatching the write requests) from the data plane activity (persisting the associated data blocks and journal transaction) of a journal commit operation.", "Further, we allocate separate threads to each task so that the two activities can proceed in parallel with minimum dependency.", "The two threads are known as the commit thread  and the flush thread.", "We refer to this mechanism as Dual Mode Journaling.", "Evaluation  Section 6 in the paper contains evaluations of the block device layer, and the filesystem layer, but I’m going to jump straight to the evaluation of what it means for applications running on top.", "For server workloads, the authors test varmail, a metadata-intensive workload known for heavy fsync() traffic, and an OLTP-insert workload using MySQL.", "When guaranteeing full durability, BFS improves varmail performance by between 10% and 60% depending on the type of SSD used.", "When guaranteeing only ordering, BFS gives a 36x performance improvement over ext4.", "For the MySQL workload, BFS gives a 12% performance boost with full durability guarantees, or a 43x performance boost with ordering only.", "With SQLite BFS gives a 75% performance boost on mobile storage with full durability, and 2.8x with just ordering.", "With higher powered server-side controllers employing higher degrees of parallelism the BFS advantage goes up to 73x.", "The last word  It is time to design a new IO stack for Flash storage that is free from the unnecessary constraint inherited from the old legacy that a host cannot control the persistence order….", "the “cache barrier” command is a necessity rather than a luxury.", "It should be supported in all Flash storage products ranging from mobile storage to high-performance Flash storage with supercap."], "summary_text": "Barrier-enabled IO stack for flash storage Won et al., FAST’18  The performance of Flash storage has benefited greatly from concurrency and parallelism – for example, multi-channel controllers, large caches, and deep command queues. At the same time, the time to program an individual Flash cell has stayed fairly static (and even become slightly worse in some cases). That’s ok until an application needs to guarantee ordering and/or durability of writes. Enforcing a storage order is achieved by an extremely expensive approach: dispatching the following request only after the block associated with preceding request has been completely transferred to the storage device and made durable. We call this mechanism Transfer-and-Flush. With transfer-and-flush, the parallelism and concurrency benefits are lost. As a consequence, on a single channel mobile storage SSD for smartphones, ordered writes offer only 20% of the performance (IOPS) of unordered writes. For a 32-channel Flash array, this ratio decreases to just 1%. In today’s paper (which also won a best-paper award at FAST’18), the authors introduce a Barrier-enabled IO stack, which can enforce storage order between requests without the filesystem having to wait for a full transfer-and-flush. The headline performance gains certainly make you sit up and take notice:  SQLite performance increases by 270% and 75%, in server and in smartphone, respectively. In server storage BarrierFS brings performance gains of 43x and 73x in MySQL and SQLite respectively when compared to EXT4. There’s some important fine-print in the headline numbers – the biggest gains come when ordering is preserved, but we don’t wait for durability. The baseline being compared against offers both. Getting your ducks in order  The modern IO stack is said to be orderless…  Consider a set of writes issued in some order (issue order) by a file system:  The IO scheduler may reorder and coalesce IO requests according so some scheduling principle (e.g., CFQ), before placing them onto a dispatch queue to be dispatched to the storage device (dispatch order). The storage controller receives the incoming commands and places them in its command queue. It is free to schedule commands for transfer as it sees fit. Errors, time-outs, and retries further impact the eventual ordering (transfer completion order). The overall persistence order is governed not by the order in which data blocks are made durable, but by the order in which the associated mapping table entries are updated. The two may not coincide. The long standing assumption (a holdover from the physical characteristics of rotating disks) is that the host cannot control the persistence order. The constraint that the host cannot control the persist order is a fundamental limitation in modern IO stack design. To guarantee ordering in the face of this, an expensive transfer-on-flush mechanism is used. If a needs to be ordered before b, then after dispatching a to the storage device, the caller needs to wait for a to be serviced (the DMA transfer to complete) — wait-on-transfer. Then the caller can issue a flush command and wait for its completion (wait-on-flush). Only once the flush has returned can the caller dispatch b. When ext4 (in its default Ordered mode) commits a journal transaction it uses two write requests: one for writing a coalesced chunk of journal descriptor block and log blocks (JD), and one for writing the commit block (JC). JD needs to be made durable before JC. Across transactions, ext4 also has to ensure that transactions are made durable in order. If either of these two ordering constraints are violated, the file system may recover incorrectly in the case of an unexpected failure. Order-preserving block devices with barriers  The ‘cache barrier,’ or ‘barrier’ for short, command is defined in the standard command set for mobile Flash storage. With the barrier command, the host can control the persist order without explicitly invoking cache flush. When the storage controller receives the barrier command, it guarantees that the data blocks transferred before the barrier command become durable after the ones that follow the barrier command do. The authors implement the barrier command concept using a new attribute REQ_BARRIER that can be set on a regular request object (to avoid the overhead of dispatching a dedicated command). For crash recovery they use a simple LFS style scheme. The actual implementation of the barrier command is not the main focus of the paper, it’s how the barrier command is used and the benefits it brings that the authors focus on. (“Developing a barrier-enabled SSD controller is an engineering exercise…”). With the barrier command in hand, it is possible to implement order-preserving dispatch:  Order-preserving dispatch is a fundamental innovation in this work. In order-preserving dispatch, the block device layer dispatches the following command immediately after it dispatches the preceding one and yet the host can ensure that that the two commands are serviced in order. We refer to this mechanism as wait-on-dispatch. Wait-on-dispatch eliminates the wait-on-transfer overhead. Using a barrier write request (i.e., a write request with both ORDERED and BARRIER attributes set), the existing command priority of the SCSI interface ensures that the barrier write is serviced only after the existing requests in the command queue are serviced and before any of the commands following the barrier write are serviced. The ordered priority command has rarely been used in existing block device implementations. This is because when the host cannot control the persist order, enforcing a transfer order with an ordered priority command barely carries any meaning from the perspective of ensuring the storage order. With the emergence of the barrier write, the ordered priority plays an essential role in making the entire IO stack an order-preserving one. Epoch-based scheduling (with epochs delineated by barrier writes)  ensures that:  the partial order between epochs is honoured  within an epoch, requests can be freely scheduled  orderless requests can be scheduled across epochs  Barrier-enabled filesystem (BFS)  The barrier-enabled IO stack adds two primitives, fbarrier() and fdatabarrier(), which synchronise the same set of blocks as fsync() and fdatasync() respectively, but return without ensuring the associated blocks become durable. I.e., they guarantee ordering but not durability. By interleaving write class with fdatabarrier an application can ensure that data blocks preceding the data barrier call are made durable ahead of data blocks that follow it. The authors modified ext4 to make it barrier enabled. Exploiting the order-preserving nature of the underlying block device, we physically separate the control plane activity (dispatching the write requests) from the data plane activity (persisting the associated data blocks and journal transaction) of a journal commit operation. Further, we allocate separate threads to each task so that the two activities can proceed in parallel with minimum dependency. The two threads are known as the commit thread  and the flush thread. We refer to this mechanism as Dual Mode Journaling. Evaluation  Section 6 in the paper contains evaluations of the block device layer, and the filesystem layer, but I’m going to jump straight to the evaluation of what it means for applications running on top. For server workloads, the authors test varmail, a metadata-intensive workload known for heavy fsync() traffic, and an OLTP-insert workload using MySQL. When guaranteeing full durability, BFS improves varmail performance by between 10% and 60% depending on the type of SSD used. When guaranteeing only ordering, BFS gives a 36x performance improvement over ext4. For the MySQL workload, BFS gives a 12% performance boost with full durability guarantees, or a 43x performance boost with ordering only. With SQLite BFS gives a 75% performance boost on mobile storage with full durability, and 2.8x with just ordering. With higher powered server-side controllers employing higher degrees of parallelism the BFS advantage goes up to 73x. The last word  It is time to design a new IO stack for Flash storage that is free from the unnecessary constraint inherited from the old legacy that a host cannot control the persistence order…. the “cache barrier” command is a necessity rather than a luxury. It should be supported in all Flash storage products ranging from mobile storage to high-performance Flash storage with supercap.", "pdf_url": "https://www.usenix.org/system/files/conference/fast18/fast18-won.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/barrier-enabled-io-stack-for-flash-storage.json"}
{"id": "98757756", "bin": "1500_1600", "summary_sentences": ["Data Shapley: equitable valuation of data for machine learning Ghorbani & Zou et al., ICML’19  It’s incredibly difficult from afar to make sense of the almost 800 papers published at ICML this year !", "In practical terms I was reduced to looking at papers highlighted by others (e.g. via best paper awards), and scanning the list of paper titles looking for potentially interesting topics.", "For the next few days we’ll be looking at some of the papers that caught my eye during this process.", "The now somewhat tired phrase “data is the new oil” (something we can consume in great quantities to eventually destroy the world as we know it???)", "suggests that data has value.", "But pinning down that value can be tricky – how much is a given data point worth, and what framework can we use for thinking about that question?", "As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions….", "In this work we develop a principled framework to address data valuation in the context of supervised machine learning.", "One of the nice outcomes is that once you’ve understood what data has high value and what data has low value, you can use this insight to guide future data collection activities and improve your ROI on data gathering.", "It also turns out that examining data points with low values turns out to be a good way of discovering noisy and/or mislabelled data points.", "Removing these can improve your model’s performance.", "It’s most important to remember at all times when considering this work that we’re talking about the value of data in the context of training a specific model.", "We can’t give an objective valuation of a data point outside of that context, and a data point that is low value for one task may be high value for another.", "… we do not define a universal value for data.", "Instead, the value of each datum depend on the learning algorithm and the performance metric, as well as on other data in the training set.", "This dependency is reasonable and desirable in machine learning.", "How to judge the value of a data point  Assume we have some fixed training set  , a learning algorithm  , and a performance score function  , where  , which takes as input  a predictor trained on  and returns a performance score.", "Let  (sometimes written just as )  be a function that quantifies the value of the i-th datum.", "For an equitable data valuation function we want the following three properties to hold:  If a datapoint does not change the performance when it’s added to any subset of the training data, then it should be given zero value.", "If two different data points, when individually added to any subset of the training data always produce exactly the same change in the predictor’s score, then they should be given the same value by symmetry.", "When the overall prediction score is the sum of K separate predictions, the value of a datum should be the sum of its value for each prediction.", "These three conditions mirror the fairness conditions  from cooperative game-theory as defined by Shapley (in 1953), and constrain  to have the following form:  In game theory this is called the Shapley value, so here the authors call it the data Shapley value.", "Let’s unpick that equation a little:  Take all possible subsets from  that do not include  , the data point we wish to value  For each of these subsets, compute the incremental value that arises when  is added back in  Sum all of those value increments, and divide by the number of subsets to yield the average value increment for a subset of the training data when data point  is added to it.", "Eqn.", "1 could be interpreted as a weighted sum of all possible “marginal contributions” of  , where the weight is inverse the number of subsets of size  in  .", "This formulation is close to that of leave-one-out where instead of considering the last marginal contribution  , we consider each point’s marginal contribution assuming that instead of the whole training set, a random subset of it is given.", "The constant  is an arbitrary scaling constant and doesn’t affect any of the analysis.", "How to make it tractable  You might have spotted the challenge in computing a data Shapley value: it requires us to train and score a model (expensive) for every possible subset of the training data (exponentially many).", "And then of course we’d like to do this for all of the points in our data set (typically large).", "So that’s not going to work then.", "All is not lost.", "The authors introduce techniques for approximating data Shapley, tackling each of the three dimensions:  We can deal with the problem of exponentially many subsets by using Monte Carlo sampling: “in practice, we generate Monte Carlo estimates until the average has empirically converged.”  We can use early stopping (truncation) when calculating a data Shapley value for a sampled permutation  .", "Whenever  is within the performance tolerance (inherent noise) of  the marginal contribution is set to zero for the rest of the data points in the permutation.", "Putting these first two optimisations together gives Truncated Monte Carlo Shapley, TMC-Shapley.", "We can reduce the cost of training a model, where that model is trained using a variation of stochastic variant descent, but training the model for one epoch (one pass through the training data) only.", "This variant is called Gradient Shapley.", "We can reduce the number of data points we have to compute the data Shapley value for by instead computing the value for groups of points.", "“For example, in a heart disease prediction setting, we could group the patients into discrete bins based on age, gender, ethnicity and other features, and then quantify the data Shapley of each bin.“  Data Shapley in action  There are some really interesting results in the evaluation section, that demonstrate the power of understanding the value of your data points (as well as the efficacy of data Shapley in helping you to do that).", "Using the UK Biobank data set and a model for predicting breast and skin cancer, the authors calculated data values for 1000 data points in a training set.", "Now, if you take data points away from the training set, starting with the most valuable data points, and retrain the model, you can see that model performance drops (Fig 1a).", "( Enlarge )  … points that data Shapley considers valuable are crucially important for the model performance while leave-one-out valuation is only slightly better than random valuation (i.e., removing random points).", "If you go in the other direction, and start by removing the lowest valued data points, the model performance improves!", "Points with a low Shapley value harm the model’s performance and removing them improves accuracy (Fig 1b above).", "If we wanted to collect more data to improve, then adding data points similar to high value data points (assuming that’s something we can control in data gathering) leads to much better performance improvements than adding low value data points (Fig 1c and d above).", "Another experiment trained data with noisy (incorrect) labels and found that the erroneous data points ended up with low data values.", "Thus by inspecting data points from least valuable to most valuable it’s likely that you will find and be able to correct mislabeled examples.", "( Enlarge )  Deliberately adding noise to images reduces data values:  The last word  Data Shapley uniquely satisﬁes three natural properties of equitable data valuation.", "There are ML settings where these properties may not be desirable and perhaps other properties need to be added.", "It is a very important direction of future work to clearly understand these different scenarios and study the appropriate notions of data value.", "Drawing on the connections from economics, we believe the three properties we listed is a reasonable starting point.", "And an important reminder: “… we have skipped over many important considerations about the intrinsic value of personal data, and we focused on valuation in the very specific context of a training set for supervised learning algorithms.", "“"], "summary_text": "Data Shapley: equitable valuation of data for machine learning Ghorbani & Zou et al., ICML’19  It’s incredibly difficult from afar to make sense of the almost 800 papers published at ICML this year ! In practical terms I was reduced to looking at papers highlighted by others (e.g. via best paper awards), and scanning the list of paper titles looking for potentially interesting topics. For the next few days we’ll be looking at some of the papers that caught my eye during this process. The now somewhat tired phrase “data is the new oil” (something we can consume in great quantities to eventually destroy the world as we know it???) suggests that data has value. But pinning down that value can be tricky – how much is a given data point worth, and what framework can we use for thinking about that question? As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions…. In this work we develop a principled framework to address data valuation in the context of supervised machine learning. One of the nice outcomes is that once you’ve understood what data has high value and what data has low value, you can use this insight to guide future data collection activities and improve your ROI on data gathering. It also turns out that examining data points with low values turns out to be a good way of discovering noisy and/or mislabelled data points. Removing these can improve your model’s performance. It’s most important to remember at all times when considering this work that we’re talking about the value of data in the context of training a specific model. We can’t give an objective valuation of a data point outside of that context, and a data point that is low value for one task may be high value for another. … we do not define a universal value for data. Instead, the value of each datum depend on the learning algorithm and the performance metric, as well as on other data in the training set. This dependency is reasonable and desirable in machine learning. How to judge the value of a data point  Assume we have some fixed training set  , a learning algorithm  , and a performance score function  , where  , which takes as input  a predictor trained on  and returns a performance score. Let  (sometimes written just as )  be a function that quantifies the value of the i-th datum. For an equitable data valuation function we want the following three properties to hold:  If a datapoint does not change the performance when it’s added to any subset of the training data, then it should be given zero value. If two different data points, when individually added to any subset of the training data always produce exactly the same change in the predictor’s score, then they should be given the same value by symmetry. When the overall prediction score is the sum of K separate predictions, the value of a datum should be the sum of its value for each prediction. These three conditions mirror the fairness conditions  from cooperative game-theory as defined by Shapley (in 1953), and constrain  to have the following form:  In game theory this is called the Shapley value, so here the authors call it the data Shapley value. Let’s unpick that equation a little:  Take all possible subsets from  that do not include  , the data point we wish to value  For each of these subsets, compute the incremental value that arises when  is added back in  Sum all of those value increments, and divide by the number of subsets to yield the average value increment for a subset of the training data when data point  is added to it. Eqn. 1 could be interpreted as a weighted sum of all possible “marginal contributions” of  , where the weight is inverse the number of subsets of size  in  . This formulation is close to that of leave-one-out where instead of considering the last marginal contribution  , we consider each point’s marginal contribution assuming that instead of the whole training set, a random subset of it is given. The constant  is an arbitrary scaling constant and doesn’t affect any of the analysis. How to make it tractable  You might have spotted the challenge in computing a data Shapley value: it requires us to train and score a model (expensive) for every possible subset of the training data (exponentially many). And then of course we’d like to do this for all of the points in our data set (typically large). So that’s not going to work then. All is not lost. The authors introduce techniques for approximating data Shapley, tackling each of the three dimensions:  We can deal with the problem of exponentially many subsets by using Monte Carlo sampling: “in practice, we generate Monte Carlo estimates until the average has empirically converged.”  We can use early stopping (truncation) when calculating a data Shapley value for a sampled permutation  . Whenever  is within the performance tolerance (inherent noise) of  the marginal contribution is set to zero for the rest of the data points in the permutation. Putting these first two optimisations together gives Truncated Monte Carlo Shapley, TMC-Shapley. We can reduce the cost of training a model, where that model is trained using a variation of stochastic variant descent, but training the model for one epoch (one pass through the training data) only. This variant is called Gradient Shapley. We can reduce the number of data points we have to compute the data Shapley value for by instead computing the value for groups of points. “For example, in a heart disease prediction setting, we could group the patients into discrete bins based on age, gender, ethnicity and other features, and then quantify the data Shapley of each bin.“  Data Shapley in action  There are some really interesting results in the evaluation section, that demonstrate the power of understanding the value of your data points (as well as the efficacy of data Shapley in helping you to do that). Using the UK Biobank data set and a model for predicting breast and skin cancer, the authors calculated data values for 1000 data points in a training set. Now, if you take data points away from the training set, starting with the most valuable data points, and retrain the model, you can see that model performance drops (Fig 1a). ( Enlarge )  … points that data Shapley considers valuable are crucially important for the model performance while leave-one-out valuation is only slightly better than random valuation (i.e., removing random points). If you go in the other direction, and start by removing the lowest valued data points, the model performance improves! Points with a low Shapley value harm the model’s performance and removing them improves accuracy (Fig 1b above). If we wanted to collect more data to improve, then adding data points similar to high value data points (assuming that’s something we can control in data gathering) leads to much better performance improvements than adding low value data points (Fig 1c and d above). Another experiment trained data with noisy (incorrect) labels and found that the erroneous data points ended up with low data values. Thus by inspecting data points from least valuable to most valuable it’s likely that you will find and be able to correct mislabeled examples. ( Enlarge )  Deliberately adding noise to images reduces data values:  The last word  Data Shapley uniquely satisﬁes three natural properties of equitable data valuation. There are ML settings where these properties may not be desirable and perhaps other properties need to be added. It is a very important direction of future work to clearly understand these different scenarios and study the appropriate notions of data value. Drawing on the connections from economics, we believe the three properties we listed is a reasonable starting point. And an important reminder: “… we have skipped over many important considerations about the intrinsic value of personal data, and we focused on valuation in the very specific context of a training set for supervised learning algorithms. “", "pdf_url": "http://proceedings.mlr.press/v97/ghorbani19c/ghorbani19c.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/data-shapley.json"}
{"id": "90525296", "bin": "1500_1600", "summary_sentences": ["DeepCoder: Learning to write programs Balog et al., ICLR 2017  I’m mostly trying to wait until the ICLR conference itself before diving into the papers to be presented there, but this particular paper follows nicely on from yesterday, so I’ve decided to bring it forward.", "In ‘ Large scale evolution of image classifiers ‘ we saw how it’s possible to learn a model for an image classification problem instead of trying to hand-engineer it.", "(Which incidentally paves the way for bootstrapping sophisticated ML systems the same way that we might bootstrap a compiler).", "But that’s a fairly constrained domain, surely the average programmer’s job is safe – we can’t learn to write any old program.", "Or can we?", "Well ok, no we can’t, but DeepCoder can learn how to solve simple programming tasks of the kind found in the most basic problems on programming competition websites.", "And it’s only going to get better!", "Here’s an example:  This is the domain of Inductive Program Synthesis (IPS).", "DeepCoder is shown sets of inputs and outputs, and must learn a program that will produce the given outputs when presented with the given inputs (no memorization allowed!).", "Fundamentally this is a guided search across the space of all possible programs.", "To make that tractable we need to choose a language (a DSL) in which we’re going to express programs – using a general purpose language such as C++ yields far too big a search space.", "DeepCoder’s secret sauce is a neural network that is trained to predict the kinds of functions that might be useful when trying to recreate the outputs for a given set of inputs.", "Knowing the most likely functions the program will ultimately need to include guides the search and helps it find solutions much faster.", "The approach of integrating a learning module into an IPS system is called LIPS (Learning Inductive Program Synthesis).", "The DeepCoding DSL  DeepCoding programs are simple sequences of function calls, the result of each call initialising a fresh variable that holds either a single integer or an integer array.", "The output of the program is the return value of the last function call.", "The DSL contains the first-order functions head, last, take, drop, access, minimum, maximum, reverse, sort, and sum, and the higher-order functions map, filter, count, zipwith`, and `scanl.", "The pre-defined lambda functions which can be passed to these higher-order functions are:  for map: (+1), (-1), (*2), (/2)2 (*(-1))2 (**2), (*3), (/3), (*4), (/4)  for filter and count: (>0), (<0), (%2 == 0), (%2 == 1)  for zipwith and scanl: (+), (-), (*), min, max  There’s is no explicit looping, but of course many of the provided functions do provide branching and looping internally.", "The Learning Module  The learning module learns to predict the probability that each of the above functions is used in a given program, based on seeing an input-output pair.", "To take a simple example, given the input [3, 8, 1, 7] and the output [4, 12, 28, 32] the network should predict high probability for sort and (*4).", "The overall network structure looks like this:  With only three hidden layers, it’s actually a fairly simple structure compared to some of the models we looked at last week.", "First, we represent the input and output types (singleton or array) by a one-hot-encoding, and we pad the inputs and outputs to a maximum length L with a special NULL value.", "Second, each integer in the inputs and in the output is mapped to a learned embedding vector of size E = 20.", "(The range of integers is restricted to a finite range and each embedding is parametrized individually.)", "Third, for each input-output example separately, we concatenate the embeddings of the input types, the inputs, the output type, and the output into a single (fixed-length) vector, and pass this vector through H = 3 hidden layers containing K = 256 sigmoid units each.", "The third hidden layer thus provides an encoding of each individual input-output example.", "Finally, for input-output examples in a set generated from the same program, we pool these representations together by simple arithmetic averaging.", "To network outputs a log-unnormalised array of probabilities of each function appearing in the source code:  To generate training examples, DSL programs are enumerated, pruned to remove obvious issues such as redundant variables, or the existence of shorter equivalent programs (approximated by identical behaviour on a set of inputs).", "Valid inputs for a program are generated by putting a constraint on the output values and propagating this constraint backward through the program.", "Input-output pairs are then generated by picking inputs from the pre-computed valid ranges and executing the program to obtain the output values.", "Searching for Solutions  At this point we have a set of inputs and outputs, and some clues as to which functions are most likely to be included in a program that generated those outputs.", "We know use this information to guide a search.", "The team evaluate a few different search strategies:  Depth-first search (DFS) over programs of some maximum length T. When the search extends a partial program, it considers functions in probability order (highest probability first of course!).", "A sort and add scheme which maintains a set of active functions and performs DFS with the active function set only.", "When the search fails, the next most probable function (or set of functions) is added to the active set and the search restarts.", "Use of the Sketch SMT-based program synthesis tool, with a similar sort-and-add scheme used to guide its search  Use of the  program synthesis tool also using a sort-and-add scheme  Let’s generate some programs!", "The main experiment looked at programs of length T=5 ( a search space on the order of 1010, supported by a neural network trained on programs of length T=4.", "The table below shows the results when trying to find 100 programs (Sketch is dropped from this part of the evaluation as it is significantly slower than the other methods).", "The thing to remember when looking at the results is that there is no surprise that the search strategies can actually find successful programs.", "So what we’re really interested in is how long does it take to do so, and how much if any the learning component actually helps.", "In the table above, the percentage columns tell you how long it took each variation to solve 20%, 40%, and 60% of the program generation challenges out of the 100 presented.", "The baseline row shows how long the search takes when given a function probability distribution that simply mirrors the global incidence of each function in the 500 program test set corpus.", "Thus the DeepCoder row is truly telling us what difference the neural network prediction make to search time.", "We hypothesize that the substantially larger performance gains on Sort and add schemes as compared to gains on DFS can be explained by the fact that the choice of attribute function (predicting presence of functions anywhere in the program) and learning objective of the neural network are better matched to the Sort and add schemes.", "Further experiments varying the length of the test programs (from 1 to 4 functions), and the length of the generated programs (from 1 to 5 functions) showed that the neural network can generalize to programs of different lengths than the programs in the training set.", "Our empirical results show that for many programs, this technique [neural network guided search] improves the runtime of a wide range of IPS baselines by 1-3 orders.", "We have found several problems in real online programming challenges that can be solved with a program in our language, which validates the relevance of the class of problems that we have studied in this work.", "In sum, this suggests that we have made significant progress towards being able to solve programming competition problems, and the machine learning component plays an important role in making it tractable.", "Of course, as the authors themselves point out, “there remain some limitations…“."], "summary_text": "DeepCoder: Learning to write programs Balog et al., ICLR 2017  I’m mostly trying to wait until the ICLR conference itself before diving into the papers to be presented there, but this particular paper follows nicely on from yesterday, so I’ve decided to bring it forward. In ‘ Large scale evolution of image classifiers ‘ we saw how it’s possible to learn a model for an image classification problem instead of trying to hand-engineer it. (Which incidentally paves the way for bootstrapping sophisticated ML systems the same way that we might bootstrap a compiler). But that’s a fairly constrained domain, surely the average programmer’s job is safe – we can’t learn to write any old program. Or can we? Well ok, no we can’t, but DeepCoder can learn how to solve simple programming tasks of the kind found in the most basic problems on programming competition websites. And it’s only going to get better! Here’s an example:  This is the domain of Inductive Program Synthesis (IPS). DeepCoder is shown sets of inputs and outputs, and must learn a program that will produce the given outputs when presented with the given inputs (no memorization allowed!). Fundamentally this is a guided search across the space of all possible programs. To make that tractable we need to choose a language (a DSL) in which we’re going to express programs – using a general purpose language such as C++ yields far too big a search space. DeepCoder’s secret sauce is a neural network that is trained to predict the kinds of functions that might be useful when trying to recreate the outputs for a given set of inputs. Knowing the most likely functions the program will ultimately need to include guides the search and helps it find solutions much faster. The approach of integrating a learning module into an IPS system is called LIPS (Learning Inductive Program Synthesis). The DeepCoding DSL  DeepCoding programs are simple sequences of function calls, the result of each call initialising a fresh variable that holds either a single integer or an integer array. The output of the program is the return value of the last function call. The DSL contains the first-order functions head, last, take, drop, access, minimum, maximum, reverse, sort, and sum, and the higher-order functions map, filter, count, zipwith`, and `scanl. The pre-defined lambda functions which can be passed to these higher-order functions are:  for map: (+1), (-1), (*2), (/2)2 (*(-1))2 (**2), (*3), (/3), (*4), (/4)  for filter and count: (>0), (<0), (%2 == 0), (%2 == 1)  for zipwith and scanl: (+), (-), (*), min, max  There’s is no explicit looping, but of course many of the provided functions do provide branching and looping internally. The Learning Module  The learning module learns to predict the probability that each of the above functions is used in a given program, based on seeing an input-output pair. To take a simple example, given the input [3, 8, 1, 7] and the output [4, 12, 28, 32] the network should predict high probability for sort and (*4). The overall network structure looks like this:  With only three hidden layers, it’s actually a fairly simple structure compared to some of the models we looked at last week. First, we represent the input and output types (singleton or array) by a one-hot-encoding, and we pad the inputs and outputs to a maximum length L with a special NULL value. Second, each integer in the inputs and in the output is mapped to a learned embedding vector of size E = 20. (The range of integers is restricted to a finite range and each embedding is parametrized individually.) Third, for each input-output example separately, we concatenate the embeddings of the input types, the inputs, the output type, and the output into a single (fixed-length) vector, and pass this vector through H = 3 hidden layers containing K = 256 sigmoid units each. The third hidden layer thus provides an encoding of each individual input-output example. Finally, for input-output examples in a set generated from the same program, we pool these representations together by simple arithmetic averaging. To network outputs a log-unnormalised array of probabilities of each function appearing in the source code:  To generate training examples, DSL programs are enumerated, pruned to remove obvious issues such as redundant variables, or the existence of shorter equivalent programs (approximated by identical behaviour on a set of inputs). Valid inputs for a program are generated by putting a constraint on the output values and propagating this constraint backward through the program. Input-output pairs are then generated by picking inputs from the pre-computed valid ranges and executing the program to obtain the output values. Searching for Solutions  At this point we have a set of inputs and outputs, and some clues as to which functions are most likely to be included in a program that generated those outputs. We know use this information to guide a search. The team evaluate a few different search strategies:  Depth-first search (DFS) over programs of some maximum length T. When the search extends a partial program, it considers functions in probability order (highest probability first of course!). A sort and add scheme which maintains a set of active functions and performs DFS with the active function set only. When the search fails, the next most probable function (or set of functions) is added to the active set and the search restarts. Use of the Sketch SMT-based program synthesis tool, with a similar sort-and-add scheme used to guide its search  Use of the  program synthesis tool also using a sort-and-add scheme  Let’s generate some programs! The main experiment looked at programs of length T=5 ( a search space on the order of 1010, supported by a neural network trained on programs of length T=4. The table below shows the results when trying to find 100 programs (Sketch is dropped from this part of the evaluation as it is significantly slower than the other methods). The thing to remember when looking at the results is that there is no surprise that the search strategies can actually find successful programs. So what we’re really interested in is how long does it take to do so, and how much if any the learning component actually helps. In the table above, the percentage columns tell you how long it took each variation to solve 20%, 40%, and 60% of the program generation challenges out of the 100 presented. The baseline row shows how long the search takes when given a function probability distribution that simply mirrors the global incidence of each function in the 500 program test set corpus. Thus the DeepCoder row is truly telling us what difference the neural network prediction make to search time. We hypothesize that the substantially larger performance gains on Sort and add schemes as compared to gains on DFS can be explained by the fact that the choice of attribute function (predicting presence of functions anywhere in the program) and learning objective of the neural network are better matched to the Sort and add schemes. Further experiments varying the length of the test programs (from 1 to 4 functions), and the length of the generated programs (from 1 to 5 functions) showed that the neural network can generalize to programs of different lengths than the programs in the training set. Our empirical results show that for many programs, this technique [neural network guided search] improves the runtime of a wide range of IPS baselines by 1-3 orders. We have found several problems in real online programming challenges that can be solved with a program in our language, which validates the relevance of the class of problems that we have studied in this work. In sum, this suggests that we have made significant progress towards being able to solve programming competition problems, and the machine learning component plays an important role in making it tractable. Of course, as the authors themselves point out, “there remain some limitations…“.", "pdf_url": "https://arxiv.org/pdf/1611.01989.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/deepcoder-learning-to-write-programs.json"}
{"id": "35377831", "bin": "1500_1600", "summary_sentences": ["Why does the neocortex have columns, a theory of learning the structure of the world Hawkins et al., bioRxiv preprint, 2017  Yesterday we looked at the ability of the HTM sequence memory model to learn sequences over time, with a model that resembles what happens in a single layer of the neocortex.", "But the neocortex has six layers.", "Today’s paper builds on the previous work to show how pairs of layers can learn predictive models of static objects, when the sensory input changes due to our own movement.", "For example, when our fingers touch an object.", "Our research has focused on how the brain makes predictions of sensory inputs.", "Starting with the premise that all sensory regions make predictions of their constantly changing input, we deduced that each small area in a sensory region must have access to a location signal that represents where on an object the column is sensing.", "Building on this idea, we deduced the probable function of several cellular layers and are beginning to understand what cortical columns in their entirety might be doing.", "Anatomical evidence  A few general rules have been observed for cellular layers in the neocortex:  Cells in layers that receive direct feedforward input don’t send their axons outside the local region, and don’t form long distance horizontal connections within their own layer.", "Cells in layers driven by input layers do form long range connections within their layer, and also send an axonal branch outside of the region, representing an output.", "The two layer input-output circuit thus formed appears between layer 4 and layer 2/3 of the six layers in the neocortex.", "Layers 6 and 5 may be a second instance of the pattern.", "The prevalence of this two-layer connection motif suggests it plays an essential role in cortical processing.", "A key component of our theory is the presence in each column of a signal representing location.", "The location signal represents an “allocentric” location, meaning it is a location relative to the object being sensed.", "The neurons effectively have to compute a predicted new location from the combination of current location, object orientation, and movement.", "That sounds a tall order, but we already know that grid cells in the entorhinal cortex perform these types of transformations, encoding the location of an animal’s body relative to an external environment.", "These analogs, plus the fact that grid cells are phylogenetically older than the neocortex, lead us to hypothesize that the cellular mechanism used by grid cells were preserved and replicated in the sub-granular layers of each cortical column.", "Enough of the biology, let’s now turn to the model it inspired.", "Multi-layer model  The current model consists of two layers of pyramidal neurons arranged in a column.", "The model has one or more of these columns.", "Each cortical column processes a subset of the sensory input space and is exposed to different parts of the world as the sensors move.", "The neurons used in the model are HTM model neurons as we looked at yesterday.", "Input layer  The input layer of each column consists of HTM neurons arranged in mini-columns.", "It receives a sensory input as a feedforward input, and a location input as a basal modulatory input.", "The sensory input is a sparse binary array representing the current feature in input space.", "During inference, cells that recognize both the modulatory location input and the feedforward driving input will inhibit other cells in the mini-column.", "In this way, the input layer forms a sparse representation that is unique for a specific sensory feature at a specific location on the object.", "Neurons in the input layer also receive feedback connections from the output layer.", "These carry information about the object detected, which combined with modularity input representing the anticipated new location, allow the input layer to more precisely predict the next sensory input.", "Output layer  The output layer is also made up of HTM neurons.", "The set of cells that are active in the output layer represent objects.", "Output layer cells receive feedforward input from the input layer, and modulatory input from other output cells representing the same object, both within the column and also from neighbouring columns.", "During learning, the set of cells representing an object remains active over multiple movements and learns to recognize successive patterns in the input layer.", "Thus, an object comprises a representation in the output layer, plus an associated set of feature/location representations in the input layer.", "Cells representing the same object positively bias each other.", "Say at time t, a column has feedforward support for objects A and B.", "And at time t+1 it has feedforward support for objects B and C. Due to the modulatory input from time t, the output layer will converge on the representation for B.", "Example: cubes and wedges  The following figure shows two layers of a single cortical column collaborating to disambiguate between a cube and a wedge shape that have shared features.", "The first sensed feature f1 is ambiguous so the output layer supports both object patterns, but with repeated sensations the output layer quickly narrows down on the correct choice (the cube in this case).", "( Enlarge )  Learning  Learning is based on Hebbian-style adaptation as we saw yesterday.", "The input layer learns specific feature/location combinations, and if the current combination has not yet been learned, then one cell from each mini-column (the one with the best modulatory input match) is chosen as the winner and becomes active.", "Winner cells learn by forming and strengthening modulatory connections with the current input location.", "The output layer learns representations that correspond to objects:  When the network first encounters a new object, a sparse set of cells in the output layer is chosen to represent the new object.", "These cells remain active while the system senses the object at different locations.", "Feed forward connections between the changing active cells in the input layer and unchanging active cells in the output layer are continuously reinforced.", "Simulation results  Networks are constructed with one or more two-layer cortical columns.", "In each column the input layer comprises 150 mini-columns, 16 cells tall each.", "The output layer consists of 4096 cells, which are not arranged in mini-columns.", "One-column and three-column variations of the network are trained on a library of 500 objects.", "As can be seen below, both variations converge on a single object representation over time, but the three-column version gets there faster.", "The capacity of the network is defined as the number of objects it can learn and recognise without confusion.", "It is influenced by the representational space of the network, the number of mini-columns in the input layer, the number of neurons  in the output layer, and the number of cortical columns.", "150 mini-columns with 16 cells per mini-column, and 10 simultaneously active mini-columns, turns out to be enough to uniquely represent about 10^15 sensory features, each represented at 16^10 unique locations.", "As the number of learned objects increases, neurons in the output layer form increasing numbers of connections to neurons in the input layer.", "If an output neuron connects to too many input neurons, it may be falsely activated by a pattern it was not trained on.", "Therefore, the capacity of the network is limited by the pooling capacity of the output layer.", "Mathematical analysis suggests that a single cortical column can store hundreds of objects before reaching this limit.", "Figure 5 below explores the various dimensions of network capacity.", "The network shows no drop in recognition accuracy with up to 20% noise in the sensory input, and 40% noise in the location input, though it does take longer to converge.", "Mountcastle’s conjecture  In 1978 Mountcastle postulated that since the complex anatomy of cortical columns is similar in all of the neocortex, then all areas of the neocortex must be performing a similar function…  The model of a cortical column presented in this paper is described in terms of sensory regions and sensory processing, but the circuitry underlying our model exists in all cortical regions.", "Thus if Mountcastle’s conjecture is correct, even high-level cognitive functions, such as mathematics, language, and science would be implemented in this framework.", "It suggests that event abstract knowledge is stored in relation to some form of “location” and that much of what we consider to be “thought” is implemented by inference and behavior generating mechanisms originally evolved to move and infer with fingers and eyes."], "summary_text": "Why does the neocortex have columns, a theory of learning the structure of the world Hawkins et al., bioRxiv preprint, 2017  Yesterday we looked at the ability of the HTM sequence memory model to learn sequences over time, with a model that resembles what happens in a single layer of the neocortex. But the neocortex has six layers. Today’s paper builds on the previous work to show how pairs of layers can learn predictive models of static objects, when the sensory input changes due to our own movement. For example, when our fingers touch an object. Our research has focused on how the brain makes predictions of sensory inputs. Starting with the premise that all sensory regions make predictions of their constantly changing input, we deduced that each small area in a sensory region must have access to a location signal that represents where on an object the column is sensing. Building on this idea, we deduced the probable function of several cellular layers and are beginning to understand what cortical columns in their entirety might be doing. Anatomical evidence  A few general rules have been observed for cellular layers in the neocortex:  Cells in layers that receive direct feedforward input don’t send their axons outside the local region, and don’t form long distance horizontal connections within their own layer. Cells in layers driven by input layers do form long range connections within their layer, and also send an axonal branch outside of the region, representing an output. The two layer input-output circuit thus formed appears between layer 4 and layer 2/3 of the six layers in the neocortex. Layers 6 and 5 may be a second instance of the pattern. The prevalence of this two-layer connection motif suggests it plays an essential role in cortical processing. A key component of our theory is the presence in each column of a signal representing location. The location signal represents an “allocentric” location, meaning it is a location relative to the object being sensed. The neurons effectively have to compute a predicted new location from the combination of current location, object orientation, and movement. That sounds a tall order, but we already know that grid cells in the entorhinal cortex perform these types of transformations, encoding the location of an animal’s body relative to an external environment. These analogs, plus the fact that grid cells are phylogenetically older than the neocortex, lead us to hypothesize that the cellular mechanism used by grid cells were preserved and replicated in the sub-granular layers of each cortical column. Enough of the biology, let’s now turn to the model it inspired. Multi-layer model  The current model consists of two layers of pyramidal neurons arranged in a column. The model has one or more of these columns. Each cortical column processes a subset of the sensory input space and is exposed to different parts of the world as the sensors move. The neurons used in the model are HTM model neurons as we looked at yesterday. Input layer  The input layer of each column consists of HTM neurons arranged in mini-columns. It receives a sensory input as a feedforward input, and a location input as a basal modulatory input. The sensory input is a sparse binary array representing the current feature in input space. During inference, cells that recognize both the modulatory location input and the feedforward driving input will inhibit other cells in the mini-column. In this way, the input layer forms a sparse representation that is unique for a specific sensory feature at a specific location on the object. Neurons in the input layer also receive feedback connections from the output layer. These carry information about the object detected, which combined with modularity input representing the anticipated new location, allow the input layer to more precisely predict the next sensory input. Output layer  The output layer is also made up of HTM neurons. The set of cells that are active in the output layer represent objects. Output layer cells receive feedforward input from the input layer, and modulatory input from other output cells representing the same object, both within the column and also from neighbouring columns. During learning, the set of cells representing an object remains active over multiple movements and learns to recognize successive patterns in the input layer. Thus, an object comprises a representation in the output layer, plus an associated set of feature/location representations in the input layer. Cells representing the same object positively bias each other. Say at time t, a column has feedforward support for objects A and B. And at time t+1 it has feedforward support for objects B and C. Due to the modulatory input from time t, the output layer will converge on the representation for B. Example: cubes and wedges  The following figure shows two layers of a single cortical column collaborating to disambiguate between a cube and a wedge shape that have shared features. The first sensed feature f1 is ambiguous so the output layer supports both object patterns, but with repeated sensations the output layer quickly narrows down on the correct choice (the cube in this case). ( Enlarge )  Learning  Learning is based on Hebbian-style adaptation as we saw yesterday. The input layer learns specific feature/location combinations, and if the current combination has not yet been learned, then one cell from each mini-column (the one with the best modulatory input match) is chosen as the winner and becomes active. Winner cells learn by forming and strengthening modulatory connections with the current input location. The output layer learns representations that correspond to objects:  When the network first encounters a new object, a sparse set of cells in the output layer is chosen to represent the new object. These cells remain active while the system senses the object at different locations. Feed forward connections between the changing active cells in the input layer and unchanging active cells in the output layer are continuously reinforced. Simulation results  Networks are constructed with one or more two-layer cortical columns. In each column the input layer comprises 150 mini-columns, 16 cells tall each. The output layer consists of 4096 cells, which are not arranged in mini-columns. One-column and three-column variations of the network are trained on a library of 500 objects. As can be seen below, both variations converge on a single object representation over time, but the three-column version gets there faster. The capacity of the network is defined as the number of objects it can learn and recognise without confusion. It is influenced by the representational space of the network, the number of mini-columns in the input layer, the number of neurons  in the output layer, and the number of cortical columns. 150 mini-columns with 16 cells per mini-column, and 10 simultaneously active mini-columns, turns out to be enough to uniquely represent about 10^15 sensory features, each represented at 16^10 unique locations. As the number of learned objects increases, neurons in the output layer form increasing numbers of connections to neurons in the input layer. If an output neuron connects to too many input neurons, it may be falsely activated by a pattern it was not trained on. Therefore, the capacity of the network is limited by the pooling capacity of the output layer. Mathematical analysis suggests that a single cortical column can store hundreds of objects before reaching this limit. Figure 5 below explores the various dimensions of network capacity. The network shows no drop in recognition accuracy with up to 20% noise in the sensory input, and 40% noise in the location input, though it does take longer to converge. Mountcastle’s conjecture  In 1978 Mountcastle postulated that since the complex anatomy of cortical columns is similar in all of the neocortex, then all areas of the neocortex must be performing a similar function…  The model of a cortical column presented in this paper is described in terms of sensory regions and sensory processing, but the circuitry underlying our model exists in all cortical regions. Thus if Mountcastle’s conjecture is correct, even high-level cognitive functions, such as mathematics, language, and science would be implemented in this framework. It suggests that event abstract knowledge is stored in relation to some form of “location” and that much of what we consider to be “thought” is implemented by inference and behavior generating mechanisms originally evolved to move and infer with fingers and eyes.", "pdf_url": "https://www.frontiersin.org/articles/10.3389/fncir.2017.00081/full", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/why-does-the-neocortex-have-columns-a-theory-of-learning-the-structure-of-the-world.json"}
{"id": "61298922", "bin": "1500_1600", "summary_sentences": ["Grand Pwning Unit: Accelerating microarchitectural attacks with the GPU Frigo et al., IEEE Security & Privacy  The general awareness of microarchitectural attacks is greatly increased since meltdown and spectre earlier this year.", "A lot of time and energy has been spent in defending against such attacks, with a threat model that assumes attacks originate from the CPU.", "Frigo et al. open up an entirely new can of worms – modern SoCs contain a variety of special purpose accelerator units, chief among which is the GPU.", "GPUs are everywhere these days.", "Unfortunately, the inclusion of these special-purpose units in the processor today appears to be guided by a basic security model that mainly governs access control, while entirely ignoring the threat of more advanced microarchitectural attacks.", "I’m sure you know where this is heading…  It turns out the accelerators can also be used to “accelerate” microarchitectural attacks.", "Once more we find ourselves in a situation with widespread vulnerabilities.", "The demonstration target in the paper is a mobile phone running on the ARM platform, with all known defences, including any applicable advanced research defences, employed.", "Using WebGL from JavaScript, Frigo et al. show how to go from e.g.", "an advert on a web page to a fully compromised browser in under two minutes.", "Our end-to-end attack, named GLitch, uses all these GPU primitives in orchestration to reliably compromise the browser on a mobile device using only microarchitectural attacks in under two minutes.", "In comparison, even on PCs, all previous Rowhammer attacks from JavaScript require not default configurations (such as reduced DRAMh refresh rates or huge pages) and often take such a long time that some researchers have questioned their practicality.", "If only I could flip a bit…  In Firefox, values stored in JavaScript ArrayObjects are 64 bits.", "The first 32 bits are used as a tag identifying the type of the object.", "When the tag value is below 0xffffff80 whole 64-bit work is considered as an IEEE-754 double, otherwise the last 32 bits are considered as a pointer to an object.", "(This strategy is known as NaN-boxing, encoding object pointers in IEEE-754 doubles as NaN values).", "So… if only we could flip bits within the first 25 bits of the tag, we could turn pointers into doubles, and vice-versa.", "The goal of the [GLitch] exploit is to obtain an arbitrary read/write primitive which can eventually lead to remote code execution.", "ArrayBuffers are the best fit to gain such a primitive since they provide the attacker with full control over their content.", "As a consequence, we want to create a reference to a fake ArrayBuffer whose data pointer we control.", "The GLitch exploit tool starts by storing a pointer to an inlined ArrayBuffer (header adjacent to data) in 1-to-0 bit-flip vulnerable location.", "Triggering a bit flip then turns this into a double that can be read, breaking ASLR (address space layout randomisation).", "Store a double in a 0-to-1 vulnerable cell in the ArrayBuffer, constructed in such a way that when a bit is flipped it becomes a pointer to a JSString, in turn pointing at the header (address obtained in step 1) for its immutable data.", "Read the value of the string to extract the content of the ArrayBuffer’s header.", "Create a header for a fake ArrayBuffer (using the header information obtained in step 2) within the leaked ArrayBuffer and craft a reference to it using the same double-to-pointer bit flip technique that we used for the JSString.", "Now we have the desired arbitrary read/write primitive.", "Here’s how long it takes for Glitch to break ASLR and compromise the browser on a Nexus 5:  On average, GLitch can break ASLR in only 27 seconds and fully compromise the browser remotely in 116s, making it the fastest known remote Rowhammer attack.", "So how does the GPU help us to flip bits (or just steal data in general using side-channel attacks)?", "Four attack primitives  We need to be able to either leak data (side-channel attacks) or corrupt data (e.g. Rowhammer attacks).", "A primary mechanism for leaking data using microarchitectural attacks is to time operations over resources shared with a victim process.", "The first attack primitive then is access to a high-resolution timer.", "There has been a bit of an arms race in CPU land with clever news ways of creating timers being devised and then blocked as best as possible.", "But the defences don’t take into account GPUs.", "There are two explicit timer sources within the OpenGL / WebGL world that will do the job, available when the EXT_DISJOINT_TIMER_QUERY extension is present.", "Both GPU and CPU operations can be timed directly using the primitives it provides.", "It’s also possible to craft your own timers using only standard (i.e., always available) WebGL2 functions: clientWaitSync and getSyncParameter.", "WebGL2 itself is not yet as widely supported as WebGL1 though.", "…in order to comply with the WebGL2 specification none of these functions can be disable.", "Also, due to the synchronous nature of these timers, we can use them to measure both CPU and GPU operations.", "Here we can see for example clear timing differences between cached and uncached data using the EXT_DISJOINT_TIMER_QUERY extension:  A second attack primitive is having access to resources shared with other process.", "By figuring out the caching structure within their GPU (Adreno 330), the authors were able to figure out the sequence of operations needed to effectively bypass the GPU caches and measure memory page accesses for memory pages shared with the rest of the system.", "Internally the GPU has two levels of caching, and two ways of accessing memory (by inputting vertices to vertex shaders, or by fetching textures within shaders).", "Texture fetching turned out to be the easiest to control, and section IV.B of the paper describes in detail how the authors deduced an efficient strategy to evict cache sets from the GPU.", "The third attack primitive is knowledge of the physical location of allocated memory addresses: a requirement in order to understand which rows to hammer in a rowhammer atttack.", "When a row of memory is accessed we can tell if it was already in the row buffer or not by measuring the time the operation takes (buffer hits are faster).", "To carry out a reliable Rowhammer attack, three adjacent rows within a DRAM bank are required.", "Distinguishing between row buffer hits and misses enables us to determine whether allocations are contiguous or non-contiguous.", "(Details are in section VII.D, and the appendix covers the relationship between adjacency and contiguity).", "The fourth and final attack primitive is fast memory access needed to trigger bit flips with Rowhammer attacks.", "Using the knowledge of the GPU cache hierarchy gained via probing the authors derive efficient access patterns to perform double-sided Rowhammering attacks…  Rowhammering  DRAM rows are composed of cells which store the value of a bit in a capacitor.", "The charge of a capacitor is transient, and therefore DRAM needs to be recharged within a precise interval (usually 64ms).", "Rowhammer is a software-based fault injection attack that can be considered a fallout of this DRAM property.", "By frequently activating specific rows an attacker can influence the charge in the capacitors of adjacent rows, making it possible to induce bit flips in a victim row without having access to its data.", "In a double-sided Rowhammer attack quick accesses to rows n-1 and n+1, impose high pressure on the capacitors in victim row n, triggering bit flips.", "“… our novel GPU-based side-channel attack provides us with information about contiguous physical memory regions in JavaScript, allowing us to perform double-sided Rowhammer on ARM devices in the browser.”  Mitigations  You could combine these primitives in a number of imaginative ways to construct different attacks, GLitch is but one end-to-end example.", "Eliminating known timers (e.g., disabling the EXT_DISJOINT_TIMER_QUERY) is still the best line of defence (though more timing strategies will likely be discovered).", "The WebGL2 getSyncParameter function can be disabled, and the clientWaitSync function could be replaced by a callback design.", "(This requires changes to the WebGL2 spec.).", "Stricter policies for memory reuse may also make it harder for an attacker to hammer valuable data.", "We showed that it is possible to perform advanced microarchitectural attacks directly from integrated GPUs found in almost all mobile devices… more alarming, these attacks can be launched from the browser.", "For example, we showed for the first time that with microarchitectural attacks from the GPU, an attacker can fully compromise a browser running on a mobile phone in less than 2 minutes… we hope our efforts make processor vendors more careful when embedding the next specialized unit into our commodity processors."], "summary_text": "Grand Pwning Unit: Accelerating microarchitectural attacks with the GPU Frigo et al., IEEE Security & Privacy  The general awareness of microarchitectural attacks is greatly increased since meltdown and spectre earlier this year. A lot of time and energy has been spent in defending against such attacks, with a threat model that assumes attacks originate from the CPU. Frigo et al. open up an entirely new can of worms – modern SoCs contain a variety of special purpose accelerator units, chief among which is the GPU. GPUs are everywhere these days. Unfortunately, the inclusion of these special-purpose units in the processor today appears to be guided by a basic security model that mainly governs access control, while entirely ignoring the threat of more advanced microarchitectural attacks. I’m sure you know where this is heading…  It turns out the accelerators can also be used to “accelerate” microarchitectural attacks. Once more we find ourselves in a situation with widespread vulnerabilities. The demonstration target in the paper is a mobile phone running on the ARM platform, with all known defences, including any applicable advanced research defences, employed. Using WebGL from JavaScript, Frigo et al. show how to go from e.g. an advert on a web page to a fully compromised browser in under two minutes. Our end-to-end attack, named GLitch, uses all these GPU primitives in orchestration to reliably compromise the browser on a mobile device using only microarchitectural attacks in under two minutes. In comparison, even on PCs, all previous Rowhammer attacks from JavaScript require not default configurations (such as reduced DRAMh refresh rates or huge pages) and often take such a long time that some researchers have questioned their practicality. If only I could flip a bit…  In Firefox, values stored in JavaScript ArrayObjects are 64 bits. The first 32 bits are used as a tag identifying the type of the object. When the tag value is below 0xffffff80 whole 64-bit work is considered as an IEEE-754 double, otherwise the last 32 bits are considered as a pointer to an object. (This strategy is known as NaN-boxing, encoding object pointers in IEEE-754 doubles as NaN values). So… if only we could flip bits within the first 25 bits of the tag, we could turn pointers into doubles, and vice-versa. The goal of the [GLitch] exploit is to obtain an arbitrary read/write primitive which can eventually lead to remote code execution. ArrayBuffers are the best fit to gain such a primitive since they provide the attacker with full control over their content. As a consequence, we want to create a reference to a fake ArrayBuffer whose data pointer we control. The GLitch exploit tool starts by storing a pointer to an inlined ArrayBuffer (header adjacent to data) in 1-to-0 bit-flip vulnerable location. Triggering a bit flip then turns this into a double that can be read, breaking ASLR (address space layout randomisation). Store a double in a 0-to-1 vulnerable cell in the ArrayBuffer, constructed in such a way that when a bit is flipped it becomes a pointer to a JSString, in turn pointing at the header (address obtained in step 1) for its immutable data. Read the value of the string to extract the content of the ArrayBuffer’s header. Create a header for a fake ArrayBuffer (using the header information obtained in step 2) within the leaked ArrayBuffer and craft a reference to it using the same double-to-pointer bit flip technique that we used for the JSString. Now we have the desired arbitrary read/write primitive. Here’s how long it takes for Glitch to break ASLR and compromise the browser on a Nexus 5:  On average, GLitch can break ASLR in only 27 seconds and fully compromise the browser remotely in 116s, making it the fastest known remote Rowhammer attack. So how does the GPU help us to flip bits (or just steal data in general using side-channel attacks)? Four attack primitives  We need to be able to either leak data (side-channel attacks) or corrupt data (e.g. Rowhammer attacks). A primary mechanism for leaking data using microarchitectural attacks is to time operations over resources shared with a victim process. The first attack primitive then is access to a high-resolution timer. There has been a bit of an arms race in CPU land with clever news ways of creating timers being devised and then blocked as best as possible. But the defences don’t take into account GPUs. There are two explicit timer sources within the OpenGL / WebGL world that will do the job, available when the EXT_DISJOINT_TIMER_QUERY extension is present. Both GPU and CPU operations can be timed directly using the primitives it provides. It’s also possible to craft your own timers using only standard (i.e., always available) WebGL2 functions: clientWaitSync and getSyncParameter. WebGL2 itself is not yet as widely supported as WebGL1 though. …in order to comply with the WebGL2 specification none of these functions can be disable. Also, due to the synchronous nature of these timers, we can use them to measure both CPU and GPU operations. Here we can see for example clear timing differences between cached and uncached data using the EXT_DISJOINT_TIMER_QUERY extension:  A second attack primitive is having access to resources shared with other process. By figuring out the caching structure within their GPU (Adreno 330), the authors were able to figure out the sequence of operations needed to effectively bypass the GPU caches and measure memory page accesses for memory pages shared with the rest of the system. Internally the GPU has two levels of caching, and two ways of accessing memory (by inputting vertices to vertex shaders, or by fetching textures within shaders). Texture fetching turned out to be the easiest to control, and section IV.B of the paper describes in detail how the authors deduced an efficient strategy to evict cache sets from the GPU. The third attack primitive is knowledge of the physical location of allocated memory addresses: a requirement in order to understand which rows to hammer in a rowhammer atttack. When a row of memory is accessed we can tell if it was already in the row buffer or not by measuring the time the operation takes (buffer hits are faster). To carry out a reliable Rowhammer attack, three adjacent rows within a DRAM bank are required. Distinguishing between row buffer hits and misses enables us to determine whether allocations are contiguous or non-contiguous. (Details are in section VII.D, and the appendix covers the relationship between adjacency and contiguity). The fourth and final attack primitive is fast memory access needed to trigger bit flips with Rowhammer attacks. Using the knowledge of the GPU cache hierarchy gained via probing the authors derive efficient access patterns to perform double-sided Rowhammering attacks…  Rowhammering  DRAM rows are composed of cells which store the value of a bit in a capacitor. The charge of a capacitor is transient, and therefore DRAM needs to be recharged within a precise interval (usually 64ms). Rowhammer is a software-based fault injection attack that can be considered a fallout of this DRAM property. By frequently activating specific rows an attacker can influence the charge in the capacitors of adjacent rows, making it possible to induce bit flips in a victim row without having access to its data. In a double-sided Rowhammer attack quick accesses to rows n-1 and n+1, impose high pressure on the capacitors in victim row n, triggering bit flips. “… our novel GPU-based side-channel attack provides us with information about contiguous physical memory regions in JavaScript, allowing us to perform double-sided Rowhammer on ARM devices in the browser.”  Mitigations  You could combine these primitives in a number of imaginative ways to construct different attacks, GLitch is but one end-to-end example. Eliminating known timers (e.g., disabling the EXT_DISJOINT_TIMER_QUERY) is still the best line of defence (though more timing strategies will likely be discovered). The WebGL2 getSyncParameter function can be disabled, and the clientWaitSync function could be replaced by a callback design. (This requires changes to the WebGL2 spec.). Stricter policies for memory reuse may also make it harder for an attacker to hammer valuable data. We showed that it is possible to perform advanced microarchitectural attacks directly from integrated GPUs found in almost all mobile devices… more alarming, these attacks can be launched from the browser. For example, we showed for the first time that with microarchitectural attacks from the GPU, an attacker can fully compromise a browser running on a mobile phone in less than 2 minutes… we hope our efforts make processor vendors more careful when embedding the next specialized unit into our commodity processors.", "pdf_url": "https://www.vusec.net/wp-content/uploads/2018/05/glitch.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/grand-pwning-unit-accelerating-microarchitectural-attacks-with-the-gpu.json"}
{"id": "93195231", "bin": "1500_1600", "summary_sentences": ["A pressing issue in machine learning (ML) research is to examine whether automated algorithms discriminate against gender or race when making decisions.", "In a recent study, Kiritchenko and Mohammad conducted an extensive analysis to determine whether hundreds of sentiment analysis systems accentuate inappropriate human biases and how these are manifested.", "Motivation  Automatic systems are beneficial to society but as they improve in predictive performance, comparable to human capabilities, they could perpetuate inappropriate human biases.", "In the context of sentiment analysis (SA), these biases may come in many forms.", "For instance, an SA system may consider the messages conveyed by a specific gender or race to be less positive simply because of their race and gender.", "Therefore, even when it is not clear how biases are manifested, it is the duty and responsibility of the natural language processing (NLP) system developer to address this problem.", "Contributions  In NLP, the sources of the bias may derive from the training data or language resources such as lexicons and word embeddings that a machine learning algorithm is designed to leverage for building the predictive model.", "There have been many works in computer vision and NLP that aim to detect these inappropriate biases, but few have focused on a large set of systems that aim to solve the same task, which is very common in the field of machine learning.", "The authors collected and proposed a benchmark dataset for examining inappropriate biases in sentiment analysis systems since none existed at the time of the study.", "Then 219 automatic sentiment analysis systems were examined for inappropriate biases using the dataset.", "The goal of the research was simply to examine whether the sentiment analysis systems were consistently assigning higher or lower sentiment intensity scores to sentence pairs involving a particular race or gender.", "Dataset  A benchmark dataset, coined as Equity Evaluation Corpus (EEC), was compiled to test the fairness of the sentiment analysis systems.", "Several sentence templates were used to generate the dataset: 7 emotional sentence templates (included gender- or race-associated word + emotional word) and 4 neutral sentence templates (only gender- or race-associated word).", "(See the 11 templates in the table below)  The <person> placeholder was instantiated by African American or European American names such as ‘Latisha’ and ‘Ellen’ and noun phrases such as ‘my daughter’ and ‘my son’.", "Whereas, the <emotion word> placeholder included an emotion state word with varying intensities or emotional situation word.", "A total of 8,640 sentences were generated with the various combination of instantiations.", "(Refer to the paper for the full list of noun phrases, names, and emotion words).", "For each template, sentence pairs were generated such as ‘The conversation with my mom was heartbreaking’ and ‘The conversation with my dad was heartbreaking’.", "And the goal of the analysis was to find if there was a significant difference in scores or average scores for these sentence pairs when fed to the SA systems.", "The Sentiment Analysis Systems  The 219 automatic sentiment analysis systems that participated in the SemEval-2018 Task 1: Affect in Tweets were individually evaluated for discrimination based on gender and race bias.", "The emotion intensity regression for four emotions (anger, fear, joy, and sadness) and valence regression outputs of the systems were analyzed for race and gender bias.", "The outputs were scores between 0 and 1.", "The participants were provided with a train dataset consisting of tweets, together with two test datasets.", "One of the test datasets included tweets and the other was the EEC dataset for which the participants were not provided any further information.", "Essentially, the first test set (tweets) was used to evaluate the predictive performance of the systems and EEC (unknown dataset) was used for the proposed bias analysis.", "Most of the systems combined sentence embeddings or emotion lexicons with traditional ML algorithms (SVM and Logistic Regression) or deep neural networks (e.g., LSTMs and Bi-LSTMs).", "The embeddings were obtained either through a distant supervision corpus , pre-trained models, or manually trained embeddings.", "The lexicons were often derived from the NRC emotion and sentiment lexicons.", "A baseline SVM system was also trained with word unigrams as features.", "To determine gender and race bias, each system’s predicted score or average score on the respective sentence pairs generated by the templates were compared.", "Very high differences in scores indicated a higher bias by the systems.", "Gender Bias Results  The study reports that on the four emotion intensity prediction tasks a whopping 75% to 86% of the systems consistently scored sentences of one gender higher than another, even when all that changed was the noun phrases or names in the sentence pairs.", "This phenomenon was more evident in the emotions of anger and joy.", "As for the emotion of fear, the systems assigned higher scores to sentences with male noun phrases.", "Sadness was more balanced than the rest of the emotions.", "These results are in line with common stereotypes, such as females are more emotional, and situations that involve males are more fearful.", "Additionally, the top 10 systems that performed the best on the first test dataset showed some sensitivity to the gender-associated words while the systems with medium to low performance had an even higher sensitivity to these words.", "Concretely, the top-ranked systems that did well on the intensity prediction task represent a high percentage of gender-biased systems.", "This raises questions about the type of resources being used for building NLP systems and what influence these are having on the bias analysis.", "Race Bias Results  The race bias analysis produced similar results as compared to the gender bias analysis.", "In fact, the majority of the systems assigned higher scores to sentences that contained African American names on the emotion intensity prediction task for anger, fear, and sadness.", "In contrast, most of the systems assigned higher scores to sentences that contained European American names on the emotion intensity prediction task for joy.", "These results reflect the stereotypes found in a previous study which report that African Americans are commonly associated with more negative emotions.", "The Future  As future work, the authors will continue to investigate the systems (and its components) to locate the exact source/s of the identified biases.", "This can provide more concrete evidence to better explain and back the findings.", "In addition, such insights can help to build more accurate systems, and more importantly, mitigate different types of biases that may naturally arise from the resources commonly used in sentiment analysis systems.", "From the results obtained by the baseline system, which only used the training dataset and no other language resource, bias was also observed in the form of some unigrams highly associated with a particular gender or race.", "Since the training dataset was collected using a distant supervision approach, the authors suspect that this could be a strong source of bias.", "Overall, race bias was more prevalent than gender bias from the systems observed.", "The authors reported that the intensity of the bias also differed for each of the emotions involved.", "More races, county names, professions, and genders will be considered in the future work.", "(Update — 8/11/2018)  Requests for Research  The EEC corpus only presents one setting to examine the fairness of sentiment analysis systems.", "This is a challenging task but it could be beneficial if the corpus can incorporate other cases of inappropriate biases to experiment on.", "Crowdsourcing efforts could help to improve the quality of the corpus.", "In addition, you can also try other types of systems besides SA.", "How about emotion recognition systems or emoji recognition systems?", "The goal of this research was only to examine whether the sentiment analysis systems were perpetrating inappropriate human biases.", "However, this idea of detecting biases can be used to improve the accuracy of the systems or mitigate biases as done in other research.", "I believe that including context (e.g., entire conversations) and real data, as opposed to only synthetic data, in the analysis can help to strengthen the findings and arguments made.", "In fact, more interesting insights may emerge from the datasets.", "Let me know if you want to work on any of these tasks.", "Reference: ACL 2018"], "summary_text": "A pressing issue in machine learning (ML) research is to examine whether automated algorithms discriminate against gender or race when making decisions. In a recent study, Kiritchenko and Mohammad conducted an extensive analysis to determine whether hundreds of sentiment analysis systems accentuate inappropriate human biases and how these are manifested. Motivation  Automatic systems are beneficial to society but as they improve in predictive performance, comparable to human capabilities, they could perpetuate inappropriate human biases. In the context of sentiment analysis (SA), these biases may come in many forms. For instance, an SA system may consider the messages conveyed by a specific gender or race to be less positive simply because of their race and gender. Therefore, even when it is not clear how biases are manifested, it is the duty and responsibility of the natural language processing (NLP) system developer to address this problem. Contributions  In NLP, the sources of the bias may derive from the training data or language resources such as lexicons and word embeddings that a machine learning algorithm is designed to leverage for building the predictive model. There have been many works in computer vision and NLP that aim to detect these inappropriate biases, but few have focused on a large set of systems that aim to solve the same task, which is very common in the field of machine learning. The authors collected and proposed a benchmark dataset for examining inappropriate biases in sentiment analysis systems since none existed at the time of the study. Then 219 automatic sentiment analysis systems were examined for inappropriate biases using the dataset. The goal of the research was simply to examine whether the sentiment analysis systems were consistently assigning higher or lower sentiment intensity scores to sentence pairs involving a particular race or gender. Dataset  A benchmark dataset, coined as Equity Evaluation Corpus (EEC), was compiled to test the fairness of the sentiment analysis systems. Several sentence templates were used to generate the dataset: 7 emotional sentence templates (included gender- or race-associated word + emotional word) and 4 neutral sentence templates (only gender- or race-associated word). (See the 11 templates in the table below)  The <person> placeholder was instantiated by African American or European American names such as ‘Latisha’ and ‘Ellen’ and noun phrases such as ‘my daughter’ and ‘my son’. Whereas, the <emotion word> placeholder included an emotion state word with varying intensities or emotional situation word. A total of 8,640 sentences were generated with the various combination of instantiations. (Refer to the paper for the full list of noun phrases, names, and emotion words). For each template, sentence pairs were generated such as ‘The conversation with my mom was heartbreaking’ and ‘The conversation with my dad was heartbreaking’. And the goal of the analysis was to find if there was a significant difference in scores or average scores for these sentence pairs when fed to the SA systems. The Sentiment Analysis Systems  The 219 automatic sentiment analysis systems that participated in the SemEval-2018 Task 1: Affect in Tweets were individually evaluated for discrimination based on gender and race bias. The emotion intensity regression for four emotions (anger, fear, joy, and sadness) and valence regression outputs of the systems were analyzed for race and gender bias. The outputs were scores between 0 and 1. The participants were provided with a train dataset consisting of tweets, together with two test datasets. One of the test datasets included tweets and the other was the EEC dataset for which the participants were not provided any further information. Essentially, the first test set (tweets) was used to evaluate the predictive performance of the systems and EEC (unknown dataset) was used for the proposed bias analysis. Most of the systems combined sentence embeddings or emotion lexicons with traditional ML algorithms (SVM and Logistic Regression) or deep neural networks (e.g., LSTMs and Bi-LSTMs). The embeddings were obtained either through a distant supervision corpus , pre-trained models, or manually trained embeddings. The lexicons were often derived from the NRC emotion and sentiment lexicons. A baseline SVM system was also trained with word unigrams as features. To determine gender and race bias, each system’s predicted score or average score on the respective sentence pairs generated by the templates were compared. Very high differences in scores indicated a higher bias by the systems. Gender Bias Results  The study reports that on the four emotion intensity prediction tasks a whopping 75% to 86% of the systems consistently scored sentences of one gender higher than another, even when all that changed was the noun phrases or names in the sentence pairs. This phenomenon was more evident in the emotions of anger and joy. As for the emotion of fear, the systems assigned higher scores to sentences with male noun phrases. Sadness was more balanced than the rest of the emotions. These results are in line with common stereotypes, such as females are more emotional, and situations that involve males are more fearful. Additionally, the top 10 systems that performed the best on the first test dataset showed some sensitivity to the gender-associated words while the systems with medium to low performance had an even higher sensitivity to these words. Concretely, the top-ranked systems that did well on the intensity prediction task represent a high percentage of gender-biased systems. This raises questions about the type of resources being used for building NLP systems and what influence these are having on the bias analysis. Race Bias Results  The race bias analysis produced similar results as compared to the gender bias analysis. In fact, the majority of the systems assigned higher scores to sentences that contained African American names on the emotion intensity prediction task for anger, fear, and sadness. In contrast, most of the systems assigned higher scores to sentences that contained European American names on the emotion intensity prediction task for joy. These results reflect the stereotypes found in a previous study which report that African Americans are commonly associated with more negative emotions. The Future  As future work, the authors will continue to investigate the systems (and its components) to locate the exact source/s of the identified biases. This can provide more concrete evidence to better explain and back the findings. In addition, such insights can help to build more accurate systems, and more importantly, mitigate different types of biases that may naturally arise from the resources commonly used in sentiment analysis systems. From the results obtained by the baseline system, which only used the training dataset and no other language resource, bias was also observed in the form of some unigrams highly associated with a particular gender or race. Since the training dataset was collected using a distant supervision approach, the authors suspect that this could be a strong source of bias. Overall, race bias was more prevalent than gender bias from the systems observed. The authors reported that the intensity of the bias also differed for each of the emotions involved. More races, county names, professions, and genders will be considered in the future work. (Update — 8/11/2018)  Requests for Research  The EEC corpus only presents one setting to examine the fairness of sentiment analysis systems. This is a challenging task but it could be beneficial if the corpus can incorporate other cases of inappropriate biases to experiment on. Crowdsourcing efforts could help to improve the quality of the corpus. In addition, you can also try other types of systems besides SA. How about emotion recognition systems or emoji recognition systems? The goal of this research was only to examine whether the sentiment analysis systems were perpetrating inappropriate human biases. However, this idea of detecting biases can be used to improve the accuracy of the systems or mitigate biases as done in other research. I believe that including context (e.g., entire conversations) and real data, as opposed to only synthetic data, in the analysis can help to strengthen the findings and arguments made. In fact, more interesting insights may emerge from the datasets. Let me know if you want to work on any of these tasks. Reference: ACL 2018", "pdf_url": "https://www.aclweb.org/anthology/S18-2005.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/examining-gender-and-race-bias-in-sentiment-analysis-systems-b04b269a653.json"}
{"id": "63188694", "bin": "1500_1600", "summary_sentences": ["Panopticon: An Omniscient Lock Broker for Efficient Distributed Transactions in the Datacenter – Tasci & Demirbas, 2015  Today we return to the theme of distributed transactions, and a paper that won a best paper award from IEEE Big Data in 2015.", "Panopticon is a centralized lock broker (like Chubby and ZooKeeper ) that manages distributed (decentralized) locks (unlike Chubby and ZooKeeper).", "Full distributed transaction support is built on top of this lock broker.", "A central idea is that the lock for a data item does not have to be held in same place as the data item itself.", "This allows locks to migrate (even if the corresponding data items don’t), and improves the efficiency of the transaction commit protocol).", "The paper is an easy read, but I found myself wishing it was a little more formal at points – these things are hard to reason about!", "(Distributed) transactions have gained a reputation for being unscalable…  The authors identify two main reasons for this:  The coordination required for acquiring multiple locks in a distributed setting (two-phase locking, two-phase commit) becomes increasingly costly as the number of servers and data items involved increases.", "There is a big latency difference between local and remote access.", "When the lock for a data item is tied to the same location as a data item (and techniques such as consistent hashing are used to place data without locality considerations) this penalises transaction latencies significantly, especially during the lock acquisition phase.", "Remember that we’re taking as a baseline the fact that an application may need to read and write multiple data items as part of some business use case.", "The overhead we’re concerned with here is doing so transactionally.", "Traditional distributed transactions employ two phase locking to prevent deadlocks, which requires that the server initiating the transaction to contact the other servers for locks in increasing order of locks.", "Instead of contacting other servers trying to acquire locks in increasing order in a serial manner, it is more efficient to go to the broker and test/set all the locks at once.", "In Panopticon, the lock request is sent at once to the broker, and the broker takes care of deadlock prevention.", "Panopticon divides locks into three categories based on their access patterns:  Locks that are accessed from multiple different servers  Locks that receive repetitive access from the same server  Locks that aren’t accessed very much  Observe that:  It is best to host type 1 locks (locks that keep receiving across-server accesses) in the lock broker.", "And it is best to assign the type 2 locks to the requesting server to avoid the overheads of repetitive requests from that server to the broker.", "Panopticon uses heuristics to migrate locks so as position them in the best place possible.", "All locks start out (as type 3 locks) on the same server as the data item they protect.", "A server contacts the lock broker only if it requires a lock that is not kept locally.", "If a server s requests some lock l from the broker (i.e., a lock that is not local to s), then the lock is migrated to the broker if it is not already there.", "The broker then grants it to s. When s releases the lock, l stays resident centrally at the broker.", "l is now treated as a type 1 lock.", "If a lock held at the broker is not accessed for a long time, and the broker needs to save space, an LRU policy can be used to migrate type 1 locks back to the server owning the corresponding data item (becoming type 3).", "We use the following rule of thumb for declaring a lock to be of type 2 and migrating that lock to a server: If two consecutive requests for a given lock l (held at the broker) comes from the same server w, then the broker migrates lock l to server w. From that point on w treats l as its local lock, the lock locality of w is improved with this, since w does not need to contact the broker for l again.", "A type 2 lock can migrate back to the broker again if a request for it is received from some other server than the one it currently resides on.", "In this manner,  As the centralized authority for mediating access to data, the broker learns about the access patterns of transactions at runtime and manages the migration of locks to servers in a way that improves lock access locality.", "The lock broker, however, is oblivious to the state of the transactions, and the servers are the ultimate transaction managers.", "Transactions are initiated and executed by the servers distributedly after checking that all the locks are available at the server.", "Panopticon uses a form of two-phase locking to manage transactions.", "When a server initiates a distributed transaction, it requests all the locks it needs in a batch request sent to the broker.", "The broker orders the locks by data item ID so that all transactions always attempt to acquire locks in the same order (preventing deadlock).", "The broker works through the requested locks in order.", "If it already owns a lock, it forwards it to the requesting server.", "If the broker does not have the lock, it adds the requesting server’s name to a request queue that the broker maintains for that lock, and forwards a request for the lock to the current owning server.", "When the lock becomes available, the broker forwards it to the server at the head of the queue.", "If the server initiating the transaction ultimately requires for example four locks – l1, l2, l3, and l4 in that order, it may be that at some point it holds locks 1,2, and 4.", "However lock l4 can be taken away from it at any point until the server has also acquired lock 3.", "Locks 1 & 2 are considered to be ‘authoritatively owned’ by the server and will never be stolen from it.", "After a transaction is finished, the server needs to unlock the data items, which means returning the locks back to the broker.", "In this phase we propose an optimization where the server lazy unlocks the locks.", "Lazy unlocking means that the locks are released locally, but not transmitted back to broker until δ time elapses, where δ is empirically determined.", "Lazy unlocking provides efficiency benefits for cases when the server needs to access the same data items used in the terminated transaction immediately in the next transaction.", "If a lazy-unlocked data item lock is requested in the meantime, it is immediately returned to the broker.", "In order to give more scalability and avoid bottlenecks when using extremely large number of servers and locks, we can employ a hierarchical composition of the brokers.", "For this we have k level-0 lock brokers each overseeing a cluster of servers and a top-level lock broker overseeing  these k lock brokers… Moreover, using hierarchical composition of brokers at different datacenters, the Panopticon system can provide a partial answer to the across-datacenter/WAN transactions problem.", "Providing an efficient and complete system for across-datacenter transactions remains part of our future work.", "Partition detection follows very simple rules, given that there is a central broker.", "If you can see the broker, you’re in the main partition, and if you can’t, you’re not!", "Providing continued operation for data items locked by locks outside of the main partition is future work.", "Panopticon is built on top of Hazelcast, and compares favourably to the default distributed transaction protocol in Hazelcast.", "The tests use artificially generated workloads.", "It would be interesting to see how Panopticon performs with real workloads (in particular, the effectiveness of Panopticon is sensitive to the parameter Phist that determines how likely it is that a server uses the same objects in consecutive transactions – just how likely is this in real workloads with load balancing etc.?", "(genuine question on my part)).", "A few other things I’m left wondering about as well: the prepare and commit phases still need to happen at every RM, even if the locks move around….", "what gets logged and where in the WAL?", "(Records normally hold information about locks held).", "How does recovery work?", "How do you prevent the broker from becoming a single point of failure?", "And if it isn’t, do you have to run some kind of consensus protocol across multiple brokers…?"], "summary_text": "Panopticon: An Omniscient Lock Broker for Efficient Distributed Transactions in the Datacenter – Tasci & Demirbas, 2015  Today we return to the theme of distributed transactions, and a paper that won a best paper award from IEEE Big Data in 2015. Panopticon is a centralized lock broker (like Chubby and ZooKeeper ) that manages distributed (decentralized) locks (unlike Chubby and ZooKeeper). Full distributed transaction support is built on top of this lock broker. A central idea is that the lock for a data item does not have to be held in same place as the data item itself. This allows locks to migrate (even if the corresponding data items don’t), and improves the efficiency of the transaction commit protocol). The paper is an easy read, but I found myself wishing it was a little more formal at points – these things are hard to reason about! (Distributed) transactions have gained a reputation for being unscalable…  The authors identify two main reasons for this:  The coordination required for acquiring multiple locks in a distributed setting (two-phase locking, two-phase commit) becomes increasingly costly as the number of servers and data items involved increases. There is a big latency difference between local and remote access. When the lock for a data item is tied to the same location as a data item (and techniques such as consistent hashing are used to place data without locality considerations) this penalises transaction latencies significantly, especially during the lock acquisition phase. Remember that we’re taking as a baseline the fact that an application may need to read and write multiple data items as part of some business use case. The overhead we’re concerned with here is doing so transactionally. Traditional distributed transactions employ two phase locking to prevent deadlocks, which requires that the server initiating the transaction to contact the other servers for locks in increasing order of locks. Instead of contacting other servers trying to acquire locks in increasing order in a serial manner, it is more efficient to go to the broker and test/set all the locks at once. In Panopticon, the lock request is sent at once to the broker, and the broker takes care of deadlock prevention. Panopticon divides locks into three categories based on their access patterns:  Locks that are accessed from multiple different servers  Locks that receive repetitive access from the same server  Locks that aren’t accessed very much  Observe that:  It is best to host type 1 locks (locks that keep receiving across-server accesses) in the lock broker. And it is best to assign the type 2 locks to the requesting server to avoid the overheads of repetitive requests from that server to the broker. Panopticon uses heuristics to migrate locks so as position them in the best place possible. All locks start out (as type 3 locks) on the same server as the data item they protect. A server contacts the lock broker only if it requires a lock that is not kept locally. If a server s requests some lock l from the broker (i.e., a lock that is not local to s), then the lock is migrated to the broker if it is not already there. The broker then grants it to s. When s releases the lock, l stays resident centrally at the broker. l is now treated as a type 1 lock. If a lock held at the broker is not accessed for a long time, and the broker needs to save space, an LRU policy can be used to migrate type 1 locks back to the server owning the corresponding data item (becoming type 3). We use the following rule of thumb for declaring a lock to be of type 2 and migrating that lock to a server: If two consecutive requests for a given lock l (held at the broker) comes from the same server w, then the broker migrates lock l to server w. From that point on w treats l as its local lock, the lock locality of w is improved with this, since w does not need to contact the broker for l again. A type 2 lock can migrate back to the broker again if a request for it is received from some other server than the one it currently resides on. In this manner,  As the centralized authority for mediating access to data, the broker learns about the access patterns of transactions at runtime and manages the migration of locks to servers in a way that improves lock access locality. The lock broker, however, is oblivious to the state of the transactions, and the servers are the ultimate transaction managers. Transactions are initiated and executed by the servers distributedly after checking that all the locks are available at the server. Panopticon uses a form of two-phase locking to manage transactions. When a server initiates a distributed transaction, it requests all the locks it needs in a batch request sent to the broker. The broker orders the locks by data item ID so that all transactions always attempt to acquire locks in the same order (preventing deadlock). The broker works through the requested locks in order. If it already owns a lock, it forwards it to the requesting server. If the broker does not have the lock, it adds the requesting server’s name to a request queue that the broker maintains for that lock, and forwards a request for the lock to the current owning server. When the lock becomes available, the broker forwards it to the server at the head of the queue. If the server initiating the transaction ultimately requires for example four locks – l1, l2, l3, and l4 in that order, it may be that at some point it holds locks 1,2, and 4. However lock l4 can be taken away from it at any point until the server has also acquired lock 3. Locks 1 & 2 are considered to be ‘authoritatively owned’ by the server and will never be stolen from it. After a transaction is finished, the server needs to unlock the data items, which means returning the locks back to the broker. In this phase we propose an optimization where the server lazy unlocks the locks. Lazy unlocking means that the locks are released locally, but not transmitted back to broker until δ time elapses, where δ is empirically determined. Lazy unlocking provides efficiency benefits for cases when the server needs to access the same data items used in the terminated transaction immediately in the next transaction. If a lazy-unlocked data item lock is requested in the meantime, it is immediately returned to the broker. In order to give more scalability and avoid bottlenecks when using extremely large number of servers and locks, we can employ a hierarchical composition of the brokers. For this we have k level-0 lock brokers each overseeing a cluster of servers and a top-level lock broker overseeing  these k lock brokers… Moreover, using hierarchical composition of brokers at different datacenters, the Panopticon system can provide a partial answer to the across-datacenter/WAN transactions problem. Providing an efficient and complete system for across-datacenter transactions remains part of our future work. Partition detection follows very simple rules, given that there is a central broker. If you can see the broker, you’re in the main partition, and if you can’t, you’re not! Providing continued operation for data items locked by locks outside of the main partition is future work. Panopticon is built on top of Hazelcast, and compares favourably to the default distributed transaction protocol in Hazelcast. The tests use artificially generated workloads. It would be interesting to see how Panopticon performs with real workloads (in particular, the effectiveness of Panopticon is sensitive to the parameter Phist that determines how likely it is that a server uses the same objects in consecutive transactions – just how likely is this in real workloads with load balancing etc.? (genuine question on my part)). A few other things I’m left wondering about as well: the prepare and commit phases still need to happen at every RM, even if the locks move around…. what gets logged and where in the WAL? (Records normally hold information about locks held). How does recovery work? How do you prevent the broker from becoming a single point of failure? And if it isn’t, do you have to run some kind of consensus protocol across multiple brokers…?", "pdf_url": "http://www.cse.buffalo.edu/tech-reports/2014-06.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/panopticon.json"}
{"id": "89628084", "bin": "1500_1600", "summary_sentences": ["BBR: Congestion-based congestion control Cardwell et al., ACM Queue Sep-Oct 2016  With thanks to Hossein Ghodse (@hossg) for recommending today’s paper selection.", "This is the story of how members of Google’s make-tcp-fast project developed and deployed a new congestion control algorithm for TCP called BBR (for Bandwidth Bottleneck and Round-trip propagation time), leading to 2-25x throughput improvement over the previous loss-based congestion control CUBIC algorithm.", "In fact, the improvements would have been even more significant but for the fact that throughput became limited by the deployed TCP receive buffer size.", "Increasing this buffer size led to a huge 133x relative improvement with BBR (2Gbps), while CUBIC remained at 15Mbps.", "BBR is also being deployed on YouTube servers, with a small percentage of users being assigned BBR playback.", "Playbacks using BBBR show significant improvement in all of YouTube’s quality-of-experience metrics, possibly because BBR’s behavior is more consistent and predictable… BBR reduces median RTT by 53 percent on average globally, and by more than 80 percent in the developing world.", "TCP congestion and bottlenecks  The Internet isn’t working as well as it should, and many of the problems relate to TCP’s loss-based congestion control, even with the current best-of-breed CUBIC algorithm.", "This ties back to design decisions taken in the 1980’s when packet loss and congestion were synonymous due to technology limitations.", "That correspondence no longer holds so directly.", "When bottleneck buffers are large, loss-based congestion control keeps them full, causing bufferbloat.", "When bottleneck buffers are small, loss-based congestion control misinterprets loss as a signal of congestion, leading to low throughput.", "Fixing these problems requires finding an alternative to loss-based congestion control.", "From the perspective of TCP, the performance of an arbitrarily complex path is bound by two constraints: round-trip propagation time (RTprop), and bottleneck bandwidth, BtlBw (the bandwidth at the slowest link in each direction).", "Here’s a picture to help make this clearer:  The RTprop time is the minimum time for round-trip propagation if there are no queuing delays and no processing delays at the receiver.", "The more familiar RTT (round-trip time) is formed of RTprop + these additional sources of noise and delay.", "Bandwidth Delay Product (BDP) is the maximum possible amount of data in transit in a network, and is obtained by multiplying the bottleneck bandwidth and round-trip propagation time.", "BDP is central to understanding network performance.", "Consider what happens to delivery rate as we gradually increase the amount of data inflight.", "When the amount of inflight data is less than BDP, then delivery rate increases as we send more data – delivery rate is limited by the application.", "Once the bandwidth at the bottleneck is saturated though, the delivery rate cannot go up anymore – we’re pushing data through that pipe just as fast as it can go.", "The buffer will fill up, eventually we’ll start dropping packets, but we still won’t increase delivery rate.", "The optimum operating point is right on the BDP threshold (blue dot above), but loss-based congestion control operates at the BDP + Bottleneck Buffer Size point (green dot above).", "Now let’s look at what happens to RTT as we increase the amount of data inflight.", "It can never be better than RTprop, so until we reach BDP, RTT ~= RTprop.", "Beyond BDP, as buffers start to fill, RTT goes up until buffers are completely full and we start dropping packets.", "Once more, the optimum operating point would be right on the BDP threshold.", "This was proved by Leonard Kleinrock in 1979, unfortunately about the same time Jeffrey M. Jaffe proved that it was impossible to create a distributed algorithm that converged to this operation point.", "Jaffe’s result rests on fundamental measurement ambiguities.", "Although it is impossible to disambiguate any single measurement, a connection’s behavior over time tells a clearer story, suggesting the possibility of measurement strategies designed to resolve ambiguity.", "Introducing BBR  BBR is a congestion control algorithm based on these two parameters that fundamentally characterise a path: bottleneck bandwidth and round-trip propagation time.", "It makes continuous estimates of these values, resulting in a distributed congestion control algorithm that reacts to actual congestion, not packet loss or transient queue delay, and converges with high probability to Kleinrock’s optimal operating point.", "(BBR is a simple instance of a Max-plus control system , a new approach to control based on nonstandard algebra.", "This approach allows the adaptation rate [controlled by the max gain] to be independent of the queue growth [controlled by the average gain].", "Applied to this problem, it results in a simple, implicit control loop where the adaptation to physical constraint changes is automatically handled by the filters representing those constraints.", "A conventional control system would require multiple loops connected by a complex state machine to accomplish the same result.)", "Since RTT can never be less than RTprop, tracking the minimum RTT provides an unbiased and efficient estimator of the round-trip propagation time.", "The existing TCP acks provide enough information for us to calculate RTT.", "Unlike RTT, nothing in the TCP spec requires implementations to track bottleneck bandwidth, but a good estimate results from tracking delivery rate.", "The average delivery rate between a send and an ack is simply the amount of data delivered divided by the time taken.", "We know that this must be less than the true bottleneck delivery rate, so we can use the highest recorded delivery rate as our running estimate of bandwidth bottleneck.", "Putting this altogether leads to a core BBR algorithm with two parts: a protocol to follow on receiving an ack, and a protocol to following when sending.", "You’ll find the pseudocode for these on pages 28 and 29-30.", "From my reading, there are a couple of small mistakes in the pseudocode (but I could be mistaken!", "), so I’ve recreated clean versions below.", "Please do check against those in the original article if you’re digging deeper…  Here’s the ack protocol:  (app_limited_until is set on the sending side, when the app is not sending enough data to reach BDP).", "This is what the sending protocol looks like:  The pacing_gain controls how fast packets are sent relative to BtlBw and is key to BBR’s ability to learn.", "A pacing_gain greater than 1 increases inflight and decreases packet inter-arrival time, moving the connection to the right on the performance charts.", "A pacing_gain less than 1 has the opposite effect, moving the connection to the left.", "BBR uses this pacing_gain to implement a simple sequential probing state machine that alternates between testing for higher bandwidths and then testing for lower round-trip times.", "The frequency, magnitude, duration and structure of these experiments differ depending on what’s already known (start-up or steady state) and the sending app’s behaviour (intermittent or continuous).", "Most time is spent in the ProbeBW state probing bandwidth.", "BBR cycles through a sequence of gains for pacing_gain, using an eight-phase cycle with values 5/4, 3/4, 1, 1, 1, 1, 1, 1.", "Each phase lasts for the estimated round-trip propagation time.", "This design allows the gain cycle first to probe for more bandwidth with a pacing_gain above 1.0, then drain any resulting queue with a pacing_gain an equal distance below 1.0, and then cruise with a short queue using a pacing_gain of 1.0.", "The result is a control loop that looks like this plot below showing the RTT (blue), inflight (green) and delivery rate (red) from 700ms of a 10Mbps, 40-ms flow.", "Here’s how BBR compares to CUBIC during the first second of a 10 Mbps, 40-ms flow.", "(BBR in green, CUBIC in red).", "We talked about the BBR benefits in Google’s high-speed WAN network (B4) and in YouTube in the introduction.", "It also has massive benefits for low bandwidth mobile subscriptions.", "More than half of the world’s 7 billion mobile Internet subscriptions connect via 8-to 114-kbps 2.5 G systems, which suffer well-documented problems because of loss-based congestion control’s buffer-filling propensities.", "The bottleneck link for these systems is usually between the SGSN (serving GPRS support node)18 and mobile device.", "SGSN software runs on a standard PC platform with ample memory, so there are frequently megabytes of buffer between the Internet and mobile device.", "Figure 10 [ below] compares (emulated) SGSN Internet-to-mobile delay for BBR and CUBIC."], "summary_text": "BBR: Congestion-based congestion control Cardwell et al., ACM Queue Sep-Oct 2016  With thanks to Hossein Ghodse (@hossg) for recommending today’s paper selection. This is the story of how members of Google’s make-tcp-fast project developed and deployed a new congestion control algorithm for TCP called BBR (for Bandwidth Bottleneck and Round-trip propagation time), leading to 2-25x throughput improvement over the previous loss-based congestion control CUBIC algorithm. In fact, the improvements would have been even more significant but for the fact that throughput became limited by the deployed TCP receive buffer size. Increasing this buffer size led to a huge 133x relative improvement with BBR (2Gbps), while CUBIC remained at 15Mbps. BBR is also being deployed on YouTube servers, with a small percentage of users being assigned BBR playback. Playbacks using BBBR show significant improvement in all of YouTube’s quality-of-experience metrics, possibly because BBR’s behavior is more consistent and predictable… BBR reduces median RTT by 53 percent on average globally, and by more than 80 percent in the developing world. TCP congestion and bottlenecks  The Internet isn’t working as well as it should, and many of the problems relate to TCP’s loss-based congestion control, even with the current best-of-breed CUBIC algorithm. This ties back to design decisions taken in the 1980’s when packet loss and congestion were synonymous due to technology limitations. That correspondence no longer holds so directly. When bottleneck buffers are large, loss-based congestion control keeps them full, causing bufferbloat. When bottleneck buffers are small, loss-based congestion control misinterprets loss as a signal of congestion, leading to low throughput. Fixing these problems requires finding an alternative to loss-based congestion control. From the perspective of TCP, the performance of an arbitrarily complex path is bound by two constraints: round-trip propagation time (RTprop), and bottleneck bandwidth, BtlBw (the bandwidth at the slowest link in each direction). Here’s a picture to help make this clearer:  The RTprop time is the minimum time for round-trip propagation if there are no queuing delays and no processing delays at the receiver. The more familiar RTT (round-trip time) is formed of RTprop + these additional sources of noise and delay. Bandwidth Delay Product (BDP) is the maximum possible amount of data in transit in a network, and is obtained by multiplying the bottleneck bandwidth and round-trip propagation time. BDP is central to understanding network performance. Consider what happens to delivery rate as we gradually increase the amount of data inflight. When the amount of inflight data is less than BDP, then delivery rate increases as we send more data – delivery rate is limited by the application. Once the bandwidth at the bottleneck is saturated though, the delivery rate cannot go up anymore – we’re pushing data through that pipe just as fast as it can go. The buffer will fill up, eventually we’ll start dropping packets, but we still won’t increase delivery rate. The optimum operating point is right on the BDP threshold (blue dot above), but loss-based congestion control operates at the BDP + Bottleneck Buffer Size point (green dot above). Now let’s look at what happens to RTT as we increase the amount of data inflight. It can never be better than RTprop, so until we reach BDP, RTT ~= RTprop. Beyond BDP, as buffers start to fill, RTT goes up until buffers are completely full and we start dropping packets. Once more, the optimum operating point would be right on the BDP threshold. This was proved by Leonard Kleinrock in 1979, unfortunately about the same time Jeffrey M. Jaffe proved that it was impossible to create a distributed algorithm that converged to this operation point. Jaffe’s result rests on fundamental measurement ambiguities. Although it is impossible to disambiguate any single measurement, a connection’s behavior over time tells a clearer story, suggesting the possibility of measurement strategies designed to resolve ambiguity. Introducing BBR  BBR is a congestion control algorithm based on these two parameters that fundamentally characterise a path: bottleneck bandwidth and round-trip propagation time. It makes continuous estimates of these values, resulting in a distributed congestion control algorithm that reacts to actual congestion, not packet loss or transient queue delay, and converges with high probability to Kleinrock’s optimal operating point. (BBR is a simple instance of a Max-plus control system , a new approach to control based on nonstandard algebra. This approach allows the adaptation rate [controlled by the max gain] to be independent of the queue growth [controlled by the average gain]. Applied to this problem, it results in a simple, implicit control loop where the adaptation to physical constraint changes is automatically handled by the filters representing those constraints. A conventional control system would require multiple loops connected by a complex state machine to accomplish the same result.) Since RTT can never be less than RTprop, tracking the minimum RTT provides an unbiased and efficient estimator of the round-trip propagation time. The existing TCP acks provide enough information for us to calculate RTT. Unlike RTT, nothing in the TCP spec requires implementations to track bottleneck bandwidth, but a good estimate results from tracking delivery rate. The average delivery rate between a send and an ack is simply the amount of data delivered divided by the time taken. We know that this must be less than the true bottleneck delivery rate, so we can use the highest recorded delivery rate as our running estimate of bandwidth bottleneck. Putting this altogether leads to a core BBR algorithm with two parts: a protocol to follow on receiving an ack, and a protocol to following when sending. You’ll find the pseudocode for these on pages 28 and 29-30. From my reading, there are a couple of small mistakes in the pseudocode (but I could be mistaken! ), so I’ve recreated clean versions below. Please do check against those in the original article if you’re digging deeper…  Here’s the ack protocol:  (app_limited_until is set on the sending side, when the app is not sending enough data to reach BDP). This is what the sending protocol looks like:  The pacing_gain controls how fast packets are sent relative to BtlBw and is key to BBR’s ability to learn. A pacing_gain greater than 1 increases inflight and decreases packet inter-arrival time, moving the connection to the right on the performance charts. A pacing_gain less than 1 has the opposite effect, moving the connection to the left. BBR uses this pacing_gain to implement a simple sequential probing state machine that alternates between testing for higher bandwidths and then testing for lower round-trip times. The frequency, magnitude, duration and structure of these experiments differ depending on what’s already known (start-up or steady state) and the sending app’s behaviour (intermittent or continuous). Most time is spent in the ProbeBW state probing bandwidth. BBR cycles through a sequence of gains for pacing_gain, using an eight-phase cycle with values 5/4, 3/4, 1, 1, 1, 1, 1, 1. Each phase lasts for the estimated round-trip propagation time. This design allows the gain cycle first to probe for more bandwidth with a pacing_gain above 1.0, then drain any resulting queue with a pacing_gain an equal distance below 1.0, and then cruise with a short queue using a pacing_gain of 1.0. The result is a control loop that looks like this plot below showing the RTT (blue), inflight (green) and delivery rate (red) from 700ms of a 10Mbps, 40-ms flow. Here’s how BBR compares to CUBIC during the first second of a 10 Mbps, 40-ms flow. (BBR in green, CUBIC in red). We talked about the BBR benefits in Google’s high-speed WAN network (B4) and in YouTube in the introduction. It also has massive benefits for low bandwidth mobile subscriptions. More than half of the world’s 7 billion mobile Internet subscriptions connect via 8-to 114-kbps 2.5 G systems, which suffer well-documented problems because of loss-based congestion control’s buffer-filling propensities. The bottleneck link for these systems is usually between the SGSN (serving GPRS support node)18 and mobile device. SGSN software runs on a standard PC platform with ample memory, so there are frequently megabytes of buffer between the Internet and mobile device. Figure 10 [ below] compares (emulated) SGSN Internet-to-mobile delay for BBR and CUBIC.", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3012426.3022184?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/bbr-congestion-based-congestion-control.json"}
{"id": "83267919", "bin": "1500_1600", "summary_sentences": ["Robust learning from untrusted sources Konstantinov & Lampert, ICML’19  Welcome back to a new term of The Morning Paper!", "Just before the break we were looking at selected papers from ICML’19, including “Data Shapley.” I’m going to pick things up pretty much where we left off with a few more ICML papers…  Data Shapley provides us with one way of finding and correcting or eliminating low-value (and potentially harmful) data points from a training set.", "In today’s paper choice, Konstantinov & Lampert provide a way of assessing the value of datasets as a whole.", "The idea is that you might be learning e.g. a classifier by combining data from multiple sources.", "By assigning a value (weighting) to each of those data sources we can intelligently combine them to get the best possible performance out of the resulting trained model.", "So if you need more data in order to improve the performance of your model, ‘Robust learning from untrusted sources’ provides an approach that lets you tap into additional, potentially noisier, sources.", "It’s similar in spirit to Snorkel which we looked at last year, and is designed to let you incorporate data from multiple ‘weakly supervised’ (i.e. noisy) data sources.", "Snorkel replaces labels with probability-weighted labels, and then trains the final classifier using those.", "The big idea  You have at least one data source that you trust to have reasonable quality, which we’ll call the reference dataset.", "In addition you have N further data sources available, of varying quality.", "Two extremes of learning from all this data are to (a) use only the known high quality reference data source, discarding the others, and (b) train using all the available data regardless of quality.", "The goal of this paper is to find a middle-ground that lets us makes use of all of the data available,  without dragging the resulting model performance down due to noisy data.", "… we propose a method that automatically assigns weights to the sources… the weights are assigned to the sources according to the quality and reliability of the data they provide, quantified by an appropriate measure of trust we introduce.", "This is achieved by comparing the data from each source to a small reference dataset, obtained or trusted by the learner.", "In the evaluation, the resulting models consistently outperform models trained on either the reference dataset only, or all of the datasets, for any amount and type of data contamination considered in the study.", "How good is that data source?", "Intuitively, a good learning algorithm will assign more weight to sources whose distribution is similar to the target one, and less weight to those that provide different or low-quality data.", "The phrase ‘similar to the target one’ implies we’re going to need a distance measure over distributions so that we can compare them.", "For this use case, the ideal distance measure would be one that gives us a strong correlation with the performance of a learned classifier.", "Now, there’s nothing that correlates better with the performance of a learned classifier than… the performance of a learned classifier!", "The discrepancy between the distribution of a given dataset  and the reference dataset  is defined to be the difference in the expected loss of  a predictor trained on the reference dataset and a predictor trained on the additional dataset.", "Intuitively, the discrepancy between the two distributions is large, if there exists a predictor that performs well on one of them and badly on the other.", "On the other hand, if all functions in the hypothesis class perform similarly on both, then  and  have low discrepancy.", "Combining data sources  At this point we have a collection of datasets, and an indication of how ‘good’ each of those datasets is for the task in hand.", "Since we have a collection of trained classifiers, one per dataset, my intuition would be to use them as an ensemble and use weighted voting to combine their outputs.", "The natural first thing I would try is to have the weighting inversely proportional to the discrepancy score.", "That’s sort of what happens here, but the authors  have a fancier way of determining the weights.", "The weights are chosen by minimising the following objective function:  Let’s pick that apart a little bit…  We have  different datasets to combine  is the weight assigned to the ith dataset  is the discrepancy score between dataset  and the reference dataset  .", "The first term therefore is learning weights based directly on the discrepancy score.", "The constraint that the learned weights must sum to 1 prevents the algorithm simply assigning a very low or zero weight to each source.", "The second term is a kind of regularisation parameter.", "is the number of data points in the ith dataset, so it’s designed to encourage more confidence to be placed in the result of classifiers trained over larger datasets.", "Note that the first term is small whenever large weights are paired with small discrepancies and hence encourages trusting sources that provide data similar to the reference target sample.", "The second term is small whenever the weights are distributed proportionally to the number of samples per source.", "Thus, it acts as a form of regularization, by encouraging the usage of information from as many sources as possible.", "is a hyperparameter controlling the balance between the two.", "Incorporating private data  If one or more of the data sources is private or has constraints (e.g., it can’t be moved to the location where the training is taking place) then it can still be incorporated into the overall process.", "If the reference (target) dataset can be moved to the location of the private data, then the discrepancy score can be calculated locally there.", "If the reference dataset also cannot be shared, then it turns out it is still possible to compute empirical discrepancies without observing the data from the source directly.", "This works because the equation for determining discrepancy breaks down into two independent terms, one concerning the reference dataset, and one concerning the comparison dataset:  Therefore, each discrepancy can be estimated by using a sequence of queries to the source about the gradient of a minibatch from its data with respect to a current candidate for the predictor…  Evaluation  The first part of the evaluation is done using an Amazon products dataset with customer reviews for 957 Amazon products.", "The task is to  learn a classifier per product which can classify reviews for that product as either positive or negative.", "The experiment focuses in on reviews for books, and provides to the learning algorithm a dataset based on reviews for the book in question, together with data from reviews of other books, and data from reviews of other non-book products.", "Each run ends up with 10 data sources to combine: 600 reviews from the target book, and 100 labelled reviews from 9 other products, varying  , the number of these other products that are not books.", "Intuitively, when learning to classify book reviews and given access to reviews from both some books and some non-books, a good learning algorithm will be able to leverage all this information, while being robust to the potentially misleading data coming from the less relevant products.", "The resulting classifiers give much better results than either of the two extremes of (a) using only the reference dataset, and (b) just lumping all the data together in one unweighted pool.", "A variation on this looked across all 957 products, giving each classifier 100 reviews from its own dataset, and an additional set of 100 labelled reviews from every other product.", "The second part of the evaluation uses the ‘Animals with Attributes 2’ dataset with 37,322 images of 50 different animal classes.", "A sample is taken of clean data to form the reference dataset, and then corrupted additional datasets are created using a variety of manipulations:  Introducing label bias by setting labels of all corrupted samples to class 1  Shuffling labels  Shuffling features  Blurring images  Introducing dead pixels (for 30% of the image)  Swapping RGB channels  For a given combination of target attribute, corruption strategy, and blend of true and corrupted datasets, the learning experiment is repeated 100 times.", "The following table summarises the results over 85 different prediction tasks:  The results in table 2 show that our method performs significantly better than all baselines for many types of corruption and many values of n, especially for high levels of contamination, while essentially never performing significantly worse than any baseline."], "summary_text": "Robust learning from untrusted sources Konstantinov & Lampert, ICML’19  Welcome back to a new term of The Morning Paper! Just before the break we were looking at selected papers from ICML’19, including “Data Shapley.” I’m going to pick things up pretty much where we left off with a few more ICML papers…  Data Shapley provides us with one way of finding and correcting or eliminating low-value (and potentially harmful) data points from a training set. In today’s paper choice, Konstantinov & Lampert provide a way of assessing the value of datasets as a whole. The idea is that you might be learning e.g. a classifier by combining data from multiple sources. By assigning a value (weighting) to each of those data sources we can intelligently combine them to get the best possible performance out of the resulting trained model. So if you need more data in order to improve the performance of your model, ‘Robust learning from untrusted sources’ provides an approach that lets you tap into additional, potentially noisier, sources. It’s similar in spirit to Snorkel which we looked at last year, and is designed to let you incorporate data from multiple ‘weakly supervised’ (i.e. noisy) data sources. Snorkel replaces labels with probability-weighted labels, and then trains the final classifier using those. The big idea  You have at least one data source that you trust to have reasonable quality, which we’ll call the reference dataset. In addition you have N further data sources available, of varying quality. Two extremes of learning from all this data are to (a) use only the known high quality reference data source, discarding the others, and (b) train using all the available data regardless of quality. The goal of this paper is to find a middle-ground that lets us makes use of all of the data available,  without dragging the resulting model performance down due to noisy data. … we propose a method that automatically assigns weights to the sources… the weights are assigned to the sources according to the quality and reliability of the data they provide, quantified by an appropriate measure of trust we introduce. This is achieved by comparing the data from each source to a small reference dataset, obtained or trusted by the learner. In the evaluation, the resulting models consistently outperform models trained on either the reference dataset only, or all of the datasets, for any amount and type of data contamination considered in the study. How good is that data source? Intuitively, a good learning algorithm will assign more weight to sources whose distribution is similar to the target one, and less weight to those that provide different or low-quality data. The phrase ‘similar to the target one’ implies we’re going to need a distance measure over distributions so that we can compare them. For this use case, the ideal distance measure would be one that gives us a strong correlation with the performance of a learned classifier. Now, there’s nothing that correlates better with the performance of a learned classifier than… the performance of a learned classifier! The discrepancy between the distribution of a given dataset  and the reference dataset  is defined to be the difference in the expected loss of  a predictor trained on the reference dataset and a predictor trained on the additional dataset. Intuitively, the discrepancy between the two distributions is large, if there exists a predictor that performs well on one of them and badly on the other. On the other hand, if all functions in the hypothesis class perform similarly on both, then  and  have low discrepancy. Combining data sources  At this point we have a collection of datasets, and an indication of how ‘good’ each of those datasets is for the task in hand. Since we have a collection of trained classifiers, one per dataset, my intuition would be to use them as an ensemble and use weighted voting to combine their outputs. The natural first thing I would try is to have the weighting inversely proportional to the discrepancy score. That’s sort of what happens here, but the authors  have a fancier way of determining the weights. The weights are chosen by minimising the following objective function:  Let’s pick that apart a little bit…  We have  different datasets to combine  is the weight assigned to the ith dataset  is the discrepancy score between dataset  and the reference dataset  . The first term therefore is learning weights based directly on the discrepancy score. The constraint that the learned weights must sum to 1 prevents the algorithm simply assigning a very low or zero weight to each source. The second term is a kind of regularisation parameter. is the number of data points in the ith dataset, so it’s designed to encourage more confidence to be placed in the result of classifiers trained over larger datasets. Note that the first term is small whenever large weights are paired with small discrepancies and hence encourages trusting sources that provide data similar to the reference target sample. The second term is small whenever the weights are distributed proportionally to the number of samples per source. Thus, it acts as a form of regularization, by encouraging the usage of information from as many sources as possible. is a hyperparameter controlling the balance between the two. Incorporating private data  If one or more of the data sources is private or has constraints (e.g., it can’t be moved to the location where the training is taking place) then it can still be incorporated into the overall process. If the reference (target) dataset can be moved to the location of the private data, then the discrepancy score can be calculated locally there. If the reference dataset also cannot be shared, then it turns out it is still possible to compute empirical discrepancies without observing the data from the source directly. This works because the equation for determining discrepancy breaks down into two independent terms, one concerning the reference dataset, and one concerning the comparison dataset:  Therefore, each discrepancy can be estimated by using a sequence of queries to the source about the gradient of a minibatch from its data with respect to a current candidate for the predictor…  Evaluation  The first part of the evaluation is done using an Amazon products dataset with customer reviews for 957 Amazon products. The task is to  learn a classifier per product which can classify reviews for that product as either positive or negative. The experiment focuses in on reviews for books, and provides to the learning algorithm a dataset based on reviews for the book in question, together with data from reviews of other books, and data from reviews of other non-book products. Each run ends up with 10 data sources to combine: 600 reviews from the target book, and 100 labelled reviews from 9 other products, varying  , the number of these other products that are not books. Intuitively, when learning to classify book reviews and given access to reviews from both some books and some non-books, a good learning algorithm will be able to leverage all this information, while being robust to the potentially misleading data coming from the less relevant products. The resulting classifiers give much better results than either of the two extremes of (a) using only the reference dataset, and (b) just lumping all the data together in one unweighted pool. A variation on this looked across all 957 products, giving each classifier 100 reviews from its own dataset, and an additional set of 100 labelled reviews from every other product. The second part of the evaluation uses the ‘Animals with Attributes 2’ dataset with 37,322 images of 50 different animal classes. A sample is taken of clean data to form the reference dataset, and then corrupted additional datasets are created using a variety of manipulations:  Introducing label bias by setting labels of all corrupted samples to class 1  Shuffling labels  Shuffling features  Blurring images  Introducing dead pixels (for 30% of the image)  Swapping RGB channels  For a given combination of target attribute, corruption strategy, and blend of true and corrupted datasets, the learning experiment is repeated 100 times. The following table summarises the results over 85 different prediction tasks:  The results in table 2 show that our method performs significantly better than all baselines for many types of corruption and many values of n, especially for high levels of contamination, while essentially never performing significantly worse than any baseline.", "pdf_url": "http://proceedings.mlr.press/v97/konstantinov19a/konstantinov19a.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/robust-learning-from-untrusted-sources.json"}
{"id": "49333267", "bin": "1500_1600", "summary_sentences": ["Twitter Heron: Towards extensible streaming engines Fu et al., ICDE 2017  We previously looked at the initial Twitter Heron paper that announced Heron to the world.", "In this ICDE 2017 paper, the team give us an update based on the work done since as a result of open-sourcing Heron .", "… we discuss the challenges we faced when transforming Heron from a system tailored for Twitter’s applications and software stack to a system using an extensible, modular architecture which provides flexibility to adapt to various environments and applications.", "We get a high level look at the architecture and an investigation into its performance (short version: having a modular architecture doesn’t mean you have to sacrifice performance).", "The part I find the most interesting though is the comparisons between Twitter Heron’s design and those of Storm and Spark Streaming.", "(Would be great to see Apache Flink included as well).", "Twitter’s daily operations rely heavily on real-time processing of billions of event per day… Heron is now the de facto stream data processing system in Twitter and is used to support various types of applications such as spam detection, real time machine learning, and real time analytics, among others.", "Why a modular architecture?", "Moving Heron from an internal project to an open source project meant that Heron could no longer be tied to any element of Twitter’s stack.", "To be externally useful to a broad audience, it needs to consider different environments (public/private cloud), a variety of software stacks, and diverse application workloads.", "A good example here is Twitter’s use of Aurora for scheduling, whereas many other organisations may already have an alternate scheduler (e.g., YARN).", "Heron allows the application developer or system administrator to create a new implementation for a specific Heron module and plug it into the system without disrupting the remaining modules or the communication between them.", "The structure also allows different applications built on top of Heron to use different module implementations while operating on the same underlying resources.", "The high-level architecture of Heron  Heron’s architecture is inspired by that of microkernel-based operating systems… due to the heterogeneity of today’s cloud environments and Big Data platforms, we decided to design Heron using extensible self-contained modules that operate on top of a kernel which provides the basic functionality needed to build a streaming engine.", "There are seven main modules, as shown in the following figure:  At runtime, everything is container-based.", "Heron Instances are essentially the spouts or bolts that run on their own JVM.", "The Metrics Manager collects process metrics, and the Topology Master manages the directed graph of spouts and bolts (a topology).", "Let’s now take a look at the remaining modules in more detail.", "Resource Manager  The resource manager is invoked on demand to manage resource assignments (CPU, memory, disk) for a particular topology.", "It does this by assigning Heron Instances to containers through a packing plan.", "An initial packing plan is created when a topology is first submitted, and can be repacked in response to user requests to scale up and down.", "Different resource management policies can be selected for the different topologies running on the same cluster.", "A generated packing plan is passed to the Scheduler.", "Scheduler  The Scheduler module interacts with an underlying scheduling framework such as YARN or Aurora to allocate the resources needed by a packing plan.", "Heron accommodates both stateful and stateless schedulers, depending on the level of support provided by the underlying scheduling framework.", "A stateful Scheduler regularly communicates with the underlying scheduling framework to monitor the state of the containers of the topology.", "In case a container has failed, the stateful Scheduler takes the necessary actions to recover from failure… A stateless Scheduler on the other hand, is not aware of the state of the containers while the topology is running.", "More specifically, it relies on the underlying scheduling framework to detect container failures and take the necessary actions to resolve them.", "Heron today has support for Aurora and YARN.", "A Mesos scheduler is being developed in the community.", "State Manager  The State Manager is used for distributed coordination and storing topology metadata.", "It stores for example topology definitions, packing plans, host and port information for all containers, and the location of the underlying scheduling framework.", "Heron provides a ZooKeeper based implementation, as well as a local file system version for local development and testing.", "Stream Manager  The Stream Manager handles all inter-process communication.", "It’s implemented in C++ to provide tighter control over the memory and CPU footprint, and to avoid copying data between the native and JVM heaps.", "The Stream Manager uses several techniques to achieve high performance:  Protocol Buffers are allocated in memory pools and reused, avoiding new/delete operations  In-place updates of Protocol Buffers are performed  Lazy deserialization is used whenever possible  The communication layer offers two important configuration parameters to tune its behaviour for a given deployment: max spout pending determines the maximum number of tuple that can be pending on a spout task at any point in time, and cache drain frequency determines how often the tuple cache is drained.", "The tuple cache store incoming and outgoing data tuples before routing them to the appropriate Heron Instances.", "Heron vs Storm  Heron is designed with the goal of operating in a cloud environment on top of a scheduling framework such as Aurora or YARN (although it can also run in local mode).", "As a result, it leverages the resource isolation mechanisms implemented by these frameworks.", "Storm, on the other hand implements parts of the functionality of the Heron Resource Manager, the Heron Scheduler and the underlying scheduling framework in the same abstraction.", "In Storm, this creates confusion between Storm’s scheduling decisions and those of any underlying scheduler.", "Furthermore, in Storm all resources for a cluster must be obtained up front, whereas Heron acquires resources on demand.", "Thus Storm clusters will tend to be over-provisioned.", "Heron provides resource isolation between topologies (through the underlying scheduling framework), and also between processes of the same topology.", "Storm can do neither – it packs multiple spout and bolt tasks into a single executor, and several executors share the same JVM.", "Finally, Heron’s Stream Manager handles all data transfers separately from processing units, which helps to make the system scalable.", "Storm shares communication threads and processing threads in the same JVM.", "“As a result, it is much harder to isolate performance bottlenecks and thus optimize the overall performance.”  Heron vs Spark Streaming  Spark Streaming depends on Spark itself for extensibility.", "Because Spark supports a wide diversity of use cases, it is not easy to customize it particularly for streaming.", "It is worth noting that Spark (and, as a result, Spark Streaming) has a similar architecture to Storm that limits the resource isolation guarantees it can provide… each executor process can run multiple tasks in different threads.", "As opposed to Heron, this model does not provide resource isolation among the tasks that are assigned to the same executor.", "All Spark Streaming communication relies on Spark, and is not customisable.", "Performance evaluation  If we compare Heron and Storm on a workload chosen to expose framework overheads we see that Heron beats storm on both throughput and latency:  The above figures are with acks enabled.", "Heron outperforms Storm by approximately 3-5x in terms of throughput and at the same time has 2-4x lower latency.", "With acks disabled, the throughput of Heron is 2-3x higher than Storm’s.", "The following two charts show the impact of the max spout pending configuration parameter.", "As you might expect, allowing more queuing tasks increases throughput up to a point, at the expense of latency.", "See section V in the paper for a more detailed performance breakdown.", "Despite the benefits of general-purpose architectures, such as Heron’s modular architecture, a common belief is that specialized solutions tend to outperform general-purpose ones because they are optimized for particular environments and applications.", "In this paper, we show that by carefully optimizing core components of the system, Heron’s general-purpose architecture can actually provide better performance than specialized solutions such as Storm."], "summary_text": "Twitter Heron: Towards extensible streaming engines Fu et al., ICDE 2017  We previously looked at the initial Twitter Heron paper that announced Heron to the world. In this ICDE 2017 paper, the team give us an update based on the work done since as a result of open-sourcing Heron . … we discuss the challenges we faced when transforming Heron from a system tailored for Twitter’s applications and software stack to a system using an extensible, modular architecture which provides flexibility to adapt to various environments and applications. We get a high level look at the architecture and an investigation into its performance (short version: having a modular architecture doesn’t mean you have to sacrifice performance). The part I find the most interesting though is the comparisons between Twitter Heron’s design and those of Storm and Spark Streaming. (Would be great to see Apache Flink included as well). Twitter’s daily operations rely heavily on real-time processing of billions of event per day… Heron is now the de facto stream data processing system in Twitter and is used to support various types of applications such as spam detection, real time machine learning, and real time analytics, among others. Why a modular architecture? Moving Heron from an internal project to an open source project meant that Heron could no longer be tied to any element of Twitter’s stack. To be externally useful to a broad audience, it needs to consider different environments (public/private cloud), a variety of software stacks, and diverse application workloads. A good example here is Twitter’s use of Aurora for scheduling, whereas many other organisations may already have an alternate scheduler (e.g., YARN). Heron allows the application developer or system administrator to create a new implementation for a specific Heron module and plug it into the system without disrupting the remaining modules or the communication between them. The structure also allows different applications built on top of Heron to use different module implementations while operating on the same underlying resources. The high-level architecture of Heron  Heron’s architecture is inspired by that of microkernel-based operating systems… due to the heterogeneity of today’s cloud environments and Big Data platforms, we decided to design Heron using extensible self-contained modules that operate on top of a kernel which provides the basic functionality needed to build a streaming engine. There are seven main modules, as shown in the following figure:  At runtime, everything is container-based. Heron Instances are essentially the spouts or bolts that run on their own JVM. The Metrics Manager collects process metrics, and the Topology Master manages the directed graph of spouts and bolts (a topology). Let’s now take a look at the remaining modules in more detail. Resource Manager  The resource manager is invoked on demand to manage resource assignments (CPU, memory, disk) for a particular topology. It does this by assigning Heron Instances to containers through a packing plan. An initial packing plan is created when a topology is first submitted, and can be repacked in response to user requests to scale up and down. Different resource management policies can be selected for the different topologies running on the same cluster. A generated packing plan is passed to the Scheduler. Scheduler  The Scheduler module interacts with an underlying scheduling framework such as YARN or Aurora to allocate the resources needed by a packing plan. Heron accommodates both stateful and stateless schedulers, depending on the level of support provided by the underlying scheduling framework. A stateful Scheduler regularly communicates with the underlying scheduling framework to monitor the state of the containers of the topology. In case a container has failed, the stateful Scheduler takes the necessary actions to recover from failure… A stateless Scheduler on the other hand, is not aware of the state of the containers while the topology is running. More specifically, it relies on the underlying scheduling framework to detect container failures and take the necessary actions to resolve them. Heron today has support for Aurora and YARN. A Mesos scheduler is being developed in the community. State Manager  The State Manager is used for distributed coordination and storing topology metadata. It stores for example topology definitions, packing plans, host and port information for all containers, and the location of the underlying scheduling framework. Heron provides a ZooKeeper based implementation, as well as a local file system version for local development and testing. Stream Manager  The Stream Manager handles all inter-process communication. It’s implemented in C++ to provide tighter control over the memory and CPU footprint, and to avoid copying data between the native and JVM heaps. The Stream Manager uses several techniques to achieve high performance:  Protocol Buffers are allocated in memory pools and reused, avoiding new/delete operations  In-place updates of Protocol Buffers are performed  Lazy deserialization is used whenever possible  The communication layer offers two important configuration parameters to tune its behaviour for a given deployment: max spout pending determines the maximum number of tuple that can be pending on a spout task at any point in time, and cache drain frequency determines how often the tuple cache is drained. The tuple cache store incoming and outgoing data tuples before routing them to the appropriate Heron Instances. Heron vs Storm  Heron is designed with the goal of operating in a cloud environment on top of a scheduling framework such as Aurora or YARN (although it can also run in local mode). As a result, it leverages the resource isolation mechanisms implemented by these frameworks. Storm, on the other hand implements parts of the functionality of the Heron Resource Manager, the Heron Scheduler and the underlying scheduling framework in the same abstraction. In Storm, this creates confusion between Storm’s scheduling decisions and those of any underlying scheduler. Furthermore, in Storm all resources for a cluster must be obtained up front, whereas Heron acquires resources on demand. Thus Storm clusters will tend to be over-provisioned. Heron provides resource isolation between topologies (through the underlying scheduling framework), and also between processes of the same topology. Storm can do neither – it packs multiple spout and bolt tasks into a single executor, and several executors share the same JVM. Finally, Heron’s Stream Manager handles all data transfers separately from processing units, which helps to make the system scalable. Storm shares communication threads and processing threads in the same JVM. “As a result, it is much harder to isolate performance bottlenecks and thus optimize the overall performance.”  Heron vs Spark Streaming  Spark Streaming depends on Spark itself for extensibility. Because Spark supports a wide diversity of use cases, it is not easy to customize it particularly for streaming. It is worth noting that Spark (and, as a result, Spark Streaming) has a similar architecture to Storm that limits the resource isolation guarantees it can provide… each executor process can run multiple tasks in different threads. As opposed to Heron, this model does not provide resource isolation among the tasks that are assigned to the same executor. All Spark Streaming communication relies on Spark, and is not customisable. Performance evaluation  If we compare Heron and Storm on a workload chosen to expose framework overheads we see that Heron beats storm on both throughput and latency:  The above figures are with acks enabled. Heron outperforms Storm by approximately 3-5x in terms of throughput and at the same time has 2-4x lower latency. With acks disabled, the throughput of Heron is 2-3x higher than Storm’s. The following two charts show the impact of the max spout pending configuration parameter. As you might expect, allowing more queuing tasks increases throughput up to a point, at the expense of latency. See section V in the paper for a more detailed performance breakdown. Despite the benefits of general-purpose architectures, such as Heron’s modular architecture, a common belief is that specialized solutions tend to outperform general-purpose ones because they are optimized for particular environments and applications. In this paper, we show that by carefully optimizing core components of the system, Heron’s general-purpose architecture can actually provide better performance than specialized solutions such as Storm.", "pdf_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/heron_icde-1.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/twitter-heron-towards-extensible-streaming-engines.json"}
{"id": "99341732", "bin": "1500_1600", "summary_sentences": ["BEAT: asynchronous BFT made practical Duan et al., CCS’18  Reaching agreement (consensus) is hard enough, doing it in the presence of active adversaries who can tamper with or destroy your communications is much harder still.", "That’s the world of Byzantine fault tolerance (BFT).", "We’ve looked at Practical BFT (PBFT) and HoneyBadger on previous editions of The Morning Paper.", "Today’s paper, BEAT, builds on top of HoneyBadger to offer BFT with even better latency and throughput.", "Asynchronous BFT protocols are arguably the most appropriate solutions for building high-assurance and intrusion-tolerant permissioned blockchains in wide-are (WAN) environments, as these asynchronous protocols are inherently more robust against timing and denial-of-service (DoS) attacks that can be mounted over an unprotected network such as the Internet.", "The best performing asynchronous BFT protocol, HoneyBadger , still lags behind the partially synchronous PBFT protocol in terms of throughput and latency.", "BEAT is actually a family of five different asynchronous BFT protocols that start from the HoneyBadger baseline and make improvements targeted at different application scenarios.", "Unlike HoneyBadgerBFT, which was designed to optimize throughput only, BEAT aims to be flexible and versatile, providing protocol instances optimized for latency, throughput, bandwidth, or scalability (in terms of the number of servers).", "The BEAT protocols divide into two groups: those supporting full (general) state-machine replication (SMR), as required e.g. for smart contract use cases (BEAT0, BEAT1, BEAT2); and those that support BFT storage (append-only ledger) use cases only (BEAT3, BEAT4).", "The following table summarises the BEAT family and the key distinguishing features of each member.", "( Enlarge )  There’s a lot of ground to cover here, but I’ll do my best to give you an overview.", "Alongside the BEAT protocols themselves, the paper also includes two new building blocks: the generalized fingerprinted cross-checksum and an asynchronous verifiable information dispersal (AVID) algorithm.", "The HoneyBadger baseline  HoneyBadger supports ACS (the asynchronous common subset) meaning that it provides these guarantees:  Validity: if a correct server delivers a set  , then  and  contains the inputs of at least  correct servers.", "Agreement: if a correct server delivers a set  , then all correct servers deliver  .", "Totality: if  correct servers submit an input, then all correct servers deliver an output.", "HoneyBadger uses reliable broadcast (RBC) and asynchronous Byzantine binary agreement (ABA) protocols to achieve its aims.", "Threshold signatures are used to provide common coins for ABA, and threshold encryption is used to avoid censorship and achieve liveness.", "In a threshold scheme the partial outputs (e.g. decryption shares) of at least t participants need to be combined in order to recover (decrypt) the intended value.", "BEAT0: improved security and performance  BEAT0, our baseline protocol, incorporates a more secure and efficient threshold encryption, a direct instantiation of threshold coin-flipping (instead of using threshold signatures), and more flexible and efficient erasure-coding support.", "BEAT0’s threshold encryption uses the TDH2 scheme by Shoup and , providing 128-bit security under elliptic curve cryptography.", "This gives stronger security and better performance than the scheme used in HoneyBadger.", "In place of the zfec erasure coding library used by HoneyBadger, which supports only Reed-Solomon codes and at most 128 servers, BEAT uses the Jerasure library giving access to more efficient erasure coding schemes and lifting the replica restriction.", "BEAT1: lower latency  Via a careful study of latency for each HoneyBadgerBFT subprotocol, we find that (1) most of the latency comes from threshold encryption and threshold signatures, and (2) somewhat surprisingly, when the load is small and there is low contention, erasure-coded reliable broadcast (AVID broadcast) causes significant latency.", "BEAT1 swaps out the AVID broadcast protocol of BEAT0 for a replication-based reliable broadcast protocol, Bracha’s broadcast .", "Under small loads BEAT1 has lower latency.", "With small batch sizes BEAT1’s throughput is higher than HoneyBadger / BEAT0, but with larger batch sizes throughput is down by 20-30%.", "BEAT2: causal ordering  BEAT2 builds on BEAT1 and also opportunistically moves the use of threshold encryption to the client side.", "In BEAT2, when the ciphertexts are delivered, it is too late for the adversary to censor transactions.", "Thus, the adversary does not know what transactions to delay, and can only delay transactions from specific clients.", "BEAT2 can be combined with anonymous communication networks to achieve full liveness.", "BEAT2 additionally achieves causal order, which prevents the adversary from inserting derived transactions before the original, causally prior transactions.", "BEAT3: higher throughput for storage use cases  BEAT3 is the first member of the BEAT family targeted for BFT-storage use cases (as opposed to general SMR).", "Recall that the safety and liveness properties of BFT storage remain the same as those of general SMR, with the only exception that the state may not be replicated at each server (but instead may be erasure-coded).", "BEAT3 can be used for blockchain applications that need append-only ledgers, and specific blockchains where the consensus protocol serves as an ordering service, such as Hyperledger Fabric.", "Whereas so far we’ve been using a reliable broadcast protocol (AVID), BEAT3 replaces this with a bandwidth-efficient information dispersal scheme called AVID-FP.", "To disperse a block  , AVID requires bandwidth  , whereas AVID-FP can do it in  .", "To order transactions of size  , the communication complexity of BEAT0 is  , of BEAT1 and BEAT2 is  , and of BEAT3 is  .", "AVID-FP is a bandwidth-efficient AVID (asynchronous verifiable information dispersal) protocol using fingerprinted cross-checksum.", "In AVID-FP, given a block B to be dispersed, the dealer applies an (m,n) erasure coding scheme, where  and  … then it generates the corresponding fingerprinted cross-checksum for B with respect to the erasure coding scheme.", "Each server verifies the correctness of its fragment with respect to the fingerprint cross-checksum, “and then, roughly speaking, leverages the (much smaller) fingerprinted cross-checksum in place of the fragment in the original AVID protocol.”  An (n,m) fingerprinted cross-checksum contains a cross-checksum array of n values, and a fingerprint array of m values.", "The ith entry in the checksum array contains the hash of the ith coded fragment.", "See section 4 in the paper for details of the fingerprint array usage.", "BEAT4: partial reads  BEAT4 further reduces read bandwidth using a novel erasure-coded reliable broadcast protocol called AVID-FP-Pyramid.", "This supports use cases where clients only need to read a fraction of a data block.", "AVID-FD-Pyramid is based on pyramid codes, which trade space for access efficiency in erasure-coded storage systems (about 10% extra space requirement for a 50% drop in access overhead).", "Pyramid codes can be efficiently built from any (n, m) systematic and MDS (maximum distance separable) code.", "See section 4 in the paper for brief details, or Huang et al. for an in-depth treatment.", "BEAT4 uses a 2-level pyramid scheme which can tolerate one failure in each level, and is able to reduce read bandwidth by 50%.", "Full details are in section 9 of the paper.", "Evaluation  The evaluation is conducted on EC2 with up to 92 nodes from ten different regions in five different continents, using a variety of network sizes and batch sizes.", "In the figures that follow,  represents the network size such that BEAT0,1,2 & 3 require  nodes and BEAT4 requires  nodes.", "When f=1, BEAT0, BEAT1, BEAT2, and BEAT3 are around 2x faster than HoneyBadger, and when f becomes larger, they are even faster than HoneyBadger.", "When f = 1, BEAT4 is about as fast as HoneyBadger… As f increases, HoneyBadger is much slower than BEAT4.", "For throughput, BEAT0 slightly outperforms HoneyBadger.", "BEAT1 and BEAT2 achieve higher throughput than HoneyBadger with small batch sizes, but have 20-30% lower throughput at larger batch sizes.", "BEAT3 and BEAT4 outperform all the other protocols consistently.", "If this write-up has captured your interest, I highly encourage you to go an and read the full paper which contains significantly more detail than I was able to convey here."], "summary_text": "BEAT: asynchronous BFT made practical Duan et al., CCS’18  Reaching agreement (consensus) is hard enough, doing it in the presence of active adversaries who can tamper with or destroy your communications is much harder still. That’s the world of Byzantine fault tolerance (BFT). We’ve looked at Practical BFT (PBFT) and HoneyBadger on previous editions of The Morning Paper. Today’s paper, BEAT, builds on top of HoneyBadger to offer BFT with even better latency and throughput. Asynchronous BFT protocols are arguably the most appropriate solutions for building high-assurance and intrusion-tolerant permissioned blockchains in wide-are (WAN) environments, as these asynchronous protocols are inherently more robust against timing and denial-of-service (DoS) attacks that can be mounted over an unprotected network such as the Internet. The best performing asynchronous BFT protocol, HoneyBadger , still lags behind the partially synchronous PBFT protocol in terms of throughput and latency. BEAT is actually a family of five different asynchronous BFT protocols that start from the HoneyBadger baseline and make improvements targeted at different application scenarios. Unlike HoneyBadgerBFT, which was designed to optimize throughput only, BEAT aims to be flexible and versatile, providing protocol instances optimized for latency, throughput, bandwidth, or scalability (in terms of the number of servers). The BEAT protocols divide into two groups: those supporting full (general) state-machine replication (SMR), as required e.g. for smart contract use cases (BEAT0, BEAT1, BEAT2); and those that support BFT storage (append-only ledger) use cases only (BEAT3, BEAT4). The following table summarises the BEAT family and the key distinguishing features of each member. ( Enlarge )  There’s a lot of ground to cover here, but I’ll do my best to give you an overview. Alongside the BEAT protocols themselves, the paper also includes two new building blocks: the generalized fingerprinted cross-checksum and an asynchronous verifiable information dispersal (AVID) algorithm. The HoneyBadger baseline  HoneyBadger supports ACS (the asynchronous common subset) meaning that it provides these guarantees:  Validity: if a correct server delivers a set  , then  and  contains the inputs of at least  correct servers. Agreement: if a correct server delivers a set  , then all correct servers deliver  . Totality: if  correct servers submit an input, then all correct servers deliver an output. HoneyBadger uses reliable broadcast (RBC) and asynchronous Byzantine binary agreement (ABA) protocols to achieve its aims. Threshold signatures are used to provide common coins for ABA, and threshold encryption is used to avoid censorship and achieve liveness. In a threshold scheme the partial outputs (e.g. decryption shares) of at least t participants need to be combined in order to recover (decrypt) the intended value. BEAT0: improved security and performance  BEAT0, our baseline protocol, incorporates a more secure and efficient threshold encryption, a direct instantiation of threshold coin-flipping (instead of using threshold signatures), and more flexible and efficient erasure-coding support. BEAT0’s threshold encryption uses the TDH2 scheme by Shoup and , providing 128-bit security under elliptic curve cryptography. This gives stronger security and better performance than the scheme used in HoneyBadger. In place of the zfec erasure coding library used by HoneyBadger, which supports only Reed-Solomon codes and at most 128 servers, BEAT uses the Jerasure library giving access to more efficient erasure coding schemes and lifting the replica restriction. BEAT1: lower latency  Via a careful study of latency for each HoneyBadgerBFT subprotocol, we find that (1) most of the latency comes from threshold encryption and threshold signatures, and (2) somewhat surprisingly, when the load is small and there is low contention, erasure-coded reliable broadcast (AVID broadcast) causes significant latency. BEAT1 swaps out the AVID broadcast protocol of BEAT0 for a replication-based reliable broadcast protocol, Bracha’s broadcast . Under small loads BEAT1 has lower latency. With small batch sizes BEAT1’s throughput is higher than HoneyBadger / BEAT0, but with larger batch sizes throughput is down by 20-30%. BEAT2: causal ordering  BEAT2 builds on BEAT1 and also opportunistically moves the use of threshold encryption to the client side. In BEAT2, when the ciphertexts are delivered, it is too late for the adversary to censor transactions. Thus, the adversary does not know what transactions to delay, and can only delay transactions from specific clients. BEAT2 can be combined with anonymous communication networks to achieve full liveness. BEAT2 additionally achieves causal order, which prevents the adversary from inserting derived transactions before the original, causally prior transactions. BEAT3: higher throughput for storage use cases  BEAT3 is the first member of the BEAT family targeted for BFT-storage use cases (as opposed to general SMR). Recall that the safety and liveness properties of BFT storage remain the same as those of general SMR, with the only exception that the state may not be replicated at each server (but instead may be erasure-coded). BEAT3 can be used for blockchain applications that need append-only ledgers, and specific blockchains where the consensus protocol serves as an ordering service, such as Hyperledger Fabric. Whereas so far we’ve been using a reliable broadcast protocol (AVID), BEAT3 replaces this with a bandwidth-efficient information dispersal scheme called AVID-FP. To disperse a block  , AVID requires bandwidth  , whereas AVID-FP can do it in  . To order transactions of size  , the communication complexity of BEAT0 is  , of BEAT1 and BEAT2 is  , and of BEAT3 is  . AVID-FP is a bandwidth-efficient AVID (asynchronous verifiable information dispersal) protocol using fingerprinted cross-checksum. In AVID-FP, given a block B to be dispersed, the dealer applies an (m,n) erasure coding scheme, where  and  … then it generates the corresponding fingerprinted cross-checksum for B with respect to the erasure coding scheme. Each server verifies the correctness of its fragment with respect to the fingerprint cross-checksum, “and then, roughly speaking, leverages the (much smaller) fingerprinted cross-checksum in place of the fragment in the original AVID protocol.”  An (n,m) fingerprinted cross-checksum contains a cross-checksum array of n values, and a fingerprint array of m values. The ith entry in the checksum array contains the hash of the ith coded fragment. See section 4 in the paper for details of the fingerprint array usage. BEAT4: partial reads  BEAT4 further reduces read bandwidth using a novel erasure-coded reliable broadcast protocol called AVID-FP-Pyramid. This supports use cases where clients only need to read a fraction of a data block. AVID-FD-Pyramid is based on pyramid codes, which trade space for access efficiency in erasure-coded storage systems (about 10% extra space requirement for a 50% drop in access overhead). Pyramid codes can be efficiently built from any (n, m) systematic and MDS (maximum distance separable) code. See section 4 in the paper for brief details, or Huang et al. for an in-depth treatment. BEAT4 uses a 2-level pyramid scheme which can tolerate one failure in each level, and is able to reduce read bandwidth by 50%. Full details are in section 9 of the paper. Evaluation  The evaluation is conducted on EC2 with up to 92 nodes from ten different regions in five different continents, using a variety of network sizes and batch sizes. In the figures that follow,  represents the network size such that BEAT0,1,2 & 3 require  nodes and BEAT4 requires  nodes. When f=1, BEAT0, BEAT1, BEAT2, and BEAT3 are around 2x faster than HoneyBadger, and when f becomes larger, they are even faster than HoneyBadger. When f = 1, BEAT4 is about as fast as HoneyBadger… As f increases, HoneyBadger is much slower than BEAT4. For throughput, BEAT0 slightly outperforms HoneyBadger. BEAT1 and BEAT2 achieve higher throughput than HoneyBadger with small batch sizes, but have 20-30% lower throughput at larger batch sizes. BEAT3 and BEAT4 outperform all the other protocols consistently. If this write-up has captured your interest, I highly encourage you to go an and read the full paper which contains significantly more detail than I was able to convey here.", "pdf_url": "https://www.csee.umbc.edu/~hbzhang/files/beat.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/beat-asynchronous-bft-made-practical.json"}
{"id": "42200029", "bin": "1500_1600", "summary_sentences": ["Growing a protocol Ramasubramanian et al., HotCloud’17  I’ve been really enjoying working my way through a selection of the HotCloud papers – they’re relatively short, thought-provoking, and designed to promote discussion (each paper has a set of discussion questions at the very end – great if you’re looking at them in a group of some kind).", "Today’s paper is certainly no exception.", "The setting is a collaboration between Elastic and Peter Alvaro’s research group at the University of California.", "The question under investigation is how best to implement and evolve distributed system protocols without introducing unintended bugs.", "In this paper, we argue that tool support for implementing and evolving fault-tolerant distributed systems needs to be rethought.", "We advocate exploration of the (sparse) middle ground between existing testing techniques practically inadequate for addressing fault tolerance concerns and traditional verification techniques ill-suited to the continual evolution of real-world evaluations.", "The status quo  If you want to solve a distributed systems problem these days, you can generally start by picking up a proven protocol for the issue at hand.", "For example, Paxos, Raft, Primary/Backup, Chain Replication, reliable broadcast, and so on.", "So we might naively expect that…  …modern system designers can merely take mechanisms “off the shelf” and enjoy the guarantees of hardened subsystems while constructing otherwise novel applications.", "Any practitioner, however, will quickly identify this as a fallacy.", "Even initial protocol implementations tend to differ significantly from their specification.", "(A sentiment echoed in the wonderful ‘ Use of Formal Methods at Amazon Web Services ’ paper).", "But it’s really the ‘little’ changes over the lifetime of the system, the protocol optimisations over time, that can have outsize unintended consequences.", "We end up with a system that is, for example, “essentially Primary/Backup.” And while it may have the essence of Primary/Backup, does it retain the guarantees?", "It’s very hard to know.", "Such a circumstance places implementors in the bad position of deriving false confidence from assertions that their implementation is “essentially Primary/Backup”.", "What we normally rely on to ensure that changes to our system don’t introduce new bugs is regression testing: “regression testing techniques ensure future optimisations do not re-introduce bugs previously encountered in early stages of system development.” In regression testing we check that inputs known to result in bad behaviour in the past no longer do.", "In the ideal case, the team would be using some kind of formal verification, and specifications would be co-evolved with the code and invariants proved afresh.", "In practice, this rarely happens.", "The correctness guarantees determined at initial verification time erode as protocols evolve.", "The authors point out that regression testing alone is not sufficient to assert fault tolerance properties.", "Inputs that trigger bugs in one version of a protocol are not guaranteed to trigger the same bug in a different version.", "In distributed systems, a large class of bugs are tied to the execution schedule, not (just) to the inputs.", "As a result, regression testing, as we currently employ it, is fundamentally too weak to prevent fault tolerance regression bugs.", "Root cause analysis is similarly inadequate, because a set of faults triggering bugs in later versions may fail to do so in an earlier version.", "This difference between input-based and schedule-based approaches necessitates the use of a different approach for fault tolerance testing and verification.", "We are left wanting something that works like verification, but feels like testing.", "Elastic meets LDFI  The team at Elastic faced a problem like the one we just described.", "They had a data replication protocol based on Primary/Backup, and were looking to introduce a faster variant that could synchronise individual operations rather than relying on file copying.", "The Elastic team then made a couple of really smart moves:  “Since this was a new algorithm, Elastic was looking for ways to formally verify it.”  “Elastic engaged our research team because they wanted a technique that strikes a balance between formal verification and testing – in particular the strong correctness guarantees of the former and the agility of the latter.”  The project used Lineage Driven Fault Injection (LDFI), which builds a model based on good system execution and explores only fault scenarios capable of forcing the system into a bad state.", "(See the earlier write-up on The Morning Paper for more details).", "We implemented a sequence of versions of the replication protocol and used LDFI to incrementally verify them as part of continuous integration.", "LDFI helped to find existing bugs relating to the implementation of the new protocol, and also demonstrated the safety (or otherwise!)", "of a couple of subsequent optimisations.", "There are many instances in the software development cycle for a bug to be introduced, the first of which is when a protocol specification is converted to an implementation.", "During our case study, we found a bug which manifested precisely from such a transaction scenario.", "(The bug related to a primary failover with partially replicated writes in progress, see section 4.1 for details).", "LDFI generated the scenario automatically, rather than requiring testers to have the foresight to manually derive such test cases.", "Originally discovered in the context of concurrent writes, subsequent analysis showed that with the right inputs a variation can occur with just a single write.", "… the two bugs are similar, but do not manifest from the same fault scenarios.", "This reinforces the claim from our motivating example that techniques such as root cause analysis as they are generally deployed would not be effective in reasoning about the fault tolerance properties of distributed systems.", "A seemingly minor optimisation was explored to avoid extra processing: requests have monotonically increasing sequence numbers, and if a sequence number associated with a write request had been seen before, instead of processing it (again?", "), the payload should be dropped and the request simply acknowledged.", "It turns out there’s a problem with this scheme that can occur during primary failover:  Fortunately, LDFI quickly and automatically discovers such a scenario by using the initial successful execution to test fault scenarios that may cause failures… Optimization carries the risk of introducing entirely new bugs capable of breaking the end-to-end properties of the system, which is best handled by verification-based tools.", "A second, substantially more complicated optimisation turned out not to produce any counterexamples when run against LDFI.", "Seemingly simple optimisations may break guarantees, while more complex ones may not – simplicity is not a guarantee of correctness.", "Improving distributed software quality  Our experience at Elastic suggests approaches like LDFI are a step towards improving the state of the art in distributed software quality.", "Where can we go from here?", "The team set out a number of future directions:  Integration with semantic-aware software model checkers: “An ideal tool solution would combine the best features of LDFI (which automatically builds models of domain knowledge, but ignore concurrency and asynchrony) with state-of-the-art combined approaches such as SAMC , since we know from Fischer et al. (FLP) that some of the most fundamental difficulties of distributed systems exist at the intersection of partial failure and asynchrony!”  Exploiting the CALM theorem : “we are developing a prototype system that combines the Lineage-Driven approach (utilizing explanations of what went right to reason about what could go wrong) and CALM analysis (using static analysis to prove commutativity of message processing logic) to simultaneously prune the space of faults and re-orderings.”  Using probabilistic system models to embrace rather than mask the inherent uncertainties in distributed executions.", "Improving the debugging experience using provenance to reason about distributed executions (“a young research area capable of radical growth”)  Our experience using LDFI at Elastic suggests the provision of high-level explanations of how a system achieves (or fails to achieve) good outcomes are a good starting point for taming the complexity of distributed debugging.", "There is wide scope for improving tools for implementing, evolving, and debugging distributed software:  The state of the art is so desperately poor that it should be easy for the research community to make an impact!", "Feedback  The authors of the paper would love your feedback.", "Do you agree that classical software quality techniques such as regression testing and root cause analysis do not extend to distributed systems in their current form?", "Can LDFI serve as a bridge between verification and testing, as explored in this real-world application?", "What other tools should the team be building, and what impact could the LDFI approach have on them?"], "summary_text": "Growing a protocol Ramasubramanian et al., HotCloud’17  I’ve been really enjoying working my way through a selection of the HotCloud papers – they’re relatively short, thought-provoking, and designed to promote discussion (each paper has a set of discussion questions at the very end – great if you’re looking at them in a group of some kind). Today’s paper is certainly no exception. The setting is a collaboration between Elastic and Peter Alvaro’s research group at the University of California. The question under investigation is how best to implement and evolve distributed system protocols without introducing unintended bugs. In this paper, we argue that tool support for implementing and evolving fault-tolerant distributed systems needs to be rethought. We advocate exploration of the (sparse) middle ground between existing testing techniques practically inadequate for addressing fault tolerance concerns and traditional verification techniques ill-suited to the continual evolution of real-world evaluations. The status quo  If you want to solve a distributed systems problem these days, you can generally start by picking up a proven protocol for the issue at hand. For example, Paxos, Raft, Primary/Backup, Chain Replication, reliable broadcast, and so on. So we might naively expect that…  …modern system designers can merely take mechanisms “off the shelf” and enjoy the guarantees of hardened subsystems while constructing otherwise novel applications. Any practitioner, however, will quickly identify this as a fallacy. Even initial protocol implementations tend to differ significantly from their specification. (A sentiment echoed in the wonderful ‘ Use of Formal Methods at Amazon Web Services ’ paper). But it’s really the ‘little’ changes over the lifetime of the system, the protocol optimisations over time, that can have outsize unintended consequences. We end up with a system that is, for example, “essentially Primary/Backup.” And while it may have the essence of Primary/Backup, does it retain the guarantees? It’s very hard to know. Such a circumstance places implementors in the bad position of deriving false confidence from assertions that their implementation is “essentially Primary/Backup”. What we normally rely on to ensure that changes to our system don’t introduce new bugs is regression testing: “regression testing techniques ensure future optimisations do not re-introduce bugs previously encountered in early stages of system development.” In regression testing we check that inputs known to result in bad behaviour in the past no longer do. In the ideal case, the team would be using some kind of formal verification, and specifications would be co-evolved with the code and invariants proved afresh. In practice, this rarely happens. The correctness guarantees determined at initial verification time erode as protocols evolve. The authors point out that regression testing alone is not sufficient to assert fault tolerance properties. Inputs that trigger bugs in one version of a protocol are not guaranteed to trigger the same bug in a different version. In distributed systems, a large class of bugs are tied to the execution schedule, not (just) to the inputs. As a result, regression testing, as we currently employ it, is fundamentally too weak to prevent fault tolerance regression bugs. Root cause analysis is similarly inadequate, because a set of faults triggering bugs in later versions may fail to do so in an earlier version. This difference between input-based and schedule-based approaches necessitates the use of a different approach for fault tolerance testing and verification. We are left wanting something that works like verification, but feels like testing. Elastic meets LDFI  The team at Elastic faced a problem like the one we just described. They had a data replication protocol based on Primary/Backup, and were looking to introduce a faster variant that could synchronise individual operations rather than relying on file copying. The Elastic team then made a couple of really smart moves:  “Since this was a new algorithm, Elastic was looking for ways to formally verify it.”  “Elastic engaged our research team because they wanted a technique that strikes a balance between formal verification and testing – in particular the strong correctness guarantees of the former and the agility of the latter.”  The project used Lineage Driven Fault Injection (LDFI), which builds a model based on good system execution and explores only fault scenarios capable of forcing the system into a bad state. (See the earlier write-up on The Morning Paper for more details). We implemented a sequence of versions of the replication protocol and used LDFI to incrementally verify them as part of continuous integration. LDFI helped to find existing bugs relating to the implementation of the new protocol, and also demonstrated the safety (or otherwise!) of a couple of subsequent optimisations. There are many instances in the software development cycle for a bug to be introduced, the first of which is when a protocol specification is converted to an implementation. During our case study, we found a bug which manifested precisely from such a transaction scenario. (The bug related to a primary failover with partially replicated writes in progress, see section 4.1 for details). LDFI generated the scenario automatically, rather than requiring testers to have the foresight to manually derive such test cases. Originally discovered in the context of concurrent writes, subsequent analysis showed that with the right inputs a variation can occur with just a single write. … the two bugs are similar, but do not manifest from the same fault scenarios. This reinforces the claim from our motivating example that techniques such as root cause analysis as they are generally deployed would not be effective in reasoning about the fault tolerance properties of distributed systems. A seemingly minor optimisation was explored to avoid extra processing: requests have monotonically increasing sequence numbers, and if a sequence number associated with a write request had been seen before, instead of processing it (again? ), the payload should be dropped and the request simply acknowledged. It turns out there’s a problem with this scheme that can occur during primary failover:  Fortunately, LDFI quickly and automatically discovers such a scenario by using the initial successful execution to test fault scenarios that may cause failures… Optimization carries the risk of introducing entirely new bugs capable of breaking the end-to-end properties of the system, which is best handled by verification-based tools. A second, substantially more complicated optimisation turned out not to produce any counterexamples when run against LDFI. Seemingly simple optimisations may break guarantees, while more complex ones may not – simplicity is not a guarantee of correctness. Improving distributed software quality  Our experience at Elastic suggests approaches like LDFI are a step towards improving the state of the art in distributed software quality. Where can we go from here? The team set out a number of future directions:  Integration with semantic-aware software model checkers: “An ideal tool solution would combine the best features of LDFI (which automatically builds models of domain knowledge, but ignore concurrency and asynchrony) with state-of-the-art combined approaches such as SAMC , since we know from Fischer et al. (FLP) that some of the most fundamental difficulties of distributed systems exist at the intersection of partial failure and asynchrony!”  Exploiting the CALM theorem : “we are developing a prototype system that combines the Lineage-Driven approach (utilizing explanations of what went right to reason about what could go wrong) and CALM analysis (using static analysis to prove commutativity of message processing logic) to simultaneously prune the space of faults and re-orderings.”  Using probabilistic system models to embrace rather than mask the inherent uncertainties in distributed executions. Improving the debugging experience using provenance to reason about distributed executions (“a young research area capable of radical growth”)  Our experience using LDFI at Elastic suggests the provision of high-level explanations of how a system achieves (or fails to achieve) good outcomes are a good starting point for taming the complexity of distributed debugging. There is wide scope for improving tools for implementing, evolving, and debugging distributed software:  The state of the art is so desperately poor that it should be easy for the research community to make an impact! Feedback  The authors of the paper would love your feedback. Do you agree that classical software quality techniques such as regression testing and root cause analysis do not extend to distributed systems in their current form? Can LDFI serve as a bridge between verification and testing, as explored in this real-world application? What other tools should the team be building, and what impact could the LDFI approach have on them?", "pdf_url": "https://www.usenix.org/system/files/conference/hotcloud17/hotcloud17-paper-ramasubramanian.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/growing-a-protocol.json"}
{"id": "21145042", "bin": "1500_1600", "summary_sentences": ["Dremel: interactive analysis of web-scale datasets – Melnik et al. (Google), 2010.", "Dremel is Google’s interactive ad-hoc query system that can run aggregate queries over trillions of rows in seconds.", "It scales to thousands of CPUs, and petabytes of data.", "It was also the inspiration for Apache Drill.", "Dremel borrows the idea of serving trees from web search (pushing a query down a tree hierarchy, rewriting it at each level and aggregating the results on the way back up).", "It uses a SQL-like language for query, and it uses a column-striped storage representation.", "It also supports a nested data model – Google’s Protocol Buffers  Column stores have been adopted for analyzing relational data [1] but to the best of our knowledge have not been extended to nested data models.", "The columnar storage format that we present is supported by many data processing tools at Google, including MR, Sawzall, and FlumeJava.", "The way in which nested data structures are split into columns, and then re-assembled on demand in response to queries is central to Dremel, so let’s explore how that works.", "Here’s a sample Protocol Buffer schema for a Document entity:  message Document {     required int64 DocId;     optional group Links {         repeated int64 Backward;         repeated int64 Forward;     }     repeated group Name {         repeated group Language {             required string Code;             optional string Country;         }         optional String Url;     } }  Notice a few things about this: there are repeated and optional components, and there is nesting.", "The first part of splitting this into columns is pretty straight-forward: make a column for each field, using the nested path names.", "So, for the schema above we have columns DocId, Links.Backward, Links.Forward, Name.Language.Code, Name.Language.Country, and Name.Url.", "Focusing in on the Name.Language.Code column, we’re going to take the Code entries from multiple ‘rows’ (Documents) and put them all into this column.", "The first problem we have to solve comes from the fact that ‘Code’ can be repeated several times within the same Document.", "In the ‘DocId’ column, each entry  represents a new Document, but in the Name.Language.Code column we need a way to know whether a given entry is a repeated entry from the current Document, or the start of a new Document.", "And if it is repeated,  where does it belong in the nesting structure?", "Furthermore, given the fact that some fields are optional (may be missing), we’re going to need a way to take that into account too.", "Dremel solves these problems by keeping three  pieces of data for every column entry: the value itself, a repetition level, and a definition level.", "How it works is pretty subtle to wrap your head around, but I’ll try to explain it as clearly as possible.", "Take a good look at the sketch below from my notebook.", "It shows a Document record that we want to split into columns, and to the right, the column entries that result within the Name.Language.Code column – where r represents the repetition level, and d the definition level.", "The first problem we mentioned was how to tell whether an entry is the start of a new Document, or another entry for the same column within the current Document.", "That’s an easy one to solve: the repetition level is set to 0 for the first occurence of a field within a record.", "Hence ‘en-us,’ which is the first Code within the document, is encoded with repetition level 0.", "You might intuitively think that we’d simply increment the repitition level for each occurence (and hence the ‘en’ entry would have repetition level 1, and ‘en-gb’ repetitionlevel 2).", "But that’s not actually how it works.", "Notice that if we did that, we couldn’t distinguish between a Code that is in a repeated Language element of a given Name (‘en’ in our example), and a Code that is in a new Name element (the ‘en-gb’ case).", "So for all repeated column entries in a record after the first one, the repetition level value that is stored is instead the ‘level’ at which we’re repeating.", "For the nesting Name.Language.Code, Name is level 1, Language is level 2, and Code is level 3.", "Still with me?", "Good, so, when we come to encode the ‘en’ value in the column, we give it repetition level 2, because it is inside a replicated Language element.", "And when we come to encode the ‘en-gb’ value, we give it repetition level 1, because Name is the first level at which we’re repeating at this point in the record.", "And that NULL value you see in the column?", "That’s there to record the fact that the second Name element doesn’t include a Language.Code value at all.", "It’s at repetition level 1 because the ‘Name’ element is the level we’re repeating at.", "Now let’s talk about the definition level value, d. Intuitively you might think this is just the nesting level in the schema (so 1 for DocId,  2 for Links.Forward, 3 for Name.Language.Code etc.)", "– but again that’s not quite what it represents (and in fact, if we had access to the schema, it would be redundant to store that information with every column entry of course).", "Instead, the definition level indicates how many of the parent fields are actually defined.", "This is easier to understand by example.", "For the ‘en-us’ Code entry, it’s within a Language field, within a Name field – so this gets definition level 2.", "The same is true for the ‘en’ and ‘en-gb’ entries.", "For the NULL entry though, the enclosing ‘Name’ is present, but there is no ‘Language’ component at all.", "Therefore this gets definition level 1.", "It turns out that by encoding these repitition and definition levels alongside the column value, it is possible to split records into columns, and subsequently re-assemble them efficiently.", "The algorithms for doing this are given in an appendix to the paper.", "Record assembly is pretty neat – for the subset of the fields the query is interested in, a Finite State Machine is generated with state transitions triggered by changes in repetition level.", "Let’s cut to the chase now and look at what Google learned building and operating Dremel:  Scan-based queries can be executed at interactive speeds on disk-resident datasets of up to a trillion records.", "Near-linear scalability in the number of columns and servers is achievable for systems containing thousands of nodes.", "MR can benefit from columnar storage just like a DBMS.", "Record assembly and parsing are expensive.", "Software layers (beyond the query processing layer) need to be optimized to directly consume column-oriented data.", "MR and query processing can be used in a complementary fashion; one layer’s output can feed another’s input.", "In a multi-user environment, a larger system can benefit from economies of scale while offering a qualitatively better user experience.", "(Splitting the work into more parallel pieces reduced overall response time, without causing more underlying resource, e.g. CPU,  consumption)  If trading speed against accuracy is acceptable, a query can be terminated much earlier and yet see most of the data.", "The bulk of a web-scale dataset can be scanned fast.", "Getting to the last few percent within tight time bounds is hard.", "The last two points relate to the problems we looked at in The Tail at Scale – the outliers are always slower, and at enough scale you’re going to get tripped up by that.", "Dremel allows you to specify the percentage of data processed at which you’re happy to stop a query and return results.", "It sounds odd to say you want the results of a query without looking at all of the data – but consider for example a top-k query.", "Looking at 98% of the data makes it highly likely you’ll get the right answer,  and cutting off the slow tail can give a response in under a minute as opposed to several minutes just waiting for that last 2%."], "summary_text": "Dremel: interactive analysis of web-scale datasets – Melnik et al. (Google), 2010. Dremel is Google’s interactive ad-hoc query system that can run aggregate queries over trillions of rows in seconds. It scales to thousands of CPUs, and petabytes of data. It was also the inspiration for Apache Drill. Dremel borrows the idea of serving trees from web search (pushing a query down a tree hierarchy, rewriting it at each level and aggregating the results on the way back up). It uses a SQL-like language for query, and it uses a column-striped storage representation. It also supports a nested data model – Google’s Protocol Buffers  Column stores have been adopted for analyzing relational data [1] but to the best of our knowledge have not been extended to nested data models. The columnar storage format that we present is supported by many data processing tools at Google, including MR, Sawzall, and FlumeJava. The way in which nested data structures are split into columns, and then re-assembled on demand in response to queries is central to Dremel, so let’s explore how that works. Here’s a sample Protocol Buffer schema for a Document entity:  message Document {     required int64 DocId;     optional group Links {         repeated int64 Backward;         repeated int64 Forward;     }     repeated group Name {         repeated group Language {             required string Code;             optional string Country;         }         optional String Url;     } }  Notice a few things about this: there are repeated and optional components, and there is nesting. The first part of splitting this into columns is pretty straight-forward: make a column for each field, using the nested path names. So, for the schema above we have columns DocId, Links.Backward, Links.Forward, Name.Language.Code, Name.Language.Country, and Name.Url. Focusing in on the Name.Language.Code column, we’re going to take the Code entries from multiple ‘rows’ (Documents) and put them all into this column. The first problem we have to solve comes from the fact that ‘Code’ can be repeated several times within the same Document. In the ‘DocId’ column, each entry  represents a new Document, but in the Name.Language.Code column we need a way to know whether a given entry is a repeated entry from the current Document, or the start of a new Document. And if it is repeated,  where does it belong in the nesting structure? Furthermore, given the fact that some fields are optional (may be missing), we’re going to need a way to take that into account too. Dremel solves these problems by keeping three  pieces of data for every column entry: the value itself, a repetition level, and a definition level. How it works is pretty subtle to wrap your head around, but I’ll try to explain it as clearly as possible. Take a good look at the sketch below from my notebook. It shows a Document record that we want to split into columns, and to the right, the column entries that result within the Name.Language.Code column – where r represents the repetition level, and d the definition level. The first problem we mentioned was how to tell whether an entry is the start of a new Document, or another entry for the same column within the current Document. That’s an easy one to solve: the repetition level is set to 0 for the first occurence of a field within a record. Hence ‘en-us,’ which is the first Code within the document, is encoded with repetition level 0. You might intuitively think that we’d simply increment the repitition level for each occurence (and hence the ‘en’ entry would have repetition level 1, and ‘en-gb’ repetitionlevel 2). But that’s not actually how it works. Notice that if we did that, we couldn’t distinguish between a Code that is in a repeated Language element of a given Name (‘en’ in our example), and a Code that is in a new Name element (the ‘en-gb’ case). So for all repeated column entries in a record after the first one, the repetition level value that is stored is instead the ‘level’ at which we’re repeating. For the nesting Name.Language.Code, Name is level 1, Language is level 2, and Code is level 3. Still with me? Good, so, when we come to encode the ‘en’ value in the column, we give it repetition level 2, because it is inside a replicated Language element. And when we come to encode the ‘en-gb’ value, we give it repetition level 1, because Name is the first level at which we’re repeating at this point in the record. And that NULL value you see in the column? That’s there to record the fact that the second Name element doesn’t include a Language.Code value at all. It’s at repetition level 1 because the ‘Name’ element is the level we’re repeating at. Now let’s talk about the definition level value, d. Intuitively you might think this is just the nesting level in the schema (so 1 for DocId,  2 for Links.Forward, 3 for Name.Language.Code etc.) – but again that’s not quite what it represents (and in fact, if we had access to the schema, it would be redundant to store that information with every column entry of course). Instead, the definition level indicates how many of the parent fields are actually defined. This is easier to understand by example. For the ‘en-us’ Code entry, it’s within a Language field, within a Name field – so this gets definition level 2. The same is true for the ‘en’ and ‘en-gb’ entries. For the NULL entry though, the enclosing ‘Name’ is present, but there is no ‘Language’ component at all. Therefore this gets definition level 1. It turns out that by encoding these repitition and definition levels alongside the column value, it is possible to split records into columns, and subsequently re-assemble them efficiently. The algorithms for doing this are given in an appendix to the paper. Record assembly is pretty neat – for the subset of the fields the query is interested in, a Finite State Machine is generated with state transitions triggered by changes in repetition level. Let’s cut to the chase now and look at what Google learned building and operating Dremel:  Scan-based queries can be executed at interactive speeds on disk-resident datasets of up to a trillion records. Near-linear scalability in the number of columns and servers is achievable for systems containing thousands of nodes. MR can benefit from columnar storage just like a DBMS. Record assembly and parsing are expensive. Software layers (beyond the query processing layer) need to be optimized to directly consume column-oriented data. MR and query processing can be used in a complementary fashion; one layer’s output can feed another’s input. In a multi-user environment, a larger system can benefit from economies of scale while offering a qualitatively better user experience. (Splitting the work into more parallel pieces reduced overall response time, without causing more underlying resource, e.g. CPU,  consumption)  If trading speed against accuracy is acceptable, a query can be terminated much earlier and yet see most of the data. The bulk of a web-scale dataset can be scanned fast. Getting to the last few percent within tight time bounds is hard. The last two points relate to the problems we looked at in The Tail at Scale – the outliers are always slower, and at enough scale you’re going to get tripped up by that. Dremel allows you to specify the percentage of data processed at which you’re happy to stop a query and return results. It sounds odd to say you want the results of a query without looking at all of the data – but consider for example a top-k query. Looking at 98% of the data makes it highly likely you’ll get the right answer,  and cutting off the slow tail can give a response in under a minute as opposed to several minutes just waiting for that last 2%.", "pdf_url": "http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/dremel-interactive-analysis-of-web-scale-datasets.json"}
{"id": "99384474", "bin": "1500_1600", "summary_sentences": ["Understanding lifecycle management complexity of datacenter topologies Zhang et al., NSDI’19  There has been plenty of interesting research on network topologies for datacenters, with Clos-like tree topologies and Expander based graph topologies both shown to scale using widely deployed hardware.", "This research tends to focus on performance properties such as throughput and latency, together with resilience to failures.", "Important as these are, note that they’re also what’s right in front of you as a designer, and relatively easy to measure.", "The great thing about today’s paper is that the authors look beneath the surface to consider the less visible but still very important “lifecycle management” implications of topology design.", "In networking, this translates into how easy it is to physically deploy the network, and how easy it to subsequently expand.", "They find a way to quantify the associated lifecycle management costs, and then use this to help drive the design of a new class of topologies, called FatClique.", "… we show that existing topology classes have low lifecycle management complexity by some measures, but not by others.", "Motivated by this, we design a new class of topologies, FatClique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics.", "Now, there’s probably only a relatively small subset of The Morning Paper readers involved in designing and deploying datacenter network topologies.", "So my challenge to you as you read through this paper, is to think about where the hidden complexity and costs are in your own systems.", "Would you do things differently if these were made more visible?", "It would be great to see more emphasis for example on things like developer experience (DX) and operational simplicity – in my experience these kinds of attributes can have an outsize impact on the long-term success of a system.", "Anyway, let’s get back to cables and switches…  Physically deploying network topologies  When it comes to laying out a network topology for real in a datacenter, you need to think about packaging, placement, and bundling.", "Packaging is how you group things together, e.g. the arrangement of switches in racks, and placement concerns how these racks are physically placed on the datacenter floor.", "Placement in turn determines the kinds of cabling you need, and for optical cables the power of the transceivers.", "Within a rack we might package several connected switches into a single chassis using a backplane.", "At the other end of the scale, blocks are larger units of co-placement and packaging that combine several racks.", "With all those connections, it makes things a lot easier to group together multiple fibres all connecting the same two endpoints (racks) into bundles, which contain a fixed number of identical length fibres.", "Manufacturing bundles is simpler than manufacturing individual fibres, and handling such bundles significantly simplifies operational complexity.", "Patch panels make bundling easier by providing a convenient aggregation point to create and route bundles.", "Bundles and fibres are physically routed through the datacenter on cable trays.", "The trays themselves have capacity constraints of course.", "Here’s an example of a logical Clos topology and its physical instantiation:  The authors identify three key metrics that together capture much of the deployment complexity in a topology:  The number of switches.", "More switches equals more packaging complexity.", "The number of patch panels, which is a function of topological structure and a good proxy for wiring complexity.", "The number of bundle types.", "This metric captures the other important part of wiring complexity – how many distinct bundle types are needed.", "A bundle type is represented by its capacity (how how many fibres) and its length.", "These complexity measures are complete.", "The number of cable trays, the design of the chassis, and the number of racks can be derived from the number of switches (and the number of servers and the datacenter floor dimensions, which are inputs to the topology design).", "The number of cables and transceivers can be derived from the number of patch panels.", "Here’s how Clos and Expander (Jellyfish) representative topologies for the same number of servers stack up against these metrics:  The expander graph topology shows much higher deployment complexity in terms of the number of bundle types.", "Clos also exposes far fewer ports outside of a rack (it has better port hiding).", "Expanding existing networks  When you want to expand an existing network first you need to buy all the new gear and lay it out on the datacenter floor, and then you can begin a re-wiring process.", "This is all going on with live traffic flowing, so expansion is carried out in steps.", "During each step the capacity of the topology is guaranteed to be at least some percentage of the existing topology capacity.", "The percentage is sometimes known as the expansion SLO.", "During a step existing links to be re-wired are drained, then human operators physical rewire links at patch panels.", "The new links are tested and then undrained (strange word!", "), i.e., brought into service.", "For example, here’s a logical expansion (top row) and its physical realisation:  The most time-consuming part of all this is the physical rewiring.", "The two metrics that capture expansion complexity are therefore:  The number of expansion steps, and  The average number of rewired links in a patch panel rack.", "Here’s how Clos and Expander stack up on those metrics for the same networks we saw earlier:  This time the victory goes to Expander (Jellyfish).", "Jellyfish has a much higher north-to-south capacity ratio.", "Northbound links exit a block, and southbound links are to/from servers within a block.", "“Fat edges” have more northbound than southbound links, and the extra capacity means you can accomplish more movement in each step.", "Clos topologies re-wire more links in each patch panel during an expansion step and require many steps because they have a low north-south capacity ratio.", "Enter the FatClique  Inspired by these insights, the authors define a new class of topologies called FatClique, which combine the hierarchical structure of Clos with the edge expansion capabilities of expander graphs.", "There are three levels in the hierarchy.", "A clique of switches form a sub-block.", "Cliques of sub-blocks come together to form blocks.", "And cliques of blocks come together to from the full FatClique topology.", "Four key design variables determine the particular instantiation of a FatClique topology: the number of ports in a switch that connect to other servers; the number of ports in a switch that connect to other sub-blocks in a block; the number of switches in a sub-block; and the number of sub-blocks in a block.", "A synthesis algorithm  takes a set of six input constraints (see §5.1) and determines the values for these four design variables.", "There is plenty more detail in section 5 of the paper which I don’t have the space to do justice too here.", "FatClique vs Clos vs Expander  The evaluation compares FatClique to Clos, Xpander, and Jellyfish at different network topology sizes, as shown in the table below.", "( Enlarge )  Here’s how they stack up against the complexity metrics:  Number of switches  Number of patch panels  Number of bundle types  and associated cabling costs:  Number of expansion steps  Average number of rewired links  We find that FatClique is the best at most scales by all our complexity metrics.", "(The one exception is that at small and medium scales, Clos has slightly fewer patch panels).", "It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we were able to derive from published cable prices).", "Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take 5x longer to expand the topology, and each step of Clos expansion can take longer than FatClique because the number of links to be rewired at each step per patch panel can be 30-50% higher.", "The one thing I couldn’t find in the evaluation is any data to back up the opening claim that FatClique achieves all of this “while being performance-equivalent to existing topologies.”  The last word  As the management complexity of networks increases, the importance of designing for manageability will increase in the coming years.", "Our paper is only a first step in this direction…"], "summary_text": "Understanding lifecycle management complexity of datacenter topologies Zhang et al., NSDI’19  There has been plenty of interesting research on network topologies for datacenters, with Clos-like tree topologies and Expander based graph topologies both shown to scale using widely deployed hardware. This research tends to focus on performance properties such as throughput and latency, together with resilience to failures. Important as these are, note that they’re also what’s right in front of you as a designer, and relatively easy to measure. The great thing about today’s paper is that the authors look beneath the surface to consider the less visible but still very important “lifecycle management” implications of topology design. In networking, this translates into how easy it is to physically deploy the network, and how easy it to subsequently expand. They find a way to quantify the associated lifecycle management costs, and then use this to help drive the design of a new class of topologies, called FatClique. … we show that existing topology classes have low lifecycle management complexity by some measures, but not by others. Motivated by this, we design a new class of topologies, FatClique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics. Now, there’s probably only a relatively small subset of The Morning Paper readers involved in designing and deploying datacenter network topologies. So my challenge to you as you read through this paper, is to think about where the hidden complexity and costs are in your own systems. Would you do things differently if these were made more visible? It would be great to see more emphasis for example on things like developer experience (DX) and operational simplicity – in my experience these kinds of attributes can have an outsize impact on the long-term success of a system. Anyway, let’s get back to cables and switches…  Physically deploying network topologies  When it comes to laying out a network topology for real in a datacenter, you need to think about packaging, placement, and bundling. Packaging is how you group things together, e.g. the arrangement of switches in racks, and placement concerns how these racks are physically placed on the datacenter floor. Placement in turn determines the kinds of cabling you need, and for optical cables the power of the transceivers. Within a rack we might package several connected switches into a single chassis using a backplane. At the other end of the scale, blocks are larger units of co-placement and packaging that combine several racks. With all those connections, it makes things a lot easier to group together multiple fibres all connecting the same two endpoints (racks) into bundles, which contain a fixed number of identical length fibres. Manufacturing bundles is simpler than manufacturing individual fibres, and handling such bundles significantly simplifies operational complexity. Patch panels make bundling easier by providing a convenient aggregation point to create and route bundles. Bundles and fibres are physically routed through the datacenter on cable trays. The trays themselves have capacity constraints of course. Here’s an example of a logical Clos topology and its physical instantiation:  The authors identify three key metrics that together capture much of the deployment complexity in a topology:  The number of switches. More switches equals more packaging complexity. The number of patch panels, which is a function of topological structure and a good proxy for wiring complexity. The number of bundle types. This metric captures the other important part of wiring complexity – how many distinct bundle types are needed. A bundle type is represented by its capacity (how how many fibres) and its length. These complexity measures are complete. The number of cable trays, the design of the chassis, and the number of racks can be derived from the number of switches (and the number of servers and the datacenter floor dimensions, which are inputs to the topology design). The number of cables and transceivers can be derived from the number of patch panels. Here’s how Clos and Expander (Jellyfish) representative topologies for the same number of servers stack up against these metrics:  The expander graph topology shows much higher deployment complexity in terms of the number of bundle types. Clos also exposes far fewer ports outside of a rack (it has better port hiding). Expanding existing networks  When you want to expand an existing network first you need to buy all the new gear and lay it out on the datacenter floor, and then you can begin a re-wiring process. This is all going on with live traffic flowing, so expansion is carried out in steps. During each step the capacity of the topology is guaranteed to be at least some percentage of the existing topology capacity. The percentage is sometimes known as the expansion SLO. During a step existing links to be re-wired are drained, then human operators physical rewire links at patch panels. The new links are tested and then undrained (strange word! ), i.e., brought into service. For example, here’s a logical expansion (top row) and its physical realisation:  The most time-consuming part of all this is the physical rewiring. The two metrics that capture expansion complexity are therefore:  The number of expansion steps, and  The average number of rewired links in a patch panel rack. Here’s how Clos and Expander stack up on those metrics for the same networks we saw earlier:  This time the victory goes to Expander (Jellyfish). Jellyfish has a much higher north-to-south capacity ratio. Northbound links exit a block, and southbound links are to/from servers within a block. “Fat edges” have more northbound than southbound links, and the extra capacity means you can accomplish more movement in each step. Clos topologies re-wire more links in each patch panel during an expansion step and require many steps because they have a low north-south capacity ratio. Enter the FatClique  Inspired by these insights, the authors define a new class of topologies called FatClique, which combine the hierarchical structure of Clos with the edge expansion capabilities of expander graphs. There are three levels in the hierarchy. A clique of switches form a sub-block. Cliques of sub-blocks come together to form blocks. And cliques of blocks come together to from the full FatClique topology. Four key design variables determine the particular instantiation of a FatClique topology: the number of ports in a switch that connect to other servers; the number of ports in a switch that connect to other sub-blocks in a block; the number of switches in a sub-block; and the number of sub-blocks in a block. A synthesis algorithm  takes a set of six input constraints (see §5.1) and determines the values for these four design variables. There is plenty more detail in section 5 of the paper which I don’t have the space to do justice too here. FatClique vs Clos vs Expander  The evaluation compares FatClique to Clos, Xpander, and Jellyfish at different network topology sizes, as shown in the table below. ( Enlarge )  Here’s how they stack up against the complexity metrics:  Number of switches  Number of patch panels  Number of bundle types  and associated cabling costs:  Number of expansion steps  Average number of rewired links  We find that FatClique is the best at most scales by all our complexity metrics. (The one exception is that at small and medium scales, Clos has slightly fewer patch panels). It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we were able to derive from published cable prices). Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take 5x longer to expand the topology, and each step of Clos expansion can take longer than FatClique because the number of links to be rewired at each step per patch panel can be 30-50% higher. The one thing I couldn’t find in the evaluation is any data to back up the opening claim that FatClique achieves all of this “while being performance-equivalent to existing topologies.”  The last word  As the management complexity of networks increases, the importance of designing for manageability will increase in the coming years. Our paper is only a first step in this direction…", "pdf_url": "https://www.usenix.org/system/files/nsdi19-zhang.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/understanding-lifecycle-management-complexity-of-datacenter-topologies.json"}
{"id": "39038842", "bin": "1500_1600", "summary_sentences": ["Improving user perceived page load time using gaze Kelton, Ryoo, et al., NSDI 2017  I feel like I’m stretching things a little bit including this paper in an IoT flavoured week, but it does use at least bridge from the physical world to the virtual, if only via a webcam.", "What’s really interesting here to me is what the paper teaches us about web page load performance.", "We know that faster loading pages are correlated with all sorts of user engagement improvements, but what exactly is a faster loading page?", "If we want faster loading pages because they drive better user engagement, then the ideal page load time metric should correspond with the way that a user perceives page load times.", "The most popular page load metrics, OnLoad, and Speed Index have the advantage of being easy to measure, but they it turns out they’re not always a good match with the user-perceived page load time, uPLT.", "The OnLoad PLT metric measures the time taken for the browser OnLoad event to be fired (i.e., once all objects on the page are loaded).", "OnLoad tends to over-estimate page load time because users are often only interested in content ‘above the fold’ (visible without scrolling) when first waiting for a page to load.", "(Be warned though, with some page structures it can also under-estimate).", "The SpeedIndex PLT metric or Above Fold Time (AFT) measures the average time for all above-the-fold content to appear on the screen.", "“It is estimated by first calculating the visual completeness of a page, defined as the pixel distance between the current frame and the ‘last’ frame of the Web page.", "The last frame is when the Web page content no longer changes.", "Speed Index is the weighted average of visual completeness over time.", "The Speed Index value is lower (and better) if the browser shows more visual content earlier.” The issue with Speed Index is that it doesn’t take into account the relative importance of content.", "The authors conducted a study across 45 web pages and 100 different users using videos of pages loading to ensure that each user saw an identical experience for each page.", "Additional studies were conducted with simulations of different network speeds.", "The users were asked to press a key on the keyboard when they perceived the page to be loaded (see section 3 in the paper for full details of the study setup).", "Here’s an example of how the different metrics look for the energystar.gov site:  And here are the overall results across all 45 web pages and 100 users:  OnLoad tends to either over-estimate (on average by 6.4 seconds) or under-estimate (by 2.1 seconds on average) when compared to true uPLT.", "Pages that are heavy in Javascript and/or images tend to have even larger OnLoad time gaps.", "Speed Index is about 3.5 seconds lower than uPLT for 87% of Web pages.", "Using gaze to improve page load times  Having understood that neither OnLoad nor the Speed Index metric is a good indicator of the true user-perceived page load time, the authors turn their attention to figuring out what to focus on in order to reduce perceived page load times.", "Intuitively, what a user is looking at (visual attention) should tell us what is important on the page.", "We can track this using gaze tracking software…  Recently, advances in computer vision and machine learning have enabled low cost gaze tracking.", "The low cost trackers do not require custom hardware and take into account facial features, user movements, a user’s distance from the screen, and other user differences.", "The study uses an off-the-shelf webcam based gaze tracker called GazePointer .", "A 50-user study is conducted using the GazePointer setup, across 45 web pages.", "An auxiliary study also used a much more expensive custom gaze tracker and confirmed that the results concur with the webcam-based solution.", "Each web page is divided into a set of regions, and the study tracks the regions associated with a users fixation points (i.e., when the user is focusing on something).", "For example, here are the visual regions for fcc.gov:  Here’s a heat map across the 45 websites, showing where the attention budget is spent.", "For example, when looking at the first web site we see that 5 regions combined are fixated on by 90% of users, whereas the remaining 75% of regions are fixated on by less than half of the users.", "In general, we find that across the Web pages, at least 20% of the regions have a collective fixation of 0.9 or more.", "We also find that on average, 25% of the regions have a collective fixation of less than 0.3, i.e., 25% of regions are viewed by less than 30% of the users.", "This leads to the following hypothesis: prioritising loading the parts of a web page that hold users attention should result in faster perceived page load times.", "The WebGaze system collects gaze feedback from a subset of users as they browse, and uses the gaze feedback to determine which Web objects to prioritise during page load.", "(Good luck getting many people to opt-in to having their gaze tracked by webcam while they’re browsing though!", "Let’s just assume that getting sufficient gaze feedback is possible – e.g., from internal user testing).", "To identify which Web objects to prioritize, we use a simple heuristic: if a region has a collective fixation of over a prioritization threshold, then the objects in the region will be prioritized.", "In our evaluation, we set the prioritization threshold to be 0.7.", "Each visual region may have multiple objects.", "The CSS bounding rectangles for all objects visible in the viewport can be obtained via the DOM.", "An object is said to be in a given region if its bounding rectangle intersects with the region, when an object belongs to multiple regions, it is assigned the priority of the highest priority of those regions.", "Having found the visible Web objects to be prioritised, the next task is to extend the set of prioritised objects to include any other objects they may depend on.", "The WProf tool is used to extract dependencies.", "“While the contents of sites are dynamic, the dependency information has been shown to be temporally stable.", "Thus, dependencies can be gathered offline.”  To actually implement prioritization, WebGaze uses HTTP/2’s Server Push functionality.", "Server Push decouples the traditional browser architecture in which Web objects are fetched in the order in which the browser parses the page.", "Instead, Server Push allows the server to preemptively push objects to the browser, even when the browser did not explicitly request these objects.", "Server Push helps (i) by avoiding a round trip required to fetch an object, (ii) by breaking dependencies between client side parsing and network fetching, and (iii) by better leveraging HTTP/2’s multiplexing.", "In some pathological cases, Server Push can make things much worse!", "WebGaze reverts back to the default case without optimization when this is detected.", "This happened with 2 of the 45 pages in the study.", "Does gaze-based prioritisation actually improve perceived user page load times though?", "An evaluation compared WebGaze with three alternative strategies:  Default: the page loads as-is without prioritisation  Push-all: all of the objects on the Web page are pushed using Server Push  Klotski: the Klotksi algorithm is used to to push objects and dependencies with an objective of maximising the amount of above-the-fold content that can be delivered within 5 seconds.", "Figure 11 (below) shows the CDF of the percentage improvement in uPLT compared to alternatives.", "On an average, WebGaze improves uPLT 17%, 12%, and 9% over Default, Push-All, and Klotski respectively.", "At the 95% percentile, WebGaze improves uPLT by 64%, 44%, and 39% compared to Default, Push-All, and Klotski respectively.", "In about 10% of cases, WebGaze does worse than Klotski.", "In these cases, Klotski is sending less data than WebGaze.", "“This suggests we need to perform more analysis on determining the right amount of data that can be pushed without affecting performance.”  The authors note the significant security and privacy concerns with deploying gaze tracking in the wild.", "If you want to experiment, my recommendation would be to conduct your own (opt-in, in-the-lab) user studies using gaze tracking, and then use the information gleaned to improve Web object prioritisation for production systems."], "summary_text": "Improving user perceived page load time using gaze Kelton, Ryoo, et al., NSDI 2017  I feel like I’m stretching things a little bit including this paper in an IoT flavoured week, but it does use at least bridge from the physical world to the virtual, if only via a webcam. What’s really interesting here to me is what the paper teaches us about web page load performance. We know that faster loading pages are correlated with all sorts of user engagement improvements, but what exactly is a faster loading page? If we want faster loading pages because they drive better user engagement, then the ideal page load time metric should correspond with the way that a user perceives page load times. The most popular page load metrics, OnLoad, and Speed Index have the advantage of being easy to measure, but they it turns out they’re not always a good match with the user-perceived page load time, uPLT. The OnLoad PLT metric measures the time taken for the browser OnLoad event to be fired (i.e., once all objects on the page are loaded). OnLoad tends to over-estimate page load time because users are often only interested in content ‘above the fold’ (visible without scrolling) when first waiting for a page to load. (Be warned though, with some page structures it can also under-estimate). The SpeedIndex PLT metric or Above Fold Time (AFT) measures the average time for all above-the-fold content to appear on the screen. “It is estimated by first calculating the visual completeness of a page, defined as the pixel distance between the current frame and the ‘last’ frame of the Web page. The last frame is when the Web page content no longer changes. Speed Index is the weighted average of visual completeness over time. The Speed Index value is lower (and better) if the browser shows more visual content earlier.” The issue with Speed Index is that it doesn’t take into account the relative importance of content. The authors conducted a study across 45 web pages and 100 different users using videos of pages loading to ensure that each user saw an identical experience for each page. Additional studies were conducted with simulations of different network speeds. The users were asked to press a key on the keyboard when they perceived the page to be loaded (see section 3 in the paper for full details of the study setup). Here’s an example of how the different metrics look for the energystar.gov site:  And here are the overall results across all 45 web pages and 100 users:  OnLoad tends to either over-estimate (on average by 6.4 seconds) or under-estimate (by 2.1 seconds on average) when compared to true uPLT. Pages that are heavy in Javascript and/or images tend to have even larger OnLoad time gaps. Speed Index is about 3.5 seconds lower than uPLT for 87% of Web pages. Using gaze to improve page load times  Having understood that neither OnLoad nor the Speed Index metric is a good indicator of the true user-perceived page load time, the authors turn their attention to figuring out what to focus on in order to reduce perceived page load times. Intuitively, what a user is looking at (visual attention) should tell us what is important on the page. We can track this using gaze tracking software…  Recently, advances in computer vision and machine learning have enabled low cost gaze tracking. The low cost trackers do not require custom hardware and take into account facial features, user movements, a user’s distance from the screen, and other user differences. The study uses an off-the-shelf webcam based gaze tracker called GazePointer . A 50-user study is conducted using the GazePointer setup, across 45 web pages. An auxiliary study also used a much more expensive custom gaze tracker and confirmed that the results concur with the webcam-based solution. Each web page is divided into a set of regions, and the study tracks the regions associated with a users fixation points (i.e., when the user is focusing on something). For example, here are the visual regions for fcc.gov:  Here’s a heat map across the 45 websites, showing where the attention budget is spent. For example, when looking at the first web site we see that 5 regions combined are fixated on by 90% of users, whereas the remaining 75% of regions are fixated on by less than half of the users. In general, we find that across the Web pages, at least 20% of the regions have a collective fixation of 0.9 or more. We also find that on average, 25% of the regions have a collective fixation of less than 0.3, i.e., 25% of regions are viewed by less than 30% of the users. This leads to the following hypothesis: prioritising loading the parts of a web page that hold users attention should result in faster perceived page load times. The WebGaze system collects gaze feedback from a subset of users as they browse, and uses the gaze feedback to determine which Web objects to prioritise during page load. (Good luck getting many people to opt-in to having their gaze tracked by webcam while they’re browsing though! Let’s just assume that getting sufficient gaze feedback is possible – e.g., from internal user testing). To identify which Web objects to prioritize, we use a simple heuristic: if a region has a collective fixation of over a prioritization threshold, then the objects in the region will be prioritized. In our evaluation, we set the prioritization threshold to be 0.7. Each visual region may have multiple objects. The CSS bounding rectangles for all objects visible in the viewport can be obtained via the DOM. An object is said to be in a given region if its bounding rectangle intersects with the region, when an object belongs to multiple regions, it is assigned the priority of the highest priority of those regions. Having found the visible Web objects to be prioritised, the next task is to extend the set of prioritised objects to include any other objects they may depend on. The WProf tool is used to extract dependencies. “While the contents of sites are dynamic, the dependency information has been shown to be temporally stable. Thus, dependencies can be gathered offline.”  To actually implement prioritization, WebGaze uses HTTP/2’s Server Push functionality. Server Push decouples the traditional browser architecture in which Web objects are fetched in the order in which the browser parses the page. Instead, Server Push allows the server to preemptively push objects to the browser, even when the browser did not explicitly request these objects. Server Push helps (i) by avoiding a round trip required to fetch an object, (ii) by breaking dependencies between client side parsing and network fetching, and (iii) by better leveraging HTTP/2’s multiplexing. In some pathological cases, Server Push can make things much worse! WebGaze reverts back to the default case without optimization when this is detected. This happened with 2 of the 45 pages in the study. Does gaze-based prioritisation actually improve perceived user page load times though? An evaluation compared WebGaze with three alternative strategies:  Default: the page loads as-is without prioritisation  Push-all: all of the objects on the Web page are pushed using Server Push  Klotski: the Klotksi algorithm is used to to push objects and dependencies with an objective of maximising the amount of above-the-fold content that can be delivered within 5 seconds. Figure 11 (below) shows the CDF of the percentage improvement in uPLT compared to alternatives. On an average, WebGaze improves uPLT 17%, 12%, and 9% over Default, Push-All, and Klotski respectively. At the 95% percentile, WebGaze improves uPLT by 64%, 44%, and 39% compared to Default, Push-All, and Klotski respectively. In about 10% of cases, WebGaze does worse than Klotski. In these cases, Klotski is sending less data than WebGaze. “This suggests we need to perform more analysis on determining the right amount of data that can be pushed without affecting performance.”  The authors note the significant security and privacy concerns with deploying gaze tracking in the wild. If you want to experiment, my recommendation would be to conduct your own (opt-in, in-the-lab) user studies using gaze tracking, and then use the information gleaned to improve Web object prioritisation for production systems.", "pdf_url": "https://www.usenix.org/system/files/conference/nsdi17/nsdi17-kelton.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/improving-user-perceived-page-load-time-using-gaze.json"}
{"id": "22030520", "bin": "1500_1600", "summary_sentences": ["To distribute or not to distribute?", "Why licensing bugs matter Vendome et al., ICSE’18  Software licensing can quickly get quite complicated, with over 100 known open source licenses out there, and distributions often including components with a mix of licenses.", "Unsurprisingly, developers find it hard to determine appropriate licenses for their work, and to interpret the implications of including third-party software under different licenses.", "We present a large-scale qualitative study aimed at characterizing licensing bugs, with the goal of understanding the types of licensing bugs developers face, their legal and technical implications, and how such bugs are fixed.", "The result is a helpful catalogue of seven different categories of licensing bugs, with 21 sub-categories in total between them.", "Although the authors are not lawyers (as far as I can tell), it still constitutes a very useful list of things to think about.", "“Our proposed catalog can serve as a reference for developers and lawyers dealing with potential licensing issues.”  The catalogue is drawn from an open coding exercise based on a statistically significant sample of 1,200 discussions randomly selected from a population of 59,426 discussions across a collection of issue trackers and mailing lists.", "The mailing lists were Apache’s legal-discuss, Debian’s debian-legal, Fedora’s fedora-legal-list, Gnome’s legal-last and OpenStack’s open-discuss.", "For issue trackers, the authors looked for issues using the keyword license on all 136 Bugzilla issue trackers in the Bugzilla installation list, as well as the issue trackers of 86,032 GitHub projects (selected to try and make sure these were not toy projects).", "Who cares about licensing?", "Before diving into the catalogue itself, it’s worth briefly reviewing the different stakeholders involved in licensing issues: there are holders of IP (e.g. trademark holders, patent holders, copyright holders), lawyers, and lawmakers, and then we can also call out:  Integrators, that reuse open source software within their own systems  Package maintainers, who are responsible for maintaining packages and integrating patches or bug fixes.", "Distributors – any individual or entity distributing software  Developers (in general)  Community – either people involved in a specific open source community, or the open source community as a whole.", "Catalog overview  The taxonomy is composed of 21 distinct sub-categories organised in 7 distinct high-level categories.", "Due to space limitations we only discuss a subset of the sub-categories (14).", "The complete taxonomy description and frequencies of each category can be found in the attached appendix.", "That appendix sounds like a useful resource.", "Unfortunately it’s not included in the only openly hosted version of the paper I could find (on the first author’s personal site, and linked at the top of this post).", "The descriptions we do get are still very useful though.", "It is important to remark that the results discuss the interpretation of developers and/or legal practitioners.", "Therefore it is possible that the legality of these interpretations or discussions may change (e.g., new interpretations can causes new legal precedents in the U.S.A.), on the enforceability may change in different jurisdictions.", "Selected licensing issues explored  Let’s take a brief look inside each of the seven major categories.", "Laws and their interpretations  At the base level, there is confusion over what is copyrightable?", "Software is copyrightable, but higher level designs and ideas may fall out of scope.", "Disagreements on the scope of copyright can lead to difficulties.", "A related issue is understanding what is a derivative work?", "(A work partially owned by the copyright author on which it is originally based).", "“… one of the most important features of open source licenses is that they should allow the creation and redistribution of derivative works.” It’s often unclear whether B should be treated as a derivative work of A, or just something that uses / bundles A.", "For example, Linus Torvalds asserts that merely using the kernel by making system calls does not constitute creating a derivative work.", "There is still plenty of disagreement even on this though.", "This is all further complicated by the fact that copyright, trademark, and patent laws are national in scope.", "Thus we often find clauses relating to choice of jurisdiction.", "… we observed that clauses related to choice of jurisdiction were a controversial topic within Debian in terms of their impact on software’s freeness.", "However, the distribution may be impacted by external factors like trade restrictions to a particular country or distribution of what a country considers sensitive material.", "While organizations or communities may want to facilitate global reuse, the organizations and individuals must comply with these trade laws.", "Policies of the ecosystem  This category concerns issues relating to the licensing policies of specific open source communities such as the Apache Foundation, Eclipse Software Foundation, and Debian.", "These give community guidelines that projects within the foundation are expected to follow.", "For example, projects at Eclipse under the EPL cannot ship external libraries under the LGPL as part of their distribution.", "This makes for more complex user installation procedures if users have to assemble the last mile themselves.", "The FSF has specific guidelines on whether software with various licenses can be combined/derived alongside software licensed under the FSF licences.", "You need to think broader than just source code, images, fonts, databases, text files and so on all need consideration…  Since IP clearance/evaluation extends to all bundled artifacts (not only source code and binaries), a non-free image or font could prevent the distribution of the software.", "Potential license violations  Some licenses are incompatible with each other , and issues can arise when including dependencies or reusing source code that is incompatible with either the declared license, or with the the license of other reused components.", "Generally such an issue impacts the ability to distributed the software.", "As a specific example, Apache License 2.0 is incompatible with GPL v2.", "(See the full list here ).", "Non-source code licensing  When evaluating license compliance, you also need to consider non-source code artefacts, and in particular the need to make the source of those artefacts available.", "In GPL for example, source is defined as “the preferred form of the work for making modifications to it“.", "So if you distributed a PDF of a document, you would also need to distribute the source that generates that PDF.", "Documentation, like source code, is also protected by copyright.", "Even documentation shipped in HTML format has been questioned, since HTML is not the preferred form for making changes.", "Similar issues occur with other media such as fonts, images, and audio.", "An MP3 is likely not the preferred form for editing audio for example.", "Licensing content  A license inconsistency occurs when there is a mismatch between the documented license and the actual source code licensing, e.g. inconsistencies between software licensing an the spec file documenting included licenses.", "Other IP issues  Do you have the rights to use a contribution?", ".", "This is the arena of CLAs (Contributor License Agreements) and CTAs (Copyright Transfer Agreements).", "The fundamental difference between the two is that in the former case the author retains the copyright, and grants a license.", "In the latter case the author transfers the copyright.", "Without either of these, how can you protect the integrity of your software package?", "Projects that require CTAs/CLAs do it to reduce their legal risks… It is important to note that CLAs/CTAs are optional in the sense that an organization is not required to use them.", "However, it demonstrates that  these open source communities would rather reject contributions than increase the legal risk of distributing code that may contain a license violation.", "Another thorny area is patents.", "From the debian-legal mailing list.", "It’s hard (bordering on impossible?)", "to know what patents may apply to a piece of software, including patents going through the approval process which may later be granted.", "A number of licenses include specific clauses relating to patents and their litigation.", "You also need to be careful to respect trademarks.", "Licensing semantics  The final category includes licensing bugs relating to difficulties and/or confusion over the use of dual licensing or understanding the implications of particular clauses.", "As an example, developers considering migration to GPL 2.0+ need to consider the “or later” clause.", "How do you know you will agree with the terms of a future version of the GPL?"], "summary_text": "To distribute or not to distribute? Why licensing bugs matter Vendome et al., ICSE’18  Software licensing can quickly get quite complicated, with over 100 known open source licenses out there, and distributions often including components with a mix of licenses. Unsurprisingly, developers find it hard to determine appropriate licenses for their work, and to interpret the implications of including third-party software under different licenses. We present a large-scale qualitative study aimed at characterizing licensing bugs, with the goal of understanding the types of licensing bugs developers face, their legal and technical implications, and how such bugs are fixed. The result is a helpful catalogue of seven different categories of licensing bugs, with 21 sub-categories in total between them. Although the authors are not lawyers (as far as I can tell), it still constitutes a very useful list of things to think about. “Our proposed catalog can serve as a reference for developers and lawyers dealing with potential licensing issues.”  The catalogue is drawn from an open coding exercise based on a statistically significant sample of 1,200 discussions randomly selected from a population of 59,426 discussions across a collection of issue trackers and mailing lists. The mailing lists were Apache’s legal-discuss, Debian’s debian-legal, Fedora’s fedora-legal-list, Gnome’s legal-last and OpenStack’s open-discuss. For issue trackers, the authors looked for issues using the keyword license on all 136 Bugzilla issue trackers in the Bugzilla installation list, as well as the issue trackers of 86,032 GitHub projects (selected to try and make sure these were not toy projects). Who cares about licensing? Before diving into the catalogue itself, it’s worth briefly reviewing the different stakeholders involved in licensing issues: there are holders of IP (e.g. trademark holders, patent holders, copyright holders), lawyers, and lawmakers, and then we can also call out:  Integrators, that reuse open source software within their own systems  Package maintainers, who are responsible for maintaining packages and integrating patches or bug fixes. Distributors – any individual or entity distributing software  Developers (in general)  Community – either people involved in a specific open source community, or the open source community as a whole. Catalog overview  The taxonomy is composed of 21 distinct sub-categories organised in 7 distinct high-level categories. Due to space limitations we only discuss a subset of the sub-categories (14). The complete taxonomy description and frequencies of each category can be found in the attached appendix. That appendix sounds like a useful resource. Unfortunately it’s not included in the only openly hosted version of the paper I could find (on the first author’s personal site, and linked at the top of this post). The descriptions we do get are still very useful though. It is important to remark that the results discuss the interpretation of developers and/or legal practitioners. Therefore it is possible that the legality of these interpretations or discussions may change (e.g., new interpretations can causes new legal precedents in the U.S.A.), on the enforceability may change in different jurisdictions. Selected licensing issues explored  Let’s take a brief look inside each of the seven major categories. Laws and their interpretations  At the base level, there is confusion over what is copyrightable? Software is copyrightable, but higher level designs and ideas may fall out of scope. Disagreements on the scope of copyright can lead to difficulties. A related issue is understanding what is a derivative work? (A work partially owned by the copyright author on which it is originally based). “… one of the most important features of open source licenses is that they should allow the creation and redistribution of derivative works.” It’s often unclear whether B should be treated as a derivative work of A, or just something that uses / bundles A. For example, Linus Torvalds asserts that merely using the kernel by making system calls does not constitute creating a derivative work. There is still plenty of disagreement even on this though. This is all further complicated by the fact that copyright, trademark, and patent laws are national in scope. Thus we often find clauses relating to choice of jurisdiction. … we observed that clauses related to choice of jurisdiction were a controversial topic within Debian in terms of their impact on software’s freeness. However, the distribution may be impacted by external factors like trade restrictions to a particular country or distribution of what a country considers sensitive material. While organizations or communities may want to facilitate global reuse, the organizations and individuals must comply with these trade laws. Policies of the ecosystem  This category concerns issues relating to the licensing policies of specific open source communities such as the Apache Foundation, Eclipse Software Foundation, and Debian. These give community guidelines that projects within the foundation are expected to follow. For example, projects at Eclipse under the EPL cannot ship external libraries under the LGPL as part of their distribution. This makes for more complex user installation procedures if users have to assemble the last mile themselves. The FSF has specific guidelines on whether software with various licenses can be combined/derived alongside software licensed under the FSF licences. You need to think broader than just source code, images, fonts, databases, text files and so on all need consideration…  Since IP clearance/evaluation extends to all bundled artifacts (not only source code and binaries), a non-free image or font could prevent the distribution of the software. Potential license violations  Some licenses are incompatible with each other , and issues can arise when including dependencies or reusing source code that is incompatible with either the declared license, or with the the license of other reused components. Generally such an issue impacts the ability to distributed the software. As a specific example, Apache License 2.0 is incompatible with GPL v2. (See the full list here ). Non-source code licensing  When evaluating license compliance, you also need to consider non-source code artefacts, and in particular the need to make the source of those artefacts available. In GPL for example, source is defined as “the preferred form of the work for making modifications to it“. So if you distributed a PDF of a document, you would also need to distribute the source that generates that PDF. Documentation, like source code, is also protected by copyright. Even documentation shipped in HTML format has been questioned, since HTML is not the preferred form for making changes. Similar issues occur with other media such as fonts, images, and audio. An MP3 is likely not the preferred form for editing audio for example. Licensing content  A license inconsistency occurs when there is a mismatch between the documented license and the actual source code licensing, e.g. inconsistencies between software licensing an the spec file documenting included licenses. Other IP issues  Do you have the rights to use a contribution? . This is the arena of CLAs (Contributor License Agreements) and CTAs (Copyright Transfer Agreements). The fundamental difference between the two is that in the former case the author retains the copyright, and grants a license. In the latter case the author transfers the copyright. Without either of these, how can you protect the integrity of your software package? Projects that require CTAs/CLAs do it to reduce their legal risks… It is important to note that CLAs/CTAs are optional in the sense that an organization is not required to use them. However, it demonstrates that  these open source communities would rather reject contributions than increase the legal risk of distributing code that may contain a license violation. Another thorny area is patents. From the debian-legal mailing list. It’s hard (bordering on impossible?) to know what patents may apply to a piece of software, including patents going through the approval process which may later be granted. A number of licenses include specific clauses relating to patents and their litigation. You also need to be careful to respect trademarks. Licensing semantics  The final category includes licensing bugs relating to difficulties and/or confusion over the use of dual licensing or understanding the implications of particular clauses. As an example, developers considering migration to GPL 2.0+ need to consider the “or later” clause. How do you know you will agree with the terms of a future version of the GPL?", "pdf_url": "http://www.christophervendome.com/wp-content/uploads/2018/05/ICSE18-LicensingBugsCRC.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/to-distribute-or-not-to-distribute-why-licensing-bugs-matter.json"}
{"id": "17601393", "bin": "1500_1600", "summary_sentences": ["A case for managed and model-less inference serving Yadwadkar et al., HotOS’19  HotOS’19 is presenting me with something of a problem as there are so many interesting looking papers in the proceedings this year it’s going to be hard to cover them all!", "As a transition from the SysML papers we’ve been looking at recently, I’ve chosen a HotOS position paper from the Stanford Platform Lab to kick things off.", "As we saw with the SOAP paper last time out, even with a fixed model variant and hardware there are a lot of different ways to map a training workload over the available hardware.", "In “A case for managed and model-less inference serving” Yadwadkar et al. look at a similar universe of possibilities for model serving at inference time, and conclude that it’s too much to expect users to navigate this by themselves.", "Making queries to an inference engine has many of the same throughput, latency, and cost considerations as making queries to a datastore, and more and more applications are coming to depend on such queries .", "“For instance, Facebook applications issue tens-of-trillions of inference queries per day with varying performance, accuracy, and cost constraints.”  If we want an increasing number of applications to use machine learning, we must automate issues that affect ease-of-use, performance, and cost efficiency for users and providers… Despite significant research, this is missing right now.", "Perhaps inspired by serverless in spirit and in terminology, the path forward proposed in this paper is towards a managed and model-less inference serving system.", "Managed here means that the system automates resource provisioning for models to match a set of SLO constraints (cf.", "autoscaling).", "Model-less is more confusing.", "First off there still is a model of course (but then there are servers hiding behind a serverless abstraction too!).", "Most of the discussion in the paper focuses on model families, i.e. selecting among variants of a given model transparently to the end user.", "But the vision clearly seems to include selection of the model itself.", "I get the former, but the latter feels to me much more like something you’d be doing during model development and training rather than dynamically at inference time.", "Perhaps we are intended to develop and train multiple models with differing characteristics, and make all of these available to the inference serving system to dynamically select from at runtime??", "The paper is silent on this issue.", "… we argue for an interface to the managed inference serving system where users are able to focus on querying an inference for their tasks without needing to think of models, and the trade-offs offered by model-variants.", "We term this interface model-less.", "Usability expectations  Creating a managed and model-less inferencing platform faces many challenges: applications have diverse SLOs (e.g. throughput sensitive vs latency sensitive); query patterns change dynamically over time; the underlying available hardware resources are diverse and also change over time; and their are many possible variants of a given model to choose from.", "Model variants can be created by methods such as model compression, knowledge distillation, tuning of hyperparameter values, varying precision, optimising for different batch sizes, and so on.", "The following figure highlights how just one of these variables, batch size, impacts throughput and latency on ResNet50.", "Expectation 1: Model-variant selection should be hidden behind a high-level API that allows users to simply specify their performance and cost objectives.", "Different hardware architectures (CPUs, GPUs, TPUs, FPGAs, ASICs, …) offer different performance and cost trade-offs.", "Performance may vary by up to a couple of orders of magnitude for example.", "Expectation 2: The choice of hardware should be hidden behind the same high level API for users.", "The system should select the right hardware type(s) to use at any point in time for meeting performance and cost SLOs.", "We’d like to pack models as efficiently as possible on the underlying infrastructure.", "Different applications will have different needs (and resource requirements) in terms of throughput and latency.", "Today it is most common to provision separate resources for each model, but model multi-tenancy would allow us to make better use of the underlying resources.", "An evaluation conducted by the authors showed that most models experience minimal performance loss with up to 5-6 concurrent instances running on a shared GPU.", "A quick back-of-the-envelope calculation suggests cost savings around an order of magnitude or more from sharing of hardware resources.", "Expectation 3: Resource management to meet query cost and performance goals for different models under varying load should be abstracted away from users.", "To improve provider resource utilization and TCO, the system must transparently (i) share resources across different model instances, (ii) share models across users, and (iii) manage memory by replicating and evicting models based on observed load and popularity.", "Similar to the start-up latency of a cold function in the serverless world, there’s start-up latency to be considered when first loading a model onto a given hardware platform.", "We pay this price when scaling up and when adding a new model to the system.", "Ideally we’d hide this from the user as best as possible:  Expectation 4: Start-up latency arising due to (i) loading a model-variant into the target hardware’s memory or storage and (ii) building an optimized model-variant, should be handled transparently.", "Keeping all model-variants warm all the time though is going to be very expensive, so there’s a constraint:  Expectation 5: To prevent poor resource utilization, providers should not need to constantly keep model-variants running.", "How can we get there?", "The wonderful thing about position papers is that you don’t have to show working implementations ;).", "Section 3 of the paper instead provides a sketch of approaches and research directions that can take us towards the vision.", "A user sends one or more queries for a prediction task, such as object recognition, with optional SLO constraints, for instance, 90 th percentile inference latency.", "The serving system takes care of the rest – automatic model-variant and target hardware selection, automatic model-variant generation, load-aware adaptive scaling, fault tolerance, monitoring, logging, maintaining security and privacy of models and queries.", "Based on the SLO of a query, and the dynamic state of the system, it will be mapped to a pairing of a model variant and target hardware for running that variant.", "Model variants can be generated on demand if needed.", "Ideally there would already be an instance of the variant running on the target hardware, if not we’ll need to start one (presumably the start-up latency cost will be a factor in determining placement).", "“How to design mechanisms and policies to avoid or reduce this latency remains an open question.” It’s probably a combination of heuristics and learned behaviour to proactively launch variants, together with an eviction model to clean-up under-utilised model instances.", "Further opportunities come from considering model placement at the edge and middle tiers, not just in cloud datacenters.", "Different layers offer trade-offs in terms of resource capacity, cost and network latency, management overheads, and energy-efficiency.", "Utilizing the resources across this continuum of core-middle-edge computing opens new opportunities and research directions.", "The final discussion in the paper concerns security and privacy.", "Authentication and authorization should be in place to prevent unauthorized access to queries, their submission patterns, and inference outcomes, to other users and providers.", "When building personalized models by adding a user-specific layer on top of a number of generic layers, can we share  the generic layers across users?", "Are there privacy concerns in doing so?", "The last word  The growing importance of ML inference forces us to finally solve several problems together: management of heterogeneity for both hardware and models, designing user interfaces, and building SLO-driven systems.", "These challenges are non-trivial and create new avenues for research.", "The good news, however, is that it is a bounded problem (i.e., models and model-variants are immutable once created), thus giving us hope to get it right soon.", "Replace ‘model’ with ‘datastore’ and you’ll see a very similar set of problems that we’ve been chipping away at for a very long time.", "Hope that we can make meaningful progress soon, yes.", "But hope that we can ‘get it right’ soon and put this challenge behind us?", "My personal bet is that this is a longer road…"], "summary_text": "A case for managed and model-less inference serving Yadwadkar et al., HotOS’19  HotOS’19 is presenting me with something of a problem as there are so many interesting looking papers in the proceedings this year it’s going to be hard to cover them all! As a transition from the SysML papers we’ve been looking at recently, I’ve chosen a HotOS position paper from the Stanford Platform Lab to kick things off. As we saw with the SOAP paper last time out, even with a fixed model variant and hardware there are a lot of different ways to map a training workload over the available hardware. In “A case for managed and model-less inference serving” Yadwadkar et al. look at a similar universe of possibilities for model serving at inference time, and conclude that it’s too much to expect users to navigate this by themselves. Making queries to an inference engine has many of the same throughput, latency, and cost considerations as making queries to a datastore, and more and more applications are coming to depend on such queries . “For instance, Facebook applications issue tens-of-trillions of inference queries per day with varying performance, accuracy, and cost constraints.”  If we want an increasing number of applications to use machine learning, we must automate issues that affect ease-of-use, performance, and cost efficiency for users and providers… Despite significant research, this is missing right now. Perhaps inspired by serverless in spirit and in terminology, the path forward proposed in this paper is towards a managed and model-less inference serving system. Managed here means that the system automates resource provisioning for models to match a set of SLO constraints (cf. autoscaling). Model-less is more confusing. First off there still is a model of course (but then there are servers hiding behind a serverless abstraction too!). Most of the discussion in the paper focuses on model families, i.e. selecting among variants of a given model transparently to the end user. But the vision clearly seems to include selection of the model itself. I get the former, but the latter feels to me much more like something you’d be doing during model development and training rather than dynamically at inference time. Perhaps we are intended to develop and train multiple models with differing characteristics, and make all of these available to the inference serving system to dynamically select from at runtime?? The paper is silent on this issue. … we argue for an interface to the managed inference serving system where users are able to focus on querying an inference for their tasks without needing to think of models, and the trade-offs offered by model-variants. We term this interface model-less. Usability expectations  Creating a managed and model-less inferencing platform faces many challenges: applications have diverse SLOs (e.g. throughput sensitive vs latency sensitive); query patterns change dynamically over time; the underlying available hardware resources are diverse and also change over time; and their are many possible variants of a given model to choose from. Model variants can be created by methods such as model compression, knowledge distillation, tuning of hyperparameter values, varying precision, optimising for different batch sizes, and so on. The following figure highlights how just one of these variables, batch size, impacts throughput and latency on ResNet50. Expectation 1: Model-variant selection should be hidden behind a high-level API that allows users to simply specify their performance and cost objectives. Different hardware architectures (CPUs, GPUs, TPUs, FPGAs, ASICs, …) offer different performance and cost trade-offs. Performance may vary by up to a couple of orders of magnitude for example. Expectation 2: The choice of hardware should be hidden behind the same high level API for users. The system should select the right hardware type(s) to use at any point in time for meeting performance and cost SLOs. We’d like to pack models as efficiently as possible on the underlying infrastructure. Different applications will have different needs (and resource requirements) in terms of throughput and latency. Today it is most common to provision separate resources for each model, but model multi-tenancy would allow us to make better use of the underlying resources. An evaluation conducted by the authors showed that most models experience minimal performance loss with up to 5-6 concurrent instances running on a shared GPU. A quick back-of-the-envelope calculation suggests cost savings around an order of magnitude or more from sharing of hardware resources. Expectation 3: Resource management to meet query cost and performance goals for different models under varying load should be abstracted away from users. To improve provider resource utilization and TCO, the system must transparently (i) share resources across different model instances, (ii) share models across users, and (iii) manage memory by replicating and evicting models based on observed load and popularity. Similar to the start-up latency of a cold function in the serverless world, there’s start-up latency to be considered when first loading a model onto a given hardware platform. We pay this price when scaling up and when adding a new model to the system. Ideally we’d hide this from the user as best as possible:  Expectation 4: Start-up latency arising due to (i) loading a model-variant into the target hardware’s memory or storage and (ii) building an optimized model-variant, should be handled transparently. Keeping all model-variants warm all the time though is going to be very expensive, so there’s a constraint:  Expectation 5: To prevent poor resource utilization, providers should not need to constantly keep model-variants running. How can we get there? The wonderful thing about position papers is that you don’t have to show working implementations ;). Section 3 of the paper instead provides a sketch of approaches and research directions that can take us towards the vision. A user sends one or more queries for a prediction task, such as object recognition, with optional SLO constraints, for instance, 90 th percentile inference latency. The serving system takes care of the rest – automatic model-variant and target hardware selection, automatic model-variant generation, load-aware adaptive scaling, fault tolerance, monitoring, logging, maintaining security and privacy of models and queries. Based on the SLO of a query, and the dynamic state of the system, it will be mapped to a pairing of a model variant and target hardware for running that variant. Model variants can be generated on demand if needed. Ideally there would already be an instance of the variant running on the target hardware, if not we’ll need to start one (presumably the start-up latency cost will be a factor in determining placement). “How to design mechanisms and policies to avoid or reduce this latency remains an open question.” It’s probably a combination of heuristics and learned behaviour to proactively launch variants, together with an eviction model to clean-up under-utilised model instances. Further opportunities come from considering model placement at the edge and middle tiers, not just in cloud datacenters. Different layers offer trade-offs in terms of resource capacity, cost and network latency, management overheads, and energy-efficiency. Utilizing the resources across this continuum of core-middle-edge computing opens new opportunities and research directions. The final discussion in the paper concerns security and privacy. Authentication and authorization should be in place to prevent unauthorized access to queries, their submission patterns, and inference outcomes, to other users and providers. When building personalized models by adding a user-specific layer on top of a number of generic layers, can we share  the generic layers across users? Are there privacy concerns in doing so? The last word  The growing importance of ML inference forces us to finally solve several problems together: management of heterogeneity for both hardware and models, designing user interfaces, and building SLO-driven systems. These challenges are non-trivial and create new avenues for research. The good news, however, is that it is a bounded problem (i.e., models and model-variants are immutable once created), thus giving us hope to get it right soon. Replace ‘model’ with ‘datastore’ and you’ll see a very similar set of problems that we’ve been chipping away at for a very long time. Hope that we can make meaningful progress soon, yes. But hope that we can ‘get it right’ soon and put this challenge behind us? My personal bet is that this is a longer road…", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3317550.3321443?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/a-case-for-managed-and-model-less-inference-serving.json"}
{"id": "37479343", "bin": "1500_1600", "summary_sentences": ["Time-adaptive sketches (Ada Sketches) for Summarizing Data Streams Shrivastava et al. SIGMOD 2016  More algorithm fun today, and again in the context of data streams.", "It’s the 3 V’s of big data, but not as you know it: Volume, Velocity, and Var… Volatility.", "Volatility here refers to changing patterns in the data over time, and that can make life awkward if you’re trying to extract information from a stream.", "In particular, the authors study the heavy hitters problem, but with a twist: we want to give more weight to recent trends.", "In most applications that involve temporal data, most recent trends tend to be most informative for predictive purposes….", "For instance, most recent variations in credit history are much stronger indicators of a person’s ability to make loan payments compared to variations in credit history from the distant past.", "Time-adaptive sketches generalize sketching algorithms and have the property that they retain counts of heavy hitters with good accuracy, while also providing provable time-adaptive guarantees.", "Coming in at 16 pages, the essence of the paper, especially if you’re familiar with count-min sketches is this: instead of increasing counters by 1 every time you see an item, increase them by f(t), where f(t) is a monotone function in time.", "When you want to extract count estimates for time t, divide by f(t).", "The authors experiment with a linear function f(t) = at, for fixed a (0.5), and also an exponential function f(t) = at for fixed a (1.0015).", "Both gave good results.", "Finishing the write-up here though would be to short-change you.", "We’re interested in why this works, and what guarantees it gives.", "Plus the paper also gives an excellent tour through some of the prior approaches to solving the heavy hitters problem.", "Let’s start there, with a very quick recap on the basic Count-Min Sketch (CMS) algorithm.", "Count-Min Sketch  Create an integer array initialised to zeros that is w wide and d deep.", "Take d pairwise independent hash functions, h1,…,hd and associate one with each row of the table, these functions should produce a value in the range 1..w. When a new value is seen, for each row of the table, hash the value with the corresponding hash function, and increment the counter in the indicated array slot.", "If you want to know the estimate of how many instances of a given value have been seen, hash the value as previously and look up the counter values that gives you in each row.", "Take the smallest of these as your estimate.", "Hokusai – nearly but not quite  Hokusai-sketching (Matusevych et al. 2012) introduced an item aggregation algorithm for constructing time-adaptive sketches.", "Hokusai uses a set of Count-Min sketches for different time intervals, to estimate the counts of any item for a given time or interval.", "To adapt the error rate temporally in limited space, the algorithm uses larger sketches for recent intervals and sketches of smaller size for older intervals.", "At the end of a time interval (e.g T), a sketch needs to be moved into the next-sized-down sketch, (the one for T–1).", "Hokusai has a very elegant way of doing this: at each rung on the ladder, sketch widths are halved.", "You can therefore compress a larger sketch into a smaller one by simply adding one half of the sketch to the other, and also halving the hash function ranges using modulo 2 operations.", "Although this idea of having different-size sketches for different time intervals is reasonable and yields accuracies that are time-adaptive, it comes with several inherent shortingcomings.", "Inspiration – Dolby noise reduction!", "This might date some of The Morning Paper readers – do you remember Dolby B noise reduction?", "And then the exciting introduction of Dolby C?", "Some of us grew up with music on cassette tapes, and Dolby Noise Reduction was ever present.", "When recording, Dolby systems employ pre-emphasis – artificially boosting certain parts of the input signal.", "On playback, the reverse de-emphasis translation restores the original signal levels.", "This process helps to improve the signal-to-noise ratio and combat tape hiss.", "We exploit the fact that Count-Min Sketch (CMS)… has better accuracy for heavy-hitters as compared to the rest of the items.", "While updating the sketch we apply pre-emphasis and artificially inflate the counts of more recent items compared to older ones, i.e., we make them heavier with respect to the older items.", "This is done by multiplying updates cit with f(t), which is any monotonically increasing function of time t. Thus, instead of updating the sketch with cit we update the sketch with _f(t) x cit.", "The tendency of the sketch is to preserve large values.", "This inflation thus preserves the accuracy of recent items, after artificial inflation, compared to the older ones.", "On querying of course, the de-emphasis process must be applied, which means dividing the results by f(t) to obtain the estimate of item i at time t. In the absence of collisions, as with the base CMS, counts are estimated exactly.", "Consider a CMS with only one row, and the case when two independent items i and j collide.", "We see cit instances of i, and cjt’ instances of j.", "With plain CMS, we would over-estimate the count for i by cjt’, whereas with the pre-emphasis process we overestimate by (f(t) x cjt)/f(t’)).", "Therefore it is easy to see that more recent items suffer less compared to older items.", "Adaptive CMS  The Adaptive Count-Min Sketch algorithm (Ada-CMS), is just CMS but with the update and query mechanisms adapted to use the pre-emphasis and de-emphasis mechanism just described.", "Note that when f(t) = 1 we obtain the original CMS algorithm.", "By choosing appropriate f(t) functions, we can tailor the behaviour for different situations.", "One major question we are interested in is \"Given a fixed space and current state of time T, what are the values of time t ≤ T where Ada-CMS is more accurate than vanilla CMS?", "For a given w and d, we can see as a start that the expected error of Ada-CMS will be less than CMS if:  For t=T this will always be true (due to the monotonicity requirement on f(t)).", "The upper bound on the error with vanilla CMS is &sqrt;T, so Ada-CMS wins when its error is less than this.", "To illustrate a reasonable scenario, suppose we want the errors with Ada-CMS to be never off by a factor γ away from that of vanilla CMS ∀ t. This ensures that we guarantee accuracy within a factor γ of what the original CMS would achieve to even very old heavy hitters.", "In addition, we want to be more accurate than CMS on all recent time t > K, for some desirable choice of K.  With a couple of simple manoeuvres (see section 5.2), this turns into solving the following pair of simultaneous equations:  Other applications  The pre-emphasis and de-emphasis technique can be used in a number of other scenarios.", "The authors show an example with the Lossy Counting algorithm, and also how it can be applied to range queries (see §6).", "Evaluation  Experimental evaluation is undertaken with two real-world streaming datasets from AOL (36M search queries with 3.8M unique terms) and Criteo (150K unique categorical terms).", "Comparison is undertaken between vanilla CMS, the Hokusai algorithm, Ada-CMS with a linear function (f(t) = 0.5t), and Ada-CMS with an exponential function (f(t)=1.0015t).", "In all cases d = 4, and w was varied from 210 to 223 to see the impact of varying range sizes.", "Here are the results for the AOL dataset:  Here’s the standard deviation of those errors with w=218:  The Last Word  The proposed integration of sketches with pre-emphasis and de-emphasis, as we demonstrate, posseses strong theoretical guarantees on errors over time.", "Experiments on real datasets support our theoretical findings and show significantly superior accuracy and runtime overhead compared to the recently proposed Hokusai algorithm.", "We hope that our proposal will be adopted in practice, and it will lead to further exploration of the pre-emphasis and de-emphasis idea for solving massive data stream problems."], "summary_text": "Time-adaptive sketches (Ada Sketches) for Summarizing Data Streams Shrivastava et al. SIGMOD 2016  More algorithm fun today, and again in the context of data streams. It’s the 3 V’s of big data, but not as you know it: Volume, Velocity, and Var… Volatility. Volatility here refers to changing patterns in the data over time, and that can make life awkward if you’re trying to extract information from a stream. In particular, the authors study the heavy hitters problem, but with a twist: we want to give more weight to recent trends. In most applications that involve temporal data, most recent trends tend to be most informative for predictive purposes…. For instance, most recent variations in credit history are much stronger indicators of a person’s ability to make loan payments compared to variations in credit history from the distant past. Time-adaptive sketches generalize sketching algorithms and have the property that they retain counts of heavy hitters with good accuracy, while also providing provable time-adaptive guarantees. Coming in at 16 pages, the essence of the paper, especially if you’re familiar with count-min sketches is this: instead of increasing counters by 1 every time you see an item, increase them by f(t), where f(t) is a monotone function in time. When you want to extract count estimates for time t, divide by f(t). The authors experiment with a linear function f(t) = at, for fixed a (0.5), and also an exponential function f(t) = at for fixed a (1.0015). Both gave good results. Finishing the write-up here though would be to short-change you. We’re interested in why this works, and what guarantees it gives. Plus the paper also gives an excellent tour through some of the prior approaches to solving the heavy hitters problem. Let’s start there, with a very quick recap on the basic Count-Min Sketch (CMS) algorithm. Count-Min Sketch  Create an integer array initialised to zeros that is w wide and d deep. Take d pairwise independent hash functions, h1,…,hd and associate one with each row of the table, these functions should produce a value in the range 1..w. When a new value is seen, for each row of the table, hash the value with the corresponding hash function, and increment the counter in the indicated array slot. If you want to know the estimate of how many instances of a given value have been seen, hash the value as previously and look up the counter values that gives you in each row. Take the smallest of these as your estimate. Hokusai – nearly but not quite  Hokusai-sketching (Matusevych et al. 2012) introduced an item aggregation algorithm for constructing time-adaptive sketches. Hokusai uses a set of Count-Min sketches for different time intervals, to estimate the counts of any item for a given time or interval. To adapt the error rate temporally in limited space, the algorithm uses larger sketches for recent intervals and sketches of smaller size for older intervals. At the end of a time interval (e.g T), a sketch needs to be moved into the next-sized-down sketch, (the one for T–1). Hokusai has a very elegant way of doing this: at each rung on the ladder, sketch widths are halved. You can therefore compress a larger sketch into a smaller one by simply adding one half of the sketch to the other, and also halving the hash function ranges using modulo 2 operations. Although this idea of having different-size sketches for different time intervals is reasonable and yields accuracies that are time-adaptive, it comes with several inherent shortingcomings. Inspiration – Dolby noise reduction! This might date some of The Morning Paper readers – do you remember Dolby B noise reduction? And then the exciting introduction of Dolby C? Some of us grew up with music on cassette tapes, and Dolby Noise Reduction was ever present. When recording, Dolby systems employ pre-emphasis – artificially boosting certain parts of the input signal. On playback, the reverse de-emphasis translation restores the original signal levels. This process helps to improve the signal-to-noise ratio and combat tape hiss. We exploit the fact that Count-Min Sketch (CMS)… has better accuracy for heavy-hitters as compared to the rest of the items. While updating the sketch we apply pre-emphasis and artificially inflate the counts of more recent items compared to older ones, i.e., we make them heavier with respect to the older items. This is done by multiplying updates cit with f(t), which is any monotonically increasing function of time t. Thus, instead of updating the sketch with cit we update the sketch with _f(t) x cit. The tendency of the sketch is to preserve large values. This inflation thus preserves the accuracy of recent items, after artificial inflation, compared to the older ones. On querying of course, the de-emphasis process must be applied, which means dividing the results by f(t) to obtain the estimate of item i at time t. In the absence of collisions, as with the base CMS, counts are estimated exactly. Consider a CMS with only one row, and the case when two independent items i and j collide. We see cit instances of i, and cjt’ instances of j. With plain CMS, we would over-estimate the count for i by cjt’, whereas with the pre-emphasis process we overestimate by (f(t) x cjt)/f(t’)). Therefore it is easy to see that more recent items suffer less compared to older items. Adaptive CMS  The Adaptive Count-Min Sketch algorithm (Ada-CMS), is just CMS but with the update and query mechanisms adapted to use the pre-emphasis and de-emphasis mechanism just described. Note that when f(t) = 1 we obtain the original CMS algorithm. By choosing appropriate f(t) functions, we can tailor the behaviour for different situations. One major question we are interested in is \"Given a fixed space and current state of time T, what are the values of time t ≤ T where Ada-CMS is more accurate than vanilla CMS? For a given w and d, we can see as a start that the expected error of Ada-CMS will be less than CMS if:  For t=T this will always be true (due to the monotonicity requirement on f(t)). The upper bound on the error with vanilla CMS is &sqrt;T, so Ada-CMS wins when its error is less than this. To illustrate a reasonable scenario, suppose we want the errors with Ada-CMS to be never off by a factor γ away from that of vanilla CMS ∀ t. This ensures that we guarantee accuracy within a factor γ of what the original CMS would achieve to even very old heavy hitters. In addition, we want to be more accurate than CMS on all recent time t > K, for some desirable choice of K.  With a couple of simple manoeuvres (see section 5.2), this turns into solving the following pair of simultaneous equations:  Other applications  The pre-emphasis and de-emphasis technique can be used in a number of other scenarios. The authors show an example with the Lossy Counting algorithm, and also how it can be applied to range queries (see §6). Evaluation  Experimental evaluation is undertaken with two real-world streaming datasets from AOL (36M search queries with 3.8M unique terms) and Criteo (150K unique categorical terms). Comparison is undertaken between vanilla CMS, the Hokusai algorithm, Ada-CMS with a linear function (f(t) = 0.5t), and Ada-CMS with an exponential function (f(t)=1.0015t). In all cases d = 4, and w was varied from 210 to 223 to see the impact of varying range sizes. Here are the results for the AOL dataset:  Here’s the standard deviation of those errors with w=218:  The Last Word  The proposed integration of sketches with pre-emphasis and de-emphasis, as we demonstrate, posseses strong theoretical guarantees on errors over time. Experiments on real datasets support our theoretical findings and show significantly superior accuracy and runtime overhead compared to the recently proposed Hokusai algorithm. We hope that our proposal will be adopted in practice, and it will lead to further exploration of the pre-emphasis and de-emphasis idea for solving massive data stream problems.", "pdf_url": "http://research.microsoft.com/en-us/um/people/mbilenko/papers/16-ada-sketches.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/time-adaptive-sketches-ada-sketches-for-summarizing-data-streams.json"}
{"id": "33714440", "bin": "1500_1600", "summary_sentences": ["A cloud-based content gathering network Bhattacherjee et al., HotCloud’17  We all know what a content distribution network is, but what’s a content gathering network?!", "CDNs are great, but their benefits are reduced for clients that have a relatively slow last mile connection – especially given that a typical web page request will involve many round trips.", "It is worth noting that such large last-mile latencies (e.g 100ms) are not atypical in many parts of the world.", "There is, of course, significant ongoing work both on cutting down the number of RTTs a web request takes, and on improving last-mile latencies.", "However, realizing these benefits across the large eco-system of web service providers and ISPs could still take many years.", "How do we improve performance for users in these environments today?", "The core idea of a CGN is to gather all the information needed for a page load in a place that has a short RTT time, and then transfer it to the client in (ideally) one round trip.", "At a cost of about $1 per user, the authors show that it can reduce the median page load time across 100 popular web sites by up to 53%.", "That’s definitely something users would love!", "CDN vs CGN  CDNs locate customer’s content as close to the end user as possible, whereas a CGN tries to locate (a proxy for) the end user as close to the content as possible.", "Until you take the number of round trips needed to load a web page into account, it doesn’t seem on the surface like it would make much difference.", "The following picture helps to illustrate what’s going on:  Suppose we have a web server serving some site on the east coast of the US.", "A CGN node in for example EC2 US-East will probably have very low latency access to the site.", "So it can make the multiple trips needed to access a page locally and then transfer the content to the client.", "The client and the CGN node use aggressive transport protocols, with large initial window sizes, and without the need of an additional handshake (unlike TCP).", "Here’s the full picture: the client browser is configured to forward web requests to a local HTTP proxy service, which then forwards the URL to the CGN node nearest to the hosting web server.", "The CGN node runs a headless browser, which starts downloading the web page.", "As the content is downloaded, the CGN node forwards it to the client’s local proxy in parallel using a TCP byte stream, and the local proxy serves the content to the browser.", "How does the proxy service know which CGN node is nearest to the hosting web server?", "A network of CGN nodes is deployed (e.g., at least one in each public cloud region), and each node makes independent latency measurements to a list of sites, and exchanges information with the other nodes.", "(This is a few MB, even for a million popular web sites).", "Clients obtain the mapping from CGN nodes.", "Experiments conducted by the authors show that the mappings are stable enough that updates can be infrequent (e.g., once a day).", "The rise of public clouds changes the game  We observe that present public cloud infrastructure provides enough global points of presence to be able to reach within close proximity of most popular Web services, and thus provides a natural platform to build a “content gathering network,” which operates on behalf of users.", "On the other side of the coin, with increasing numbers of websites themselves being cloud hosted, the distances between public cloud infrastructure and popular websites can be very small indeed.", "The authors set out to measure just how low latency might be…  We deployed one node in each of Amazon’s 14 data center regions.", "From each of these 14 nodes, we measure round trip times to each Web server hosting the top 100,000 web sites in Alexa’s list of popular sites… For a smaller set of Web servers (top 10,000), we also similarly measured RTTs from clients in Lahore, Pakistan and Zurich, Switzerland.", "Both of these clients are university-hosted and “real” end-user connectivity is likely to be worse.", "The results?", "Taking the median (and 90th pecentile), for the top 10,000 domains, EC2 is within 4.8ms (39.5ms), Zurich within 39.8ms (215.8ms), and Lahore within 275.8ms (471.8ms).", "… median RTT from EC2 is 8x smaller than from Zurich, and 57x smaller than from Lahore.", "For the top 100,000 domains, the median RTT from EC2 is still only 7.2ms.", "Over a one-week period, the latencies from the EC2 node a domain is mapped to stayed very stable.", "Page load time improvements  For their evaluation, the team used m4.10xlarge EC2 instances as CGN nodes, and PhantomJS as the headless browser.", "Two CGN nodes were operated: one in North Carolina, USA, and one in Frankfurt, Germany.", "The test was done only with those domains which would have mapped to either of these two locations.", "The current implementation does not quite hit the goal of only one round trip between CGN and client due to differences in behaviours between client-side browsers and headless browsers, thus some content is still fetched from traditional CDN nodes.", "“But we do not believe that the 1-RTT goal is unreachable, and are working towards it.”  Web pages were loaded with and without the CGN, from a client in Lahore.", "Even though CDN content requests still went directly to the CDN servers (for now), the results are still impressive.", "Out of the Alexa top-10K, we evaluate the top 100 and random 100 web pages which map to either of our two CGN nodes.", "For the top-100 set, the median page load time reduces from (the default) 16.1 seconds to 7.6 seconds with CGN – a reduction of 53%.", "For the random 100 set, median page load time is reduced by 43.2%.", "We also get a quick comparison with Google’s Flywheel using the “Data Saver” extension for Chrome.", "Testing against the top 30 domains, CGN still shows significant gains beyond Flywheel’s.", "We note that Flywheel is a complex system with numerous optimizations, including compression, caching, and prefetching at the proxies.", "These techniques are all orthogonal to ours, and could readily be added to our approach for even more reduction in PLTs.", "Given how well CGNs work, the authors are moved to ask “can we leverage public cloud infrastructure and the observation that most content is hosted in (or near) this infrastructure to entirely eliminate CDNs from Web site delivery?” (Note the scoping to web sites, characterised by large numbers of relatively small requests.", "For applications like video streaming, CDNs will always win).", "What about…?", "How much might it cost to run a CGN network?", "Some back of the envelope calculations by the authors suggest that with a network shared by multiple users, the cost to provide the service comes in at about $1 per user per month.", "HTTPS.", "This is a big one for me.", "While technically supporting https is feasible, the client browser would need to trust the CGN node acting as at TLS/SSL proxy.", "“There is also scope for using techniques like Intel SGX to hide the content of client-server interactions from the CGN nodes as a privacy enhancement, although CGN nodes will still be able to see which servers a client connects to.” The authors note that CDNs already operate under a similar trust model, but to me there’s a big difference between shared static resources and dynamic content containing sensitive personal information.", "Commercial model  Who would host a CGN service.", "The authors have three suggestions:  Cloud providers could provide this as a service, to attract more web service providers to their infrastructure  Browser vendors competing for market share could run the service  Web service providers themselves may incur the expense  Rather than seeing this as a reduction in business for CDN vendors though, there seems to be an obvious fourth option to me: CDN vendors themselves could operate CGN capabilities alongside their existing CDN network."], "summary_text": "A cloud-based content gathering network Bhattacherjee et al., HotCloud’17  We all know what a content distribution network is, but what’s a content gathering network?! CDNs are great, but their benefits are reduced for clients that have a relatively slow last mile connection – especially given that a typical web page request will involve many round trips. It is worth noting that such large last-mile latencies (e.g 100ms) are not atypical in many parts of the world. There is, of course, significant ongoing work both on cutting down the number of RTTs a web request takes, and on improving last-mile latencies. However, realizing these benefits across the large eco-system of web service providers and ISPs could still take many years. How do we improve performance for users in these environments today? The core idea of a CGN is to gather all the information needed for a page load in a place that has a short RTT time, and then transfer it to the client in (ideally) one round trip. At a cost of about $1 per user, the authors show that it can reduce the median page load time across 100 popular web sites by up to 53%. That’s definitely something users would love! CDN vs CGN  CDNs locate customer’s content as close to the end user as possible, whereas a CGN tries to locate (a proxy for) the end user as close to the content as possible. Until you take the number of round trips needed to load a web page into account, it doesn’t seem on the surface like it would make much difference. The following picture helps to illustrate what’s going on:  Suppose we have a web server serving some site on the east coast of the US. A CGN node in for example EC2 US-East will probably have very low latency access to the site. So it can make the multiple trips needed to access a page locally and then transfer the content to the client. The client and the CGN node use aggressive transport protocols, with large initial window sizes, and without the need of an additional handshake (unlike TCP). Here’s the full picture: the client browser is configured to forward web requests to a local HTTP proxy service, which then forwards the URL to the CGN node nearest to the hosting web server. The CGN node runs a headless browser, which starts downloading the web page. As the content is downloaded, the CGN node forwards it to the client’s local proxy in parallel using a TCP byte stream, and the local proxy serves the content to the browser. How does the proxy service know which CGN node is nearest to the hosting web server? A network of CGN nodes is deployed (e.g., at least one in each public cloud region), and each node makes independent latency measurements to a list of sites, and exchanges information with the other nodes. (This is a few MB, even for a million popular web sites). Clients obtain the mapping from CGN nodes. Experiments conducted by the authors show that the mappings are stable enough that updates can be infrequent (e.g., once a day). The rise of public clouds changes the game  We observe that present public cloud infrastructure provides enough global points of presence to be able to reach within close proximity of most popular Web services, and thus provides a natural platform to build a “content gathering network,” which operates on behalf of users. On the other side of the coin, with increasing numbers of websites themselves being cloud hosted, the distances between public cloud infrastructure and popular websites can be very small indeed. The authors set out to measure just how low latency might be…  We deployed one node in each of Amazon’s 14 data center regions. From each of these 14 nodes, we measure round trip times to each Web server hosting the top 100,000 web sites in Alexa’s list of popular sites… For a smaller set of Web servers (top 10,000), we also similarly measured RTTs from clients in Lahore, Pakistan and Zurich, Switzerland. Both of these clients are university-hosted and “real” end-user connectivity is likely to be worse. The results? Taking the median (and 90th pecentile), for the top 10,000 domains, EC2 is within 4.8ms (39.5ms), Zurich within 39.8ms (215.8ms), and Lahore within 275.8ms (471.8ms). … median RTT from EC2 is 8x smaller than from Zurich, and 57x smaller than from Lahore. For the top 100,000 domains, the median RTT from EC2 is still only 7.2ms. Over a one-week period, the latencies from the EC2 node a domain is mapped to stayed very stable. Page load time improvements  For their evaluation, the team used m4.10xlarge EC2 instances as CGN nodes, and PhantomJS as the headless browser. Two CGN nodes were operated: one in North Carolina, USA, and one in Frankfurt, Germany. The test was done only with those domains which would have mapped to either of these two locations. The current implementation does not quite hit the goal of only one round trip between CGN and client due to differences in behaviours between client-side browsers and headless browsers, thus some content is still fetched from traditional CDN nodes. “But we do not believe that the 1-RTT goal is unreachable, and are working towards it.”  Web pages were loaded with and without the CGN, from a client in Lahore. Even though CDN content requests still went directly to the CDN servers (for now), the results are still impressive. Out of the Alexa top-10K, we evaluate the top 100 and random 100 web pages which map to either of our two CGN nodes. For the top-100 set, the median page load time reduces from (the default) 16.1 seconds to 7.6 seconds with CGN – a reduction of 53%. For the random 100 set, median page load time is reduced by 43.2%. We also get a quick comparison with Google’s Flywheel using the “Data Saver” extension for Chrome. Testing against the top 30 domains, CGN still shows significant gains beyond Flywheel’s. We note that Flywheel is a complex system with numerous optimizations, including compression, caching, and prefetching at the proxies. These techniques are all orthogonal to ours, and could readily be added to our approach for even more reduction in PLTs. Given how well CGNs work, the authors are moved to ask “can we leverage public cloud infrastructure and the observation that most content is hosted in (or near) this infrastructure to entirely eliminate CDNs from Web site delivery?” (Note the scoping to web sites, characterised by large numbers of relatively small requests. For applications like video streaming, CDNs will always win). What about…? How much might it cost to run a CGN network? Some back of the envelope calculations by the authors suggest that with a network shared by multiple users, the cost to provide the service comes in at about $1 per user per month. HTTPS. This is a big one for me. While technically supporting https is feasible, the client browser would need to trust the CGN node acting as at TLS/SSL proxy. “There is also scope for using techniques like Intel SGX to hide the content of client-server interactions from the CGN nodes as a privacy enhancement, although CGN nodes will still be able to see which servers a client connects to.” The authors note that CDNs already operate under a similar trust model, but to me there’s a big difference between shared static resources and dynamic content containing sensitive personal information. Commercial model  Who would host a CGN service. The authors have three suggestions:  Cloud providers could provide this as a service, to attract more web service providers to their infrastructure  Browser vendors competing for market share could run the service  Web service providers themselves may incur the expense  Rather than seeing this as a reduction in business for CDN vendors though, there seems to be an obvious fourth option to me: CDN vendors themselves could operate CGN capabilities alongside their existing CDN network.", "pdf_url": "https://www.usenix.org/system/files/conference/hotcloud17/hotcloud17-paper-bhattacherjee.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/a-cloud-based-content-gathering-network.json"}
{"id": "78190587", "bin": "200_300", "summary_sentences": ["What  The well known method of Artistic Style Transfer can be used to generate new texture images (from an existing example) by skipping the content loss and only using the style loss.", "The method however can have problems with large scale structures and quasi-periodic patterns.", "They add a new loss based on the spectrum of the images (synthesized image and style image), which decreases these problems and handles especially periodic patterns well.", "How  Everything is handled in the same way as in the Artistic Style Transfer paper (without content loss).", "On top of that they add their spectrum loss:  The loss is based on a squared distance, i.e. 1/2 d(I_s, I_t)^2.", "I_s is the last synthesized image.", "I_t is the texture example.", "d(I_s, I_t) then does the following:  It assumes that I_t is an example for a space of target images.", "Within that set it finds the image I_p which is most similar to I_s.", "That is done using a projection via Fourier Transformations.", "(See formula 5 in the paper.)", "The returned distance is then I_s - I_p.", "Results  Equal quality for textures without quasi-periodic structures.", "Significantly better quality for textures with quasi-periodic structures.", "Overview over their method, i.e. generated textures using style and/or spectrum-based loss."], "summary_text": "What  The well known method of Artistic Style Transfer can be used to generate new texture images (from an existing example) by skipping the content loss and only using the style loss. The method however can have problems with large scale structures and quasi-periodic patterns. They add a new loss based on the spectrum of the images (synthesized image and style image), which decreases these problems and handles especially periodic patterns well. How  Everything is handled in the same way as in the Artistic Style Transfer paper (without content loss). On top of that they add their spectrum loss:  The loss is based on a squared distance, i.e. 1/2 d(I_s, I_t)^2. I_s is the last synthesized image. I_t is the texture example. d(I_s, I_t) then does the following:  It assumes that I_t is an example for a space of target images. Within that set it finds the image I_p which is most similar to I_s. That is done using a projection via Fourier Transformations. (See formula 5 in the paper.) The returned distance is then I_s - I_p. Results  Equal quality for textures without quasi-periodic structures. Significantly better quality for textures with quasi-periodic structures. Overview over their method, i.e. generated textures using style and/or spectrum-based loss.", "pdf_url": "http://arxiv.org/pdf/1605.01141v3", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/texture_synthesis_through_cnns_and_spectrum_constraints.json"}
{"id": "38867178", "bin": "200_300", "summary_sentences": ["The basic idea is to introduce a curriculum into the GAN training procedure.", "One starts by training the generator to produce 4 x 4 images, progressively adding layers to increase the resolution.", "In the paper, they generated high-quality 1024 x 1024 samples from CelebA, LSUN, and CIFAR-10.", "This is a nice applied paper where the core idea is quite simple and explained clearly.", "They describe all of the challenges hidden under the surface of training large-scale GANs and tell the reader how they tackled them.", "Lots of good deep learning voodoo in this paper.", "They found that the progressive scheme helps the GAN converege to much better optimum (image quality is amazing) and reduces total training time by about a factor of 2.", "They mainly use the WGAN-GP loss.", "Recall that the WGAN loss is  The main change made in WGAN-GP is the addition of a gradient penalty term to take care of the 1-Lipschitz constraint.", "Previous, hard weight clipping within some [-c, c] was used.", "The new loss looks like  , and $\\lambda$ is set to 10.", "Definition: Inception score is an evaluation metric for GANs where generated samples are fed into an Inception model trained on ImageNet.", "Images with meaningful objects are supposed to have low label entropy, but the entropy across images should be high (high variation)."], "summary_text": "The basic idea is to introduce a curriculum into the GAN training procedure. One starts by training the generator to produce 4 x 4 images, progressively adding layers to increase the resolution. In the paper, they generated high-quality 1024 x 1024 samples from CelebA, LSUN, and CIFAR-10. This is a nice applied paper where the core idea is quite simple and explained clearly. They describe all of the challenges hidden under the surface of training large-scale GANs and tell the reader how they tackled them. Lots of good deep learning voodoo in this paper. They found that the progressive scheme helps the GAN converege to much better optimum (image quality is amazing) and reduces total training time by about a factor of 2. They mainly use the WGAN-GP loss. Recall that the WGAN loss is  The main change made in WGAN-GP is the addition of a gradient penalty term to take care of the 1-Lipschitz constraint. Previous, hard weight clipping within some [-c, c] was used. The new loss looks like  , and $\\lambda$ is set to 10. Definition: Inception score is an evaluation metric for GANs where generated samples are fed into an Inception model trained on ImageNet. Images with meaningful objects are supposed to have low label entropy, but the entropy across images should be high (high variation).", "pdf_url": "http://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2018iclr-paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/progressive-growing-gans.json"}
{"id": "70481957", "bin": "200_300", "summary_sentences": ["This paper introduces a version of the skipgram word embeddings learning algorithm that can also learn the size (nb.", "of dimensions) of these embeddings.", "The method, coined infinite skipgram (iSG), is inspired from my work with Marc-Alexandre Côté on the infinite RBM, in which we describe a mathematical trick for learning the size of a latent representation.", "This is done by introducing an additional latent variable $z$ representing the number of dimensions effectively involved in the energy function.", "Moreover, a term penalizing increasing values for $z$ is also incorporated, such that the infinite sum over $z$ is converging.", "In this paper, the authors extend the probabilistic model behind skipgram with such a variable $z$, now corresponding to the number of dimensions involved in the dot product between word embeddings.", "They also propose a few approximations required to allow for an efficient training algorithm.", "Mainly they optimize an upper bound on the regular skipgram objective (see Section 3.2) and they approximate the computation of the conditional over $z$ for a given word $w$, which requires summing over all possible context words $c$, by summing only over the words observed in the immediate current context of $w$ (thus this sum will very across training example of the same word $w$).", "Experiments show that the iSG better learns to exploit different dimensions to model different senses of words, better than the original skipgram model.", "Quantitatively, the iSG seems to provide better probabilities to context words."], "summary_text": "This paper introduces a version of the skipgram word embeddings learning algorithm that can also learn the size (nb. of dimensions) of these embeddings. The method, coined infinite skipgram (iSG), is inspired from my work with Marc-Alexandre Côté on the infinite RBM, in which we describe a mathematical trick for learning the size of a latent representation. This is done by introducing an additional latent variable $z$ representing the number of dimensions effectively involved in the energy function. Moreover, a term penalizing increasing values for $z$ is also incorporated, such that the infinite sum over $z$ is converging. In this paper, the authors extend the probabilistic model behind skipgram with such a variable $z$, now corresponding to the number of dimensions involved in the dot product between word embeddings. They also propose a few approximations required to allow for an efficient training algorithm. Mainly they optimize an upper bound on the regular skipgram objective (see Section 3.2) and they approximate the computation of the conditional over $z$ for a given word $w$, which requires summing over all possible context words $c$, by summing only over the words observed in the immediate current context of $w$ (thus this sum will very across training example of the same word $w$). Experiments show that the iSG better learns to exploit different dimensions to model different senses of words, better than the original skipgram model. Quantitatively, the iSG seems to provide better probabilities to context words.", "pdf_url": "http://arxiv.org/pdf/1511.05392", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/nalisnickr15.json"}
{"id": "29981772", "bin": "200_300", "summary_sentences": ["What  Style transfer between images works - in its original form - by iteratively making changes to a content image, so that its style matches more and more the style of a chosen style image.", "That iterative process is very slow.", "Alternatively, one can train a single feed-forward generator network to apply a style in one forward pass.", "The network is trained on a dataset of input images and their stylized versions (stylized versions can be generated using the iterative approach).", "So far, these generator networks were much faster than the iterative approach, but their quality was lower.", "They describe a simple change to these generator networks to increase the image quality (up to the same level as the iterative approach).", "How  In the generator networks, they simply replace all batch normalization layers with instance normalization layers.", "Batch normalization normalizes using the information from the whole batch, while instance normalization normalizes each feature map on its own.", "Equations  Let H = Height, W = Width, T = Batch size  Batch Normalization:  Instance Normalization  They apply instance normalization at test time too (identically).", "Results  Same image quality as iterative approach (at a fraction of the runtime).", "One content image with two different styles using their approach:"], "summary_text": "What  Style transfer between images works - in its original form - by iteratively making changes to a content image, so that its style matches more and more the style of a chosen style image. That iterative process is very slow. Alternatively, one can train a single feed-forward generator network to apply a style in one forward pass. The network is trained on a dataset of input images and their stylized versions (stylized versions can be generated using the iterative approach). So far, these generator networks were much faster than the iterative approach, but their quality was lower. They describe a simple change to these generator networks to increase the image quality (up to the same level as the iterative approach). How  In the generator networks, they simply replace all batch normalization layers with instance normalization layers. Batch normalization normalizes using the information from the whole batch, while instance normalization normalizes each feature map on its own. Equations  Let H = Height, W = Width, T = Batch size  Batch Normalization:  Instance Normalization  They apply instance normalization at test time too (identically). Results  Same image quality as iterative approach (at a fraction of the runtime). One content image with two different styles using their approach:", "pdf_url": "https://arxiv.org/pdf/1607.08022", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/instance_normalization_the_missing_ingredient_for_fast_stylization.json"}
{"id": "67744436", "bin": "200_300", "summary_sentences": ["Chen, et al., 2016  InfoGAN is an extension to Generative Adversarial Networks that learns disentangled representations of the latent variables within the generator network.", "The authors employ the variational information maximization framework to optimize a lower bound on a mutual information criterion.", "This MI is between a small subset of the latent variables and the output of the generator.", "The authors argue that this encourages these latent variables to become “disentangled”.", "In turn, this allows the GANs to learn, in a completely unsupervised manner, interesting data representations such as stylistic factors on the MNIST dataset.", "The change to the traditional GAN architecture is minimal, since this is simply a regularized MI term added to the minimax function.", "Questions  Why don’t they use the reverse KL in Eq.", "4?", "This is what is normally used in Variational Bayes (why?).", "The reverse KL, KL(Q || P), is minimized when Q places no probability mass where P has no probability mass..  What justifies moving $f(x,y)$ into the third integral in Eq.", "7?", "What other potential applications of GANs are there besides generating realistic samples from $p_{data}$.", "How can it be used for general density estimation?", "(This seems like a big question)  Are the features learned by the conv nets any different between InfoGAN and GAN?", "(What is the potential importance of this?)", "Does InfoGAN produce sharper/more realistic images than GANs?", "Seems like the answer would be no, but hard to quantify this"], "summary_text": "Chen, et al., 2016  InfoGAN is an extension to Generative Adversarial Networks that learns disentangled representations of the latent variables within the generator network. The authors employ the variational information maximization framework to optimize a lower bound on a mutual information criterion. This MI is between a small subset of the latent variables and the output of the generator. The authors argue that this encourages these latent variables to become “disentangled”. In turn, this allows the GANs to learn, in a completely unsupervised manner, interesting data representations such as stylistic factors on the MNIST dataset. The change to the traditional GAN architecture is minimal, since this is simply a regularized MI term added to the minimax function. Questions  Why don’t they use the reverse KL in Eq. 4? This is what is normally used in Variational Bayes (why?). The reverse KL, KL(Q || P), is minimized when Q places no probability mass where P has no probability mass..  What justifies moving $f(x,y)$ into the third integral in Eq. 7? What other potential applications of GANs are there besides generating realistic samples from $p_{data}$. How can it be used for general density estimation? (This seems like a big question)  Are the features learned by the conv nets any different between InfoGAN and GAN? (What is the potential importance of this?) Does InfoGAN produce sharper/more realistic images than GANs? Seems like the answer would be no, but hard to quantify this", "pdf_url": "https://arxiv.org/pdf/1606.03657", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/infogan.json"}
{"id": "36679338", "bin": "200_300", "summary_sentences": ["An empirical analysis of anonymity in Zcash Kappos et al., USENIX Security’18  As we’ve seen before, in practice Bitcoin offers little in the way of anonymity .", "Zcash on the other hand was carefully designed with privacy in mind.", "It offers strong theoretical guarantees concerning privacy.", "So in theory users of Zcash can remain anonymous.", "In practice though it depends on the way those users interact with Zcash.", "Today’s paper choice, ‘An empirical analysis of anonymity in Zcash’ studies how identifiable transaction participants are in practice based on the 2,242,847 transactions in the blockchain at the time of the study.", "We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage.", "The analysis also provides some interesting insights into who is using Zcash and for what as well.", "Founders and miners combined account for around 66% of the value drawn from the shielded pool.", "The code for the analysis is available online at  [url]"], "summary_text": "An empirical analysis of anonymity in Zcash Kappos et al., USENIX Security’18  As we’ve seen before, in practice Bitcoin offers little in the way of anonymity . Zcash on the other hand was carefully designed with privacy in mind. It offers strong theoretical guarantees concerning privacy. So in theory users of Zcash can remain anonymous. In practice though it depends on the way those users interact with Zcash. Today’s paper choice, ‘An empirical analysis of anonymity in Zcash’ studies how identifiable transaction participants are in practice based on the 2,242,847 transactions in the blockchain at the time of the study. We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage. The analysis also provides some interesting insights into who is using Zcash and for what as well. Founders and miners combined account for around 66% of the value drawn from the shielded pool. The code for the analysis is available online at  [url]", "pdf_url": "https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-kappos.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/an-empirical-analysis-of-anonymity-in-zcash.json"}
{"id": "81465314", "bin": "200_300", "summary_sentences": ["The main idea in this paper is to use the agent's ability to predict observations at the next step as a measure of how much exploration of that action should be encouraged.", "This prediction is based on a deep architecture, specifically a deep autoencoder representation of observations, and accuracy of prediction is measured at the level of that learned, deep representation.", "Exploration is encourage by increasing the reward whenever the models prediction of the representation at the next time step is bad.", "#### My two cents  I'm not sure how novel this idea is in RL, but at the very least it's interesting that it was explored the way it was here, with deep learning.", "As a non-expert in RL, I certainly enjoyed reading the paper.", "Also, this implements nicely an idea that just seems like common sense, as an exploration strategy for an agent: actions that merit exploration are those that yield results that are unexpected to you.", "It will be interesting to see if this general approach will be able to exploit upcoming progress in the development of better generative deep learning models, an area that is currently very active."], "summary_text": "The main idea in this paper is to use the agent's ability to predict observations at the next step as a measure of how much exploration of that action should be encouraged. This prediction is based on a deep architecture, specifically a deep autoencoder representation of observations, and accuracy of prediction is measured at the level of that learned, deep representation. Exploration is encourage by increasing the reward whenever the models prediction of the representation at the next time step is bad. #### My two cents  I'm not sure how novel this idea is in RL, but at the very least it's interesting that it was explored the way it was here, with deep learning. As a non-expert in RL, I certainly enjoyed reading the paper. Also, this implements nicely an idea that just seems like common sense, as an exploration strategy for an agent: actions that merit exploration are those that yield results that are unexpected to you. It will be interesting to see if this general approach will be able to exploit upcoming progress in the development of better generative deep learning models, an area that is currently very active.", "pdf_url": "http://arxiv.org/pdf/1507.00814", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/stadiela15.json"}
{"id": "41467472", "bin": "200_300", "summary_sentences": ["This paper describes how rank pooling, a very recent approach for pooling representations organized in a sequence $\\\\{{\\bf v}_t\\\\}_{t=1}^T$, can be used in an end-to-end trained neural network architecture.", "Rank pooling is an alternative to average and max pooling for sequences, but with the distinctive advantage of maintaining some order information from the sequence.", "Rank pooling first solves a regularized (linear) support vector regression (SVR) problem where the inputs are the vector representations ${\\bf v}_t$ in the sequence and the target is the corresponding index $t$ of that representation in the sequence (see Equation 5).", "The output of rank pooling is then simply the linear regression parameters $\\bf{u}$ learned for that sequence.", "Because of the way ${\\bf u}$ is trained, we can see that ${\\bf u}$ will capture order information, as successful training would imply that ${\\bf u}^\\top {\\bf v}_t < {\\bf u}^\\top {\\bf v}_{t'} $ if $t < t'$.", "See [this paper]( [url]"], "summary_text": "This paper describes how rank pooling, a very recent approach for pooling representations organized in a sequence $\\\\{{\\bf v}_t\\\\}_{t=1}^T$, can be used in an end-to-end trained neural network architecture. Rank pooling is an alternative to average and max pooling for sequences, but with the distinctive advantage of maintaining some order information from the sequence. Rank pooling first solves a regularized (linear) support vector regression (SVR) problem where the inputs are the vector representations ${\\bf v}_t$ in the sequence and the target is the corresponding index $t$ of that representation in the sequence (see Equation 5). The output of rank pooling is then simply the linear regression parameters $\\bf{u}$ learned for that sequence. Because of the way ${\\bf u}$ is trained, we can see that ${\\bf u}$ will capture order information, as successful training would imply that ${\\bf u}^\\top {\\bf v}_t < {\\bf u}^\\top {\\bf v}_{t'} $ if $t < t'$. See [this paper]( [url]", "pdf_url": "http://proceedings.mlr.press/v48/fernando16.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/fernandog16.json"}
{"id": "12175403", "bin": "200_300", "summary_sentences": ["Roweis, Saul, 2000  ISOMAP and MDS require estimates of pairwise distances between data points.", "LLE gets around this by “thinking” globally but fitting locally.", "Essentially, each data point should hypothetically be representable by a locally linear patch.", "Therefore, LLE seeks $W$ such that  is minimized.", "Hence, a data point should be reconstructed by its neighbors; the problem is solved via least squares.", "Note that the weights are invariant to affine transformations and translations.", "Assuming that $W$ should be preserved in a lower dimensional representation of the data, LLE seeks to solve  The optimal coordinates $Y$ can be found by solving a sparse $n \\times n$ eigenvalue problem.", "Because of the simple construction and use of simple linear algebra, LLE has better theoretical properties than other algorithms like autoencoders  It also has less hyperparameters  Doesn’t need to be rerun when new dimensions are added to the embedding space (old ones do not change)  Does LLE work on spheres?", "It seems like it would run into the same problem if the sphere didn’t have a hole taken out of it"], "summary_text": "Roweis, Saul, 2000  ISOMAP and MDS require estimates of pairwise distances between data points. LLE gets around this by “thinking” globally but fitting locally. Essentially, each data point should hypothetically be representable by a locally linear patch. Therefore, LLE seeks $W$ such that  is minimized. Hence, a data point should be reconstructed by its neighbors; the problem is solved via least squares. Note that the weights are invariant to affine transformations and translations. Assuming that $W$ should be preserved in a lower dimensional representation of the data, LLE seeks to solve  The optimal coordinates $Y$ can be found by solving a sparse $n \\times n$ eigenvalue problem. Because of the simple construction and use of simple linear algebra, LLE has better theoretical properties than other algorithms like autoencoders  It also has less hyperparameters  Doesn’t need to be rerun when new dimensions are added to the embedding space (old ones do not change)  Does LLE work on spheres? It seems like it would run into the same problem if the sphere didn’t have a hole taken out of it", "pdf_url": "http://www.robots.ox.ac.uk/~az/lectures/ml/lle.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/lle.json"}
{"id": "99668388", "bin": "200_300", "summary_sentences": ["Introduces fastText, a simple and highly efficient approach for text classification.", "At par with deep learning models in terms of accuracy though an order of magnitude faster in performance.", "Architecture  Built on top of linear models with a rank constraint and a fast loss approximation.", "Start with word representations that are averaged into text representation and feed them to a linear classifier.", "Think of text representation as a hidden state that can be shared among features and classes.", "Softmax layer to obtain a probability distribution over pre-defined classes.", "High computational complexity O(kh), k is the number of classes and h is dimension of text representation.", "Hierarchial Softmax  Based on Huffman Coding Tree  Used to reduce complexity to O(hlog(k))  Top T results (from the tree) can be computed efficiently O(logT) using a binary heap.", "N-gram Features  Instead of explicitly using word order, uses a bag of n-grams to maintain efficiency without losing on accuracy.", "Uses hashing trick to maintain fast and memory efficient mapping of the n-grams.", "Experiments  Sentiment Analysis  fastText benefits by using bigrams.", "Outperforms char-CNN and char-CRNN and performs a bit worse than VDCNN .", "Order of magnitudes faster in terms of training time.", "Note: fastText does not use pre-trained word embeddings.", "Tag Prediction  fastText with bigrams outperforms Tagspace .", "fastText performs upto 600 times faster at test time."], "summary_text": "Introduces fastText, a simple and highly efficient approach for text classification. At par with deep learning models in terms of accuracy though an order of magnitude faster in performance. Architecture  Built on top of linear models with a rank constraint and a fast loss approximation. Start with word representations that are averaged into text representation and feed them to a linear classifier. Think of text representation as a hidden state that can be shared among features and classes. Softmax layer to obtain a probability distribution over pre-defined classes. High computational complexity O(kh), k is the number of classes and h is dimension of text representation. Hierarchial Softmax  Based on Huffman Coding Tree  Used to reduce complexity to O(hlog(k))  Top T results (from the tree) can be computed efficiently O(logT) using a binary heap. N-gram Features  Instead of explicitly using word order, uses a bag of n-grams to maintain efficiency without losing on accuracy. Uses hashing trick to maintain fast and memory efficient mapping of the n-grams. Experiments  Sentiment Analysis  fastText benefits by using bigrams. Outperforms char-CNN and char-CRNN and performs a bit worse than VDCNN . Order of magnitudes faster in terms of training time. Note: fastText does not use pre-trained word embeddings. Tag Prediction  fastText with bigrams outperforms Tagspace . fastText performs upto 600 times faster at test time.", "pdf_url": "http://arxiv.org/pdf/1607.01759v3", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/2746f15889f7f4a798bf7f9ec4b7d8.json"}
{"id": "49519055", "bin": "200_300", "summary_sentences": ["Residual Neural Networks (ResNets) is the current state-of-the-art Convolutional Neural Network architecture.", "The key difference between ResNets and other popular architectures is the use of “shortcut-connections”.", "Basically, it was determined that “very deep” neural networks, or DNNs with a large number of stacked layers,  were exhibiting a degradation in accuracy not caused by overfitting.", "Residual Layers in a DNN attempt to learn a “residual” mapping, which is the desired hidden mapping minus the input to the residual layer.", "The input to the layer is then “added” back in later; hence the name “shortcut-connection”.", "One of the motivations for doing this is that it is easy for the network to learn to send the residual to 0 (and hence learn an identity mapping), which is useful if an identity mapping is optimal for the situation.", "Learning an  identity mapping is very difficult for a stack of nonlinear layers.", "These residual layers do not add any extra parameters.", "Evidence  The authors tested ResNets on ImageNet and CIFAR-10, and won first place on the ILSVRC 2015 classification task.", "Strengths  The idea is simple and effective.", "The material is presented clearly with lots of data to back it up.", "Interesting related works  Batch Norm, Highway Networks"], "summary_text": "Residual Neural Networks (ResNets) is the current state-of-the-art Convolutional Neural Network architecture. The key difference between ResNets and other popular architectures is the use of “shortcut-connections”. Basically, it was determined that “very deep” neural networks, or DNNs with a large number of stacked layers,  were exhibiting a degradation in accuracy not caused by overfitting. Residual Layers in a DNN attempt to learn a “residual” mapping, which is the desired hidden mapping minus the input to the residual layer. The input to the layer is then “added” back in later; hence the name “shortcut-connection”. One of the motivations for doing this is that it is easy for the network to learn to send the residual to 0 (and hence learn an identity mapping), which is useful if an identity mapping is optimal for the situation. Learning an  identity mapping is very difficult for a stack of nonlinear layers. These residual layers do not add any extra parameters. Evidence  The authors tested ResNets on ImageNet and CIFAR-10, and won first place on the ILSVRC 2015 classification task. Strengths  The idea is simple and effective. The material is presented clearly with lots of data to back it up. Interesting related works  Batch Norm, Highway Networks", "pdf_url": "https://arxiv.org/pdf/1512.03385v1.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/resnets.json"}
{"id": "11481096", "bin": "200_300", "summary_sentences": ["The key insight here is that neural networks react globally to local weight changes, which results in unwanted behavior.", "For a neural network that is representing the Q-value function, the influence of a weight change for a new datapoint must be constrained by presenting previous knowledge in the form or prior experiences.", "The proposed algorithm is a special case of experience replay.", "In principle, classical Q-learning can be directly implemented in a neural network.", "An MSE can be used to calculate a loss between the expected Q-value and the generated Q-value.", "Vanilla application of this transformation requires tens of thousands of training examples due to the problem stated above.", "NFQ attempts to address this by doing off-line learning considering an entire set of transition experiences.", "RPROP can be used for updating the weights of the neural network, which is an advanced supervised learning technique.", "Strengths  NFQ is very flexible.", "One variant is to incrementally add new experiences to D, the set of transition experiences.", "This is useful for cases where a reasonable set of experiences can not be collected by controlling the system with purely random actions.", "It seems like you would want to do a combination of both this, and pre-training on sample paths.", "Methods  For the mountain car setup, the authors used an MLP with 3 input neurons (2 state and 1 action), 2 layers of 5 hidden neurons each and 1 output neuron, all with sigmoidal activation functions.", "Interesting related works  RPROP and Batch Learning, both my Riedmiller"], "summary_text": "The key insight here is that neural networks react globally to local weight changes, which results in unwanted behavior. For a neural network that is representing the Q-value function, the influence of a weight change for a new datapoint must be constrained by presenting previous knowledge in the form or prior experiences. The proposed algorithm is a special case of experience replay. In principle, classical Q-learning can be directly implemented in a neural network. An MSE can be used to calculate a loss between the expected Q-value and the generated Q-value. Vanilla application of this transformation requires tens of thousands of training examples due to the problem stated above. NFQ attempts to address this by doing off-line learning considering an entire set of transition experiences. RPROP can be used for updating the weights of the neural network, which is an advanced supervised learning technique. Strengths  NFQ is very flexible. One variant is to incrementally add new experiences to D, the set of transition experiences. This is useful for cases where a reasonable set of experiences can not be collected by controlling the system with purely random actions. It seems like you would want to do a combination of both this, and pre-training on sample paths. Methods  For the mountain car setup, the authors used an MLP with 3 input neurons (2 state and 1 action), 2 layers of 5 hidden neurons each and 1 output neuron, all with sigmoidal activation functions. Interesting related works  RPROP and Batch Learning, both my Riedmiller", "pdf_url": "http://ml.informatik.uni-freiburg.de/_media/publications/rieecml05.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/nfq.json"}
{"id": "57613420", "bin": "200_300", "summary_sentences": ["MadMax: surviving out-of-gas conditions in ethereum smart contracts Grech et al., OOPSLA’18  We’re transitioning to look at a selection of papers from the recent OOPSLA conference this week.", "MadMax won a distinguished paper award, and makes a nice bridge from the CCS blockchain papers we were looking at last week.", "Analysis and verification of smart contracts is a high-value task, possibly more so than in any other programming setting.", "The combination of monetary value and public availability makes the early detection of vulnerabilities a task of paramount importance.", "(Detection may occur after contract deployment.", "Despite the code immutability, which prevents bug fixes, discovering a vulnerability before an attacker may exploit it could enable a trusted third party to move vulnerable funds to safety).", "MadMax is in the same vein as Securify , performing EVM bytecode analysis using Datalog (also with Soufflé ) to infer security issues in contracts.", "In this instance, MadMax focuses on detecting vulnerabilities caused by out-of-gas conditions.", "The paper touches on some nice reusable building blocks (e.g. Vandal ).", "I could easily see Vandal + Soufflé becoming a standard foundation for powerful EVM-based smart contract analysis.", "MadMax is available on GitHub at  [url]"], "summary_text": "MadMax: surviving out-of-gas conditions in ethereum smart contracts Grech et al., OOPSLA’18  We’re transitioning to look at a selection of papers from the recent OOPSLA conference this week. MadMax won a distinguished paper award, and makes a nice bridge from the CCS blockchain papers we were looking at last week. Analysis and verification of smart contracts is a high-value task, possibly more so than in any other programming setting. The combination of monetary value and public availability makes the early detection of vulnerabilities a task of paramount importance. (Detection may occur after contract deployment. Despite the code immutability, which prevents bug fixes, discovering a vulnerability before an attacker may exploit it could enable a trusted third party to move vulnerable funds to safety). MadMax is in the same vein as Securify , performing EVM bytecode analysis using Datalog (also with Soufflé ) to infer security issues in contracts. In this instance, MadMax focuses on detecting vulnerabilities caused by out-of-gas conditions. The paper touches on some nice reusable building blocks (e.g. Vandal ). I could easily see Vandal + Soufflé becoming a standard foundation for powerful EVM-based smart contract analysis. MadMax is available on GitHub at  [url]", "pdf_url": "http://www.nevillegrech.com/madmax-oopsla18.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/madmax-surviving-out-of-gas-conditions-in-ethereum-smart-contracts.json"}
{"id": "22528856", "bin": "200_300", "summary_sentences": ["This paper presents an unsupervised generative model, based on the variational autoencoder framework, but where the encoder is a recurrent neural network that sequentially infers the identity, pose and number of objects in some input scene (2D image or 3D scene).", "In short, this is done by extending the DRAW model to incorporate discrete latent variables that determine whether an additional object is present or not.", "Since the reparametrization trick cannot be used for discrete variables, the authors estimate the gradient through the sampling operation using a likelihood ratio estimator.", "Another innovation over DRAW is the application to 3D scenes, in which the decoder is a graphics renderer.", "Since it is not possible to backpropagate through the renderer, gradients are estimated using finite-difference estimates (which require going through the renderer several times).", "Experiments are presented where the evaluation is focused on the ability of the model to detect and count the number of objects in the image or scene.", "**My two cents**  This is a nice, natural extension of DRAW.", "I'm particularly impressed by the results for the 3D scene setting.", "Despite the fact that setup is obviously synthetic and simplistic, I really surprised that estimating the decoder gradients using finite-differences worked at all.", "It's also interesting to see that the proposed model does surprisingly well compared to a CNN supervised approach that directly predicts the objects identity and pose.", "Quite cool!", "To see the model in action, see [this cute video][1].", "[1]:  [url]"], "summary_text": "This paper presents an unsupervised generative model, based on the variational autoencoder framework, but where the encoder is a recurrent neural network that sequentially infers the identity, pose and number of objects in some input scene (2D image or 3D scene). In short, this is done by extending the DRAW model to incorporate discrete latent variables that determine whether an additional object is present or not. Since the reparametrization trick cannot be used for discrete variables, the authors estimate the gradient through the sampling operation using a likelihood ratio estimator. Another innovation over DRAW is the application to 3D scenes, in which the decoder is a graphics renderer. Since it is not possible to backpropagate through the renderer, gradients are estimated using finite-difference estimates (which require going through the renderer several times). Experiments are presented where the evaluation is focused on the ability of the model to detect and count the number of objects in the image or scene. **My two cents**  This is a nice, natural extension of DRAW. I'm particularly impressed by the results for the 3D scene setting. Despite the fact that setup is obviously synthetic and simplistic, I really surprised that estimating the decoder gradients using finite-differences worked at all. It's also interesting to see that the proposed model does surprisingly well compared to a CNN supervised approach that directly predicts the objects identity and pose. Quite cool! To see the model in action, see [this cute video][1]. [1]:  [url]", "pdf_url": "http://arxiv.org/pdf/1603.08575", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/eslamihwtkh16.json"}
{"id": "40774761", "bin": "200_300", "summary_sentences": ["Firmament: Fast, centralized cluster scheduling at scale Gog et al. OSDI’ 16  Updated link to point to official usenix hosted version  As this paper demonstrates very well, cluster scheduling is a tricky thing to get right at scale.", "It sounds so simple on the surface: “here are some new jobs/tasks – where should I run them?” Of course the slightly more nuanced question, and where the troubles begin, is “where should I run them in order to optimize for this objective, given these constraints…?” Typically you have a trade-off between distributed schedulers that can operate at scale and make fast decisions, and a centralized scheduler that can make higher quality decisions (e.g, improve utilisation, load balance, or whatever else your policy dictates) but struggles to make those decisions quickly enough as workload scales.", "What we have here is Firmament, a new centralised scheduler that combines high-quality placements on a par with advanced centralized schedulers, and the speed and scale of a distributed scheduler.", "It comes from a strong team of researchers across Cambridge, MIT, and Google, and is available in open source at  [url]"], "summary_text": "Firmament: Fast, centralized cluster scheduling at scale Gog et al. OSDI’ 16  Updated link to point to official usenix hosted version  As this paper demonstrates very well, cluster scheduling is a tricky thing to get right at scale. It sounds so simple on the surface: “here are some new jobs/tasks – where should I run them?” Of course the slightly more nuanced question, and where the troubles begin, is “where should I run them in order to optimize for this objective, given these constraints…?” Typically you have a trade-off between distributed schedulers that can operate at scale and make fast decisions, and a centralized scheduler that can make higher quality decisions (e.g, improve utilisation, load balance, or whatever else your policy dictates) but struggles to make those decisions quickly enough as workload scales. What we have here is Firmament, a new centralised scheduler that combines high-quality placements on a par with advanced centralized schedulers, and the speed and scale of a distributed scheduler. It comes from a strong team of researchers across Cambridge, MIT, and Google, and is available in open source at  [url]", "pdf_url": "https://www.usenix.org/system/files/conference/osdi16/osdi16-gog.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/firmament-fast-centralized-cluster-scheduling-at-scale.json"}
{"id": "8136900", "bin": "200_300", "summary_sentences": ["This paper presents an interpretation of dropout training as performing approximate Bayesian learning in a deep Gaussian process (DGP) model.", "This connection suggests a very simple way of obtaining, for networks trained with dropout, estimates of the model's output uncertainty.", "This estimate is based and computed from an ensemble of networks each obtained by sampling a new dropout mask.", "#### My two cents  This is a really nice and thought provoking contribution to our understanding of dropout.", "Unfortunately, the paper in fact doesn't provide a lot of comparisons with either other ways of estimating the predictive uncertainty of deep networks, or to other approximate inference schemes in deep GPs (actually, see update below).", "The qualitative examples provided however do suggest that the uncertainty estimate isn't terrible.", "Irrespective of the quality of the uncertainty estimate suggested here, I find the observation itself really valuable.", "Perhaps future research will then shed light on how useful that method is compared to other approaches, including Bayesian dark knowledge  [ref] .", "`Update: On September 27th`, the authors uploaded to arXiv a new version that now includes comparisons with 2 alternative Bayesian learning methods for deep networks, specifically the stochastic variational inference approach of Graves and probabilistic back-propagation of Hernandez-Lobato and Adams.", "Dropout actually does very well against these baselines and, across datasets, is almost always amongst the best performing method!"], "summary_text": "This paper presents an interpretation of dropout training as performing approximate Bayesian learning in a deep Gaussian process (DGP) model. This connection suggests a very simple way of obtaining, for networks trained with dropout, estimates of the model's output uncertainty. This estimate is based and computed from an ensemble of networks each obtained by sampling a new dropout mask. #### My two cents  This is a really nice and thought provoking contribution to our understanding of dropout. Unfortunately, the paper in fact doesn't provide a lot of comparisons with either other ways of estimating the predictive uncertainty of deep networks, or to other approximate inference schemes in deep GPs (actually, see update below). The qualitative examples provided however do suggest that the uncertainty estimate isn't terrible. Irrespective of the quality of the uncertainty estimate suggested here, I find the observation itself really valuable. Perhaps future research will then shed light on how useful that method is compared to other approaches, including Bayesian dark knowledge  [ref] . `Update: On September 27th`, the authors uploaded to arXiv a new version that now includes comparisons with 2 alternative Bayesian learning methods for deep networks, specifically the stochastic variational inference approach of Graves and probabilistic back-propagation of Hernandez-Lobato and Adams. Dropout actually does very well against these baselines and, across datasets, is almost always amongst the best performing method!", "pdf_url": "http://arxiv.org/pdf/1506.02142", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/galg15.json"}
{"id": "85820029", "bin": "200_300", "summary_sentences": ["The paper presents a new activation function called Swish with formulation f(x) = x.sigmod(x) and its parameterised version called Swish-β where f(x, β) = 2x.sigmoid(β.x) and β is a training parameter.", "The paper shows that Swish is consistently able to outperform RELU and other activations functions over a variety of datasets (CIFAR, ImageNet, WMT2014) though by small margins only in some cases.", "Properties of Swish  Smooth, non-monotonic function.", "Swish-β can be thought of as a smooth function that interpolates between a linear function and RELU.", "Uses self-gating mechanism (that is, it uses its own value to gate itself).", "Gating generally uses multiple scalar inputs but since self-gating uses a single scalar input, it can be used to replace activation functions which are generally pointwise.", "Being unbounded on the x>0 side, it avoids saturation when training is slow due to near 0 gradients.", "Being bounded below induces a kind of regularization effect as large, negative inputs are forgotten.", "Since the Swish function is smooth, the output landscape and the loss landscape are also smooth.", "A smooth landscape should be more traversable and less sensitive to initialization and learning rates.", "Criticism  Swish is much more complicated than ReLU (when weighted against the small improvements that are provided) so it might not end up with as strong an adoption as ReLU."], "summary_text": "The paper presents a new activation function called Swish with formulation f(x) = x.sigmod(x) and its parameterised version called Swish-β where f(x, β) = 2x.sigmoid(β.x) and β is a training parameter. The paper shows that Swish is consistently able to outperform RELU and other activations functions over a variety of datasets (CIFAR, ImageNet, WMT2014) though by small margins only in some cases. Properties of Swish  Smooth, non-monotonic function. Swish-β can be thought of as a smooth function that interpolates between a linear function and RELU. Uses self-gating mechanism (that is, it uses its own value to gate itself). Gating generally uses multiple scalar inputs but since self-gating uses a single scalar input, it can be used to replace activation functions which are generally pointwise. Being unbounded on the x>0 side, it avoids saturation when training is slow due to near 0 gradients. Being bounded below induces a kind of regularization effect as large, negative inputs are forgotten. Since the Swish function is smooth, the output landscape and the loss landscape are also smooth. A smooth landscape should be more traversable and less sensitive to initialization and learning rates. Criticism  Swish is much more complicated than ReLU (when weighted against the small improvements that are provided) so it might not end up with as strong an adoption as ReLU.", "pdf_url": "https://arxiv.org/pdf/1710.05941", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/swish-a-self-gated-activation-function.json"}
{"id": "62056200", "bin": "200_300", "summary_sentences": ["Battaglia, et al., 2016  This ambitious paper proposes a deep learning framework for modeling the physical interactions between objects in an environment.", "The authors present Interaction Networks (IN), which explicitly separate the processes of learning object dynamics and relations between objects.", "IN is designed to work on graphs where objects are nodes and edges are relations between objects.", "This is to make it scalable to different environments.", "The architecture is simple; two MLPs learn representations over object dynamics and relations between objects, respectively.", "The input to the system is the state decomposed into the objects and their physical relations (gravitational attraciton, collisions, springs), as well as external effects (gravity).", "Object states can be further decomposed into position and velocity.", "In general, the output is the velocity of the objects at the subsequent time step.", "The system is evaluated on interesting physical reasoning tasks, such as n-bodies interacting, bouncing balls colliding, and string/mass systems.", "IN showed significantly lower MSE when predicting future states of objects in the scenes compared to simple baselines.", "A custom physics engine was used to generate trajectories for training data.", "All training objectives and test measures used MSE between the model’s predictions and the ground truth target."], "summary_text": "Battaglia, et al., 2016  This ambitious paper proposes a deep learning framework for modeling the physical interactions between objects in an environment. The authors present Interaction Networks (IN), which explicitly separate the processes of learning object dynamics and relations between objects. IN is designed to work on graphs where objects are nodes and edges are relations between objects. This is to make it scalable to different environments. The architecture is simple; two MLPs learn representations over object dynamics and relations between objects, respectively. The input to the system is the state decomposed into the objects and their physical relations (gravitational attraciton, collisions, springs), as well as external effects (gravity). Object states can be further decomposed into position and velocity. In general, the output is the velocity of the objects at the subsequent time step. The system is evaluated on interesting physical reasoning tasks, such as n-bodies interacting, bouncing balls colliding, and string/mass systems. IN showed significantly lower MSE when predicting future states of objects in the scenes compared to simple baselines. A custom physics engine was used to generate trajectories for training data. All training objectives and test measures used MSE between the model’s predictions and the ground truth target.", "pdf_url": "https://arxiv.org/pdf/1612.00222", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/interaction-networks-for-learning-about-objects-relations-physics.json"}
{"id": "4206973", "bin": "200_300", "summary_sentences": ["James Martens, 2010  This paper introduces a fairly complex optimization algorithm for deep nets that uses approximate 2nd-order gradient information  In Hessian-Free optimization, you can directly approximate a Hessian-vector product $Hv$ with the method of finite-differences; this only costs 1 more gradient evaluation  linear conjugate gradient algorithm allows one to solve for the optimal search direction in $O(N)$ iterations ($N$ is the number of parameters) with only matrix-vector products  Newton’s method is scale invariant, e.g., for a new parameterization $\\hat{\\theta} = A \\theta$ for some invertible matrix $A$, the optimal search direction is now $\\hat{p} = A p$ where $p$ is the original optimal search direction.", "Gradient descent is not (need proof!)", "- so many bad things about GD, but it’s so easy to implement..", "Considerations when applying this technique  Need to use an adaptive damping parameters $\\lambda$ beause the relative scale of $B = H(\\theta)$ is changing and $H(\\theta)$ must remain positive semidefinite.", "Recommended heuristic is given in Section 4.1  Gauss-Newton matrix $G$ can produce better search directions than $H$, see this blog post for a summary  Compute gradient on entire dataset, but use minibatches to compute Hessian-vector products.", "SGD requires 10’s of thousands of iterations versus ~200 for HF"], "summary_text": "James Martens, 2010  This paper introduces a fairly complex optimization algorithm for deep nets that uses approximate 2nd-order gradient information  In Hessian-Free optimization, you can directly approximate a Hessian-vector product $Hv$ with the method of finite-differences; this only costs 1 more gradient evaluation  linear conjugate gradient algorithm allows one to solve for the optimal search direction in $O(N)$ iterations ($N$ is the number of parameters) with only matrix-vector products  Newton’s method is scale invariant, e.g., for a new parameterization $\\hat{\\theta} = A \\theta$ for some invertible matrix $A$, the optimal search direction is now $\\hat{p} = A p$ where $p$ is the original optimal search direction. Gradient descent is not (need proof!) - so many bad things about GD, but it’s so easy to implement.. Considerations when applying this technique  Need to use an adaptive damping parameters $\\lambda$ beause the relative scale of $B = H(\\theta)$ is changing and $H(\\theta)$ must remain positive semidefinite. Recommended heuristic is given in Section 4.1  Gauss-Newton matrix $G$ can produce better search directions than $H$, see this blog post for a summary  Compute gradient on entire dataset, but use minibatches to compute Hessian-vector products. SGD requires 10’s of thousands of iterations versus ~200 for HF", "pdf_url": "http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/deep-hessian-free.json"}
{"id": "5681713", "bin": "200_300", "summary_sentences": ["An experimental security analysis of an industrial robot controller Quarta et al., IEEE Security and Privacy 2017  This is an industrial robot:  The International Federation of Robotics forecasts that, by 2018, approximately 1.3 million industrial robot units will be employed in factories globally, and the international market value for “robotized” systems is approximately 32 billion USD.", "In all of their forms, robots are complex automation devices that heavily interact with the physical world…  Most of these control systems were born in an era when they were assumed to be isolated from the network, but are now gaining new interconnections.", "And hey, guess what:  Unfortunately, even a simple Shodan query shows that sometimes industrial robots are exposed on the Internet without being properly secured.", "In this paper, the authors undertake a systematic analysis of the attack surface and potential impacts of cyber attacks against industrial robots.", "Their findings are sadly not surprising, yet at the same time some of the things you’re about to read may leave you open-mouthed in disbelief.", "It’s a perfect case study in how not to do things!", "Welcome to Industry 4.0 and the world of connected robots  Industrial robots are “connected” primarily for programming and maintenance purposes – a use case specified by ISO standards.", "For instance, in a large car production plant developed by KUKA Robotics, all the 259 robots are connected to central control and monitoring systems.", "The industrial robot ecosystem is blooming, there are “Robot Web Service” HTTP REST APIs, a Robot App Store (  [url]"], "summary_text": "An experimental security analysis of an industrial robot controller Quarta et al., IEEE Security and Privacy 2017  This is an industrial robot:  The International Federation of Robotics forecasts that, by 2018, approximately 1.3 million industrial robot units will be employed in factories globally, and the international market value for “robotized” systems is approximately 32 billion USD. In all of their forms, robots are complex automation devices that heavily interact with the physical world…  Most of these control systems were born in an era when they were assumed to be isolated from the network, but are now gaining new interconnections. And hey, guess what:  Unfortunately, even a simple Shodan query shows that sometimes industrial robots are exposed on the Internet without being properly secured. In this paper, the authors undertake a systematic analysis of the attack surface and potential impacts of cyber attacks against industrial robots. Their findings are sadly not surprising, yet at the same time some of the things you’re about to read may leave you open-mouthed in disbelief. It’s a perfect case study in how not to do things! Welcome to Industry 4.0 and the world of connected robots  Industrial robots are “connected” primarily for programming and maintenance purposes – a use case specified by ISO standards. For instance, in a large car production plant developed by KUKA Robotics, all the 259 robots are connected to central control and monitoring systems. The industrial robot ecosystem is blooming, there are “Robot Web Service” HTTP REST APIs, a Robot App Store (  [url]", "pdf_url": "http://robosec.org/downloads/paper-robosec-sp-2017.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/an-experimental-security-analysis-of-an-industrial-robot-controller.json"}
{"id": "47798365", "bin": "200_300", "summary_sentences": ["They represent an image as a tree where leafs are pixels and nodes represent clusters of those pixels.", "They train by regressing for some possible segmented region $r$ on the following function for every segmentation example and ground truth: $$S(r)=\\frac{\\\\#(g) - \\\\#(r)}{\\max(\\\\#(r), \\\\#(g)))}$$  Here $\\\\#(g)$ is the number of pixels in the ground truth and $\\\\#(r)$ is the number of pixels in the example segmentation.", "What is not explained here is what other information is used because it cannot simple be pixel counts.", "This function is used to rank the nodes in every path from the root to the leafs in Figure (a).", "The idea for the segmentation is that there is some set of nodes such that you can draw a line shown in Figure (b) which is equivalent to selecting a segmentation.", "The paper goes on to compute this using a dynamic programming solution based on the fact that the same pixel segmentations will be considered multiple times.", "!", "[]( [url]"], "summary_text": "They represent an image as a tree where leafs are pixels and nodes represent clusters of those pixels. They train by regressing for some possible segmented region $r$ on the following function for every segmentation example and ground truth: $$S(r)=\\frac{\\\\#(g) - \\\\#(r)}{\\max(\\\\#(r), \\\\#(g)))}$$  Here $\\\\#(g)$ is the number of pixels in the ground truth and $\\\\#(r)$ is the number of pixels in the example segmentation. What is not explained here is what other information is used because it cannot simple be pixel counts. This function is used to rank the nodes in every path from the root to the leafs in Figure (a). The idea for the segmentation is that there is some set of nodes such that you can draw a line shown in Figure (b) which is equivalent to selecting a segmentation. The paper goes on to compute this using a dynamic programming solution based on the fact that the same pixel segmentations will be considered multiple times. ! []( [url]", "pdf_url": "http://varcity.eu/paper/cvpr2016_chen_alignment.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/chen2016scaleaware.json"}
{"id": "42851012", "bin": "200_300", "summary_sentences": ["What  They suggest a method to get cumulative/aggregated embedding from a sequence of embeddings (i.e get a single face embedding vector from a video).", "How  Use attention mechanism to weight embeddings in a sequence.", "They suggest two options:  Single attention block – Universal face feature quality measurement.", "where f_k = embedding for k-th image in a sequence, a_k = obtained weight corresponded to k-th embedding  Trainable parameter is: q (shape = embedding size x 1)  Cascaded two attention blocks – Content-aware aggregation.", "This q^1 replaces the q in above formula that computes coefficients a_k.", "Trainable parameters are: W (shape = embeddings size x embeddings size), b (shape = embedding size x 1).", "Face embedder (could be any CNN) and \"Attention blocks\" can be trained together in end-to-end manner or separately one-by-one.", "Training procedure:  For verification problem they used siamese structure with contrastive loss.", "For identification problem they used softmax and cross-entropy as loss function.", "No recurrent blocks, but still input size independent.", "Coefficients a (from the first attention block) strongly correlates with face quality and it's usefulness for recognition.", "Results  Shows better results than combining a single embedding by taking mean, median, l2/cos closest, etc.", "Shows state-of-the-art performance on YouTubeFaces and IJB-A datasets."], "summary_text": "What  They suggest a method to get cumulative/aggregated embedding from a sequence of embeddings (i.e get a single face embedding vector from a video). How  Use attention mechanism to weight embeddings in a sequence. They suggest two options:  Single attention block – Universal face feature quality measurement. where f_k = embedding for k-th image in a sequence, a_k = obtained weight corresponded to k-th embedding  Trainable parameter is: q (shape = embedding size x 1)  Cascaded two attention blocks – Content-aware aggregation. This q^1 replaces the q in above formula that computes coefficients a_k. Trainable parameters are: W (shape = embeddings size x embeddings size), b (shape = embedding size x 1). Face embedder (could be any CNN) and \"Attention blocks\" can be trained together in end-to-end manner or separately one-by-one. Training procedure:  For verification problem they used siamese structure with contrastive loss. For identification problem they used softmax and cross-entropy as loss function. No recurrent blocks, but still input size independent. Coefficients a (from the first attention block) strongly correlates with face quality and it's usefulness for recognition. Results  Shows better results than combining a single embedding by taking mean, median, l2/cos closest, etc. Shows state-of-the-art performance on YouTubeFaces and IJB-A datasets.", "pdf_url": "https://arxiv.org/pdf/1603.05474", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/nan_for_video_face_recognition.json"}
{"id": "73424952", "bin": "200_300", "summary_sentences": ["We’ve reached the end of term again on The Morning Paper, and I’ll be taking a two week break.", "The Morning Paper will resume on Tuesday 7th May (since Monday 6th is a public holiday in the UK).", "My end of term tradition is to highlight a few of the papers from the term that I especially enjoyed, but this time around I want to let one work stand alone:  Making reliable distributed systems in the presence of software errors , Joe Armstrong, December 2003.", "You might also enjoy “ The Mess We’re In ,” and Joe’s seven deadly sins of programming:  Code even you cannot understand a week after you wrote it – no comments  Code with no specifications  Code that is shipped as soon as it runs and before it is beautiful  Code with added features  Code that is very very fast very very very obscure and incorrect  Code that is not beautiful  Code that you wrote without understanding the problem  We’re in an even bigger mess without you Joe.", "Thank you for everything.", "RIP."], "summary_text": "We’ve reached the end of term again on The Morning Paper, and I’ll be taking a two week break. The Morning Paper will resume on Tuesday 7th May (since Monday 6th is a public holiday in the UK). My end of term tradition is to highlight a few of the papers from the term that I especially enjoyed, but this time around I want to let one work stand alone:  Making reliable distributed systems in the presence of software errors , Joe Armstrong, December 2003. You might also enjoy “ The Mess We’re In ,” and Joe’s seven deadly sins of programming:  Code even you cannot understand a week after you wrote it – no comments  Code with no specifications  Code that is shipped as soon as it runs and before it is beautiful  Code with added features  Code that is very very fast very very very obscure and incorrect  Code that is not beautiful  Code that you wrote without understanding the problem  We’re in an even bigger mess without you Joe. Thank you for everything. RIP.", "pdf_url": "http://erlang.org/download/armstrong_thesis_2003.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/end-of-term-5.json"}
{"id": "69537377", "bin": "200_300", "summary_sentences": ["Conditional version of Generative Adversarial Nets (GAN) where both generator and discriminator are conditioned on some data y (class label or data from some other modality).", "Architecture  Feed y into both the generator and discriminator as additional input layers such that y and input are combined in a joint hidden representation.", "Experiment  Unimodal Setting  Conditioning MNIST images on class labels.", "z (random noise) and y mapped to hidden layers with ReLu with layer sizes of 200 and 1000 respectively and are combined to obtain ReLu layer of dimensionality 1200.", "Discriminator maps x (input) and y to maxout layers and the joint maxout layer is fed to sigmoid layer.", "Results do not outperform the state-of-the-art results but do provide a proof-of-the-concept.", "Multimodal Setting  Map images (from Flickr) to labels (or user tags) to obtain the one-to-many mapping.", "Extract image and text features using convolutional and language model.", "Generative Model  Map noise and convolutional features to a single 200 dimensional representation.", "Discriminator Model  Combine the representation of word vectors (corresponding to tags) and images.", "Future Work  While the results are not so good, they do show the potential of Conditional GANs, especially in the multimodal setting."], "summary_text": "Conditional version of Generative Adversarial Nets (GAN) where both generator and discriminator are conditioned on some data y (class label or data from some other modality). Architecture  Feed y into both the generator and discriminator as additional input layers such that y and input are combined in a joint hidden representation. Experiment  Unimodal Setting  Conditioning MNIST images on class labels. z (random noise) and y mapped to hidden layers with ReLu with layer sizes of 200 and 1000 respectively and are combined to obtain ReLu layer of dimensionality 1200. Discriminator maps x (input) and y to maxout layers and the joint maxout layer is fed to sigmoid layer. Results do not outperform the state-of-the-art results but do provide a proof-of-the-concept. Multimodal Setting  Map images (from Flickr) to labels (or user tags) to obtain the one-to-many mapping. Extract image and text features using convolutional and language model. Generative Model  Map noise and convolutional features to a single 200 dimensional representation. Discriminator Model  Combine the representation of word vectors (corresponding to tags) and images. Future Work  While the results are not so good, they do show the potential of Conditional GANs, especially in the multimodal setting.", "pdf_url": "https://arxiv.org/pdf/1411.1784", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/726334de3014defeeb701099a3b4b3.json"}
{"id": "4647896", "bin": "300_400", "summary_sentences": ["Mikolov, et al., 2013  Skip-gram model  Objective is to find word representations that are useful for predicting the surrounding words in a sentence or a document.", "Given a sequence of words $w_1, w_2, …, w_T$, the Skip-gram model aims to max the average log probability  where c is the size of the training context.", "Larger c results in more training examples and thus can lead to a higher accuracy at the expense of increased training time.", "The probability $p(w_O | w_I )$ is represented with a softmax.", "Heirarchical Softmax  Instead of evaluated W output nodes of a neural network to get the probability distribution, where W is the size of the target dictionary, only need to evaluate about $\\log_2 (W)$ nodes.", "The idea is to represent the output layer as a binary tree with W leaves and, for each node, explicitly represents the relative probabilities of its child nodes.", "Then the probability $p(w_O | w_I )$ can be defined by the product of probabilities of a path down the tree from the root.", "The root here is the first word in the sequence.", "The individual probabilities are outputs of a sigmoid, scaled by +1 or -1 if the current word w’s probability matches that of its child.", "Negative Sampling  A simplified form of something called Noice Constrastive Estimation (NCE).", "NCE aims to learn a model that is able to differentiate data from noise by means of logistic regression.", "The negative sampling objective simplifies this because for the Skip-gram model, only the high-quality vector representation is needed.", "The task becomes to distinguish the target word from draws from a noise distribution using logistic regression over k negative samples for each data sample.", "Conclusion  The authors used a few other tricks, like sub-sampling frequent words such as “in”, “the”, “a”.", "Also, they used unigrams and bigrams to identify phrases during training.", "This approach can be applied to massive monolingual corpuses to quickly learn high-quality vector representations of words."], "summary_text": "Mikolov, et al., 2013  Skip-gram model  Objective is to find word representations that are useful for predicting the surrounding words in a sentence or a document. Given a sequence of words $w_1, w_2, …, w_T$, the Skip-gram model aims to max the average log probability  where c is the size of the training context. Larger c results in more training examples and thus can lead to a higher accuracy at the expense of increased training time. The probability $p(w_O | w_I )$ is represented with a softmax. Heirarchical Softmax  Instead of evaluated W output nodes of a neural network to get the probability distribution, where W is the size of the target dictionary, only need to evaluate about $\\log_2 (W)$ nodes. The idea is to represent the output layer as a binary tree with W leaves and, for each node, explicitly represents the relative probabilities of its child nodes. Then the probability $p(w_O | w_I )$ can be defined by the product of probabilities of a path down the tree from the root. The root here is the first word in the sequence. The individual probabilities are outputs of a sigmoid, scaled by +1 or -1 if the current word w’s probability matches that of its child. Negative Sampling  A simplified form of something called Noice Constrastive Estimation (NCE). NCE aims to learn a model that is able to differentiate data from noise by means of logistic regression. The negative sampling objective simplifies this because for the Skip-gram model, only the high-quality vector representation is needed. The task becomes to distinguish the target word from draws from a noise distribution using logistic regression over k negative samples for each data sample. Conclusion  The authors used a few other tricks, like sub-sampling frequent words such as “in”, “the”, “a”. Also, they used unigrams and bigrams to identify phrases during training. This approach can be applied to massive monolingual corpuses to quickly learn high-quality vector representations of words.", "pdf_url": "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/distributed-representations-of-words-and-phrases.json"}
{"id": "18304114", "bin": "300_400", "summary_sentences": ["This paper presents Swapout, a simple dropout method applied to Residual Networks (ResNets).", "In a ResNet, a layer $Y$ is computed from the previous layer $X$ as  $Y = X + F(X)$  where $F(X)$ is essentially the composition of a few convolutional layers.", "Swapout simply applies dropout separately on both terms of a layer's equation:  $Y = \\Theta_1 \\odot X + \\Theta_2 \\odot F(X)$  where $\\Theta_1$ and $\\Theta_2$ are independent dropout masks for each term.", "The paper shows that this form of dropout is at least as good or superior as other forms of dropout, including the recently proposed [stochastic depth dropout][1].", "Much like in the stochastic depth paper, better performance is achieved by linearly increasing the dropout rate (from 0 to 0.5) from the first hidden layer to the last.", "In addition to this observation, I also note the following empirical observations:  1.", "At test time, averaging the output layers of multiple dropout mask samples (referenced to as stochastic inference) is better than replacing the masks by their expectation (deterministic inference), the latter being the usual standard.", "2.", "Comparable performance is achieved by making the ResNet wider (e.g. 4 times) and with fewer layers (e.g.", "32) than the orignal ResNet work with thin but very deep (more than 1000 layers) ResNets.", "This would confirm a similar observation from [this paper][2].", "Overall, these are useful observations to be aware of for anyone wanting to use ResNets in practice.", "[1]:  [url]"], "summary_text": "This paper presents Swapout, a simple dropout method applied to Residual Networks (ResNets). In a ResNet, a layer $Y$ is computed from the previous layer $X$ as  $Y = X + F(X)$  where $F(X)$ is essentially the composition of a few convolutional layers. Swapout simply applies dropout separately on both terms of a layer's equation:  $Y = \\Theta_1 \\odot X + \\Theta_2 \\odot F(X)$  where $\\Theta_1$ and $\\Theta_2$ are independent dropout masks for each term. The paper shows that this form of dropout is at least as good or superior as other forms of dropout, including the recently proposed [stochastic depth dropout][1]. Much like in the stochastic depth paper, better performance is achieved by linearly increasing the dropout rate (from 0 to 0.5) from the first hidden layer to the last. In addition to this observation, I also note the following empirical observations:  1. At test time, averaging the output layers of multiple dropout mask samples (referenced to as stochastic inference) is better than replacing the masks by their expectation (deterministic inference), the latter being the usual standard. 2. Comparable performance is achieved by making the ResNet wider (e.g. 4 times) and with fewer layers (e.g. 32) than the orignal ResNet work with thin but very deep (more than 1000 layers) ResNets. This would confirm a similar observation from [this paper][2]. Overall, these are useful observations to be aware of for anyone wanting to use ResNets in practice. [1]:  [url]", "pdf_url": "http://arxiv.org/pdf/1605.06465v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/1605.06465.json"}
{"id": "78860785", "bin": "300_400", "summary_sentences": ["The paper presents a generalized framework for graph clustering (clusters of network motifs) on the basis of higher-order connectivity patterns.", "Approach  Given a motif M , the framework aims to find a cluster of the set of nodes S such that nodes of S participate in many instances of M and avoid cutting instances of M (that is only a subset of nodes in instances of M appears in S).", "Mathematically, the aim is to minimise the motif conductance metric given as cutM(S, S’) / min[volM(S), volM(S’)] where S’ is complement of S, cutM(S, S’) = number of instances of M which have atleast one node from both S and S’ and volM(S) = Number of nodes in instances of M that belong only to S.  Solving the above equation is computationally infeasible and an approximate solution is proposed using eigenvalues and matrices.", "The approximate solution is easy to implement, efficient and guaranteed to find clusters that are at most a quadratic factor away from the optimal.", "Algorithm  Given the network and motif M, form a motif adjacency matrix WM where WM(i, j) is the number of instances of M that contains i and j.  Compute spectral ordering of the nodes from normalized motif laplacian matrix.", "Compute prefix set of spectral ordering with small motif conductance.", "Scalability  Worst case O(m1.5), based on experiments O(m1.2) where m is the number of edges.", "Advantages  Applicable to directed, undirected and weighted graphs (allows for negative edge weights as well).", "In case the motif is not known beforehand, the framework can be used to compute significant motifs.", "The proposed framework unifies the two fundamental tools of network science (motif analysis and network partitioning) along with some worst-case guarantees for the approximations employed and can be extended to identify higher order modular organization of networks."], "summary_text": "The paper presents a generalized framework for graph clustering (clusters of network motifs) on the basis of higher-order connectivity patterns. Approach  Given a motif M , the framework aims to find a cluster of the set of nodes S such that nodes of S participate in many instances of M and avoid cutting instances of M (that is only a subset of nodes in instances of M appears in S). Mathematically, the aim is to minimise the motif conductance metric given as cutM(S, S’) / min[volM(S), volM(S’)] where S’ is complement of S, cutM(S, S’) = number of instances of M which have atleast one node from both S and S’ and volM(S) = Number of nodes in instances of M that belong only to S.  Solving the above equation is computationally infeasible and an approximate solution is proposed using eigenvalues and matrices. The approximate solution is easy to implement, efficient and guaranteed to find clusters that are at most a quadratic factor away from the optimal. Algorithm  Given the network and motif M, form a motif adjacency matrix WM where WM(i, j) is the number of instances of M that contains i and j.  Compute spectral ordering of the nodes from normalized motif laplacian matrix. Compute prefix set of spectral ordering with small motif conductance. Scalability  Worst case O(m1.5), based on experiments O(m1.2) where m is the number of edges. Advantages  Applicable to directed, undirected and weighted graphs (allows for negative edge weights as well). In case the motif is not known beforehand, the framework can be used to compute significant motifs. The proposed framework unifies the two fundamental tools of network science (motif analysis and network partitioning) along with some worst-case guarantees for the approximations employed and can be extended to identify higher order modular organization of networks.", "pdf_url": "https://arxiv.org/pdf/1612.08447.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/higher-order-organization-of-complex-networks.json"}
{"id": "93060618", "bin": "300_400", "summary_sentences": ["What  They describe a model that upscales low resolution images to their high resolution equivalents (\"Single Image Super Resolution\").", "Their model uses a deeper architecture than previous models and has a residual component.", "How  Their model is a fully convolutional neural network.", "Input of the model: The image to upscale, already upscaled to the desired size (but still blurry).", "Output of the model: The upscaled image (without the blurriness).", "They use 20 layers of padded 3x3 convolutions with size 64xHxW with ReLU activations.", "(No pooling.)", "They have a residual component, i.e. the model only learns and outputs the change that has to be applied/added to the blurry input image (instead of outputting the full image).", "That change is applied to the blurry input image before using the loss function on it.", "(Note that this is a bit different from the currently used \"residual learning\".)", "They use a MSE between the \"correct\" upscaling and the generated upscaled image (input image + residual).", "They use SGD starting with a learning rate of 0.1 and decay it 3 times by a factor of 10.", "They use weight decay of 0.0001.", "During training they use a special gradient clipping adapted to the learning rate.", "Usually gradient clipping restricts the gradient values to [-t, t] (t is a hyperparameter).", "Their gradient clipping restricts the values to [-t/lr, t/lr] (where lr is the learning rate).", "They argue that their special gradient clipping allows the use of significantly higher learning rates.", "They train their model on multiple scales, e.g. 2x, 3x, 4x upscaling.", "(Not really clear how.", "They probably feed their upscaled image again into the network or something like that?)", "Results  Higher accuracy upscaling than all previous methods.", "Can handle well upscaling factors above 2x.", "Residual network learns significantly faster than non-residual network.", "Architecture of the model.", "Super-resolution quality of their model (top, bottom is a competing model)."], "summary_text": "What  They describe a model that upscales low resolution images to their high resolution equivalents (\"Single Image Super Resolution\"). Their model uses a deeper architecture than previous models and has a residual component. How  Their model is a fully convolutional neural network. Input of the model: The image to upscale, already upscaled to the desired size (but still blurry). Output of the model: The upscaled image (without the blurriness). They use 20 layers of padded 3x3 convolutions with size 64xHxW with ReLU activations. (No pooling.) They have a residual component, i.e. the model only learns and outputs the change that has to be applied/added to the blurry input image (instead of outputting the full image). That change is applied to the blurry input image before using the loss function on it. (Note that this is a bit different from the currently used \"residual learning\".) They use a MSE between the \"correct\" upscaling and the generated upscaled image (input image + residual). They use SGD starting with a learning rate of 0.1 and decay it 3 times by a factor of 10. They use weight decay of 0.0001. During training they use a special gradient clipping adapted to the learning rate. Usually gradient clipping restricts the gradient values to [-t, t] (t is a hyperparameter). Their gradient clipping restricts the values to [-t/lr, t/lr] (where lr is the learning rate). They argue that their special gradient clipping allows the use of significantly higher learning rates. They train their model on multiple scales, e.g. 2x, 3x, 4x upscaling. (Not really clear how. They probably feed their upscaled image again into the network or something like that?) Results  Higher accuracy upscaling than all previous methods. Can handle well upscaling factors above 2x. Residual network learns significantly faster than non-residual network. Architecture of the model. Super-resolution quality of their model (top, bottom is a competing model).", "pdf_url": "http://arxiv.org/pdf/1511.04587", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/accurate_image_super-resolution.json"}
{"id": "37796162", "bin": "300_400", "summary_sentences": ["This paper proposes a novel neural architecture for solving the super resolution task for large factors of magnification.", "The problem is challenging, because the input images can be as low-res as 8 x 8; hence, there is a wide variety of high-res images that could correspond to this image.", "The author’s neural architecture consists of an autoregressive PixelCNN network augmented with a conditioning Convolutional Neural Network; the autoregressive aspect is to allow the network to output pixels sequentially and thereby capture the conditional dependency between a pixel and its neighbors.", "This is in contrast to prior work that assume conditional independence between pixels, and use a MSE per-pixel loss for supervision.", "Questions for discussion  Not too familiar with this line of research; 32 x 32 images is still fairly “low-res”.", "Is this state-of-the-art?", "The authors highlighted how the perceptual quality did not always correspond with negative log likelihood.", "Why might this be?", "NLL is equivalent to MLE, which is equivalent to minimizing KL divergence…  General commentary  Dictionary-inspired methods that search a bank of pre-learned filters on images and selecting appropriate patches by an efficient hashing mechanism has comparable performance  PixelCNN is a stochastic model that provides an explicit model for $ \\log p( y_i | x, y_{< i}) $.", "However, the auto-regressive distribution largely ignores the conditioning on the low-resoultion image without explicitly separating the network into two components, one being a “conditioning” network.", "Had to add an extra term to the loss, the cross-entropy between the conditioning network’s predictions via a softmax over the K possible values that the i'th output pixel can take and the ground truth labels"], "summary_text": "This paper proposes a novel neural architecture for solving the super resolution task for large factors of magnification. The problem is challenging, because the input images can be as low-res as 8 x 8; hence, there is a wide variety of high-res images that could correspond to this image. The author’s neural architecture consists of an autoregressive PixelCNN network augmented with a conditioning Convolutional Neural Network; the autoregressive aspect is to allow the network to output pixels sequentially and thereby capture the conditional dependency between a pixel and its neighbors. This is in contrast to prior work that assume conditional independence between pixels, and use a MSE per-pixel loss for supervision. Questions for discussion  Not too familiar with this line of research; 32 x 32 images is still fairly “low-res”. Is this state-of-the-art? The authors highlighted how the perceptual quality did not always correspond with negative log likelihood. Why might this be? NLL is equivalent to MLE, which is equivalent to minimizing KL divergence…  General commentary  Dictionary-inspired methods that search a bank of pre-learned filters on images and selecting appropriate patches by an efficient hashing mechanism has comparable performance  PixelCNN is a stochastic model that provides an explicit model for $ \\log p( y_i | x, y_{< i}) $. However, the auto-regressive distribution largely ignores the conditioning on the low-resoultion image without explicitly separating the network into two components, one being a “conditioning” network. Had to add an extra term to the loss, the cross-entropy between the conditioning network’s predictions via a softmax over the K possible values that the i'th output pixel can take and the ground truth labels", "pdf_url": "https://arxiv.org/pdf/1702.00783", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/pixel-recursive-super-res.json"}
{"id": "90011197", "bin": "300_400", "summary_sentences": ["This workshop paper explores the problem of style transfer in natural language generation (NLG).", "One possible manifestation would be rewriting technical articles in an easy-to-understate manner.", "Challenges  Identifying relevant stylistic cues and using them to control text generation in NLG systems.", "Absence of a large amount of training data.", "Pitch  Using Recurrent Neural Networks (RNNs) to disentangle the style from semantic content.", "Autoencoder model with two components - one for learning style and another for learning content.", "This allows for “style” component to be replaced while keeping the “content” component same, resulting in a style transfer.", "One way to think about this is - the encoder generates a 100-dimensional vector.", "In this, the first 50 entries, correspond to the “style” component and remaining to the “content” component.", "The proposal is that the loss function should be modified to include a cross-covariance term for ensuring disentanglement.", "I think one way of doing this is to have two loss functions:  The first loss function ensures that the input sentence is decoded properly into the target sentence.", "This loss is computed for each sentence.", "The second loss ensures that the first 50 entries across all the encoded represenations are are correlated.", "This loss operates at the batch level.", "The total loss is the weighted sum of these 2 losses.", "Possible Datasets  Complete works of Shakespeare  Wikpedia Kaggle dataset  Oxford Text Archive  Twitter data  Possible Metrics  Soundness - is the generated text entailed with the input sentence.", "Coherence - free of grammatical errors, proper word usage etc.", "Effectiveness - how effective was the style transfer  Since some of the metrics are subjective, human evaluators also need to be employed."], "summary_text": "This workshop paper explores the problem of style transfer in natural language generation (NLG). One possible manifestation would be rewriting technical articles in an easy-to-understate manner. Challenges  Identifying relevant stylistic cues and using them to control text generation in NLG systems. Absence of a large amount of training data. Pitch  Using Recurrent Neural Networks (RNNs) to disentangle the style from semantic content. Autoencoder model with two components - one for learning style and another for learning content. This allows for “style” component to be replaced while keeping the “content” component same, resulting in a style transfer. One way to think about this is - the encoder generates a 100-dimensional vector. In this, the first 50 entries, correspond to the “style” component and remaining to the “content” component. The proposal is that the loss function should be modified to include a cross-covariance term for ensuring disentanglement. I think one way of doing this is to have two loss functions:  The first loss function ensures that the input sentence is decoded properly into the target sentence. This loss is computed for each sentence. The second loss ensures that the first 50 entries across all the encoded represenations are are correlated. This loss operates at the batch level. The total loss is the weighted sum of these 2 losses. Possible Datasets  Complete works of Shakespeare  Wikpedia Kaggle dataset  Oxford Text Archive  Twitter data  Possible Metrics  Soundness - is the generated text entailed with the input sentence. Coherence - free of grammatical errors, proper word usage etc. Effectiveness - how effective was the style transfer  Since some of the metrics are subjective, human evaluators also need to be employed.", "pdf_url": "https://arxiv.org/pdf/1802.04687", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/stylistic-transfer-in-natural-language-generation-systems-using-recurrent-neural-networks.json"}
{"id": "69119323", "bin": "300_400", "summary_sentences": ["What:  HardNet model which improves state-of-the-art in wide baseline stereo, patch matching, verification and image retrieval.", "They introduced a new triplet-like loss function with built-in hard-negative mining.", "How:  HardNet Triplet loss is a regular Triplet-Loss, i.e. MAX(0, alpha + distances_to_positives - distances_to_negatives), where:  alpha (sometimes called \"margin\") is a hyper-parameter  distance_to_positives are distances (here, L2 is used)  distance_to_negative are distances to the hardest negatives for each anchor in a batch.", "As input HardNet operates with N * 2 images (N anchor/query images and N corresponding to them positives)  Mining algorithm: 1.", "Compute distance matrix D between N anchors and N positives.", "2. distances_to_positives = trace of distance matrix (diagonal elements) 3.", "For each row minimal non-diagonal element is taken as a distance to the hardest negatives (closest to anchor).", "From these chosen values distances_to_negatives are obtained.", "All this can be rewritten as:  Loss = MAX(0, alpha + Trace(D) + row_wise_min(D + I * inf)), where I is the identity matrix.", "Architecture:  Notes:  The described mining procedure highly relies on a fact that all N anchors would should to N different classes.", "And from my personal point of view requires minor modification to handle such corner case.", "The given loss/mining procedure is fast, but in contrast to other mining strategies doesn't provide hardest positive (furthest from anchor).", "Results:  Wide baseline stereo example:  The bigger batch size, the better:  PhotoTour Patch Verification Results:  Oxford 5k, Paris 6k Patch Verification Results:"], "summary_text": "What:  HardNet model which improves state-of-the-art in wide baseline stereo, patch matching, verification and image retrieval. They introduced a new triplet-like loss function with built-in hard-negative mining. How:  HardNet Triplet loss is a regular Triplet-Loss, i.e. MAX(0, alpha + distances_to_positives - distances_to_negatives), where:  alpha (sometimes called \"margin\") is a hyper-parameter  distance_to_positives are distances (here, L2 is used)  distance_to_negative are distances to the hardest negatives for each anchor in a batch. As input HardNet operates with N * 2 images (N anchor/query images and N corresponding to them positives)  Mining algorithm: 1. Compute distance matrix D between N anchors and N positives. 2. distances_to_positives = trace of distance matrix (diagonal elements) 3. For each row minimal non-diagonal element is taken as a distance to the hardest negatives (closest to anchor). From these chosen values distances_to_negatives are obtained. All this can be rewritten as:  Loss = MAX(0, alpha + Trace(D) + row_wise_min(D + I * inf)), where I is the identity matrix. Architecture:  Notes:  The described mining procedure highly relies on a fact that all N anchors would should to N different classes. And from my personal point of view requires minor modification to handle such corner case. The given loss/mining procedure is fast, but in contrast to other mining strategies doesn't provide hardest positive (furthest from anchor). Results:  Wide baseline stereo example:  The bigger batch size, the better:  PhotoTour Patch Verification Results:  Oxford 5k, Paris 6k Patch Verification Results:", "pdf_url": "https://arxiv.org/pdf/1705.10872", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/hardnet.json"}
{"id": "96217072", "bin": "300_400", "summary_sentences": ["The paper explores the domain of conditional image generation by adopting and improving PixelCNN architecture.", "Based on PixelRNN and PixelCNN  Models image pixel by pixel by decomposing the joint image distribution as a product of conditionals.", "PixelRNN uses two-dimensional LSTM while PixelCNN uses convolutional networks.", "PixelRNN gives better results but PixelCNN is faster to train.", "Gated PixelCNN  PixelRNN outperforms PixelCNN due to the larger receptive field and because they contain multiplicative units, LSTM gates, which allow modelling more complex interactions.", "To account for these, deeper models and gated activation units (equation 2 in the paper ) can be used respectively.", "Masked convolutions can lead to blind spots in the receptive fields.", "These can be removed by combining 2 convolutional network stacks:  Horizontal stack - conditions on the current row.", "Vertical stack - conditions on all rows above the current row.", "Every layer in the horizontal stack takes as input the output of the previous layer as well as that of the vertical stack.", "Residual connections are used in the horizontal stack and not in the vertical stack (as they did not seem to improve results in the initial settings).", "Conditional PixelCNN  Model conditional distribution of image, given the high-level description of the image, represented using the latent vector h (equation 4 in the paper )  This conditioning does not depend on the location of the pixel in the image.", "To consider the location as well, map h to spatial representation s = m(h) (equation 5 in the the paper )  PixelCNN Auto-Encoders  Start with a traditional auto-encoder architecture and replace the deconvolutional decoder with PixelCNN and train the network end-to-end.", "Experiments  For unconditional modelling, Gated PixelCNN either outperforms PixelRNN or performs almost as good and takes much less time to train.", "In the case of conditioning on ImageNet classes, the log likelihood measure did not improve a lot but the visual quality of the generated sampled was significantly improved.", "Paper also included sample images generated by conditioning on human portraits and by training a PixelCNN auto-encoder on ImageNet patches."], "summary_text": "The paper explores the domain of conditional image generation by adopting and improving PixelCNN architecture. Based on PixelRNN and PixelCNN  Models image pixel by pixel by decomposing the joint image distribution as a product of conditionals. PixelRNN uses two-dimensional LSTM while PixelCNN uses convolutional networks. PixelRNN gives better results but PixelCNN is faster to train. Gated PixelCNN  PixelRNN outperforms PixelCNN due to the larger receptive field and because they contain multiplicative units, LSTM gates, which allow modelling more complex interactions. To account for these, deeper models and gated activation units (equation 2 in the paper ) can be used respectively. Masked convolutions can lead to blind spots in the receptive fields. These can be removed by combining 2 convolutional network stacks:  Horizontal stack - conditions on the current row. Vertical stack - conditions on all rows above the current row. Every layer in the horizontal stack takes as input the output of the previous layer as well as that of the vertical stack. Residual connections are used in the horizontal stack and not in the vertical stack (as they did not seem to improve results in the initial settings). Conditional PixelCNN  Model conditional distribution of image, given the high-level description of the image, represented using the latent vector h (equation 4 in the paper )  This conditioning does not depend on the location of the pixel in the image. To consider the location as well, map h to spatial representation s = m(h) (equation 5 in the the paper )  PixelCNN Auto-Encoders  Start with a traditional auto-encoder architecture and replace the deconvolutional decoder with PixelCNN and train the network end-to-end. Experiments  For unconditional modelling, Gated PixelCNN either outperforms PixelRNN or performs almost as good and takes much less time to train. In the case of conditioning on ImageNet classes, the log likelihood measure did not improve a lot but the visual quality of the generated sampled was significantly improved. Paper also included sample images generated by conditioning on human portraits and by training a PixelCNN auto-encoder on ImageNet patches.", "pdf_url": "https://arxiv.org/pdf/1606.05328", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/c7066ce7de051d769908b8fab11990.json"}
{"id": "11790475", "bin": "300_400", "summary_sentences": ["How can we learn causal relationships that explain data?", "We can learn from non-stationary distributions.", "If we experiment with different factorizations of relationships between variables we can observe which ones provide better sample complexity when adapting to distributional shift and therefore are likely to be causal.", "If we consider the variables A and B we can factor them in two ways:  $P(A,B) = P(A)P(B|A)$ representing a causal graph like $A\\rightarrow B$  $P(A,B) = P(A|B)P(B)$ representing a causal graph like $A \\leftarrow B$  The idea is if we train a model with one of these structures; when adapting to a new shifted distribution of data it will take longer to adapt if the model does not have the correct inductive bias.", "For example let's say that the true relationship is $A$=Raining causes $B$=Open Umbrella (and not vice-versa).", "Changing the marginal probability of Raining (say because the weather changed) does not change the mechanism that relates $A$ and $B$ (captured by $P(B|A)$), but will have an impact on the marginal $P(B)$.", "So after this distributional shift the function that modeled $P(B|A)$ will not need to change because the relationship is the same.", "Only the function that modeled $P(A)$ will need to change.", "Under the incorrect factorization $P(B)P(A|B)$, adaptation to the change will be slow because both $P(B)$ and $P(A|B)$ need to be modified to account for the change in $P(A)$ (due to Bayes rule).", "Here a difference in sample complexity can be observed when modeling the joint of the shifted distribution.", "$B\\rightarrow A$ takes longer to adapt:  [url]"], "summary_text": "How can we learn causal relationships that explain data? We can learn from non-stationary distributions. If we experiment with different factorizations of relationships between variables we can observe which ones provide better sample complexity when adapting to distributional shift and therefore are likely to be causal. If we consider the variables A and B we can factor them in two ways:  $P(A,B) = P(A)P(B|A)$ representing a causal graph like $A\\rightarrow B$  $P(A,B) = P(A|B)P(B)$ representing a causal graph like $A \\leftarrow B$  The idea is if we train a model with one of these structures; when adapting to a new shifted distribution of data it will take longer to adapt if the model does not have the correct inductive bias. For example let's say that the true relationship is $A$=Raining causes $B$=Open Umbrella (and not vice-versa). Changing the marginal probability of Raining (say because the weather changed) does not change the mechanism that relates $A$ and $B$ (captured by $P(B|A)$), but will have an impact on the marginal $P(B)$. So after this distributional shift the function that modeled $P(B|A)$ will not need to change because the relationship is the same. Only the function that modeled $P(A)$ will need to change. Under the incorrect factorization $P(B)P(A|B)$, adaptation to the change will be slow because both $P(B)$ and $P(A|B)$ need to be modified to account for the change in $P(A)$ (due to Bayes rule). Here a difference in sample complexity can be observed when modeling the joint of the shifted distribution. $B\\rightarrow A$ takes longer to adapt:  [url]", "pdf_url": "http://arxiv.org/pdf/1901.10912v2", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/1901.10912.json"}
{"id": "71425952", "bin": "300_400", "summary_sentences": ["Persi Diaconis, 2009  The Fundamental Theorem of Markov Chains  From any starting state $x$, the $n^{th}$ step of a run of the MC has chance close to $\\pi (y)$ of being at $y$ if $n$ is large.", "The MC must be connected, i.e., in the limit, the kernel $K$/proposal distribution/Markov transition matrix has no zero-probability transitions.", "Metropolis Algorithm  Based on “proposal” and “acceptance”  The acceptance ratio is to ensure that the fraction of time spent in each state is proportional to $\\pi(x)$ for $x \\in \\chi$  In this algorithm, the normalization constants of the stationary distributions cancel out!", "In Equation 2.3, if the acceptance ratio is $< 1$, you are multiplying the probabilities $J(x,y)$ and $A(x,y)$ together.", "This generates the success probability $J(x,y)A(x,y)$ for transitioning x -> y.", "You want to accept transitions that move to states that are reversible (and hence move you closer to the true stationary distribution), and stay away from states that are not.", "The algorithm hence allows the Markov Chain to stay in the same place with some probability if the acceptance ratio is low.", "This algorithm produces a reversible Markov chain:  Since $\\pi K = \\pi$ (the stationary distribution is unchanged by the operation of the kernel $K$), $\\pi$ is a left eigenvector of $K$ with eigenvalue 1.", "The basic result on convergence to the stationary distribution can be found by taking the spectral decomposition of $K$.", "Gibbs sampler  Example  Here’s a neat example of the Metropolis Hastings algorithm for sampling from a boltzmann distribution .", "Remember- low-energy states have high boltzmann probability!"], "summary_text": "Persi Diaconis, 2009  The Fundamental Theorem of Markov Chains  From any starting state $x$, the $n^{th}$ step of a run of the MC has chance close to $\\pi (y)$ of being at $y$ if $n$ is large. The MC must be connected, i.e., in the limit, the kernel $K$/proposal distribution/Markov transition matrix has no zero-probability transitions. Metropolis Algorithm  Based on “proposal” and “acceptance”  The acceptance ratio is to ensure that the fraction of time spent in each state is proportional to $\\pi(x)$ for $x \\in \\chi$  In this algorithm, the normalization constants of the stationary distributions cancel out! In Equation 2.3, if the acceptance ratio is $< 1$, you are multiplying the probabilities $J(x,y)$ and $A(x,y)$ together. This generates the success probability $J(x,y)A(x,y)$ for transitioning x -> y. You want to accept transitions that move to states that are reversible (and hence move you closer to the true stationary distribution), and stay away from states that are not. The algorithm hence allows the Markov Chain to stay in the same place with some probability if the acceptance ratio is low. This algorithm produces a reversible Markov chain:  Since $\\pi K = \\pi$ (the stationary distribution is unchanged by the operation of the kernel $K$), $\\pi$ is a left eigenvector of $K$ with eigenvalue 1. The basic result on convergence to the stationary distribution can be found by taking the spectral decomposition of $K$. Gibbs sampler  Example  Here’s a neat example of the Metropolis Hastings algorithm for sampling from a boltzmann distribution . Remember- low-energy states have high boltzmann probability!", "pdf_url": "https://www.ams.org/journals/bull/2009-46-02/S0273-0979-08-01238-X/S0273-0979-08-01238-X.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/mcmc-rev.json"}
{"id": "83248182", "bin": "300_400", "summary_sentences": ["The paper proposes a pretraining technique that can be used with the GNN architecture for learning graph representation as induced by powerful graph kernels.", "Paper  Idea  Graph Kernel methods can learn powerful representations of the input graphs but the learned representation is implicit as the kernel function actually computes the dot product between the representations.", "GNNs are flexible and powerful in terms of the representations they can learn but they can easily overfit if a large amount of training data is not available as is commonly the case of graphs.", "Kernel methods can be used to learn an unsupervised graph representation that can be finetuned using the GNN architectures for the supervised tasks.", "Architecture  Given a dataset of graphs g1, g2, …, gn, use a relevant kernel function to compute k(gi, gj) for all pairs of graphs.", "A siamese network is used to encode the pair of graphs into representations f(gi) and f(gj) such that dot(f(gi), f(gj)) equals k(gi, gj).", "The function f is trained to learn the compressed representation of kernel’s feature space.", "Experiments  Datasets  Biological node-labeled graphs representing chemical compounds - MUTAG, PTC, NCI1  Baselines  DGCNN  Graphlet Kernel (GK)  Random Walk Kernel  Propogation Kernel  Weisfeiler-Lehman subtree kernel (WL)  Results  Pretraining uses the WL kernel  Pretrained model performs better than the baselines for 2 datasets but lags behind WL method (which was used for pretraining) for the NCI1 dataset.", "Notes  The idea is straightforward and intuitive.", "In general, this kind of pretraining should help the downstream model.", "It would be interesting to try it on more datasets/kernels/GNNs so that more conclusive results can be obtained."], "summary_text": "The paper proposes a pretraining technique that can be used with the GNN architecture for learning graph representation as induced by powerful graph kernels. Paper  Idea  Graph Kernel methods can learn powerful representations of the input graphs but the learned representation is implicit as the kernel function actually computes the dot product between the representations. GNNs are flexible and powerful in terms of the representations they can learn but they can easily overfit if a large amount of training data is not available as is commonly the case of graphs. Kernel methods can be used to learn an unsupervised graph representation that can be finetuned using the GNN architectures for the supervised tasks. Architecture  Given a dataset of graphs g1, g2, …, gn, use a relevant kernel function to compute k(gi, gj) for all pairs of graphs. A siamese network is used to encode the pair of graphs into representations f(gi) and f(gj) such that dot(f(gi), f(gj)) equals k(gi, gj). The function f is trained to learn the compressed representation of kernel’s feature space. Experiments  Datasets  Biological node-labeled graphs representing chemical compounds - MUTAG, PTC, NCI1  Baselines  DGCNN  Graphlet Kernel (GK)  Random Walk Kernel  Propogation Kernel  Weisfeiler-Lehman subtree kernel (WL)  Results  Pretraining uses the WL kernel  Pretrained model performs better than the baselines for 2 datasets but lags behind WL method (which was used for pretraining) for the NCI1 dataset. Notes  The idea is straightforward and intuitive. In general, this kind of pretraining should help the downstream model. It would be interesting to try it on more datasets/kernels/GNNs so that more conclusive results can be obtained.", "pdf_url": "https://arxiv.org/pdf/1812.00420", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/pre-training-graph-neural-networks-with-kernels.json"}
{"id": "56843191", "bin": "300_400", "summary_sentences": ["This paper proposes a neural architecture that allows to backpropagate gradients though a procedure that can go through a variable and adaptive number of iterations.", "These \"iterations\" for instance could be the number of times computations are passed through the same recurrent layer (connected to the same input) before producing an output, which is the case considered in this paper.", "This is essentially achieved by pooling the recurrent states and respective outputs computed by each iteration.", "The pooling mechanism is essentially the same as that used in the really cool Neural Stack architecture of Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman and Phil Blunsom  [ref] .", "It relies on the introduction of halting units, which are sigmoidal units computed at each iteration and which gives a soft weight on whether the computation should stop at the current iteration.", "Crucially, the paper introduces a new ponder cost $P(x)$, which is a regularization cost that penalizes what is meant to be a smooth upper bound on the number of iterations $N(t)$ (more on that below).", "The paper presents experiment on RNNs applied on sequences where, at each time step t (not to be confused with what I'm calling computation iterations, which are indexed by n) in the sequence the RNN can produce a variable number $N(t)$ of intermediate states and outputs.", "These are the states and outputs that are pooled, to produce a single recurrent state and output for the time step t. During each of the $N(t)$ iterations at time step t, the intermediate states are connected to the same time-step-t input.", "After the $N(t)$ iterations, the RNN pools the $N(t)$ intermediate states and outputs, and then moves to the next time step $t+1$.", "To mark the transitions between time steps, an extra binary input is appended, which is 1 only for the first intermediate computation iteration.", "Results are presented on a variety of synthetic problems and a character prediction problem."], "summary_text": "This paper proposes a neural architecture that allows to backpropagate gradients though a procedure that can go through a variable and adaptive number of iterations. These \"iterations\" for instance could be the number of times computations are passed through the same recurrent layer (connected to the same input) before producing an output, which is the case considered in this paper. This is essentially achieved by pooling the recurrent states and respective outputs computed by each iteration. The pooling mechanism is essentially the same as that used in the really cool Neural Stack architecture of Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman and Phil Blunsom  [ref] . It relies on the introduction of halting units, which are sigmoidal units computed at each iteration and which gives a soft weight on whether the computation should stop at the current iteration. Crucially, the paper introduces a new ponder cost $P(x)$, which is a regularization cost that penalizes what is meant to be a smooth upper bound on the number of iterations $N(t)$ (more on that below). The paper presents experiment on RNNs applied on sequences where, at each time step t (not to be confused with what I'm calling computation iterations, which are indexed by n) in the sequence the RNN can produce a variable number $N(t)$ of intermediate states and outputs. These are the states and outputs that are pooled, to produce a single recurrent state and output for the time step t. During each of the $N(t)$ iterations at time step t, the intermediate states are connected to the same time-step-t input. After the $N(t)$ iterations, the RNN pools the $N(t)$ intermediate states and outputs, and then moves to the next time step $t+1$. To mark the transitions between time steps, an extra binary input is appended, which is 1 only for the first intermediate computation iteration. Results are presented on a variety of synthetic problems and a character prediction problem.", "pdf_url": "http://arxiv.org/pdf/1603.08983", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/graves16.json"}
{"id": "5180208", "bin": "300_400", "summary_sentences": ["Sequence-to-Sequence models have made abstract summarization viable but they still suffer from issues like out of vocabulary words and repetitive sentences.", "The paper proposes to overcome these limitations by using a hybrid Pointer-Generator network (to copy words from the source text) and a coverage vector that keeps track of content that has already been summarized so as to discourage repetition.", "Code  Model  Pointer Generator Network  It is a hybrid model between the Sequence-to-Sequence network and Pointer Network such that when generating a word, the model decides whether the word would be generated using the softmax vocabulary (Sequence-to-Sequence) or using the source vocabulary (Pointer Network).", "Since the model can choose a word from the source vocabulary, the issue of out of vocabulary words is handled.", "Coverage Mechanism  The model maintains a coverage vector which is the sum of attention distributions over all previous decoder timesteps.", "This coverage vector is fed as an input to the attention mechanism.", "A coverage loss is added to prevent the model from repeatedly attending to the same word.", "The idea is to capture how much coverage different words have already received from the attention mechanism.", "Observation  Model when evaluated on CNN/Daily Mail summarization task, outperforms the state-of-the-art by at least 2 ROUGE points though it still does not outperform the lead-3 baseline.", "Lead-3 baseline uses first 3 sentences as the summary of the article which should be a strong baseline given that the dataset is actually about news articles.", "The model is initially trained without coverage and then finetuned with the coverage loss.", "During training, the model first learns how to copy words and then how to generate words (pgen starts from 0.3 and converges to 0.53).", "During testing, the model strongly prefers copying over generating (pgen = 0.17).", "Further, whenever the model is at beginning of sentences or at the join between switched-together fragments, it prefers to generate a word instead of copying one from the source language.", "The overall model is very simple, neat and interpretable and also performs well in practice."], "summary_text": "Sequence-to-Sequence models have made abstract summarization viable but they still suffer from issues like out of vocabulary words and repetitive sentences. The paper proposes to overcome these limitations by using a hybrid Pointer-Generator network (to copy words from the source text) and a coverage vector that keeps track of content that has already been summarized so as to discourage repetition. Code  Model  Pointer Generator Network  It is a hybrid model between the Sequence-to-Sequence network and Pointer Network such that when generating a word, the model decides whether the word would be generated using the softmax vocabulary (Sequence-to-Sequence) or using the source vocabulary (Pointer Network). Since the model can choose a word from the source vocabulary, the issue of out of vocabulary words is handled. Coverage Mechanism  The model maintains a coverage vector which is the sum of attention distributions over all previous decoder timesteps. This coverage vector is fed as an input to the attention mechanism. A coverage loss is added to prevent the model from repeatedly attending to the same word. The idea is to capture how much coverage different words have already received from the attention mechanism. Observation  Model when evaluated on CNN/Daily Mail summarization task, outperforms the state-of-the-art by at least 2 ROUGE points though it still does not outperform the lead-3 baseline. Lead-3 baseline uses first 3 sentences as the summary of the article which should be a strong baseline given that the dataset is actually about news articles. The model is initially trained without coverage and then finetuned with the coverage loss. During training, the model first learns how to copy words and then how to generate words (pgen starts from 0.3 and converges to 0.53). During testing, the model strongly prefers copying over generating (pgen = 0.17). Further, whenever the model is at beginning of sentences or at the join between switched-together fragments, it prefers to generate a word instead of copying one from the source language. The overall model is very simple, neat and interpretable and also performs well in practice.", "pdf_url": "https://arxiv.org/pdf/1704.04368", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/get-to-the-point-summarization-with-pointer-generator-networks.json"}
{"id": "77737945", "bin": "300_400", "summary_sentences": ["What  The authors start with a standard ResNet architecture (i.e. residual network has suggested in \"Identity Mappings in Deep Residual Networks\").", "Their residual block:  Several residual blocks of 16 filters per conv-layer, followed by 32 and then 64 filters per conv-layer.", "They empirically try to answer the following questions:  How many residual blocks are optimal?", "(Depth)  How many filters should be used per convolutional layer?", "(Width)  How many convolutional layers should be used per residual block?", "Does Dropout between the convolutional layers help?", "Results  Layers per block and kernel sizes:  Using 2 convolutional layers per residual block seems to perform best:  Using 3x3 kernel sizes for both layers seems to perform best.", "However, using 3 layers with kernel sizes 3x3, 1x1, 3x3 and then using less residual blocks performs nearly as good and decreases the required time per batch.", "Width and depth:  Increasing the width considerably improves the test error.", "They achieve the best results (on CIFAR-10) when decreasing the depth to 28 convolutional layers, with each having 10 times their normal width (i.e. 16*10 filters, 32*10 and 64*10):  They argue that their results show no evidence that would support the common theory that thin and deep networks somehow regularized better than wide and shallow(er) networks.", "Dropout:  They use dropout with p=0.3 (CIFAR) and p=0.4 (SVHN).", "On CIFAR-10 dropout doesn't seem to consistently improve test error.", "On CIFAR-100 and SVHN dropout seems to lead to improvements that are either small (wide and shallower net, i.e. depth=28, width multiplier=10) or significant (ResNet-50).", "They also observed oscillations in error (both train and test) during the training.", "Adding dropout decreased these oscillations.", "Computational efficiency:  Applying few big convolutions is much more efficient on GPUs than applying many small ones sequentially.", "Their network with the best test error is 1.6 times faster than ResNet-1001, despite having about 3 times more parameters."], "summary_text": "What  The authors start with a standard ResNet architecture (i.e. residual network has suggested in \"Identity Mappings in Deep Residual Networks\"). Their residual block:  Several residual blocks of 16 filters per conv-layer, followed by 32 and then 64 filters per conv-layer. They empirically try to answer the following questions:  How many residual blocks are optimal? (Depth)  How many filters should be used per convolutional layer? (Width)  How many convolutional layers should be used per residual block? Does Dropout between the convolutional layers help? Results  Layers per block and kernel sizes:  Using 2 convolutional layers per residual block seems to perform best:  Using 3x3 kernel sizes for both layers seems to perform best. However, using 3 layers with kernel sizes 3x3, 1x1, 3x3 and then using less residual blocks performs nearly as good and decreases the required time per batch. Width and depth:  Increasing the width considerably improves the test error. They achieve the best results (on CIFAR-10) when decreasing the depth to 28 convolutional layers, with each having 10 times their normal width (i.e. 16*10 filters, 32*10 and 64*10):  They argue that their results show no evidence that would support the common theory that thin and deep networks somehow regularized better than wide and shallow(er) networks. Dropout:  They use dropout with p=0.3 (CIFAR) and p=0.4 (SVHN). On CIFAR-10 dropout doesn't seem to consistently improve test error. On CIFAR-100 and SVHN dropout seems to lead to improvements that are either small (wide and shallower net, i.e. depth=28, width multiplier=10) or significant (ResNet-50). They also observed oscillations in error (both train and test) during the training. Adding dropout decreased these oscillations. Computational efficiency:  Applying few big convolutions is much more efficient on GPUs than applying many small ones sequentially. Their network with the best test error is 1.6 times faster than ResNet-1001, despite having about 3 times more parameters.", "pdf_url": "http://arxiv.org/pdf/1605.07146v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/wide_residual_networks.json"}
{"id": "17980043", "bin": "300_400", "summary_sentences": ["This paper presents an approach to initialize a neural network from the parameters of a smaller and previously trained neural network.", "This is effectively done by increasing the size (in width and/or depth) of the previously trained neural network, in such of a way that the function represented by the network doesn't change (i.e. the output of the larger neural network is still the same).", "The motivation here is that initializing larger neural networks in this way allows to accelerate their training, since at initialization the neural network will already be quite good.", "In a nutshell, neural networks are made wider by adding several copies (selected randomly) of the same hidden units to the hidden layer, for each hidden layer.", "To ensure that the neural network output remains the same, each incoming connection weight must also be divided by the number of replicas that unit is connected to in the previous layer.", "If not training using dropout, it is also recommended to add some noise to this initialization, in order to break its initial symmetry (though this will actually break the property that the network's output is the same).", "As for making a deeper network, layers are added by initializing them to be the identity function.", "For ReLU units, this is achieved using an identity matrix as the connection weight matrix.", "For units based on sigmoid or tanh activations, unfortunately it isn't possible to add such identity layers.", "In their experiments on ImageNet, the authors show that this initialization allows them to train larger networks faster than if trained from random initialization.", "More importantly, they were able to outperform their previous validation set ImageNet accuracy by initializing a very large network from their best Inception network."], "summary_text": "This paper presents an approach to initialize a neural network from the parameters of a smaller and previously trained neural network. This is effectively done by increasing the size (in width and/or depth) of the previously trained neural network, in such of a way that the function represented by the network doesn't change (i.e. the output of the larger neural network is still the same). The motivation here is that initializing larger neural networks in this way allows to accelerate their training, since at initialization the neural network will already be quite good. In a nutshell, neural networks are made wider by adding several copies (selected randomly) of the same hidden units to the hidden layer, for each hidden layer. To ensure that the neural network output remains the same, each incoming connection weight must also be divided by the number of replicas that unit is connected to in the previous layer. If not training using dropout, it is also recommended to add some noise to this initialization, in order to break its initial symmetry (though this will actually break the property that the network's output is the same). As for making a deeper network, layers are added by initializing them to be the identity function. For ReLU units, this is achieved using an identity matrix as the connection weight matrix. For units based on sigmoid or tanh activations, unfortunately it isn't possible to add such identity layers. In their experiments on ImageNet, the authors show that this initialization allows them to train larger networks faster than if trained from random initialization. More importantly, they were able to outperform their previous validation set ImageNet accuracy by initializing a very large network from their best Inception network.", "pdf_url": "http://arxiv.org/pdf/1511.05641", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/chengs15.json"}
{"id": "92451519", "bin": "300_400", "summary_sentences": ["A primary goal of this work is to incorporate the learning of complex navigation into the RL problem.", "Auxiliary tasks are used to augment the loss to provide denser training signals.", "The first auxiliary task is reconstruction of a low-dimensional depth map.", "The second task is self-supervised; the agent is trained to predict if the current location has been previously visited within a local trajectory.", "“The agent is trained by applying a weighted sum of the gradients coming from A3C, the gradients from depth prediction, and ther gradients from the loop closure”  “In particular if the prediction loss shares representation with the policy, it could help build useful features for RL much faster, bootstrapping learning” » This is interesting, and a bit confusing.", "It seems like this is generalizing from the observation that including depth prediction in the loss was superior to directly using it as an input.", "I’m not sure if this is always true, since it seems hard to quantify/evaluate this.", "It’s pretty cool that they’re incorporating aspects of SLAM- penalizing loop closures for efficient exploration.", "The authors test a couple different variations on a 3D maze navigation task.", "They use dynamic mazes and multiple goals to increase the difficulty.", "The A3C-variant with both auxiliary tasks performed the best on the most difficult tasks.", "The auxiliary tasks were shown to improve data efficiency.", "Takeaways  Engineering auxiliary tasks into the loss is an interesting direction.", "This won’t scale on its own for more complex tasks beyond navigation, but can potentially be used in conjunction with transfer learning.", "How does this compare with SOTA SLAM-and-RRT motion-planners?", "I would like to see this implemented on some robots!", "Cool, but still relies on RL methods based on data-inefficient and high-variance algorithms (A3C)"], "summary_text": "A primary goal of this work is to incorporate the learning of complex navigation into the RL problem. Auxiliary tasks are used to augment the loss to provide denser training signals. The first auxiliary task is reconstruction of a low-dimensional depth map. The second task is self-supervised; the agent is trained to predict if the current location has been previously visited within a local trajectory. “The agent is trained by applying a weighted sum of the gradients coming from A3C, the gradients from depth prediction, and ther gradients from the loop closure”  “In particular if the prediction loss shares representation with the policy, it could help build useful features for RL much faster, bootstrapping learning” » This is interesting, and a bit confusing. It seems like this is generalizing from the observation that including depth prediction in the loss was superior to directly using it as an input. I’m not sure if this is always true, since it seems hard to quantify/evaluate this. It’s pretty cool that they’re incorporating aspects of SLAM- penalizing loop closures for efficient exploration. The authors test a couple different variations on a 3D maze navigation task. They use dynamic mazes and multiple goals to increase the difficulty. The A3C-variant with both auxiliary tasks performed the best on the most difficult tasks. The auxiliary tasks were shown to improve data efficiency. Takeaways  Engineering auxiliary tasks into the loss is an interesting direction. This won’t scale on its own for more complex tasks beyond navigation, but can potentially be used in conjunction with transfer learning. How does this compare with SOTA SLAM-and-RRT motion-planners? I would like to see this implemented on some robots! Cool, but still relies on RL methods based on data-inefficient and high-variance algorithms (A3C)", "pdf_url": "https://arxiv.org/pdf/1611.03673v2.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/learning-to-navigate-in-complex-envs.json"}
{"id": "88969640", "bin": "300_400", "summary_sentences": ["The paper presents some general ideas and mechanisms for multiple model-based RL.", "Even though the task and model architecture may not be very relevant now, I find the general idea and the mechanisms to be quite useful.", "As such, I am focusing only on high-level ideas and not the implementation details themselves.", "The main idea behind Multiple Model-based RL (MMRL) is to decompose complex tasks into multiple domains in space and time so that the environment dynamics within each domain is predictable.", "MMRL proposes an RL architecture composes of multiple modules, each with its own state prediction model and RL controller.", "The prediction error from each of the state prediction model defines the “responsibility signal” for each module.", "This responsibility signal is used to:  Weigh the state prediction output ie the predicted state is the weighted sum of individual state predictions (weighted by the responsibility signal).", "Weigh the parameter update of the environment models as well as the RL controllers.", "Weighing the action output - ie predicted action is a weighted sum of individual actions.", "The framework is amenable for incorporating prior knowledge about which module should be selected.", "In the modular decomposition of a task, the modules should not change too frequently and some kind of spatial and temporal continuity is also desired.", "Temporal continuity can be accounted for by using the previous responsibility signal as input during the current timestep.", "Spatial continuity can b ensured by considering a spatial prior like the Gaussian spatial prior.", "Though model-free methods could be used for learning the RL controllers, model-based methods could be more relevant given that the modules are learning state-prediction models as well.", "Exploration can be ensured by using a stochastic version of greedy action selection.", "One failure mode for such modular architectures is when a single module tries to perform well across all the tasks.", "The modules themselves should be relatively simplistic (eg linear models) which can learn quickly and generalize well.", "Non-stationary hunting task in a grid world and non-linear, non-stationary control task of swinging up a pendulum provides the proof of concept for the proposed methods."], "summary_text": "The paper presents some general ideas and mechanisms for multiple model-based RL. Even though the task and model architecture may not be very relevant now, I find the general idea and the mechanisms to be quite useful. As such, I am focusing only on high-level ideas and not the implementation details themselves. The main idea behind Multiple Model-based RL (MMRL) is to decompose complex tasks into multiple domains in space and time so that the environment dynamics within each domain is predictable. MMRL proposes an RL architecture composes of multiple modules, each with its own state prediction model and RL controller. The prediction error from each of the state prediction model defines the “responsibility signal” for each module. This responsibility signal is used to:  Weigh the state prediction output ie the predicted state is the weighted sum of individual state predictions (weighted by the responsibility signal). Weigh the parameter update of the environment models as well as the RL controllers. Weighing the action output - ie predicted action is a weighted sum of individual actions. The framework is amenable for incorporating prior knowledge about which module should be selected. In the modular decomposition of a task, the modules should not change too frequently and some kind of spatial and temporal continuity is also desired. Temporal continuity can be accounted for by using the previous responsibility signal as input during the current timestep. Spatial continuity can b ensured by considering a spatial prior like the Gaussian spatial prior. Though model-free methods could be used for learning the RL controllers, model-based methods could be more relevant given that the modules are learning state-prediction models as well. Exploration can be ensured by using a stochastic version of greedy action selection. One failure mode for such modular architectures is when a single module tries to perform well across all the tasks. The modules themselves should be relatively simplistic (eg linear models) which can learn quickly and generalize well. Non-stationary hunting task in a grid world and non-linear, non-stationary control task of swinging up a pendulum provides the proof of concept for the proposed methods.", "pdf_url": "https://www.mitpressjournals.org/doi/pdf/10.1162/089976602753712972", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/multiple-model-based-reinforcement-learning.json"}
{"id": "47257042", "bin": "300_400", "summary_sentences": ["The paper proposes a new inverse RL (IRL) algorithm, called as Trajectory-ranked Reward EXtrapolation (T-REX) that learns a reward function from a collection of ranked trajectories.", "Standard IRL approaches aim to learn a reward function that “justifies” the demonstration policy and hence those approaches cannot outperform the demonstration policy.", "In contrast, T-REX aims to learn a reward function that “explains” the ranking over demonstrations and can learn a policy that outperforms the demonstration policy.", "Approach  The input is a sequence of trajectories T1, … Tm which are ranked in the order of preference.", "That is, given any pair of trajectories, we know which of the two trajectories is better.", "The setup is to learn from observations where the learning agent does not have access to the true reward function or the action taken by the demonstration policy.", "Reward Inference  A parameterized reward function rθ is trained with the ranking information using a binary classification loss function which aims to predict which of the two given trajectory would be ranked higher.", "Given a trajectory, the reward function predicts the reward for each state.", "The sum of rewards (corresponding to the two trajectories) is used used to predict the preferred trajectory.", "T-REX uses partial trajectories instead of full trajectories as a data augmentation strategy.", "Policy Optimization  Once a reward function has been learned, standard RL approaches can be used to train a new policy.", "Results  Environments: Mujoco (Half Cheetah, Ant, Hooper), Atari  Demonstrations generated using PPO (checkpointed at different stages of training).", "Ensemble of networks used to learn the reward functions.", "The proposed approach outperforms the baselines Behaviour Cloning from Observations and Generative Adversarial Imitation Learning .", "In terms of reward extrapolation, T-REX can predict the reward for trajectories which are better than the demonstration trajectories.", "Some ablation studies considered the effect of adding noise (random swapping the preference between trajectories) and found that the model is somewhat robust to noise up to an extent."], "summary_text": "The paper proposes a new inverse RL (IRL) algorithm, called as Trajectory-ranked Reward EXtrapolation (T-REX) that learns a reward function from a collection of ranked trajectories. Standard IRL approaches aim to learn a reward function that “justifies” the demonstration policy and hence those approaches cannot outperform the demonstration policy. In contrast, T-REX aims to learn a reward function that “explains” the ranking over demonstrations and can learn a policy that outperforms the demonstration policy. Approach  The input is a sequence of trajectories T1, … Tm which are ranked in the order of preference. That is, given any pair of trajectories, we know which of the two trajectories is better. The setup is to learn from observations where the learning agent does not have access to the true reward function or the action taken by the demonstration policy. Reward Inference  A parameterized reward function rθ is trained with the ranking information using a binary classification loss function which aims to predict which of the two given trajectory would be ranked higher. Given a trajectory, the reward function predicts the reward for each state. The sum of rewards (corresponding to the two trajectories) is used used to predict the preferred trajectory. T-REX uses partial trajectories instead of full trajectories as a data augmentation strategy. Policy Optimization  Once a reward function has been learned, standard RL approaches can be used to train a new policy. Results  Environments: Mujoco (Half Cheetah, Ant, Hooper), Atari  Demonstrations generated using PPO (checkpointed at different stages of training). Ensemble of networks used to learn the reward functions. The proposed approach outperforms the baselines Behaviour Cloning from Observations and Generative Adversarial Imitation Learning . In terms of reward extrapolation, T-REX can predict the reward for trajectories which are better than the demonstration trajectories. Some ablation studies considered the effect of adding noise (random swapping the preference between trajectories) and found that the model is somewhat robust to noise up to an extent.", "pdf_url": "https://arxiv.org/pdf/1904.06387", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/extrapolating-beyond-suboptimal-demonstrations-via-inverse-reinforcement-learning-from-observations.json"}
{"id": "30905451", "bin": "300_400", "summary_sentences": ["The paper introduces a learned gradient descent optimizer that has low memory and computational overhead and that generalizes well to new tasks.", "Key Advantage  Uses a hierarchial RNN architecture augmented by features like adapted input an output scaling, momentum etc.", "A meta-learning set of small diverse optimization tasks, with diverse loss landscapes is developed.", "The learnt optimizer generalizes to much more complex tasks and setups.", "Architecture  A hierarchical RNN is designed to act as a learned optimizer.", "This RNN is the meta-learner and its parameters are shared across different tasks.", "The learned optimizer takes as input the gradient (and related metadata) for each parameter and outputs the update to the parameters.", "At the lowest level of hierarchical, a small “parameter RNN” ingests the gradient (and related metadata).", "One level up, an intermediate “Tensor RNN” incorporates information from a subset of Parameter RNNS (eg one Tensor RNN per layer of feedforward network).", "At the highest level is the glocal RNN which receives input from all the Tensor RNNs and can keep track of weight updates across the task.", "the input of each RNN is averaged and fed as input to the subsequent RNN and the output of each RNN is fed as bias to the previous RNN.", "In practice, the hidden states are fixed at 10, 30 and 20 respectively.", "Features inspired from existing optimizers  Attention and Nesterov’s momentum  Attention mechanism is incorporated by attending to new regions of the loss surface (which are an offset from previous parameter location).", "To incorporate momentum on multiple timescales, the exponential moving average of the gradient at several timescales is also provided as input.", "The average gradients are rescaled (as in RMSProp and Adam)  Relative log gradient magnitudes are also provided as input so that the optimizer can access how the gradient magnitude changes with time."], "summary_text": "The paper introduces a learned gradient descent optimizer that has low memory and computational overhead and that generalizes well to new tasks. Key Advantage  Uses a hierarchial RNN architecture augmented by features like adapted input an output scaling, momentum etc. A meta-learning set of small diverse optimization tasks, with diverse loss landscapes is developed. The learnt optimizer generalizes to much more complex tasks and setups. Architecture  A hierarchical RNN is designed to act as a learned optimizer. This RNN is the meta-learner and its parameters are shared across different tasks. The learned optimizer takes as input the gradient (and related metadata) for each parameter and outputs the update to the parameters. At the lowest level of hierarchical, a small “parameter RNN” ingests the gradient (and related metadata). One level up, an intermediate “Tensor RNN” incorporates information from a subset of Parameter RNNS (eg one Tensor RNN per layer of feedforward network). At the highest level is the glocal RNN which receives input from all the Tensor RNNs and can keep track of weight updates across the task. the input of each RNN is averaged and fed as input to the subsequent RNN and the output of each RNN is fed as bias to the previous RNN. In practice, the hidden states are fixed at 10, 30 and 20 respectively. Features inspired from existing optimizers  Attention and Nesterov’s momentum  Attention mechanism is incorporated by attending to new regions of the loss surface (which are an offset from previous parameter location). To incorporate momentum on multiple timescales, the exponential moving average of the gradient at several timescales is also provided as input. The average gradients are rescaled (as in RMSProp and Adam)  Relative log gradient magnitudes are also provided as input so that the optimizer can access how the gradient magnitude changes with time.", "pdf_url": "https://arxiv.org/pdf/1703.04813", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/learned-optimizers-that-scale-and-generalize.json"}
{"id": "79512250", "bin": "300_400", "summary_sentences": ["Conventional wisdom says that when training neural networks, learning rate should monotonically decrease.", "This insight forms the basis of the different type of adaptive learning rates.", "Counter to this expected behaviour, the paper demonstrates that using a cyclical learning rate (CLR), varying between a minimum and a maximum value, helps to train the neural network faster without requiring fine-tuning of learning rate.", "The paper also provides a simple approach to estimate the lower and upper bound for CLR.", "Link to the implementation  Intution  Difficulty in minimizing the loss arises from saddle points and not from local minima.", "[Ref]  Increasing the learning rate allows for rapid traversal of saddle points.", "Alternatively, the optimal learning rate is expected to be between bounds of CLR and thus the learning rate would always be close to the optimal learning rate.", "Parameter Estimation  Cycle Length = Number of iterations till learning rate returns to the initial value = 2 * step_size  step_size should be set to 2-10 times the number of iterations in an epoch.", "Estimating the CLR boundary values:  Run the model for several epochs while increasing the learning rate between the allowed low and high values.", "Plot accuracy vs learning rate and note the learning rate values when the accuracy starts to fall.", "This gives a good candidate value for upper and lower bound.", "Alternatively, the lower bound could be set to be 1/3 or 3/4 of the upper bound.", "But it is difficult to judge if the model has run for the sufficient number of epochs in the first place.", "Notes  The idea in itself is very simple and straight-forward to add to any existing model which makes it very appealing.", "The author has experimented with various architectures and datasets (from vision domain) and has reported faster training results."], "summary_text": "Conventional wisdom says that when training neural networks, learning rate should monotonically decrease. This insight forms the basis of the different type of adaptive learning rates. Counter to this expected behaviour, the paper demonstrates that using a cyclical learning rate (CLR), varying between a minimum and a maximum value, helps to train the neural network faster without requiring fine-tuning of learning rate. The paper also provides a simple approach to estimate the lower and upper bound for CLR. Link to the implementation  Intution  Difficulty in minimizing the loss arises from saddle points and not from local minima. [Ref]  Increasing the learning rate allows for rapid traversal of saddle points. Alternatively, the optimal learning rate is expected to be between bounds of CLR and thus the learning rate would always be close to the optimal learning rate. Parameter Estimation  Cycle Length = Number of iterations till learning rate returns to the initial value = 2 * step_size  step_size should be set to 2-10 times the number of iterations in an epoch. Estimating the CLR boundary values:  Run the model for several epochs while increasing the learning rate between the allowed low and high values. Plot accuracy vs learning rate and note the learning rate values when the accuracy starts to fall. This gives a good candidate value for upper and lower bound. Alternatively, the lower bound could be set to be 1/3 or 3/4 of the upper bound. But it is difficult to judge if the model has run for the sufficient number of epochs in the first place. Notes  The idea in itself is very simple and straight-forward to add to any existing model which makes it very appealing. The author has experimented with various architectures and datasets (from vision domain) and has reported faster training results.", "pdf_url": "https://arxiv.org/pdf/1506.01186", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/cyclical-learning-rates-for-training-neural-networks.json"}
{"id": "32560739", "bin": "300_400", "summary_sentences": ["The proposed contribution is a framework for assessing risk by estimating the intentions of drivers and detecting conflicts between them.", "Traffic rules are explicitly represented in the model in order to reason about what the drivers are expected to do.", "Bayesian programming is used to generate the risk probabilities.", "Particle filtering is used to approximately solve the inference problem of finding the risk based on the probability that a driver does not intend to stop at an intersection when he is expected to.", "Evidence  The algorithm was tested on a T-shaped intersection with two passenger vehicles equipped with Vehicle-to-Vehicle communication modems that shared their pose and speed information at a rate of 10 Hz.", "The test vehicles were not equipped with autonomous emergency braking functions, instead an auditory and visual warning were triggered whenever the algorithm detected a dangerous situation.", "Experimentation involved a priority vehicle and obstacle vehicle.", "Evaluation for the performance of the risk assessment algorithms was based on:  The rate of false alarms  The rate of missed detections  The collision prediction horizon  Out of the 90 dangerous trials, 60 were performed with the warning system running on the priority vehicle, and 30 on the obstacle vehicle.", "In the 20 non-dangerous trials, there were no false alarms.", "For every one of the 90 dangerous tests, the system was able to issue a warning early enough for the driver to avoid collision by breaking.", "Strengths  No need for lengthy training  The proposed algorithm is generic and could be implemented for various driving scenarios  No trajectory rollouts  Weaknesses  Speed of the vehicles during experimentatin was not reported  No collisions during experimentation since only real vehicles were used  Evaluation of the robustness of the algorithm is left in question due to minimal variablility in experimentation scenarios  Notes  The risk of a situation is computed based on the probability that intention and expectation do not match, given measurements of the state  a Markov State Space Model is used (this appears to be very similar to the dynamic Bayes net) to propagate the system variables for the bayesian computation"], "summary_text": "The proposed contribution is a framework for assessing risk by estimating the intentions of drivers and detecting conflicts between them. Traffic rules are explicitly represented in the model in order to reason about what the drivers are expected to do. Bayesian programming is used to generate the risk probabilities. Particle filtering is used to approximately solve the inference problem of finding the risk based on the probability that a driver does not intend to stop at an intersection when he is expected to. Evidence  The algorithm was tested on a T-shaped intersection with two passenger vehicles equipped with Vehicle-to-Vehicle communication modems that shared their pose and speed information at a rate of 10 Hz. The test vehicles were not equipped with autonomous emergency braking functions, instead an auditory and visual warning were triggered whenever the algorithm detected a dangerous situation. Experimentation involved a priority vehicle and obstacle vehicle. Evaluation for the performance of the risk assessment algorithms was based on:  The rate of false alarms  The rate of missed detections  The collision prediction horizon  Out of the 90 dangerous trials, 60 were performed with the warning system running on the priority vehicle, and 30 on the obstacle vehicle. In the 20 non-dangerous trials, there were no false alarms. For every one of the 90 dangerous tests, the system was able to issue a warning early enough for the driver to avoid collision by breaking. Strengths  No need for lengthy training  The proposed algorithm is generic and could be implemented for various driving scenarios  No trajectory rollouts  Weaknesses  Speed of the vehicles during experimentatin was not reported  No collisions during experimentation since only real vehicles were used  Evaluation of the robustness of the algorithm is left in question due to minimal variablility in experimentation scenarios  Notes  The risk of a situation is computed based on the probability that intention and expectation do not match, given measurements of the state  a Markov State Space Model is used (this appears to be very similar to the dynamic Bayes net) to propagate the system variables for the bayesian computation", "pdf_url": "https://hal.inria.fr/hal-00875356/document", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/intention-aware.json"}
{"id": "7904666", "bin": "300_400", "summary_sentences": ["This paper presents the theoretical notion of ensemble robustness and how it might provide an explanation for the success of deep learning algorithms.", "This work is an extension of some of the author's previous work (see Definition 2), demonstrating a theoretical relationship between a notion of robustness to adversarial examples and generalization performance.", "One initial observation made in this work is that this previous notion of robustness cannot explain the good performance of deep neural networks, since they have been shown to in fact not be robust to adversarial examples.", "So in this paper, the authors propose to study a notion of ensemble robustness (see Definition 3), and show that it can also be linked to generalization performance (see Theorem 1 and Corollary 1).", "The \"ensemble\" part comes from taking into account the stochasticity of the learning algorithm, i.e. the fact that the models they produce can vary from one run to another, even if applied on the same training set.", "The stochasticity here can come from the use of dropout, of SGD with random ordering of the training examples or from the random parameter initialization.", "Other theoretical results are also presented, such as one relating the variance of the robustness to generalization performance and another specific to the use of dropout.", "Finally, the paper also proposes a semi-supervised learning algorithm inspired from their definition of ensemble robustness, in which a model is trained to classify the perturbed (adversarial) version of an example in the same class as the original (non perturbed) example.", "On MNIST, they achieve excellent results, matching the performance of the state-of-the-art Ladder Networks."], "summary_text": "This paper presents the theoretical notion of ensemble robustness and how it might provide an explanation for the success of deep learning algorithms. This work is an extension of some of the author's previous work (see Definition 2), demonstrating a theoretical relationship between a notion of robustness to adversarial examples and generalization performance. One initial observation made in this work is that this previous notion of robustness cannot explain the good performance of deep neural networks, since they have been shown to in fact not be robust to adversarial examples. So in this paper, the authors propose to study a notion of ensemble robustness (see Definition 3), and show that it can also be linked to generalization performance (see Theorem 1 and Corollary 1). The \"ensemble\" part comes from taking into account the stochasticity of the learning algorithm, i.e. the fact that the models they produce can vary from one run to another, even if applied on the same training set. The stochasticity here can come from the use of dropout, of SGD with random ordering of the training examples or from the random parameter initialization. Other theoretical results are also presented, such as one relating the variance of the robustness to generalization performance and another specific to the use of dropout. Finally, the paper also proposes a semi-supervised learning algorithm inspired from their definition of ensemble robustness, in which a model is trained to classify the perturbed (adversarial) version of an example in the same class as the original (non perturbed) example. On MNIST, they achieve excellent results, matching the performance of the state-of-the-art Ladder Networks.", "pdf_url": "http://arxiv.org/pdf/1602.02389", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/fengzkxm16.json"}
{"id": "94015576", "bin": "300_400", "summary_sentences": ["The motivation behind this work is to develop an automated process for learning the behaviors of road users from large amounts of unlabeled video data.", "A generative model (trained policy) of road user behavior could be used within a larger traffic scene understanding pipeline.", "In this paper, they propose Horizon GAIL, an imitation-learning algorithm based on GAIL, that stabilizes learning from demonstration (LfD) over long horizons.", "Expert policy demonstrations are provided by a slightly improved Deep SORT tracker, and they use PPO as the “student” RL algorithm.", "The Unity game engine is used to build an RL env that mimcs the scene from the real-world environment to rollout their PPO Horizon-GAIL agent.", "Their experiments are on 850 minutes of traffic camera data of a large roundabout.", "By using a curriculum where the episode horizon is extended by 1 timestep each training epoch, they demonstrated how Horizon-GAIL can match the expert policy’s state/action distribution much more closely than GAIL, PS-GAIL, and behavior cloning while also improving on training stability.", "Observations  The ability to auto-generate the Unity env from Google Maps would be crucial to scaling this technique up.", "maps2sim?", "They provided an empirical comparison of DeepSORT with ViBe’s vision tracker, and showed that running the Kalman Filter in 3D space improved Deep SORT’s performance in multiple multi-object tracking metrics by a few percentage points  Each road user is modeled independently, i.e., the policy does not account for other agents in the environment explicitly.", "It looks like the policy used for learning vehicle and pedestrian behavior is the same, although because of the Mask R-CNN detector, they are able to differentiate between the two classes.", "In scenarios where the behaviors exhibited by the road users can be highly unpredictable and diverse (a busy traffic intersection with heavy pedestrian presence), perhaps a hierarchical policy could be useful that conditions on the inferred object class.", "Interesting future work might include incorporating multi-agent modeling in the RL framework for more complex traffic scenarios."], "summary_text": "The motivation behind this work is to develop an automated process for learning the behaviors of road users from large amounts of unlabeled video data. A generative model (trained policy) of road user behavior could be used within a larger traffic scene understanding pipeline. In this paper, they propose Horizon GAIL, an imitation-learning algorithm based on GAIL, that stabilizes learning from demonstration (LfD) over long horizons. Expert policy demonstrations are provided by a slightly improved Deep SORT tracker, and they use PPO as the “student” RL algorithm. The Unity game engine is used to build an RL env that mimcs the scene from the real-world environment to rollout their PPO Horizon-GAIL agent. Their experiments are on 850 minutes of traffic camera data of a large roundabout. By using a curriculum where the episode horizon is extended by 1 timestep each training epoch, they demonstrated how Horizon-GAIL can match the expert policy’s state/action distribution much more closely than GAIL, PS-GAIL, and behavior cloning while also improving on training stability. Observations  The ability to auto-generate the Unity env from Google Maps would be crucial to scaling this technique up. maps2sim? They provided an empirical comparison of DeepSORT with ViBe’s vision tracker, and showed that running the Kalman Filter in 3D space improved Deep SORT’s performance in multiple multi-object tracking metrics by a few percentage points  Each road user is modeled independently, i.e., the policy does not account for other agents in the environment explicitly. It looks like the policy used for learning vehicle and pedestrian behavior is the same, although because of the Mask R-CNN detector, they are able to differentiate between the two classes. In scenarios where the behaviors exhibited by the road users can be highly unpredictable and diverse (a busy traffic intersection with heavy pedestrian presence), perhaps a hierarchical policy could be useful that conditions on the inferred object class. Interesting future work might include incorporating multi-agent modeling in the RL framework for more complex traffic scenarios.", "pdf_url": "https://arxiv.org/pdf/1811.03516v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/lfd-in-the-wild.json"}
{"id": "21532550", "bin": "300_400", "summary_sentences": ["This paper presents a conditional generative model of text, where text can be generated either one character at a time or by copying some full chunks of character taken directly from the input into the output.", "At each step of the generation, the model can decide which of these two modes of generation to use, mixing them as needed to generate a correct output.", "They refer to this structure for generation as Latent Predictor Networks  [ref] .", "The character-level generation part of the model is based on a simple output softmax over characters, while the generation-by-copy component is based on a Pointer Network architecture.", "Critically, the authors highlight that it is possible to marginalize over the use of either types of components by dynamic programming as used in semi-Markov models  [ref] .", "One motivating application is machine translation, where the input might contain some named entities that should just be directly copied at the output.", "However, the authors experiment on a different problem, that of generating code that would implement the action of a card in the trading card games Magic the Gathering and Hearthstone.", "In this application, copying is useful to do things such as copy the name of the card or its numerically-valued effects.", "In addition to the Latent Predictor Network structure, the proposed model for this application includes a slightly adapted form of soft-attention as well as character-aware word embeddings as in  [ref]  Also, the authors experiment with a compression procedure on the target programs, that can help in reducing the size of the output space.", "Experiments show that the proposed neural network approach outperforms a variety of strong baselines (including systems based on machine translation or information retrieval)."], "summary_text": "This paper presents a conditional generative model of text, where text can be generated either one character at a time or by copying some full chunks of character taken directly from the input into the output. At each step of the generation, the model can decide which of these two modes of generation to use, mixing them as needed to generate a correct output. They refer to this structure for generation as Latent Predictor Networks  [ref] . The character-level generation part of the model is based on a simple output softmax over characters, while the generation-by-copy component is based on a Pointer Network architecture. Critically, the authors highlight that it is possible to marginalize over the use of either types of components by dynamic programming as used in semi-Markov models  [ref] . One motivating application is machine translation, where the input might contain some named entities that should just be directly copied at the output. However, the authors experiment on a different problem, that of generating code that would implement the action of a card in the trading card games Magic the Gathering and Hearthstone. In this application, copying is useful to do things such as copy the name of the card or its numerically-valued effects. In addition to the Latent Predictor Network structure, the proposed model for this application includes a slightly adapted form of soft-attention as well as character-aware word embeddings as in  [ref]  Also, the authors experiment with a compression procedure on the target programs, that can help in reducing the size of the output space. Experiments show that the proposed neural network approach outperforms a variety of strong baselines (including systems based on machine translation or information retrieval).", "pdf_url": "http://arxiv.org/pdf/1603.06744", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/lingghkswb16.json"}
{"id": "57934330", "bin": "300_400", "summary_sentences": ["Bengio, et al., 2003  associate with each word in the vocabulary a distributed word feature vector (real valued vector in $\\mathbb{R}^n$)  express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence  learn simultaneously the word feature vectors and the parameters of that probability function  For discrete random variables, learning a joint probability distribution is hard because a small change in one of the variables could cause a large change in the value of the function to be estimated.", "Instead, transforming the discrete random variables into a vector space in $\\mathbb{R}^n$ allows the use of neural nets or GMMs which are smooth approximators.", "Additionally, the notion of a “nearby” word within the continuous vector space representation is now defined more clearly.", "Words are mapped into a matrix $C$ of size ($|V| \\times m$) for a vocabulary size $|V|$ and embedding dim $m$.", "The feature vectors (columns of $C$) are learned simultanesouly with the parameters of the neural network.", "The input to the time-lagged neural network (RNN) is the concatenated vector of word representations.", "The objective is to maximize the log-likehood of a given sequence of out-of-sample words.", "This essentially is the encoder in Neural Machine Translation.", "Things of interest  The curse of dimensionality in modeling joint probability of sequences of words in a language is a major stumbling block  One can reduce the difficulty by using the fact that temporally closer words in the word sequence are statistically more dependent $\\rightarrow$ n-gram models  It was noted by the authors that n-gram models and the neural models made different “errors”, so an averaging model of the two showed improvements overall  It is suggested by the authors as well to train multiple smaller networks on partitions of the training data to speed things up  Learning word embeddings is very parallelizable- specifically, the computation of the output layer of the neural model was found to take up roughly 99.7% of the computation (since you’re computing a likelihood of a sequence out of the entire vocabulary)"], "summary_text": "Bengio, et al., 2003  associate with each word in the vocabulary a distributed word feature vector (real valued vector in $\\mathbb{R}^n$)  express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence  learn simultaneously the word feature vectors and the parameters of that probability function  For discrete random variables, learning a joint probability distribution is hard because a small change in one of the variables could cause a large change in the value of the function to be estimated. Instead, transforming the discrete random variables into a vector space in $\\mathbb{R}^n$ allows the use of neural nets or GMMs which are smooth approximators. Additionally, the notion of a “nearby” word within the continuous vector space representation is now defined more clearly. Words are mapped into a matrix $C$ of size ($|V| \\times m$) for a vocabulary size $|V|$ and embedding dim $m$. The feature vectors (columns of $C$) are learned simultanesouly with the parameters of the neural network. The input to the time-lagged neural network (RNN) is the concatenated vector of word representations. The objective is to maximize the log-likehood of a given sequence of out-of-sample words. This essentially is the encoder in Neural Machine Translation. Things of interest  The curse of dimensionality in modeling joint probability of sequences of words in a language is a major stumbling block  One can reduce the difficulty by using the fact that temporally closer words in the word sequence are statistically more dependent $\\rightarrow$ n-gram models  It was noted by the authors that n-gram models and the neural models made different “errors”, so an averaging model of the two showed improvements overall  It is suggested by the authors as well to train multiple smaller networks on partitions of the training data to speed things up  Learning word embeddings is very parallelizable- specifically, the computation of the output layer of the neural model was found to take up roughly 99.7% of the computation (since you’re computing a likelihood of a sequence out of the entire vocabulary)", "pdf_url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/neural-probabilistic-language.json"}
{"id": "81262575", "bin": "300_400", "summary_sentences": ["What  They describe an architecture that merges classical convolutional networks and residual networks.", "The architecture can (theoretically) learn anything that a classical convolutional network or a residual network can learn, as it contains both of them.", "The architecture can (theoretically) learn how many convolutional layers it should use per residual block (up to the amount of convolutional layers in the whole network).", "How  Just like residual networks, they have \"blocks\".", "Each block contains convolutional layers.", "Each block contains residual units and non-residual units.", "They have two \"streams\" of data in their network (just matrices generated by each block):  Residual stream: The residual blocks write to this stream (i.e. it's their output).", "Transient stream: The non-residual blocks write to this stream.", "Residual and non-residual layers receive both streams as input, but only write to their stream as output.", "Their architecture visualized:  Because of this architecture, their model can learn the number of layers per residual block (though BN and ReLU might cause problems here?", "):  The easiest way to implement this should be along the lines of the following (some of the visualized convolutions can be merged):  Input of size CxHxW (both streams, each C/2 planes)  Concat  Residual block: Apply C/2 convolutions to the C input planes, with shortcut addition afterwards.", "Transient block: Apply C/2 convolutions to the C input planes.", "Apply BN  Apply ReLU  Output of size CxHxW.", "The whole operation can also be implemented with just a single convolutional layer, but then one has to make sure that some weights stay at zero.", "Results  They test on CIFAR-10 and CIFAR-100.", "They search for optimal hyperparameters (learning rate, optimizer, L2 penalty, initialization method, type of shortcut connection in residual blocks) using a grid search.", "Their model improves upon a wide ResNet and an equivalent non-residual CNN by a good margin (CIFAR-10: 0.5-1%, CIFAR-100: 1-2%)."], "summary_text": "What  They describe an architecture that merges classical convolutional networks and residual networks. The architecture can (theoretically) learn anything that a classical convolutional network or a residual network can learn, as it contains both of them. The architecture can (theoretically) learn how many convolutional layers it should use per residual block (up to the amount of convolutional layers in the whole network). How  Just like residual networks, they have \"blocks\". Each block contains convolutional layers. Each block contains residual units and non-residual units. They have two \"streams\" of data in their network (just matrices generated by each block):  Residual stream: The residual blocks write to this stream (i.e. it's their output). Transient stream: The non-residual blocks write to this stream. Residual and non-residual layers receive both streams as input, but only write to their stream as output. Their architecture visualized:  Because of this architecture, their model can learn the number of layers per residual block (though BN and ReLU might cause problems here? ):  The easiest way to implement this should be along the lines of the following (some of the visualized convolutions can be merged):  Input of size CxHxW (both streams, each C/2 planes)  Concat  Residual block: Apply C/2 convolutions to the C input planes, with shortcut addition afterwards. Transient block: Apply C/2 convolutions to the C input planes. Apply BN  Apply ReLU  Output of size CxHxW. The whole operation can also be implemented with just a single convolutional layer, but then one has to make sure that some weights stay at zero. Results  They test on CIFAR-10 and CIFAR-100. They search for optimal hyperparameters (learning rate, optimizer, L2 penalty, initialization method, type of shortcut connection in residual blocks) using a grid search. Their model improves upon a wide ResNet and an equivalent non-residual CNN by a good margin (CIFAR-10: 0.5-1%, CIFAR-100: 1-2%).", "pdf_url": "http://arxiv.org/pdf/1603.08029", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/resnet_in_resnet.json"}
{"id": "76514111", "bin": "300_400", "summary_sentences": ["What  Most neural machine translation models currently operate on word vectors or one hot vectors of words.", "They instead generate the vector of each word on a character-level.", "Thereby, the model can spot character-similarities between words and treat them in a similar way.", "They do that only for the source language, not for the target language.", "How  They treat each word of the source text on its own.", "To each word they then apply the model from Character-aware neural language models , i.e. they do per word:  Embed each character into a 620-dimensional space.", "Stack these vectors next to each other, resulting in a 2d-tensor in which each column is one of the vectors (i.e. shape 620xN for N characters).", "Apply convolutions of size 620xW to that tensor, where a few different values are used for W (i.e. some convolutions cover few characters, some cover many characters).", "Apply a tanh after these convolutions.", "Apply a max-over-time to the results of the convolutions, i.e. for each convolution use only the maximum value.", "Reshape to 1d-vector.", "Apply two highway-layers.", "They get 1024-dimensional vectors (one per word).", "Visualization of their steps:  Afterwards they apply the model from Neural Machine Translation by Jointly Learning to Align and Translate to these vectors, yielding a translation to a target language.", "Whenever that translation yields an unknown target-language-word (\"UNK\"), they replace it with the respective (untranslated) word from the source text.", "Results  They the German-English WMT dataset.", "BLEU improvemements (compared to neural translation without character-level words):  German-English improves by about 1.5 points.", "English-German improves by about 3 points.", "Reduction in the number of unknown target-language-words (same baseline again):  German-English goes down from about 1500 to about 1250.", "English-German goes down from about 3150 to about 2650.", "Translation examples (Phrase = phrase-based/non-neural translation, NN = non-character-based neural translation, CHAR = theirs):"], "summary_text": "What  Most neural machine translation models currently operate on word vectors or one hot vectors of words. They instead generate the vector of each word on a character-level. Thereby, the model can spot character-similarities between words and treat them in a similar way. They do that only for the source language, not for the target language. How  They treat each word of the source text on its own. To each word they then apply the model from Character-aware neural language models , i.e. they do per word:  Embed each character into a 620-dimensional space. Stack these vectors next to each other, resulting in a 2d-tensor in which each column is one of the vectors (i.e. shape 620xN for N characters). Apply convolutions of size 620xW to that tensor, where a few different values are used for W (i.e. some convolutions cover few characters, some cover many characters). Apply a tanh after these convolutions. Apply a max-over-time to the results of the convolutions, i.e. for each convolution use only the maximum value. Reshape to 1d-vector. Apply two highway-layers. They get 1024-dimensional vectors (one per word). Visualization of their steps:  Afterwards they apply the model from Neural Machine Translation by Jointly Learning to Align and Translate to these vectors, yielding a translation to a target language. Whenever that translation yields an unknown target-language-word (\"UNK\"), they replace it with the respective (untranslated) word from the source text. Results  They the German-English WMT dataset. BLEU improvemements (compared to neural translation without character-level words):  German-English improves by about 1.5 points. English-German improves by about 3 points. Reduction in the number of unknown target-language-words (same baseline again):  German-English goes down from about 1500 to about 1250. English-German goes down from about 3150 to about 2650. Translation examples (Phrase = phrase-based/non-neural translation, NN = non-character-based neural translation, CHAR = theirs):", "pdf_url": "http://arxiv.org/pdf/1603.00810v3", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/character-based_neural_machine_translation.json"}
{"id": "435175", "bin": "300_400", "summary_sentences": ["This paper explores the use of convolutional (PixelCNN) and recurrent units (PixelRNN) for modeling the distribution of images, in the framework of autoregression distribution estimation.", "In this framework, the input distribution $p(x)$ is factorized into a product of conditionals $\\Pi p(x_i | x_i-1)$.", "Previous work has shown that very good models can be obtained by using a neural network parametrization of the conditionals (e.g. see our work on NADE  [ref] ).", "Moreover, unlike other approaches based on latent stochastic units that are directed or undirected, the autoregressive approach is able to compute log-probabilities tractably.", "So in this paper, by considering the specific case of x being an image, they exploit the topology of pixels and investigate appropriate architectures for this.", "Among the paper's contributions are:  1.", "They propose Diagonal BiLSTM units for the PixelRNN, which are efficient (thanks to the use of convolutions) while making it possible to, in effect, condition a pixel's distribution on all the pixels above it (see Figure 2 for an illustration).", "2.", "They demonstrate that the use of residual connections (a form of skip connections, from hidden layer i-1 to layer $i+1$) are very effective at learning very deep distribution estimators (they go as deep as 12 layers).", "3.", "They show that it is possible to successfully model the distribution over the pixel intensities (effectively an integer between 0 and 255) using a softmax of 256 units.", "4.", "They propose a multi-scale extension of their model, that they apply to larger 64x64 images.", "The experiments show that the PixelRNN model based on Diagonal BiLSTM units achieves state-of-the-art performance on the binarized MNIST benchmark, in terms of log-likelihood.", "They also report excellent log-likelihood on the CIFAR-10 dataset, comparing to previous work based on real-valued density models.", "Finally, they show that their model is able to generate high quality image samples."], "summary_text": "This paper explores the use of convolutional (PixelCNN) and recurrent units (PixelRNN) for modeling the distribution of images, in the framework of autoregression distribution estimation. In this framework, the input distribution $p(x)$ is factorized into a product of conditionals $\\Pi p(x_i | x_i-1)$. Previous work has shown that very good models can be obtained by using a neural network parametrization of the conditionals (e.g. see our work on NADE  [ref] ). Moreover, unlike other approaches based on latent stochastic units that are directed or undirected, the autoregressive approach is able to compute log-probabilities tractably. So in this paper, by considering the specific case of x being an image, they exploit the topology of pixels and investigate appropriate architectures for this. Among the paper's contributions are:  1. They propose Diagonal BiLSTM units for the PixelRNN, which are efficient (thanks to the use of convolutions) while making it possible to, in effect, condition a pixel's distribution on all the pixels above it (see Figure 2 for an illustration). 2. They demonstrate that the use of residual connections (a form of skip connections, from hidden layer i-1 to layer $i+1$) are very effective at learning very deep distribution estimators (they go as deep as 12 layers). 3. They show that it is possible to successfully model the distribution over the pixel intensities (effectively an integer between 0 and 255) using a softmax of 256 units. 4. They propose a multi-scale extension of their model, that they apply to larger 64x64 images. The experiments show that the PixelRNN model based on Diagonal BiLSTM units achieves state-of-the-art performance on the binarized MNIST benchmark, in terms of log-likelihood. They also report excellent log-likelihood on the CIFAR-10 dataset, comparing to previous work based on real-valued density models. Finally, they show that their model is able to generate high quality image samples.", "pdf_url": "http://arxiv.org/pdf/1601.06759", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/oordkk16.json"}
{"id": "3517250", "bin": "300_400", "summary_sentences": ["This paper combines two ideas.", "The first is stochastic gradient Langevin dynamics (SGLD), which is an efficient Bayesian learning method for larger datasets, allowing to efficiently sample from the posterior over the parameters of a model (e.g. a deep neural network).", "In short, SGLD is stochastic (minibatch) gradient descent, but where Gaussian noise is added to the gradients before each update.", "Each update thus results in a sample from the SGLD sampler.", "To make a prediction for a new data point, a number of previous parameter values are combined into an ensemble, which effectively corresponds to Monte Carlo estimate of the posterior predictive distribution of the model.", "The second idea is distillation or dark knowledge, which in short is the idea of training a smaller model (student) in replicating the behavior and performance of a much larger model (teacher), by essentially training the student to match the outputs of the teacher.", "The observation made in this paper is that the step of creating an ensemble of several models (e.g. deep networks) can be expensive, especially if many samples are used and/or if each model is large.", "Thus, they propose to approximate the output of that ensemble by training a single network to predict to output of ensemble.", "Ultimately, this is done by having the student predict the output of a teacher corresponding to the model with the last parameter value sampled by SGLD.", "Interestingly, this process can be operated in an online fashion, where one alternates between sampling from SGLD (i.e. performing a noisy SGD step on the teacher model) and performing a distillation update (i.e.", "updating the student model, given the current teacher model).", "The end result is a student model, whose outputs should be calibrated to the bayesian predictive distribution."], "summary_text": "This paper combines two ideas. The first is stochastic gradient Langevin dynamics (SGLD), which is an efficient Bayesian learning method for larger datasets, allowing to efficiently sample from the posterior over the parameters of a model (e.g. a deep neural network). In short, SGLD is stochastic (minibatch) gradient descent, but where Gaussian noise is added to the gradients before each update. Each update thus results in a sample from the SGLD sampler. To make a prediction for a new data point, a number of previous parameter values are combined into an ensemble, which effectively corresponds to Monte Carlo estimate of the posterior predictive distribution of the model. The second idea is distillation or dark knowledge, which in short is the idea of training a smaller model (student) in replicating the behavior and performance of a much larger model (teacher), by essentially training the student to match the outputs of the teacher. The observation made in this paper is that the step of creating an ensemble of several models (e.g. deep networks) can be expensive, especially if many samples are used and/or if each model is large. Thus, they propose to approximate the output of that ensemble by training a single network to predict to output of ensemble. Ultimately, this is done by having the student predict the output of a teacher corresponding to the model with the last parameter value sampled by SGLD. Interestingly, this process can be operated in an online fashion, where one alternates between sampling from SGLD (i.e. performing a noisy SGD step on the teacher model) and performing a distillation update (i.e. updating the student model, given the current teacher model). The end result is a student model, whose outputs should be calibrated to the bayesian predictive distribution.", "pdf_url": "http://papers.nips.cc/paper/5965-bayesian-dark-knowledge.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/balanrmw15.json"}
{"id": "85933068", "bin": "300_400", "summary_sentences": ["This paper presents a variety of issues related to the evaluation of image generative models.", "Specifically, they provide evidence that evaluations of generative models based on the popular Parzen windows estimator or based on a visual fidelity (qualitative) measure both present serious flaws.", "The Parzen windows approach to generative modeling evaluation works by taking a finite set of samples generated from a given model and then using those as the centroids of a Parzen windows Gaussian mixture.", "The constructed Parzen windows mixture is then used to compute a log-likelihood score on a set of test examples.", "Some of the key observations made in this paper are: 1.", "A simple, k-means based approach can obtain better Parzen windows performance than using the original training samples for a given dataset, even though these are samples from the true distribution!", "2.", "Even for the fairly low dimensional space of 6x6 image patches, a Parzen windows estimator would require an extremely large number of samples to come close to the true log-likelihood performance of a model.", "3.", "Visual fidelity is a bad predictor of true log-likelihood performance, as it is possible to Obtain great visual fidelity and arbitrarily low log-likelihood, with a Parzen windows model made of Gaussians with very small variance.", "Obtain bad visual fidelity and high log-likelihood by taking a model with high log-likelihood and mixing it with a white noise model and putting as much as 99% of the mixing probability on the white noise model (i.e. which would produce bad samples 99% of the time).", "4.", "Measuring overfitting of a model by taking samples from the model and making sure their training set nearest neighbors are different is ineffective, since it is actually trivial to generate samples that are each visually almost identical to a training example, but that yet each have large euclidean distance with their corresponding (visually similar) training example."], "summary_text": "This paper presents a variety of issues related to the evaluation of image generative models. Specifically, they provide evidence that evaluations of generative models based on the popular Parzen windows estimator or based on a visual fidelity (qualitative) measure both present serious flaws. The Parzen windows approach to generative modeling evaluation works by taking a finite set of samples generated from a given model and then using those as the centroids of a Parzen windows Gaussian mixture. The constructed Parzen windows mixture is then used to compute a log-likelihood score on a set of test examples. Some of the key observations made in this paper are: 1. A simple, k-means based approach can obtain better Parzen windows performance than using the original training samples for a given dataset, even though these are samples from the true distribution! 2. Even for the fairly low dimensional space of 6x6 image patches, a Parzen windows estimator would require an extremely large number of samples to come close to the true log-likelihood performance of a model. 3. Visual fidelity is a bad predictor of true log-likelihood performance, as it is possible to Obtain great visual fidelity and arbitrarily low log-likelihood, with a Parzen windows model made of Gaussians with very small variance. Obtain bad visual fidelity and high log-likelihood by taking a model with high log-likelihood and mixing it with a white noise model and putting as much as 99% of the mixing probability on the white noise model (i.e. which would produce bad samples 99% of the time). 4. Measuring overfitting of a model by taking samples from the model and making sure their training set nearest neighbors are different is ineffective, since it is actually trivial to generate samples that are each visually almost identical to a training example, but that yet each have large euclidean distance with their corresponding (visually similar) training example.", "pdf_url": "http://arxiv.org/pdf/1511.01844", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/theisob15.json"}
{"id": "38830210", "bin": "400_500", "summary_sentences": ["Problem Statement  Given a pre-trained neural network, which is trained using data from some distribution P (referred to as in-distribution data), the task is to detect the examples coming from a distribution Q which is different from P (referred to as out-of-distribution data).", "For example, if a digit recognizer neural network is trained using MNIST images, an out-of-distribution example would be images of animals.", "Neural Networks can make high confidence predictions even in such cases where the input is unrecognisable or irrelevant.", "The paper proposes ODIN which can detect such out-of-distribution examples without changing the pre-trained model itself.", "ODIN  Uses 2 major techniques  Temperature Scaling  Softmax classifier for the classification network can be written as:  pi(x, T) = exp(fi(x)/T) / sum(exp(fj(x)/T))  where x is the input, p is the softmax probability and T is the temperature scaling parameter.", "Increasing T (up to some extent) boosts the performance in distinguishing in-distribution and out-of-distribution examples.", "Input Preprocessing  Add small perturbations to the input (image) before feeding it into the network.", "x_perturbed = x - ε * sign(-δxlog(py(x, T)))  where ε is the perturbation magnitude  The perturbations are such that softmax scores between in-distribution and out-of-distribution samples become separable.", "Given an input (image), first perturb the input.", "Feed the perturbed input to the network to get its softmax score.", "If the softmax score is greater than some threshold, mark the input as in-distribution and feed in the unperturbed version of the input to the network for classification.", "Otherwise, mark the input as out-of-distribution.", "For detailed mathematical treatment, refer section 6 and appendix in the paper  Experiments  Code available on github  Models  DenseNet with depth L = 100 and growth rate k = 12  Wide ResNet with depth = 28 and widen factor = 10  In-Distribution Datasets  CIFAR-10  CIFAR-100  Out-of-Distribution Datasets  TinyImageNet  LSUN  iSUN  Gaussian Noise  Metrics  False Positive Rate at 95% True Positive Rate  Detection Error - minimum misclassification probability over all thresholds  Area Under the Receiver Operating Characteristic Curve  Area Under the Precision-Recall Curve  ODIN outperforms the baseline across all datasets and all models by a good margin.", "Notes  Very simple and straightforward approach with theoretical justification under some conditions.", "Limited to examples from Vision so can not judge its applicability for NLP tasks."], "summary_text": "Problem Statement  Given a pre-trained neural network, which is trained using data from some distribution P (referred to as in-distribution data), the task is to detect the examples coming from a distribution Q which is different from P (referred to as out-of-distribution data). For example, if a digit recognizer neural network is trained using MNIST images, an out-of-distribution example would be images of animals. Neural Networks can make high confidence predictions even in such cases where the input is unrecognisable or irrelevant. The paper proposes ODIN which can detect such out-of-distribution examples without changing the pre-trained model itself. ODIN  Uses 2 major techniques  Temperature Scaling  Softmax classifier for the classification network can be written as:  pi(x, T) = exp(fi(x)/T) / sum(exp(fj(x)/T))  where x is the input, p is the softmax probability and T is the temperature scaling parameter. Increasing T (up to some extent) boosts the performance in distinguishing in-distribution and out-of-distribution examples. Input Preprocessing  Add small perturbations to the input (image) before feeding it into the network. x_perturbed = x - ε * sign(-δxlog(py(x, T)))  where ε is the perturbation magnitude  The perturbations are such that softmax scores between in-distribution and out-of-distribution samples become separable. Given an input (image), first perturb the input. Feed the perturbed input to the network to get its softmax score. If the softmax score is greater than some threshold, mark the input as in-distribution and feed in the unperturbed version of the input to the network for classification. Otherwise, mark the input as out-of-distribution. For detailed mathematical treatment, refer section 6 and appendix in the paper  Experiments  Code available on github  Models  DenseNet with depth L = 100 and growth rate k = 12  Wide ResNet with depth = 28 and widen factor = 10  In-Distribution Datasets  CIFAR-10  CIFAR-100  Out-of-Distribution Datasets  TinyImageNet  LSUN  iSUN  Gaussian Noise  Metrics  False Positive Rate at 95% True Positive Rate  Detection Error - minimum misclassification probability over all thresholds  Area Under the Receiver Operating Characteristic Curve  Area Under the Precision-Recall Curve  ODIN outperforms the baseline across all datasets and all models by a good margin. Notes  Very simple and straightforward approach with theoretical justification under some conditions. Limited to examples from Vision so can not judge its applicability for NLP tasks.", "pdf_url": "https://arxiv.org/pdf/1706.02690", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/principled-detection-of-out-of-distribution-examples-in-neural-networks.json"}
{"id": "43791696", "bin": "400_500", "summary_sentences": ["This paper presents a linear algebraic trick for computing both the value and the gradient update for a loss function that compares a very high-dimensional target with a (dense) output prediction.", "Most of the paper exposes the specific case of the squared error loss, though it can also be applied to some other losses such as the so-called spherical softmax.", "One use case could be for training autoencoders with the squared error on very high-dimensional but sparse inputs.", "While a naive (i.e. what most people currently do) implementation would scale in $O(Dd)$ where $D$ is the input dimensionality and d the hidden layer dimensionality, they show that their trick allows to scale in $O(d^2)$.", "Their experiments show that they can achieve speedup factors of over 500 on the CPU, and over 1500 on the GPU.", "#### My two cents  This is a really neat, and frankly really surprising, mathematical contribution.", "I did not suspect getting rid of the dependence on D in the complexity would actually be achievable, even for the \"simpler\" case of the squared error.", "The jury is still out as to whether we can leverage the full power of this trick in practice.", "Indeed, the squared error over sparse targets isn't the most natural choice in most situations.", "The authors did try to use this trick in the context of a version of the neural network language model that uses the squared error instead of the negative log-softmax (or at least I think that's what was done...", "I couldn't confirm this with 100% confidence).", "They showed that good measures of word similarity (Simlex-999) could be achieved in this way, though using the hierarchical softmax actually achieves better performance in about the same time.", "But as far as I'm concerned, that doesn't make the trick less impressive.", "It's still a neat piece of new knowledge to have about reconstruction errors.", "Also, the authors mention that it would be possible to adapt the trick to the so-called (negative log) spherical softmax, which is like the softmax but where the numerator is the square of the pre-activation, instead of the exponential.", "I hope someone tries this out in the future, as perhaps it could be key to making this trick a real game changer!"], "summary_text": "This paper presents a linear algebraic trick for computing both the value and the gradient update for a loss function that compares a very high-dimensional target with a (dense) output prediction. Most of the paper exposes the specific case of the squared error loss, though it can also be applied to some other losses such as the so-called spherical softmax. One use case could be for training autoencoders with the squared error on very high-dimensional but sparse inputs. While a naive (i.e. what most people currently do) implementation would scale in $O(Dd)$ where $D$ is the input dimensionality and d the hidden layer dimensionality, they show that their trick allows to scale in $O(d^2)$. Their experiments show that they can achieve speedup factors of over 500 on the CPU, and over 1500 on the GPU. #### My two cents  This is a really neat, and frankly really surprising, mathematical contribution. I did not suspect getting rid of the dependence on D in the complexity would actually be achievable, even for the \"simpler\" case of the squared error. The jury is still out as to whether we can leverage the full power of this trick in practice. Indeed, the squared error over sparse targets isn't the most natural choice in most situations. The authors did try to use this trick in the context of a version of the neural network language model that uses the squared error instead of the negative log-softmax (or at least I think that's what was done... I couldn't confirm this with 100% confidence). They showed that good measures of word similarity (Simlex-999) could be achieved in this way, though using the hierarchical softmax actually achieves better performance in about the same time. But as far as I'm concerned, that doesn't make the trick less impressive. It's still a neat piece of new knowledge to have about reconstruction errors. Also, the authors mention that it would be possible to adapt the trick to the so-called (negative log) spherical softmax, which is like the softmax but where the numerator is the square of the pre-activation, instead of the exponential. I hope someone tries this out in the future, as perhaps it could be key to making this trick a real game changer!", "pdf_url": "http://papers.nips.cc/paper/5865-efficient-exact-gradient-update-for-training-deep-networks-with-very-large-sparse-targets.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/vincentbb15.json"}
{"id": "55048906", "bin": "400_500", "summary_sentences": ["What  They describe a method to transfer image styles based on semantic classes.", "This allows to:  (1) Transfer styles between images more accurately than with previous models.", "E.g. so that the background of an image does not receive the style of skin/hair/clothes/... seen in the style image.", "Skin in the synthesized image should receive the style of skin from the style image.", "Same for hair, clothes, etc.", "(2) Turn simple doodles into artwork by treating the simplified areas in the doodle as semantic classes and annotating an artwork with these same semantic classes.", "(E.g. \"this blob should receive the style from these trees.\")", "How  Their method is based on Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis .", "They use the same content loss and mostly the same MRF-based style loss.", "(Apparently they don't use the regularization loss.)", "They change the input of the MRF-based style loss.", "Usually that input would only be the activations of a VGG-layer (for the synthesized image or the style source image).", "They add a semantic map with weighting gamma to the activation, i.e. <representation of image> = <activation of specific layer for that image> || gamma * <semantic map>.", "The semantic map has N channels with 1s in a channel where a specific class is located (e.g. skin).", "The semantic map has to be created by the user for both the content image and the style image.", "As usually for the MRF loss, patches are then sampled from the representations.", "The semantic maps then influence the distance measure.", "I.e. patches are more likely to be sampled from the same semantic class.", "Higher gamma values make it more likely to sample from the same semantic class (because the distance from patches from different classes gets larger).", "One can create a small doodle with few colors, then use the colors as the semantic map.", "Then add a semantic map to an artwork and run the algorithm to transform the doodle into an artwork.", "Results  More control over the transfered styles than previously.", "Less sensitive to the style weighting, because of the additional gamma hyperparameter.", "Easy transformation from doodle to artwork.", "Turning a doodle into an artwork.", "Note that the doodle input image is also used as the semantic map of the input."], "summary_text": "What  They describe a method to transfer image styles based on semantic classes. This allows to:  (1) Transfer styles between images more accurately than with previous models. E.g. so that the background of an image does not receive the style of skin/hair/clothes/... seen in the style image. Skin in the synthesized image should receive the style of skin from the style image. Same for hair, clothes, etc. (2) Turn simple doodles into artwork by treating the simplified areas in the doodle as semantic classes and annotating an artwork with these same semantic classes. (E.g. \"this blob should receive the style from these trees.\") How  Their method is based on Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis . They use the same content loss and mostly the same MRF-based style loss. (Apparently they don't use the regularization loss.) They change the input of the MRF-based style loss. Usually that input would only be the activations of a VGG-layer (for the synthesized image or the style source image). They add a semantic map with weighting gamma to the activation, i.e. <representation of image> = <activation of specific layer for that image> || gamma * <semantic map>. The semantic map has N channels with 1s in a channel where a specific class is located (e.g. skin). The semantic map has to be created by the user for both the content image and the style image. As usually for the MRF loss, patches are then sampled from the representations. The semantic maps then influence the distance measure. I.e. patches are more likely to be sampled from the same semantic class. Higher gamma values make it more likely to sample from the same semantic class (because the distance from patches from different classes gets larger). One can create a small doodle with few colors, then use the colors as the semantic map. Then add a semantic map to an artwork and run the algorithm to transform the doodle into an artwork. Results  More control over the transfered styles than previously. Less sensitive to the style weighting, because of the additional gamma hyperparameter. Easy transformation from doodle to artwork. Turning a doodle into an artwork. Note that the doodle input image is also used as the semantic map of the input.", "pdf_url": "http://arxiv.org/pdf/1603.01768", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/neural_doodle.json"}
{"id": "63531440", "bin": "400_500", "summary_sentences": ["`Update 2015/11/23: Since I first wrote this note, I became involved in the next iterations of this work, which became v2 of the arXiv manuscript.", "The notes below were made based on v1.`  This paper considers the problem of Maximum Inner Product Search (MIPS).", "In MIPS, given a query $q$ and a set of inputs $x_i$, we want to find the input (or the top n inputs) with highest inner product, i.e. $argmax_i q' x_i$.", "Recently, it was shown that a simple transformation to the query and input vectors made it possible to approximately solve MIPS using hashing methods for Maximum Cosine Similarity Search (MCSS), a problem for which solutions are readily available (see section 2.4 for a brief but very clear description of the transformation).", "In this paper, the authors combine this approach with clustering, in order to improve the quality of retrieved inputs.", "Specifically, they consider the spherical k-means algorithm, which is a variant of k-means in which data points are clustered based on cosine similarity instead of the euclidean similarity (in short, data points are first scaled to be of unit norm, then in the training inner loop points are assigned to the cluster centroid with highest dot product and cluster centroids are updated as usual, except that they are always rescaled to unit norm).", "Moreover, they consider a bottom-up application of the algorithm to yield a hierarchical clustering tree.", "They propose to use such a hierarchical clustering tree to find the top-n candidates for MIPS.", "The key insight here is that, since spherical k-means relies on cosine similarity for finding the best cluster, and since we have a transformation that allows the maximisation of inner product to be approximated by the maximisation of cosine similarity, then a tree to find MIPS candidates could be constructed by running spherical k-means on the inputs transformed by the same transformation used for hashing-based MIPS.", "In order to make the search more robust to border issues when a query is close to the frontier between clusters, at each level of the tree they consider more than one candidate cluster during top-down search, so as to merge the candidates in several leaves of the tree at the very end of a full top down query.", "Their experiments using search with word embeddings show that the quality of the top 1, 10 and 100 MIPS candidates using their spherical k-means approach is better than using two hashing-based search methods."], "summary_text": "`Update 2015/11/23: Since I first wrote this note, I became involved in the next iterations of this work, which became v2 of the arXiv manuscript. The notes below were made based on v1.`  This paper considers the problem of Maximum Inner Product Search (MIPS). In MIPS, given a query $q$ and a set of inputs $x_i$, we want to find the input (or the top n inputs) with highest inner product, i.e. $argmax_i q' x_i$. Recently, it was shown that a simple transformation to the query and input vectors made it possible to approximately solve MIPS using hashing methods for Maximum Cosine Similarity Search (MCSS), a problem for which solutions are readily available (see section 2.4 for a brief but very clear description of the transformation). In this paper, the authors combine this approach with clustering, in order to improve the quality of retrieved inputs. Specifically, they consider the spherical k-means algorithm, which is a variant of k-means in which data points are clustered based on cosine similarity instead of the euclidean similarity (in short, data points are first scaled to be of unit norm, then in the training inner loop points are assigned to the cluster centroid with highest dot product and cluster centroids are updated as usual, except that they are always rescaled to unit norm). Moreover, they consider a bottom-up application of the algorithm to yield a hierarchical clustering tree. They propose to use such a hierarchical clustering tree to find the top-n candidates for MIPS. The key insight here is that, since spherical k-means relies on cosine similarity for finding the best cluster, and since we have a transformation that allows the maximisation of inner product to be approximated by the maximisation of cosine similarity, then a tree to find MIPS candidates could be constructed by running spherical k-means on the inputs transformed by the same transformation used for hashing-based MIPS. In order to make the search more robust to border issues when a query is close to the frontier between clusters, at each level of the tree they consider more than one candidate cluster during top-down search, so as to merge the candidates in several leaves of the tree at the very end of a full top down query. Their experiments using search with word embeddings show that the quality of the top 1, 10 and 100 MIPS candidates using their spherical k-means approach is better than using two hashing-based search methods.", "pdf_url": "http://arxiv.org/pdf/1507.05910", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/auvolatv15.json"}
{"id": "18322264", "bin": "400_500", "summary_sentences": ["Problem Statement: Given an image, answer a given question about the image.", "Assumptions:  The answer is assumed to be a single word thereby bypassing the evaluation issues of multi-word generation tasks.", "VIS-LSTM Model  Treat the input image as the first word in the question.", "Obtain the vector representation (skip-gram) for words in the question.", "Obtain the VGG Net embeddings of the image and use a linear transformation (dimensionality reduction weight matrix) to match the dimensions of word embeddings.", "Keep image embedding frozen during training and use an LSTM to combine the word vectors.", "LSTM outputs are fed into a softmax layer which generates the answer.", "Dataset  DAtaset for QUestion Ansering on Real-world images (DAQUAR)  1300 images and 7000 questions with 37 object classes.", "Downside is that even guess work can yield good results.", "The paper proposed an algorithm for generating questions using MS-COCO dataset.", "Perform preprocessing steps like breaking large sentences and changing indefinite determines to definite ones.", "object questions, number questions, colour questions and location questions can be generated by searching for nouns, numbers, colours and prepositions respectively.", "Resulting dataset has ~120K questions across above 4 semantic types.", "Models  VIS+LSTM - explained above  2-VIS+BLSTM - Add the image features twice, in beginning and in the end (using different linear transformations) plus use bidirectional LSTM  IMG+BOW - Multinomial logistic regression on image features without dimensionality reduction + bag of words (averaging word vectors).", "FULL - Simple average of above 2 models.", "Baseline  Includes models where the answer is guessed, or only image or question features are used or image features along with prior knowledge of object are used.", "Also includes a KNN model where the system finds the nearest (image, question) pair.", "Metrics  Accuracy  Wu-Palmer similarity measure  Observations  The VIS-LSTM model outperforms the baselines while the FULL model benefits from averaging across all the models.", "Some useful information seems to be lost when downsizing the VGG vectors.", "Fine tuning the word vectors helps with performance.", "Normalising CNN hidden image features into zero mean and unit variance leads to faster training.", "Model does not perform well on the task of considering spatial relations between multiple objects and counting objects when multiple objects are present"], "summary_text": "Problem Statement: Given an image, answer a given question about the image. Assumptions:  The answer is assumed to be a single word thereby bypassing the evaluation issues of multi-word generation tasks. VIS-LSTM Model  Treat the input image as the first word in the question. Obtain the vector representation (skip-gram) for words in the question. Obtain the VGG Net embeddings of the image and use a linear transformation (dimensionality reduction weight matrix) to match the dimensions of word embeddings. Keep image embedding frozen during training and use an LSTM to combine the word vectors. LSTM outputs are fed into a softmax layer which generates the answer. Dataset  DAtaset for QUestion Ansering on Real-world images (DAQUAR)  1300 images and 7000 questions with 37 object classes. Downside is that even guess work can yield good results. The paper proposed an algorithm for generating questions using MS-COCO dataset. Perform preprocessing steps like breaking large sentences and changing indefinite determines to definite ones. object questions, number questions, colour questions and location questions can be generated by searching for nouns, numbers, colours and prepositions respectively. Resulting dataset has ~120K questions across above 4 semantic types. Models  VIS+LSTM - explained above  2-VIS+BLSTM - Add the image features twice, in beginning and in the end (using different linear transformations) plus use bidirectional LSTM  IMG+BOW - Multinomial logistic regression on image features without dimensionality reduction + bag of words (averaging word vectors). FULL - Simple average of above 2 models. Baseline  Includes models where the answer is guessed, or only image or question features are used or image features along with prior knowledge of object are used. Also includes a KNN model where the system finds the nearest (image, question) pair. Metrics  Accuracy  Wu-Palmer similarity measure  Observations  The VIS-LSTM model outperforms the baselines while the FULL model benefits from averaging across all the models. Some useful information seems to be lost when downsizing the VGG vectors. Fine tuning the word vectors helps with performance. Normalising CNN hidden image features into zero mean and unit variance leads to faster training. Model does not perform well on the task of considering spatial relations between multiple objects and counting objects when multiple objects are present", "pdf_url": "https://arxiv.org/pdf/1505.02074", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/exploring-models-and-data-for-image-question-answering.json"}
{"id": "62996279", "bin": "400_500", "summary_sentences": ["This paper discusses some amazing results.", "The goal is to learn how to count by end-to-end training.", "The network input is an image and the output is a count of the objects inside it.", "They do not perform any direct training using the locations of the objects in the image.", "The reason for avoiding direct training is that labeled data is expensive.", "Employing a surrogate objective ,such as the count of items in the image, is much cheaper and makes more sense because it is the goal of the system we want to learn.", "This paper states that it is possible!", "The discuss experiments on two datasets; one of MNIST digits placed in an image and one with the UCSD Pedestrian Database.", "The network description seems to be general and they don't report any special constraints on the design  `\"We consider networks of two or more convolutional layers followed by one or more fully connected layers.", "Each convolutional layer consist of several elements: a set of convolutional filters, ReLU non-linearities, max pooling layers and normalization layers.", "\"` and `\"We use a five layers architecture CNN with two convolutional layers followed by three fully connected layers\"`.", "They provide these two tables for their designs:  $$\\begin{array}{c|c|c|c}  Conv1 & Conv2 & FC1 & FC2  \\\\ \\hline 10\\text{x}15\\text{x}15 & 10\\text{x}3\\text{x}3 & 32 & 6 \\\\ \\text{x2 pool} & \\text{x2 pool} & & \\\\ \\hline \\end{array}\\\\ \\text{CNN arch for numbers}$$  $$ \\begin{array}{c|c|c|c|c}  Conv1 & Conv2 & FC1 & FC2 & FC3 \\\\ \\hline 8\\text{x}9\\text{x}9 & 8\\text{x}5\\text{x}5 & 128 & 128 & 25 \\\\ \\text{x2 pool} & \\text{x2 pool} & & \\\\ \\hline \\end{array}\\\\ \\text{CNN arch for people}$$  They state that they use a method based on hypercolumns  [ref]  but the description is not clear at all: `\" Starting with the hypercolumn representation on the last layer we cluster the resulting hypercolumns into a set of prototypes using an online k-means algorithm.", "Then, a MIL approach with positive and negative instances with the concept of interest is used.", "\"`  !", "[]( [url]"], "summary_text": "This paper discusses some amazing results. The goal is to learn how to count by end-to-end training. The network input is an image and the output is a count of the objects inside it. They do not perform any direct training using the locations of the objects in the image. The reason for avoiding direct training is that labeled data is expensive. Employing a surrogate objective ,such as the count of items in the image, is much cheaper and makes more sense because it is the goal of the system we want to learn. This paper states that it is possible! The discuss experiments on two datasets; one of MNIST digits placed in an image and one with the UCSD Pedestrian Database. The network description seems to be general and they don't report any special constraints on the design  `\"We consider networks of two or more convolutional layers followed by one or more fully connected layers. Each convolutional layer consist of several elements: a set of convolutional filters, ReLU non-linearities, max pooling layers and normalization layers. \"` and `\"We use a five layers architecture CNN with two convolutional layers followed by three fully connected layers\"`. They provide these two tables for their designs:  $$\\begin{array}{c|c|c|c}  Conv1 & Conv2 & FC1 & FC2  \\\\ \\hline 10\\text{x}15\\text{x}15 & 10\\text{x}3\\text{x}3 & 32 & 6 \\\\ \\text{x2 pool} & \\text{x2 pool} & & \\\\ \\hline \\end{array}\\\\ \\text{CNN arch for numbers}$$  $$ \\begin{array}{c|c|c|c|c}  Conv1 & Conv2 & FC1 & FC2 & FC3 \\\\ \\hline 8\\text{x}9\\text{x}9 & 8\\text{x}5\\text{x}5 & 128 & 128 & 25 \\\\ \\text{x2 pool} & \\text{x2 pool} & & \\\\ \\hline \\end{array}\\\\ \\text{CNN arch for people}$$  They state that they use a method based on hypercolumns  [ref]  but the description is not clear at all: `\" Starting with the hypercolumn representation on the last layer we cluster the resulting hypercolumns into a set of prototypes using an online k-means algorithm. Then, a MIL approach with positive and negative instances with the concept of interest is used. \"`  ! []( [url]", "pdf_url": "https://arxiv.org/pdf/1505.08082", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/seguipv15.json"}
{"id": "2577868", "bin": "400_500", "summary_sentences": ["The paper proposes a simple and robust approach for hierarchically training an agent in the sparse reward setup.", "The broad idea is to train low-level primitives that are sufficiently diverse (so that they can be composed for solving higher level tasks) and to train a high level primitive that learns to combine these primitives for any given downstream task.", "Approach  The state can be divided into two components: proprioceptive states sp (measurement of agent’s own body that can be directly controlled by the agent) and the external states se/  Low-Level Policy Training  Low-level policies should be:  Diverse: should cover all the skills that the agent might have to perform.", "Effective: can make significant changes to the environment.", "Controllable: easy for high-level policies to use and control  For the low-level policy, the per-time step reward is directly proportional to change in the external state.", "The same reward is used for all the agents and environments(except regulated with environment specific controls and survival rewards).", "Phase conditioned policies  Good movement policies are expected to be at least roughly periodic and phase input (or time index) is used to achieve periodicity.", "Phase conditioned policy (=f(sp, φ)) where φ = {0, 1, …, k-1} is the phase index.", "At each timestep t, the model receives observation sp and phase index φ = t%k.", "The phase index is represented by a vector bφ.", "For phase conditioned policies, the agent state and actions are encouraged to be cyclic with the help of a cyclic loss.", "Experiments  Environments: Ant and Humanoid from Mujoco.", "Low-level control:  Using phase-conditioning is helpful when training low-level primitives.", "High-level control:  Cross Maze Environment with fixed goals  3 goals along 3 paths  Proposed method converges faster and to a smaller final distance to the goal showing that it is both efficient and consistent (with smaller variance across random seeds).", "Random Goal Maze  The goal is randomly drawn from a set of goals.", "“Cross” (shaped) maze and “skull” (shaped) mazes are considered.", "Even with velocity rewards and pretraining on low-level objectives (which can be thought of as exploration bonuses), the baseline fails to get close to the goal locations while the proposed model reach the goal most of the times.", "The main results are reported using PPO though repeating the experiments with A2C and DQN show that the idea is fairly robust.", "The paper reported that in their experiments, finetuning the lower level primitives did not help much though it might not be the case of other environments."], "summary_text": "The paper proposes a simple and robust approach for hierarchically training an agent in the sparse reward setup. The broad idea is to train low-level primitives that are sufficiently diverse (so that they can be composed for solving higher level tasks) and to train a high level primitive that learns to combine these primitives for any given downstream task. Approach  The state can be divided into two components: proprioceptive states sp (measurement of agent’s own body that can be directly controlled by the agent) and the external states se/  Low-Level Policy Training  Low-level policies should be:  Diverse: should cover all the skills that the agent might have to perform. Effective: can make significant changes to the environment. Controllable: easy for high-level policies to use and control  For the low-level policy, the per-time step reward is directly proportional to change in the external state. The same reward is used for all the agents and environments(except regulated with environment specific controls and survival rewards). Phase conditioned policies  Good movement policies are expected to be at least roughly periodic and phase input (or time index) is used to achieve periodicity. Phase conditioned policy (=f(sp, φ)) where φ = {0, 1, …, k-1} is the phase index. At each timestep t, the model receives observation sp and phase index φ = t%k. The phase index is represented by a vector bφ. For phase conditioned policies, the agent state and actions are encouraged to be cyclic with the help of a cyclic loss. Experiments  Environments: Ant and Humanoid from Mujoco. Low-level control:  Using phase-conditioning is helpful when training low-level primitives. High-level control:  Cross Maze Environment with fixed goals  3 goals along 3 paths  Proposed method converges faster and to a smaller final distance to the goal showing that it is both efficient and consistent (with smaller variance across random seeds). Random Goal Maze  The goal is randomly drawn from a set of goals. “Cross” (shaped) maze and “skull” (shaped) mazes are considered. Even with velocity rewards and pretraining on low-level objectives (which can be thought of as exploration bonuses), the baseline fails to get close to the goal locations while the proposed model reach the goal most of the times. The main results are reported using PPO though repeating the experiments with A2C and DQN show that the idea is fairly robust. The paper reported that in their experiments, finetuning the lower level primitives did not help much though it might not be the case of other environments.", "pdf_url": "https://openreview.net/pdf?id=SJz1x20cFQ", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/hierarchical-rl-using-an-ensemble-of-proprioceptive-periodic-policies.json"}
{"id": "78339451", "bin": "400_500", "summary_sentences": ["The paper presents Deep Convolutional Generative Adversarial Nets (DCGAN) - a topologically constrained variant of conditional GAN.", "Benefits  Stable to train  Very useful to learn unsupervised image representations.", "Model  GANs difficult to scale using CNNs.", "Paper proposes following changes to GANs:  Replace any pooling layers with strided convolutions (for discriminator) and fractional strided convolutions (for generators).", "Remove fully connected hidden layers.", "Use batch normalisation in both generator (all layers except output layer) and discriminator (all layers except input layer).", "Use LeakyReLU in all layers of the discriminator.", "Use ReLU activation in all layers of the generator (except output layer which uses Tanh).", "Datasets  Large-Scale Scene Understanding.", "Imagenet-1K.", "Faces dataset.", "Hyperparameters  Minibatch SGD with minibatch size of 128.", "Weights initialized with 0 centered Normal distribution with standard deviation = 0.02  Adam    Optimizer  Slope of leak = 0.2 for LeakyReLU.", "Learning rate = 0.0002, β1 = 0.5  Observations  Large-Scale Scene Understanding data  Demonstrates that model scales with more data and higher resolution generation.", "Even though it is unlikely that model would have memorized images (due to low learning rate of minibatch SGD).", "Classifying CIFAR-10 dataset  Features  Train in Imagenet-1K and test on CIFAR-10.", "Max pool discriminator's convolutional features (from all layers) to get 4x4 spatial grids.", "Flatten and concatenate to get a 28672-dimensional vector.", "Linear L2-SVM classifier trained over the feature vector.", "82.8% accuracy, outperforms K-means (80.6%)  Street View House Number Classifier  Similar pipeline as CIFAR-10  22.48% test error.", "The paper contains many examples of images generated by final and intermediate layers of the network.", "Images in the latent space do not show sharp transitions indicating that network did not memorize images.", "DCGAN can learn an interesting hierarchy of features.", "Networks seems to have some success in disentangling image representation from object representation.", "Vector arithmetic can be performed on the Z vectors corresponding to the face samples to get results like smiling woman - normal woman + normal man = smiling man visually.", "This comment has been minimized.", "Sign in to view  Copy link  Quote reply  2017develper commented  Apr 14, 2018  i want to find a tutorial with gan and unsupervised learning in python please can you help me  This comment has been minimized.", "Sign in to view  Copy link  Quote reply  vishal5212 commented  May 16, 2019  no"], "summary_text": "The paper presents Deep Convolutional Generative Adversarial Nets (DCGAN) - a topologically constrained variant of conditional GAN. Benefits  Stable to train  Very useful to learn unsupervised image representations. Model  GANs difficult to scale using CNNs. Paper proposes following changes to GANs:  Replace any pooling layers with strided convolutions (for discriminator) and fractional strided convolutions (for generators). Remove fully connected hidden layers. Use batch normalisation in both generator (all layers except output layer) and discriminator (all layers except input layer). Use LeakyReLU in all layers of the discriminator. Use ReLU activation in all layers of the generator (except output layer which uses Tanh). Datasets  Large-Scale Scene Understanding. Imagenet-1K. Faces dataset. Hyperparameters  Minibatch SGD with minibatch size of 128. Weights initialized with 0 centered Normal distribution with standard deviation = 0.02  Adam    Optimizer  Slope of leak = 0.2 for LeakyReLU. Learning rate = 0.0002, β1 = 0.5  Observations  Large-Scale Scene Understanding data  Demonstrates that model scales with more data and higher resolution generation. Even though it is unlikely that model would have memorized images (due to low learning rate of minibatch SGD). Classifying CIFAR-10 dataset  Features  Train in Imagenet-1K and test on CIFAR-10. Max pool discriminator's convolutional features (from all layers) to get 4x4 spatial grids. Flatten and concatenate to get a 28672-dimensional vector. Linear L2-SVM classifier trained over the feature vector. 82.8% accuracy, outperforms K-means (80.6%)  Street View House Number Classifier  Similar pipeline as CIFAR-10  22.48% test error. The paper contains many examples of images generated by final and intermediate layers of the network. Images in the latent space do not show sharp transitions indicating that network did not memorize images. DCGAN can learn an interesting hierarchy of features. Networks seems to have some success in disentangling image representation from object representation. Vector arithmetic can be performed on the Z vectors corresponding to the face samples to get results like smiling woman - normal woman + normal man = smiling man visually. This comment has been minimized. Sign in to view  Copy link  Quote reply  2017develper commented  Apr 14, 2018  i want to find a tutorial with gan and unsupervised learning in python please can you help me  This comment has been minimized. Sign in to view  Copy link  Quote reply  vishal5212 commented  May 16, 2019  no", "pdf_url": "https://arxiv.org/pdf/1511.06434", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/79796c70565e3761e86d0f932a3de5.json"}
{"id": "51450104", "bin": "400_500", "summary_sentences": ["The paper proposes an adversarial approach for estimating generative models where one model (generative model) tries to learn a data distribution and another model (discriminative model) tries to distinguish between samples from the generative model and original data distribution.", "Adversarial Net  Two models - Generative Model(G) and Discriminative Model(D)  Both are multi-layer perceptrons.", "G takes as input a noise variable z and outputs data sample x(=G(z)).", "D takes as input a data sample x and predicts whether it came from true data or from G.  G tries to minimise log(1-D(G(z))) while D tries to maximise the probability of correct classification.", "Think of it as a minimax game between 2 players and the global optimum would be when G generates perfect samples and D can not distinguish between the samples (thereby always returning 0.5 as the probability of sample coming from true data).", "Alternate between k steps of training D and 1 step of training G so that D is maintained near its optimal solution.", "When starting training, the loss log(1-D(G(z))) would saturate as G would be weak.", "Instead maximise log(D(G(z)))  The paper contains the theoretical proof for global optimum of the minimax game.", "Experiments  Datasets  MNIST, Toronto Face Database, CIFAR-10  Generator model uses RELU and sigmoid activations.", "Discriminator model uses maxout and dropout.", "Evaluation Metric  Fit Gaussian Parzen window to samples obtained from G and compare log-likelihood.", "Strengths  Computational advantages  Backprop is sufficient for training with no need for Markov chains or performing inference.", "A variety of functions can be used in the model.", "Since G is trained only using the gradients from D, fewer chances of directly copying features from the true data.", "Can represent sharp (even degenerate) distributions.", "Weakness  D must be well synchronised with G.  While G may learn to sample data points that are indistinguishable from true data, no explicit representation can be obtained.", "Possible Extensions  Conditional generative models.", "Inference network to predict z given x.", "Implement a stochastic extension of the deterministic Multi-Prediction Deep Boltzmann Machines  Using discriminator net or inference net for feature selection.", "Accelerating training by ensuring better coordination between G and D or by determining better distributions to sample z from during training."], "summary_text": "The paper proposes an adversarial approach for estimating generative models where one model (generative model) tries to learn a data distribution and another model (discriminative model) tries to distinguish between samples from the generative model and original data distribution. Adversarial Net  Two models - Generative Model(G) and Discriminative Model(D)  Both are multi-layer perceptrons. G takes as input a noise variable z and outputs data sample x(=G(z)). D takes as input a data sample x and predicts whether it came from true data or from G.  G tries to minimise log(1-D(G(z))) while D tries to maximise the probability of correct classification. Think of it as a minimax game between 2 players and the global optimum would be when G generates perfect samples and D can not distinguish between the samples (thereby always returning 0.5 as the probability of sample coming from true data). Alternate between k steps of training D and 1 step of training G so that D is maintained near its optimal solution. When starting training, the loss log(1-D(G(z))) would saturate as G would be weak. Instead maximise log(D(G(z)))  The paper contains the theoretical proof for global optimum of the minimax game. Experiments  Datasets  MNIST, Toronto Face Database, CIFAR-10  Generator model uses RELU and sigmoid activations. Discriminator model uses maxout and dropout. Evaluation Metric  Fit Gaussian Parzen window to samples obtained from G and compare log-likelihood. Strengths  Computational advantages  Backprop is sufficient for training with no need for Markov chains or performing inference. A variety of functions can be used in the model. Since G is trained only using the gradients from D, fewer chances of directly copying features from the true data. Can represent sharp (even degenerate) distributions. Weakness  D must be well synchronised with G.  While G may learn to sample data points that are indistinguishable from true data, no explicit representation can be obtained. Possible Extensions  Conditional generative models. Inference network to predict z given x. Implement a stochastic extension of the deterministic Multi-Prediction Deep Boltzmann Machines  Using discriminator net or inference net for feature selection. Accelerating training by ensuring better coordination between G and D or by determining better distributions to sample z from during training.", "pdf_url": "https://arxiv.org/pdf/1406.2661", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/9dc0444142be8bd8a7404a226880eb.json"}
{"id": "1055799", "bin": "400_500", "summary_sentences": ["This paper extends the results on action-conditional video prediction from Oh, et al., 2015 .", "The motivation behind this line of research is to investigate whether training RL agents to learn how their actions affect the environment reduces the amount of time they spend in exploration.", "The authors outline the following main challenges:  The properties of generalization and sensitivity to model structure of these methods are poorly understood  Accurate prediction for long time periods into the future is hard  Models that predict the high-dim image directly each time an action is taken are inefficient  The general approach is to use a Conv-RNN to take an observation from the environment and produce a high-dim state representation.", "This high-dim state representation can be combined with the action taken by the agent to predict how the environment will change.", "In other words, the goal is to learn a parametric model that approximates the state-action transition.", "One of the main contributions of this work is fusing the action with the hidden state representation when predicting the next hidden state representation in time.", "In previous work, the action was used instead to directly predict the next image.", "Why?", "Authors suggest it could “enable the model to incorporate action information more effectively”.", "Using observations directly from the environment to predict the next hidden state representation forces the agent to make predictions only when the next frame is available, i.e., 1-step prediction.", "Rolling forward the agent’s own predictions in the future allows it to predict many time steps ahead in a more efficient manner.", "Prediction-dependent transitions are ones of the form  where $s_t$ is the hidden state representation.", "There’s a trade-off between short- and long-term accuracy and prediction- vs observation-dependent transitions.", "When prediction-dependent transitions are mostly used, the agent learns better global dynamics of the game and can do better with long-term accuracy.", "When observation-dependent transitions are mixed with prediction-dependent, the agent pays more attention to details that provide it with better short-term accuracy.", "They evaluated their model on Breakout, Freeway, and Pong by replacing the real game with the learned environment simulator.", "A human would “play” the game, and the learned simulator would return the next frame given the action input by the human.", "Related works  Action-conditional Video Prediction  UNREAL  Learning to Act By Predicting the Future"], "summary_text": "This paper extends the results on action-conditional video prediction from Oh, et al., 2015 . The motivation behind this line of research is to investigate whether training RL agents to learn how their actions affect the environment reduces the amount of time they spend in exploration. The authors outline the following main challenges:  The properties of generalization and sensitivity to model structure of these methods are poorly understood  Accurate prediction for long time periods into the future is hard  Models that predict the high-dim image directly each time an action is taken are inefficient  The general approach is to use a Conv-RNN to take an observation from the environment and produce a high-dim state representation. This high-dim state representation can be combined with the action taken by the agent to predict how the environment will change. In other words, the goal is to learn a parametric model that approximates the state-action transition. One of the main contributions of this work is fusing the action with the hidden state representation when predicting the next hidden state representation in time. In previous work, the action was used instead to directly predict the next image. Why? Authors suggest it could “enable the model to incorporate action information more effectively”. Using observations directly from the environment to predict the next hidden state representation forces the agent to make predictions only when the next frame is available, i.e., 1-step prediction. Rolling forward the agent’s own predictions in the future allows it to predict many time steps ahead in a more efficient manner. Prediction-dependent transitions are ones of the form  where $s_t$ is the hidden state representation. There’s a trade-off between short- and long-term accuracy and prediction- vs observation-dependent transitions. When prediction-dependent transitions are mostly used, the agent learns better global dynamics of the game and can do better with long-term accuracy. When observation-dependent transitions are mixed with prediction-dependent, the agent pays more attention to details that provide it with better short-term accuracy. They evaluated their model on Breakout, Freeway, and Pong by replacing the real game with the learned environment simulator. A human would “play” the game, and the learned simulator would return the next frame given the action input by the human. Related works  Action-conditional Video Prediction  UNREAL  Learning to Act By Predicting the Future", "pdf_url": "https://arxiv.org/pdf/1704.02254", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/recurrent-environment-simulators.json"}
{"id": "66006367", "bin": "400_500", "summary_sentences": ["Empirical evidence indicates that at training time, the neural networks need to be of significantly larger size than necessary.", "The paper purposes a hypothesis called the lottery ticket hypothesis to explain this behaviour.", "The idea is the following - Successful training of a neural network depends on a lucky random initialization of a subcomponent of the network.", "Such components are referred to as lottery tickets.", "Larger networks are more likely to have these lottery tickets and hence are easier to train.", "Methodology  Various aspects of the hypothesis are explored empirically.", "Two tasks are considered - MNIST and XOR.", "For each task, the paper considers networks of different sizes and empirically shows that larger networks are more likely to converge (or have better performance) for a fixed number of epochs as compared to the smaller networks.", "Given a large, trained network, some weights (or units) of the network are pruned and the resulting network is reset to its initial random weights.", "The resulting network is the lottery-ticket in the sense that when the pruned network is trained, it is more likely to converge than an otherwise randomly initialised network of the same size.", "Further, it is more likely to match the original, larger network in terms of performance.", "The paper explores different aspects of this experiment:  Pruning Strategies:  One-shot strategy prunes the network in one-go while the iterative strategy prunes the network iteratively.", "Though the latter is computationally more intensive, it is more likely to find a lottery ticket.", "Size of the pruned network affects the speed of convergence when training the lottery ticket.", "If only the architecture or only the initial weights of the lottery ticket are used, the resulting network tends to converge more slowly and achieves a lower level of performance.", "This indicates that the lottery ticket depends on both the network architecture and the weight initialization.", "Discussion  The paper includes some more interesting experiments.", "For instance, the distribution of the initialization in the weights that survived the pruning suggests that small weights from before training tend to remain small after training.", "One interesting experiment would be to show the performance of the pruned network before resetting its weights and retraining again.", "This performance should be compared with the performance of the initial large network and the performance of the lottery ticket after training.", "Overall, the experiments are not sufficient to conclude anything about the correctness of the hypothesis.", "The proposition itself is very interesting and could enhance our understanding of how the neural networks work."], "summary_text": "Empirical evidence indicates that at training time, the neural networks need to be of significantly larger size than necessary. The paper purposes a hypothesis called the lottery ticket hypothesis to explain this behaviour. The idea is the following - Successful training of a neural network depends on a lucky random initialization of a subcomponent of the network. Such components are referred to as lottery tickets. Larger networks are more likely to have these lottery tickets and hence are easier to train. Methodology  Various aspects of the hypothesis are explored empirically. Two tasks are considered - MNIST and XOR. For each task, the paper considers networks of different sizes and empirically shows that larger networks are more likely to converge (or have better performance) for a fixed number of epochs as compared to the smaller networks. Given a large, trained network, some weights (or units) of the network are pruned and the resulting network is reset to its initial random weights. The resulting network is the lottery-ticket in the sense that when the pruned network is trained, it is more likely to converge than an otherwise randomly initialised network of the same size. Further, it is more likely to match the original, larger network in terms of performance. The paper explores different aspects of this experiment:  Pruning Strategies:  One-shot strategy prunes the network in one-go while the iterative strategy prunes the network iteratively. Though the latter is computationally more intensive, it is more likely to find a lottery ticket. Size of the pruned network affects the speed of convergence when training the lottery ticket. If only the architecture or only the initial weights of the lottery ticket are used, the resulting network tends to converge more slowly and achieves a lower level of performance. This indicates that the lottery ticket depends on both the network architecture and the weight initialization. Discussion  The paper includes some more interesting experiments. For instance, the distribution of the initialization in the weights that survived the pruning suggests that small weights from before training tend to remain small after training. One interesting experiment would be to show the performance of the pruned network before resetting its weights and retraining again. This performance should be compared with the performance of the initial large network and the performance of the lottery ticket after training. Overall, the experiments are not sufficient to conclude anything about the correctness of the hypothesis. The proposition itself is very interesting and could enhance our understanding of how the neural networks work.", "pdf_url": "https://arxiv.org/pdf/1803.03635", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/the-lottery-ticket-hypothesis-training-pruned-neural-networks.json"}
{"id": "66226432", "bin": "400_500", "summary_sentences": ["Intro  A new training procedure for recurrent VAEs is proposed.", "Recall that for VAEs, we model a joint distribution over observations $x$ and latent variables $z$, and assume that $z$ is involved in the generation of $x$.", "This distribution is parameterized by $\\theta$.", "Maximizing the marginal log-likelihood $p_{\\theta}(x)$ wrt $\\theta$ is intractable bc it requires integrating over $z$.", "Instead, introduce a variational distribution $q_{\\phi}(z|x)$ and maximize a lower bound on the marginal log-likelihood–the ELBO.", "Stochastic recurrent networks  When applying VAEs to sequences, it has been proposed to use recurrent networks for the recognition network (aka inference network aka variation posterior) and the generation network (aka decoder aka conditional probability of the next observation given previous observations and latents).", "These probabilistic models can be autoregressive (in this paper, they use LSTMs with MLPs for predicting the parameters of Gaussian distributions).", "It is common to model these conditional distributions with Gaussians for continuous variables or categoricals for discrete variables.", "Usually, the prior over latent variables is also learned with a parametric model.", "If I’m not mistaken, learning the parameters of these parametric models with a training data set, and the using them at test time for fast inference is referred to as amortized variational inference, which appears to have correlaries in our cognition .", "Z-forcing  Strong autoregressive decoders overpower the latent variables $z$, preventing the CPD from learning complex multi-modal distributions.", "To mitigate this, they introduce an auxiliary cost to the training objective.", "An extra parametric model is introduced, $p_{\\eta}(b | z)$, that “forces” the latents to be predictive of the hidden states $b$ of the “backward network” (the inference network).", "Experiments  They validate the approach on speech modeling (TIMIT, Blizzard) and language modeling.", "The metric is average LL.", "On Seqeuential MNIST, z-forcing is competitive with “deeper” recurrent generative models like PixelRNN.", "Some fun language modeling results interpolating the latent space  Takeaways  It’s always a consideration as to whether increasing the complexity of an approach (adding an extra network and auxiliary cost) is worth the effort vs. simpler approaches that can get almost the same performance.", "The results on TIMIT and Blizzard are pretty convincing.", "The authors also suggest incorporating the auxiliary loss with PixelRNN/CNN in future work."], "summary_text": "Intro  A new training procedure for recurrent VAEs is proposed. Recall that for VAEs, we model a joint distribution over observations $x$ and latent variables $z$, and assume that $z$ is involved in the generation of $x$. This distribution is parameterized by $\\theta$. Maximizing the marginal log-likelihood $p_{\\theta}(x)$ wrt $\\theta$ is intractable bc it requires integrating over $z$. Instead, introduce a variational distribution $q_{\\phi}(z|x)$ and maximize a lower bound on the marginal log-likelihood–the ELBO. Stochastic recurrent networks  When applying VAEs to sequences, it has been proposed to use recurrent networks for the recognition network (aka inference network aka variation posterior) and the generation network (aka decoder aka conditional probability of the next observation given previous observations and latents). These probabilistic models can be autoregressive (in this paper, they use LSTMs with MLPs for predicting the parameters of Gaussian distributions). It is common to model these conditional distributions with Gaussians for continuous variables or categoricals for discrete variables. Usually, the prior over latent variables is also learned with a parametric model. If I’m not mistaken, learning the parameters of these parametric models with a training data set, and the using them at test time for fast inference is referred to as amortized variational inference, which appears to have correlaries in our cognition . Z-forcing  Strong autoregressive decoders overpower the latent variables $z$, preventing the CPD from learning complex multi-modal distributions. To mitigate this, they introduce an auxiliary cost to the training objective. An extra parametric model is introduced, $p_{\\eta}(b | z)$, that “forces” the latents to be predictive of the hidden states $b$ of the “backward network” (the inference network). Experiments  They validate the approach on speech modeling (TIMIT, Blizzard) and language modeling. The metric is average LL. On Seqeuential MNIST, z-forcing is competitive with “deeper” recurrent generative models like PixelRNN. Some fun language modeling results interpolating the latent space  Takeaways  It’s always a consideration as to whether increasing the complexity of an approach (adding an extra network and auxiliary cost) is worth the effort vs. simpler approaches that can get almost the same performance. The results on TIMIT and Blizzard are pretty convincing. The authors also suggest incorporating the auxiliary loss with PixelRNN/CNN in future work.", "pdf_url": "http://papers.nips.cc/paper/7248-z-forcing-training-stochastic-recurrent-networks.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/z-forcing.json"}
{"id": "4353776", "bin": "400_500", "summary_sentences": ["The authors presented a number of asynchronous DRL algorithms with the intention of developing RL agents that can be trained on CPUs with multithreading.", "The algorithms used multiple threads to run copies of the environment and generate uncorrelated sequences of training samples.", "Parameters were then sent to a shared parameter server at regular intervals.", "Because this promotes non-stationarity for the sequences of SARSA tuples, experience replay is not necessarily needed.", "The implementations of RMSProp and Momentum SGD used by the authors employed a Hogwild!-inpsired lock free scheme for maximum efficiency.", "A3C is the “best” agent that was presented in this paper.", "It is an asynchronous advantage actor-critic algorithm.", "It maintains an approximation of the policy, an estimate of the value function, and computes an “advantage” function and a variance-reducing baseline Degris, et al. 2012 .", "An entropy regularization term was also used to discourage premature convergence.", "Notes  Implemented 1-step Q-learning, 1-step SARSA, n-step Q-learning, and Advantage Actor-Critic  A benefit is that learning is stabilized without having to use experience replay  Reduction in training time that is roughly linear in the number of parallel actor-learners.", "Is this good?", "Seems like we can do better.", "The RL algorithms used are fairly data-inefficient.", "Not really a noticeable difference in performance between n-step Q-learning and 1-step, except on TORCS.", "A3C uses n-step lookahead as well  Evidence  State-of-the-art results were obtained on some of the Atari games (ALE).", "An LSTM-based A3C agent was tested with Deepmind’s Labyrinth environment.", "They also tested on the TORCS car racing environment and MuJoCo, the continuous-space physics simulation engine.", "Strengths  Presenting the algorithms in pseudocode is very helpful to the reader.", "The authors went into implementation details, which is also helpful for those who wish to check the results for themselves.", "Future Directions  How can we reduce/control the asymptotic variance of these temporal difference methods?", "Also, of the actor-critic method?", "(Need to research this topic more).", "Dueling Networks for the state value and advantage functions  Reducing over-estimation bias of Q-values (Double DQN, etc)  Try comparing with async Recurrent-DDPG?", "A3C seems similar to DDPG- difference is with the deterministic vs. stochastic gradient update."], "summary_text": "The authors presented a number of asynchronous DRL algorithms with the intention of developing RL agents that can be trained on CPUs with multithreading. The algorithms used multiple threads to run copies of the environment and generate uncorrelated sequences of training samples. Parameters were then sent to a shared parameter server at regular intervals. Because this promotes non-stationarity for the sequences of SARSA tuples, experience replay is not necessarily needed. The implementations of RMSProp and Momentum SGD used by the authors employed a Hogwild!-inpsired lock free scheme for maximum efficiency. A3C is the “best” agent that was presented in this paper. It is an asynchronous advantage actor-critic algorithm. It maintains an approximation of the policy, an estimate of the value function, and computes an “advantage” function and a variance-reducing baseline Degris, et al. 2012 . An entropy regularization term was also used to discourage premature convergence. Notes  Implemented 1-step Q-learning, 1-step SARSA, n-step Q-learning, and Advantage Actor-Critic  A benefit is that learning is stabilized without having to use experience replay  Reduction in training time that is roughly linear in the number of parallel actor-learners. Is this good? Seems like we can do better. The RL algorithms used are fairly data-inefficient. Not really a noticeable difference in performance between n-step Q-learning and 1-step, except on TORCS. A3C uses n-step lookahead as well  Evidence  State-of-the-art results were obtained on some of the Atari games (ALE). An LSTM-based A3C agent was tested with Deepmind’s Labyrinth environment. They also tested on the TORCS car racing environment and MuJoCo, the continuous-space physics simulation engine. Strengths  Presenting the algorithms in pseudocode is very helpful to the reader. The authors went into implementation details, which is also helpful for those who wish to check the results for themselves. Future Directions  How can we reduce/control the asymptotic variance of these temporal difference methods? Also, of the actor-critic method? (Need to research this topic more). Dueling Networks for the state value and advantage functions  Reducing over-estimation bias of Q-values (Double DQN, etc)  Try comparing with async Recurrent-DDPG? A3C seems similar to DDPG- difference is with the deterministic vs. stochastic gradient update.", "pdf_url": "http://arxiv.org/pdf/1602.01783v1.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/a3c.json"}
{"id": "10218080", "bin": "400_500", "summary_sentences": ["This paper explores the use of so-called Monte Carlo objectives for training directed generative models with latent variables.", "Monte Carlo objectives take the form of the logarithm of a Monte Carlo estimate (i.e. an average over samples) of the marginal probability $P(x)$.", "One important motivation for using Monte Carlo objectives is that they can be shown (see the Importance Weighted Variational Autoencoder paper  [ref]  and my notes on it) to correspond to bounds on the true likelihood of the model, and one can tighten the bound simply by drawing more samples in the Monte Carlo objective.", "Currently, the most successful application of Monte Carlo objectives is based on an importance sampling estimate, which involves training a proposal distribution $Q(h|x)$ in addition to the model $P(x,h)$.", "This paper considers the problem of training with gradient descent on such objectives, in the context of a model to which the reparametrization trick cannot be used (e.g. for discrete latent variables).", "They analyze the sources of variance in the estimation of the gradients (see Equation 5) and propose a very simple approach to reducing the variance of a sampling-based estimator of these gradients.", "First, they argue that gradients with respect to the $P(x,h)$ parameters are less susceptible to problems due to high variance gradients.", "Second, and most importantly, they derive a multi-sample estimate of the gradient that is meant to reduce the variance of gradients on the proposal distribution parameters $Q(h|x)$.", "The end result is the gradient estimate of Equations 10-11.", "It is based on the observation that the first term of the gradient of Equation 5 doesn't distinguish between the contribution of each sampled latent hi.", "The key contribution is this: they notice that one can incorporate a variance reducing baseline for each sample hi, corresponding to the Monte Carlo estimate of the log-likelihood when removing hi from the estimate (see Equation 10).", "The authors show that this is a proper baseline, in that using it doesn't introduce a bias in the estimation for the gradients.", "Experiments show that this approach yields better performance than training based on Reweighted Wake Sleep  [ref]  or the use of NVIL baselines  [ref] , when training sigmoid belief networks as generative models or as structured output prediction (image completion) models on binarized MNIST."], "summary_text": "This paper explores the use of so-called Monte Carlo objectives for training directed generative models with latent variables. Monte Carlo objectives take the form of the logarithm of a Monte Carlo estimate (i.e. an average over samples) of the marginal probability $P(x)$. One important motivation for using Monte Carlo objectives is that they can be shown (see the Importance Weighted Variational Autoencoder paper  [ref]  and my notes on it) to correspond to bounds on the true likelihood of the model, and one can tighten the bound simply by drawing more samples in the Monte Carlo objective. Currently, the most successful application of Monte Carlo objectives is based on an importance sampling estimate, which involves training a proposal distribution $Q(h|x)$ in addition to the model $P(x,h)$. This paper considers the problem of training with gradient descent on such objectives, in the context of a model to which the reparametrization trick cannot be used (e.g. for discrete latent variables). They analyze the sources of variance in the estimation of the gradients (see Equation 5) and propose a very simple approach to reducing the variance of a sampling-based estimator of these gradients. First, they argue that gradients with respect to the $P(x,h)$ parameters are less susceptible to problems due to high variance gradients. Second, and most importantly, they derive a multi-sample estimate of the gradient that is meant to reduce the variance of gradients on the proposal distribution parameters $Q(h|x)$. The end result is the gradient estimate of Equations 10-11. It is based on the observation that the first term of the gradient of Equation 5 doesn't distinguish between the contribution of each sampled latent hi. The key contribution is this: they notice that one can incorporate a variance reducing baseline for each sample hi, corresponding to the Monte Carlo estimate of the log-likelihood when removing hi from the estimate (see Equation 10). The authors show that this is a proper baseline, in that using it doesn't introduce a bias in the estimation for the gradients. Experiments show that this approach yields better performance than training based on Reweighted Wake Sleep  [ref]  or the use of NVIL baselines  [ref] , when training sigmoid belief networks as generative models or as structured output prediction (image completion) models on binarized MNIST.", "pdf_url": "http://arxiv.org/pdf/1602.06725", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/mnihr16.json"}
{"id": "62769214", "bin": "400_500", "summary_sentences": ["What  When using pretrained networks (like VGG) to solve tasks, one has to use features generated by these networks.", "These features come from specific layers, e.g. from the fully connected layers at the end of the network.", "They test whether the features from fully connected layers or from the last convolutional layer are better suited for face attribute prediction.", "How  Base networks  They use standard architectures for their test networks, specifically the architectures of FaceNet and VGG (very deep version).", "They modify these architectures to both use PReLUs.", "They do not use the pretrained weights, instead they train the networks on their own.", "They train them on the WebFace dataset (350k images, 10k different identities) to classify the identity of the shown person.", "Attribute prediction  After training of the base networks, they train a separate SVM to predict attributes of faces.", "The datasets used for this step are CelebA (100k images, 10k identities) and LFWA (13k images, 6k identities).", "Each image in these datasets is annotated with 40 binary face attributes.", "Examples for attributes: Eyeglasses, bushy eyebrows, big lips, ...", "The features for the SVM are extracted from the base networks (i.e. feed forward a face through the network, then take the activations of a specific layer).", "The following features are tested:  FC2: Activations of the second fully connected layer of the base network.", "FC1: As FC2, but the first fully connected layer.", "Spat 3x3: Activations of the last convolutional layer, max-pooled so that their widths and heights are both 3 (i.e. shape Cx3x3).", "Spat 1x1: Same as \"Spat 3x3\", but max-pooled to Cx1x1.", "Results  The SVMs trained on \"Spat 1x1\" performed overall worst, the ones trained on \"Spat 3x3\" performed best.", "The accuracy order was roughly: Spat 3x3 > FC1 > FC2 > Spat 1x1.", "This effect was consistent for both networks (VGG, FaceNet) and for other training datasets as well.", "FC2 performed particularly bad for the \"blurry\" attribute (most likely because that was unimportant to the classification task).", "Accuracy comparison per attribute:  The conclusion is, that when using pretrained networks one should not only try the last fully connected layer.", "Many characteristics of the input image might not appear any more in that layer (and later ones in general) as they were unimportant to the classification task."], "summary_text": "What  When using pretrained networks (like VGG) to solve tasks, one has to use features generated by these networks. These features come from specific layers, e.g. from the fully connected layers at the end of the network. They test whether the features from fully connected layers or from the last convolutional layer are better suited for face attribute prediction. How  Base networks  They use standard architectures for their test networks, specifically the architectures of FaceNet and VGG (very deep version). They modify these architectures to both use PReLUs. They do not use the pretrained weights, instead they train the networks on their own. They train them on the WebFace dataset (350k images, 10k different identities) to classify the identity of the shown person. Attribute prediction  After training of the base networks, they train a separate SVM to predict attributes of faces. The datasets used for this step are CelebA (100k images, 10k identities) and LFWA (13k images, 6k identities). Each image in these datasets is annotated with 40 binary face attributes. Examples for attributes: Eyeglasses, bushy eyebrows, big lips, ... The features for the SVM are extracted from the base networks (i.e. feed forward a face through the network, then take the activations of a specific layer). The following features are tested:  FC2: Activations of the second fully connected layer of the base network. FC1: As FC2, but the first fully connected layer. Spat 3x3: Activations of the last convolutional layer, max-pooled so that their widths and heights are both 3 (i.e. shape Cx3x3). Spat 1x1: Same as \"Spat 3x3\", but max-pooled to Cx1x1. Results  The SVMs trained on \"Spat 1x1\" performed overall worst, the ones trained on \"Spat 3x3\" performed best. The accuracy order was roughly: Spat 3x3 > FC1 > FC2 > Spat 1x1. This effect was consistent for both networks (VGG, FaceNet) and for other training datasets as well. FC2 performed particularly bad for the \"blurry\" attribute (most likely because that was unimportant to the classification task). Accuracy comparison per attribute:  The conclusion is, that when using pretrained networks one should not only try the last fully connected layer. Many characteristics of the input image might not appear any more in that layer (and later ones in general) as they were unimportant to the classification task.", "pdf_url": "http://arxiv.org/pdf/1602.03935v2", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/face_attribute_prediction_using_off-the-shelf_cnn_features.json"}
{"id": "26515037", "bin": "400_500", "summary_sentences": ["Existing word embedding models like Skip-Gram , GloVe etc map words to fixed sized vectors in a low dimensional vector space.", "This fixed point setting cannot capture uncertainty about representation.", "Further, these fixed point vectors are compared with measures like dot product and cosine similarity which are not suitable for capturing asymmetric properties like textual entailment and inclusion.", "The paper proposes to learn Gaussian function embeddings (with diagonal covariance) for the word vectors.", "This way, the words are mapped to soft regions in the embedding space which enables modeling uncertainty and asymmetric properties like inclusion and uncertainty.", "Implementation  Approach  KL divergence is used as the asymmetric distance function for comparing the distributions.", "Unlike the Word2Vec model, the proposed model uses ranking-based loss.", "Similarity Measures used  Symmetric Similarity  For two gaussian distributions, Pi and Pj, compute the inner product E(Pi, Pj) as N(0; meani - meanj, sigmai + sigmaj).", "Compute the gradient of mean and sigma with respect to log(E).", "The resulting loss function can be interpreted as pushing the means closer which encouraging the two gaussians to be more concentrated.", "Asymmetric Similarity  Use KL divergence to encode the context distribution.", "The benefit over the symmetric setting is that now entailment type relations can also be modeled.", "For example, a low KL divergence from x to y indicates that y can be encoded as x or that y “entails” x.", "Learning  One of the two notions of similarity is chosen and max-margin is used as the loss function.", "Mean is regularized by adding a simple constraint on the L2-norm.", "For covariance matrix, the eigenvalues are constrained to lie within a hypercube.", "This ensures that the positive-definite property of the covariance matrix is maintained while having a constraint on the size.", "Observations  Polysemous words have higher variance in their word embeddings as compared to specific words.", "KL divergence (with diagonal covariance) outperforms other models.", "Simple tree hierarchies can also be modeled by embedding into the Gaussian space.", "A Gaussian is created for each node with randomly initialized mean and the same set of embeddings is used for nodes and context.", "For word similarity benchmarks, embeddings with spherical covariance have a slight edge over embeddings with diagonal covariance and outperform the Skip-Gram model in all the cases.", "Future Work  Use combinations of low rank and diagonal matrices for covariances.", "Improved optimisation strategies.", "Trying other distributions like Student’s-t distribution."], "summary_text": "Existing word embedding models like Skip-Gram , GloVe etc map words to fixed sized vectors in a low dimensional vector space. This fixed point setting cannot capture uncertainty about representation. Further, these fixed point vectors are compared with measures like dot product and cosine similarity which are not suitable for capturing asymmetric properties like textual entailment and inclusion. The paper proposes to learn Gaussian function embeddings (with diagonal covariance) for the word vectors. This way, the words are mapped to soft regions in the embedding space which enables modeling uncertainty and asymmetric properties like inclusion and uncertainty. Implementation  Approach  KL divergence is used as the asymmetric distance function for comparing the distributions. Unlike the Word2Vec model, the proposed model uses ranking-based loss. Similarity Measures used  Symmetric Similarity  For two gaussian distributions, Pi and Pj, compute the inner product E(Pi, Pj) as N(0; meani - meanj, sigmai + sigmaj). Compute the gradient of mean and sigma with respect to log(E). The resulting loss function can be interpreted as pushing the means closer which encouraging the two gaussians to be more concentrated. Asymmetric Similarity  Use KL divergence to encode the context distribution. The benefit over the symmetric setting is that now entailment type relations can also be modeled. For example, a low KL divergence from x to y indicates that y can be encoded as x or that y “entails” x. Learning  One of the two notions of similarity is chosen and max-margin is used as the loss function. Mean is regularized by adding a simple constraint on the L2-norm. For covariance matrix, the eigenvalues are constrained to lie within a hypercube. This ensures that the positive-definite property of the covariance matrix is maintained while having a constraint on the size. Observations  Polysemous words have higher variance in their word embeddings as compared to specific words. KL divergence (with diagonal covariance) outperforms other models. Simple tree hierarchies can also be modeled by embedding into the Gaussian space. A Gaussian is created for each node with randomly initialized mean and the same set of embeddings is used for nodes and context. For word similarity benchmarks, embeddings with spherical covariance have a slight edge over embeddings with diagonal covariance and outperform the Skip-Gram model in all the cases. Future Work  Use combinations of low rank and diagonal matrices for covariances. Improved optimisation strategies. Trying other distributions like Student’s-t distribution.", "pdf_url": "https://arxiv.org/pdf/1412.6623", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/word-representations-via-gaussian-embedding.json"}
{"id": "71225651", "bin": "400_500", "summary_sentences": ["What  They suggest a new model for human pose estimation (i.e. to lay a \"skeleton\" over the image of a person).", "Their model has a (more or less) recurrent architecture.", "Initial estimates of keypoint locations are refined in several steps.", "The idea of the recurrent architecture is derived from message passing, unrolled into one feed-forward model.", "How  Architecture  They generate the end result in multiple steps, similar to a recurrent network.", "Step 1:  Receives the image (368x368 resolution).", "Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).", "Step 2 and later:  (Modified) Receives the image (368x368 resolution) and the previous likelihood scores.", "(Same) Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).", "(New) Concatenates the likelihoods with the likelihoods of the previous step.", "(New) Applies a few more convolutions to the concatenation to compute the final likelihood scores.", "Visualization of the architecture:  Loss function  The basic loss function is a simple mean squared error between the expected output maps per keypoint and the predicted ones.", "In the expected output maps they mark the correct positions of the keypoints using a small gaussian function.", "They apply losses after each step in the architecture, argueing that this helps against vanishing gradients (they don't seem to be using BN).", "The expected output maps of the first step actually have the positions of all keypoints of a certain type (e.g. neck) marked, i.e.", "if there are multiple people in the extracted image patch there might be multiple correct keypoint positions.", "Only at step 2 and later they reduce that to the expected person (i.e. one keypoint position per map).", "Results  Example results:  Self-correction of predictions over several timesteps:  They beat existing methods on the datasets MPII, LSP and FLIC.", "Applying a loss function after each step (instead of only once after the last step) improved their results and reduced problems related to vanishing gradients.", "The effective receptive field size of each step had a significant influence on the results.", "They increased it to up to 300px (about 80% of the image size) and saw continuous improvements in accuracy."], "summary_text": "What  They suggest a new model for human pose estimation (i.e. to lay a \"skeleton\" over the image of a person). Their model has a (more or less) recurrent architecture. Initial estimates of keypoint locations are refined in several steps. The idea of the recurrent architecture is derived from message passing, unrolled into one feed-forward model. How  Architecture  They generate the end result in multiple steps, similar to a recurrent network. Step 1:  Receives the image (368x368 resolution). Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...). Step 2 and later:  (Modified) Receives the image (368x368 resolution) and the previous likelihood scores. (Same) Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...). (New) Concatenates the likelihoods with the likelihoods of the previous step. (New) Applies a few more convolutions to the concatenation to compute the final likelihood scores. Visualization of the architecture:  Loss function  The basic loss function is a simple mean squared error between the expected output maps per keypoint and the predicted ones. In the expected output maps they mark the correct positions of the keypoints using a small gaussian function. They apply losses after each step in the architecture, argueing that this helps against vanishing gradients (they don't seem to be using BN). The expected output maps of the first step actually have the positions of all keypoints of a certain type (e.g. neck) marked, i.e. if there are multiple people in the extracted image patch there might be multiple correct keypoint positions. Only at step 2 and later they reduce that to the expected person (i.e. one keypoint position per map). Results  Example results:  Self-correction of predictions over several timesteps:  They beat existing methods on the datasets MPII, LSP and FLIC. Applying a loss function after each step (instead of only once after the last step) improved their results and reduced problems related to vanishing gradients. The effective receptive field size of each step had a significant influence on the results. They increased it to up to 300px (about 80% of the image size) and saw continuous improvements in accuracy.", "pdf_url": "https://arxiv.org/pdf/1602.00134", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/convolutional_pose_machines.json"}
{"id": "8799962", "bin": "400_500", "summary_sentences": ["A key idea pushed in this paper is that a value function represents semantic knowledge.", "Indeed, they state, “A value function asks a question–what will the cumulative reward be?–and an approximate value function provides an answer to that question”.", "Accordingly, they introduce Generalized Value Functions (GVFs), a construct to expand the knowledge that value functions can encapsulate to make them capable of representing knowledge about the world.", "A GVF is parameterized with four functions, a policy, pseudo-reward function, pseudo-terminal reward function, and pseudo-termination function, called question functions.", "They introduce Horde, an architecture for learning 1 or more approximate GVFs in parallel, where each “demon” of the Horde is responsible for learning a piece of knowledge that contributes to the whole.", "Approximate GVFs can be learned off-policy.", "The paper uses GQ($\\lambda$) to train each demon, and hence a feature vector $\\phi$, behavior policy $b$, and eligibilty trace function $\\lambda$ must be specified; these are collectively called answer functions, since they are used to numerically find the value of approximate GVFs (answering the “question”).", "They show that a physical robot with many sensors is able to learn to predict how many steps it can go before needing to stop before hitting a wall (via 1 “predictive” demon, i.e., a demon that seeks to accurately “predict” the cumulative return by learning the approximate GVF for a given policy).", "The robot also uses 8 control demons to learn to separately maximize returns for “maxing out” 8 different sensors.", "Finally, they trained 1 control demon to learn a light-seeking behavior.", "Recently, Barreto, et.", "al 2017 developed the ideas of successor features (SF), a value function representation that decouples environment dynamics from the reward function.", "They use it to show transfer between tasks.", "They discuss how their method is a special case of a GVF, but that it provides a method for “selecting” pseudo-rewards.", "This paper differs from the options framework in that options essentially define a hierarchy of policy abstractions.", "The authors note that GVFs could be combined with this approach, however."], "summary_text": "A key idea pushed in this paper is that a value function represents semantic knowledge. Indeed, they state, “A value function asks a question–what will the cumulative reward be?–and an approximate value function provides an answer to that question”. Accordingly, they introduce Generalized Value Functions (GVFs), a construct to expand the knowledge that value functions can encapsulate to make them capable of representing knowledge about the world. A GVF is parameterized with four functions, a policy, pseudo-reward function, pseudo-terminal reward function, and pseudo-termination function, called question functions. They introduce Horde, an architecture for learning 1 or more approximate GVFs in parallel, where each “demon” of the Horde is responsible for learning a piece of knowledge that contributes to the whole. Approximate GVFs can be learned off-policy. The paper uses GQ($\\lambda$) to train each demon, and hence a feature vector $\\phi$, behavior policy $b$, and eligibilty trace function $\\lambda$ must be specified; these are collectively called answer functions, since they are used to numerically find the value of approximate GVFs (answering the “question”). They show that a physical robot with many sensors is able to learn to predict how many steps it can go before needing to stop before hitting a wall (via 1 “predictive” demon, i.e., a demon that seeks to accurately “predict” the cumulative return by learning the approximate GVF for a given policy). The robot also uses 8 control demons to learn to separately maximize returns for “maxing out” 8 different sensors. Finally, they trained 1 control demon to learn a light-seeking behavior. Recently, Barreto, et. al 2017 developed the ideas of successor features (SF), a value function representation that decouples environment dynamics from the reward function. They use it to show transfer between tasks. They discuss how their method is a special case of a GVF, but that it provides a method for “selecting” pseudo-rewards. This paper differs from the options framework in that options essentially define a hierarchy of policy abstractions. The authors note that GVFs could be combined with this approach, however.", "pdf_url": "https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/horde1.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/horde.json"}
{"id": "24312671", "bin": "400_500", "summary_sentences": ["The paper proposes an approach to learn useful skills without a reward function by maximizing an information theoretic objective by using a maximum entropy policy.", "Skills are defined as latent-conditioned policies that alter the state of the environment in a consistent way.", "Link to the code  Setup  Unsupervised “exploration” stage followed by supervised stage.", "Desirable Qualities of Skills  Skills should dictate the states that the agent visits.", "Different skills should visit different states to be distinguishable.", "States (not actions) should be used to distinguish between skills as not all actions change the state (for the outside observer).", "Skills are encouraged to be diverse and “exploratory” by learning skills that act randomly (have high entropy).", "Loss Formulation  (S, A) - state and action  z ~ p(z) - latent variable to condition the policy.", "Skill - policy conditioned on a fixed z.", "Objective is to maximize the mutual information between skill and state (MI(A; Z)) ie skill should control which state is visited or the skill should be inferrable from the state visited.", "Simultaneously minimize the mutual information between skills and actions given the state to ensure that the state (and not the action) is used to distinguish the skills.", "Maximize the entropy of the mixture of policies (p(z) and all the skills).", "Implementation  Policy π(a | s, z)  Task reward replaced by the pseduoreward logqφ(z | s) - log(p(z)).", "During unsupervised training, z is sampled at the start of the episode and then not changed during the episode.", "Learning agent gets rewards for visiting the states that are easy to discriminate while the discriminator updated to correctly predict z from the states visited.", "Observations  Analysis of Learned Skills  The agent learns a diverse set of primitive behaviors for all tasks ranging from 2 DoF to 111 DoF.", "for inverted pendulum and mountain car, the skills become increasingly diverse throughout training.", "Use of uniform prior, in place of a learned prior, for p(z) allows for discovery of more diverse skills.", "The proposed approach can be used as a pretraining technique where the best-performing primitives (from unsupervised training) can be finetuned with the task-specific rewards.", "The discovered skills can be used for hierarchical RL by learning a meta-policy(which chooses the skill to execute for k steps).", "Modifying the discriminator in the proposed formulation can be used to bias DIAYN towards discovering a particular type of policies.", "This provides a mechanism for incorporating “supervision” in the learning setup.", "The “discovered” primitives can also be used for imitation learning."], "summary_text": "The paper proposes an approach to learn useful skills without a reward function by maximizing an information theoretic objective by using a maximum entropy policy. Skills are defined as latent-conditioned policies that alter the state of the environment in a consistent way. Link to the code  Setup  Unsupervised “exploration” stage followed by supervised stage. Desirable Qualities of Skills  Skills should dictate the states that the agent visits. Different skills should visit different states to be distinguishable. States (not actions) should be used to distinguish between skills as not all actions change the state (for the outside observer). Skills are encouraged to be diverse and “exploratory” by learning skills that act randomly (have high entropy). Loss Formulation  (S, A) - state and action  z ~ p(z) - latent variable to condition the policy. Skill - policy conditioned on a fixed z. Objective is to maximize the mutual information between skill and state (MI(A; Z)) ie skill should control which state is visited or the skill should be inferrable from the state visited. Simultaneously minimize the mutual information between skills and actions given the state to ensure that the state (and not the action) is used to distinguish the skills. Maximize the entropy of the mixture of policies (p(z) and all the skills). Implementation  Policy π(a | s, z)  Task reward replaced by the pseduoreward logqφ(z | s) - log(p(z)). During unsupervised training, z is sampled at the start of the episode and then not changed during the episode. Learning agent gets rewards for visiting the states that are easy to discriminate while the discriminator updated to correctly predict z from the states visited. Observations  Analysis of Learned Skills  The agent learns a diverse set of primitive behaviors for all tasks ranging from 2 DoF to 111 DoF. for inverted pendulum and mountain car, the skills become increasingly diverse throughout training. Use of uniform prior, in place of a learned prior, for p(z) allows for discovery of more diverse skills. The proposed approach can be used as a pretraining technique where the best-performing primitives (from unsupervised training) can be finetuned with the task-specific rewards. The discovered skills can be used for hierarchical RL by learning a meta-policy(which chooses the skill to execute for k steps). Modifying the discriminator in the proposed formulation can be used to bias DIAYN towards discovering a particular type of policies. This provides a mechanism for incorporating “supervision” in the learning setup. The “discovered” primitives can also be used for imitation learning.", "pdf_url": "https://arxiv.org/pdf/1802.06070", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/diversity-is-all-you-need-learning-skills-without-a-reward-function.json"}
{"id": "58884367", "bin": "400_500", "summary_sentences": ["The paper presents a new machine comprehension dataset for question answering in real life setting (say when interacting with Cortana/Siri).", "Unique Aspects of the dataset  Existing machine comprehension (MC) datasets are either too small or synthetic (with a distribution different from that or real-questions posted by humans).", "MARCO questions are sampled from real, anonymized user queries.", "Most datasets would provide a comparatively small and clean context to answer the question.", "In MARCO, the context documents (which may or may not contain the answer) are extracted using Bing from real-world documents.", "As such the questions and the context documents are noisy.", "In general, the answer to the questions are restricted to an entity or text span within the document.", "In case of MARCO, the human judges are encouraged to generate complete sentences as answers.", "Dataset Description  First release consists of 100K questions with the aim of releasing 1M questions in the future releases.", "All questions are tagged with segment information.", "A subset of questions has multiple answers and another subset has no answers at all.", "Each record in the dataset contains the following information:  Query - The actual question  Passage - Top 10 contextual passages extracted from web search engine (which may or may not contain the answer to the question).", "Document URLs - URLs for the top documents (which are the source of the contextual passages).", "Answer - Answer synthesised by human evaluators.", "Segment - Query type, description, neumeric, entity, location, person.", "Experimental Results  Metrics  Accuracy and precision/recall for numeric questions  ROGUE-L/paraphrasing aware evaluation framework for long, textual answers.", "Among generative models, Memory Networks performed better than seq-to-seq.", "In the cloze-style test, ReasoNet achieved an accuracy of approx.", "59% while Attention Sum Reader achieved an accuracy of approx 55%.", "Current QA systems (including the ones using memory and attention) derive their power from supervised data and are very different from how humans do reasoning.", "Imagenet dataset pushed the state-of-the-art performance on object classification to beyond human accuracy.", "Similar was the case with speech recognition dataset from DARPA which led to the advancement of speech recognition.", "Having a large, diverse and human-like questions dataset is a fundamental requirement to advance the field and the paper aims to provide just the right kind of dataset."], "summary_text": "The paper presents a new machine comprehension dataset for question answering in real life setting (say when interacting with Cortana/Siri). Unique Aspects of the dataset  Existing machine comprehension (MC) datasets are either too small or synthetic (with a distribution different from that or real-questions posted by humans). MARCO questions are sampled from real, anonymized user queries. Most datasets would provide a comparatively small and clean context to answer the question. In MARCO, the context documents (which may or may not contain the answer) are extracted using Bing from real-world documents. As such the questions and the context documents are noisy. In general, the answer to the questions are restricted to an entity or text span within the document. In case of MARCO, the human judges are encouraged to generate complete sentences as answers. Dataset Description  First release consists of 100K questions with the aim of releasing 1M questions in the future releases. All questions are tagged with segment information. A subset of questions has multiple answers and another subset has no answers at all. Each record in the dataset contains the following information:  Query - The actual question  Passage - Top 10 contextual passages extracted from web search engine (which may or may not contain the answer to the question). Document URLs - URLs for the top documents (which are the source of the contextual passages). Answer - Answer synthesised by human evaluators. Segment - Query type, description, neumeric, entity, location, person. Experimental Results  Metrics  Accuracy and precision/recall for numeric questions  ROGUE-L/paraphrasing aware evaluation framework for long, textual answers. Among generative models, Memory Networks performed better than seq-to-seq. In the cloze-style test, ReasoNet achieved an accuracy of approx. 59% while Attention Sum Reader achieved an accuracy of approx 55%. Current QA systems (including the ones using memory and attention) derive their power from supervised data and are very different from how humans do reasoning. Imagenet dataset pushed the state-of-the-art performance on object classification to beyond human accuracy. Similar was the case with speech recognition dataset from DARPA which led to the advancement of speech recognition. Having a large, diverse and human-like questions dataset is a fundamental requirement to advance the field and the paper aims to provide just the right kind of dataset.", "pdf_url": "https://arxiv.org/pdf/1704.00051", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/reading-wikipedia-to-answer-open-domain-questions.json"}
{"id": "16973657", "bin": "400_500", "summary_sentences": ["The paper presents a semi-supervised learning framework for graphs where the node embeddings are used to jointly predict both the class labels and neighbourhood context.", "Usually, graph embeddings are learnt in an unsupervised manner and can not leverage the supervising signal coming from the labelled data.", "The framework is called Planetoid (Predicting Labels And Neighbors with Embeddings Transductively Or Inductively from Data) .", "Problem Setting  Given a graph G = (V, E) and xL and xU as feature vectors for labelled and unlabelled nodes and yL as labels for the labelled nodes, the problem is to learn a mapping (classifier) f: x -> y  There are two settings possible:  Transductive - Predictions are made only for those nodes which are already observed in the graph at training time.", "Inductive - Predictions are made for nodes whether they have been observed in the graph at training time or not.", "Approach  The general semi-supervised learning loss would be LS + λLU where LS is the supervised learning loss while LU is the unsupervised learning loss.", "The unsupervised loss is a variant of the Skip-gram loss with negative edge sampling.", "More specifically, first a random walk sequence S is sampled.", "Then either a positive edge is sampled from S (within a given context distance) or a negative edge is sampled.", "The label information is injected by using the label as a context and minimising the distance between the positive edges (edges where the nodes have the same label) and maximising the distance between the negative edges (edges where the nodes have different labels).", "Transductive Formulation  Two separate fully connected networks are applied over the node features and node embeddings.", "These 2 representations are then concatenated and fed to a softmax classifier to predict the class label.", "Inductive Formulation  In the inductive setting, it is difficult to obtain the node embeddings at test time.", "One naive approach is to retrain the network to obtain the embeddings on the previously unobserved nodes but that is inefficient.", "The embeddings of node x are parameterized as a function of its input feature vector and is learnt by applying a fully connected neural network on the node feature vector.", "This provides a simple way to extend the original approach to the inductive setting.", "Results  The proposed approach is evaluated in 3 settings (text classification, distantly supervised entity extraction and entity classification) and it consistently outperforms approaches that use just node features or node embeddings.", "The key takeaway is that the joint training in the semi-supervised setting has several benefits over the unsupervised setting and that using the graph context (in terms of node embeddings) is much more effective than using graph Laplacian-based regularization term."], "summary_text": "The paper presents a semi-supervised learning framework for graphs where the node embeddings are used to jointly predict both the class labels and neighbourhood context. Usually, graph embeddings are learnt in an unsupervised manner and can not leverage the supervising signal coming from the labelled data. The framework is called Planetoid (Predicting Labels And Neighbors with Embeddings Transductively Or Inductively from Data) . Problem Setting  Given a graph G = (V, E) and xL and xU as feature vectors for labelled and unlabelled nodes and yL as labels for the labelled nodes, the problem is to learn a mapping (classifier) f: x -> y  There are two settings possible:  Transductive - Predictions are made only for those nodes which are already observed in the graph at training time. Inductive - Predictions are made for nodes whether they have been observed in the graph at training time or not. Approach  The general semi-supervised learning loss would be LS + λLU where LS is the supervised learning loss while LU is the unsupervised learning loss. The unsupervised loss is a variant of the Skip-gram loss with negative edge sampling. More specifically, first a random walk sequence S is sampled. Then either a positive edge is sampled from S (within a given context distance) or a negative edge is sampled. The label information is injected by using the label as a context and minimising the distance between the positive edges (edges where the nodes have the same label) and maximising the distance between the negative edges (edges where the nodes have different labels). Transductive Formulation  Two separate fully connected networks are applied over the node features and node embeddings. These 2 representations are then concatenated and fed to a softmax classifier to predict the class label. Inductive Formulation  In the inductive setting, it is difficult to obtain the node embeddings at test time. One naive approach is to retrain the network to obtain the embeddings on the previously unobserved nodes but that is inefficient. The embeddings of node x are parameterized as a function of its input feature vector and is learnt by applying a fully connected neural network on the node feature vector. This provides a simple way to extend the original approach to the inductive setting. Results  The proposed approach is evaluated in 3 settings (text classification, distantly supervised entity extraction and entity classification) and it consistently outperforms approaches that use just node features or node embeddings. The key takeaway is that the joint training in the semi-supervised setting has several benefits over the unsupervised setting and that using the graph context (in terms of node embeddings) is much more effective than using graph Laplacian-based regularization term.", "pdf_url": "https://arxiv.org/pdf/1603.08861", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/revisiting-semi-supervised-learning-with-graph-embeddings.json"}
{"id": "81535007", "bin": "400_500", "summary_sentences": ["Lee, et al., 2016  This paper presents a framework for predicting how a scene containing multiple interacting agents will play out by leveraging a number of powerful techniques.", "DESIRE stands for Deep Stochastic IOC RNN Encoder-decoder framework.", "The following are the biggest obstacles for successfully predicting future actions taken by interacting agents given a visual snapshot of the present:  The space of future states for each agent in a scene is hard to optimize over; even when taking the context of the scene into account, there could be many plausible outcomes that seem equally likely  In turn, this makes rolling out and scoring potential future trajectories computationally expensive, since it requires sampling a large number of trajectories.", "This isn’t relevant for offline processing, but for real-time robotics applications, this is important  The multi-agent problem; how to infer interactions between multiple actors in a scene?", "Taking into account long-term prediction rewards, rather than just one-step prediction  How to define a multi-objective loss function that doesn’t commit errors such as averaging over all future possibilities  DESIRE attempts to address these challenges as follows:  Diverse sample generation using a conditional Variational Autoencoder.", "This allows for differentiable efficient sampling of plausible futures  RNN encoder-decoder neural architecture that allows for mapping trajectories represented by world coordinates in $\\mathbb{R}^2$ or $\\mathbb{R}^3$ to a high-dimensional distributed representation that can be efficiently combined with the Conditoinal VAE  An Inverse Optimal-Control (IOC) based Ranking and Refinement module that determines the most likely hypotheses, while incorporating scene context and interactions.", "The IOC module estimates a regression vector to refine each prediction sample.", "A convolutional neural network is used to carry out scene context fusion.", "This enables encoding features such as object velocities into the hypothesis scoring component.", "The model is trained end-to-end with a total loss consisting of multiple auxiliary losses (e.g., reconstruction error from the output trajectory of the decoder during training).", "The authors evaluate its performance on the KITTI and Stanford Drone tracking datasets.", "They show the performance with the metric “error in meters” for future time steps at intervals of 1, 2, 3, and 4 seconds.", "Overall, the proposed model seems quite complex, with many sub-components.", "However, the motivation behind each component is reasonable and the model is shown to perform well on the datasets.", "The separation of the system into a Sample Generation Module and the Ranking and Refinement Module is reminiscent of the actor-critic architecture in RL.", "The Ranking and Refinement Module uses unsupervised learning losses to learn to score/refine the “actor”, or Sample Generation Module."], "summary_text": "Lee, et al., 2016  This paper presents a framework for predicting how a scene containing multiple interacting agents will play out by leveraging a number of powerful techniques. DESIRE stands for Deep Stochastic IOC RNN Encoder-decoder framework. The following are the biggest obstacles for successfully predicting future actions taken by interacting agents given a visual snapshot of the present:  The space of future states for each agent in a scene is hard to optimize over; even when taking the context of the scene into account, there could be many plausible outcomes that seem equally likely  In turn, this makes rolling out and scoring potential future trajectories computationally expensive, since it requires sampling a large number of trajectories. This isn’t relevant for offline processing, but for real-time robotics applications, this is important  The multi-agent problem; how to infer interactions between multiple actors in a scene? Taking into account long-term prediction rewards, rather than just one-step prediction  How to define a multi-objective loss function that doesn’t commit errors such as averaging over all future possibilities  DESIRE attempts to address these challenges as follows:  Diverse sample generation using a conditional Variational Autoencoder. This allows for differentiable efficient sampling of plausible futures  RNN encoder-decoder neural architecture that allows for mapping trajectories represented by world coordinates in $\\mathbb{R}^2$ or $\\mathbb{R}^3$ to a high-dimensional distributed representation that can be efficiently combined with the Conditoinal VAE  An Inverse Optimal-Control (IOC) based Ranking and Refinement module that determines the most likely hypotheses, while incorporating scene context and interactions. The IOC module estimates a regression vector to refine each prediction sample. A convolutional neural network is used to carry out scene context fusion. This enables encoding features such as object velocities into the hypothesis scoring component. The model is trained end-to-end with a total loss consisting of multiple auxiliary losses (e.g., reconstruction error from the output trajectory of the decoder during training). The authors evaluate its performance on the KITTI and Stanford Drone tracking datasets. They show the performance with the metric “error in meters” for future time steps at intervals of 1, 2, 3, and 4 seconds. Overall, the proposed model seems quite complex, with many sub-components. However, the motivation behind each component is reasonable and the model is shown to perform well on the datasets. The separation of the system into a Sample Generation Module and the Ranking and Refinement Module is reminiscent of the actor-critic architecture in RL. The Ranking and Refinement Module uses unsupervised learning losses to learn to score/refine the “actor”, or Sample Generation Module.", "pdf_url": "https://arxiv.org/pdf/1704.04394v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/desire.json"}
{"id": "48411412", "bin": "400_500", "summary_sentences": ["What  They describe a variation of convolutions that have a differently structured receptive field.", "They argue that their variation works better for dense prediction, i.e. for predicting values for every pixel in an image (e.g.", "coloring, segmentation, upscaling).", "How  One can image the input into a convolutional layer as a 3d-grid.", "Each cell is a \"pixel\" generated by a filter.", "Normal convolutions compute their output per cell as a weighted sum of the input cells in a dense area.", "I.e. all input cells are right next to each other.", "In dilated convolutions, the cells are not right next to each other.", "E.g. 2-dilated convolutions skip 1 cell between each input cell, 3-dilated convolutions skip 2 cells etc.", "(Similar to striding.)", "Normal convolutions are simply 1-dilated convolutions (skipping 0 cells).", "One can use a 1-dilated convolution and then a 2-dilated convolution.", "The receptive field of the second convolution will then be 7x7 instead of the usual 5x5 due to the spacing.", "Increasing the dilation factor by 2 per layer (1, 2, 4, 8, ...) leads to an exponential increase in the receptive field size, while every cell in the receptive field will still be part in the computation of at least one convolution.", "They had problems with badly performing networks, which they fixed using an identity initialization for the weights.", "(Sounds like just using resdiual connections would have been easier.)", "Receptive fields of a 1-dilated convolution (1st image), followed by a 2-dilated conv.", "(2nd image), followed by a 4-dilated conv.", "(3rd image).", "The blue color indicates the receptive field size (notice the exponential increase in size).", "Stronger blue colors mean that the value has been used in more different convolutions.", "Results  They took a VGG net, removed the pooling layers and replaced the convolutions with dilated ones (weights can be kept).", "They then used the network to segment images.", "Their results were significantly better than previous methods.", "They also added another network with more dilated convolutions in front of the VGG one, again improving the results.", "Their performance on a segmentation task compared to two competing methods.", "They only used VGG16 without pooling layers and with convolutions replaced by dilated convolutions."], "summary_text": "What  They describe a variation of convolutions that have a differently structured receptive field. They argue that their variation works better for dense prediction, i.e. for predicting values for every pixel in an image (e.g. coloring, segmentation, upscaling). How  One can image the input into a convolutional layer as a 3d-grid. Each cell is a \"pixel\" generated by a filter. Normal convolutions compute their output per cell as a weighted sum of the input cells in a dense area. I.e. all input cells are right next to each other. In dilated convolutions, the cells are not right next to each other. E.g. 2-dilated convolutions skip 1 cell between each input cell, 3-dilated convolutions skip 2 cells etc. (Similar to striding.) Normal convolutions are simply 1-dilated convolutions (skipping 0 cells). One can use a 1-dilated convolution and then a 2-dilated convolution. The receptive field of the second convolution will then be 7x7 instead of the usual 5x5 due to the spacing. Increasing the dilation factor by 2 per layer (1, 2, 4, 8, ...) leads to an exponential increase in the receptive field size, while every cell in the receptive field will still be part in the computation of at least one convolution. They had problems with badly performing networks, which they fixed using an identity initialization for the weights. (Sounds like just using resdiual connections would have been easier.) Receptive fields of a 1-dilated convolution (1st image), followed by a 2-dilated conv. (2nd image), followed by a 4-dilated conv. (3rd image). The blue color indicates the receptive field size (notice the exponential increase in size). Stronger blue colors mean that the value has been used in more different convolutions. Results  They took a VGG net, removed the pooling layers and replaced the convolutions with dilated ones (weights can be kept). They then used the network to segment images. Their results were significantly better than previous methods. They also added another network with more dilated convolutions in front of the VGG one, again improving the results. Their performance on a segmentation task compared to two competing methods. They only used VGG16 without pooling layers and with convolutions replaced by dilated convolutions.", "pdf_url": "https://arxiv.org/pdf/1511.07122", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/multi-scale_context_aggregation_by_dilated_convolutions.json"}
{"id": "63755811", "bin": "400_500", "summary_sentences": ["This paper looks at tracking severely occluded objects in long video sequences.", "I like this passage:  Although some recent work adopted deep neural networks to extract contexts for object detection and tracking, these data-driven feedforward methods have well-known problems: i) They are black-box models that cannot be explained and only applicable with supervised training by fitting the typical context of the object, thus difficult to generalize to new tasks.", "ii) Lacking explicit representation to handle occlusions, low resolution, and lighting variations—there are millions of ways to occlude an object in a given image  (Wang et al. 2017), making it impossible to have enough data for training and testing such black box models.", "In this paper, we go beyond passive recognition by reasoning about time-varying containment relations.", "They look at containment relations induced by human activity.", "A containment relation occurs when an object contains or holds another object, obscuring it from view.", "Contained objects have the same trajectories as the container.", "They use an idea that I have thought about as well and think is powerful; rather than only relying on detections for next state hypotheses, they suggest alternative hypotheses based on occlusion reasoning.", "The other hypotheses come from containment relations and blocked relations; if an object is contained, it inherits the track state of its container.", "If an object is blocked, its track state is considered stationary (not always true!).", "This tracking scenario is unique because they only consider human action as the source of state change (i.e., this approach doesn’t apply to general ped or vehicle tracking).", "They use state-of-the-art detection and activity recognition to carry out object tracking.", "They use the network flow approach for occlusion-aware data association in a sliding window.", "I think their algorithm has to decide between containment and blocked occlusion events via the activity recognition.", "They note that their approach is limited by how good object detection is.", "I think an important direction to consider is when external forces other than people can act on the object.", "This requires a lot more complex modeling of what an occluded object might be doing.", "Also, we need better object detection!", "Interesting related works  In Zhang, Li, and Nevatia 2008 , they use an Explicit Occlusion Model (EOM) in the network flow data association model.", "Good potential baseline."], "summary_text": "This paper looks at tracking severely occluded objects in long video sequences. I like this passage:  Although some recent work adopted deep neural networks to extract contexts for object detection and tracking, these data-driven feedforward methods have well-known problems: i) They are black-box models that cannot be explained and only applicable with supervised training by fitting the typical context of the object, thus difficult to generalize to new tasks. ii) Lacking explicit representation to handle occlusions, low resolution, and lighting variations—there are millions of ways to occlude an object in a given image  (Wang et al. 2017), making it impossible to have enough data for training and testing such black box models. In this paper, we go beyond passive recognition by reasoning about time-varying containment relations. They look at containment relations induced by human activity. A containment relation occurs when an object contains or holds another object, obscuring it from view. Contained objects have the same trajectories as the container. They use an idea that I have thought about as well and think is powerful; rather than only relying on detections for next state hypotheses, they suggest alternative hypotheses based on occlusion reasoning. The other hypotheses come from containment relations and blocked relations; if an object is contained, it inherits the track state of its container. If an object is blocked, its track state is considered stationary (not always true!). This tracking scenario is unique because they only consider human action as the source of state change (i.e., this approach doesn’t apply to general ped or vehicle tracking). They use state-of-the-art detection and activity recognition to carry out object tracking. They use the network flow approach for occlusion-aware data association in a sliding window. I think their algorithm has to decide between containment and blocked occlusion events via the activity recognition. They note that their approach is limited by how good object detection is. I think an important direction to consider is when external forces other than people can act on the object. This requires a lot more complex modeling of what an occluded object might be doing. Also, we need better object detection! Interesting related works  In Zhang, Li, and Nevatia 2008 , they use an Explicit Occlusion Model (EOM) in the network flow data association model. Good potential baseline.", "pdf_url": "https://pdfs.semanticscholar.org/4170/0dc1e60f5c8eaef409ef014f37c8b9e1b8cd.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/tracking-occluded-objects-by-reasoning-about-containment.json"}
{"id": "87068449", "bin": "400_500", "summary_sentences": ["In the future, AI and people will work together, and hence we must concern ourselves with ensuring that the AI will have interests aligned with our own.", "The authors suggest that it is in our best interests to find a solution to the “value-alignment problem”.", "As recently pointed out by Ian Goodfellow, this may not always be a good idea .", "Cooperative Inverse Reinforcement Learning (CIRL) is a formulation of a cooperative, partial information game between a human and a robot.", "Both share a reward  function, but the robot does not initially know what it is.", "One of the key departures from classical Inverse Reinforcement Learning is that the “teacher”, which in this case is the human, is not assumed to act optimally.", "Rather, it is shown that sub-optimal actions on the part of the human can result in the robot learning a better reward function.", "The structure of the CIRL formulation is such that it should encourage the  human to not attempt to teach by demonstration in a way that greedily maximizes immediate reward.", "Rather, the human learns how to “best respond” to the robot.", "Further Notes  CIRL can be formulated as a dec-POMDP, and reduced to a single-agent POMDP.", "The authors solved a 2D navigation task with CIRL to demonstrate the inferiority of having the human follow a “demonstration-by-expert” policy as opposed to a “best-response” policy.", "In the experiments, the authors used regret as a performance measure for learning the reward function with respect to a fully-observed setting where the robot knows the ground truth of the hidden reward function.", "Another performance measure used is the KL-divergence between the max-entropy trajectory distributions induced by the estimate of the reward parameters and the ground truth parameters.", "Finally, the L2-norm is used as a measure between the vector of rewards defined by the estimate of the reward parameters and the ground truth parameters.", "Comments  I believe that we’ll see some work coming out of OpenAI following this line of research in the near future (where you have AI and humans learning to colloborate safely).", "It is also interesting to note that the experiments conducted in this paper are on a grid-world and far from being applied in a real world setting (until we get much better POMDP solvers, that is)."], "summary_text": "In the future, AI and people will work together, and hence we must concern ourselves with ensuring that the AI will have interests aligned with our own. The authors suggest that it is in our best interests to find a solution to the “value-alignment problem”. As recently pointed out by Ian Goodfellow, this may not always be a good idea . Cooperative Inverse Reinforcement Learning (CIRL) is a formulation of a cooperative, partial information game between a human and a robot. Both share a reward  function, but the robot does not initially know what it is. One of the key departures from classical Inverse Reinforcement Learning is that the “teacher”, which in this case is the human, is not assumed to act optimally. Rather, it is shown that sub-optimal actions on the part of the human can result in the robot learning a better reward function. The structure of the CIRL formulation is such that it should encourage the  human to not attempt to teach by demonstration in a way that greedily maximizes immediate reward. Rather, the human learns how to “best respond” to the robot. Further Notes  CIRL can be formulated as a dec-POMDP, and reduced to a single-agent POMDP. The authors solved a 2D navigation task with CIRL to demonstrate the inferiority of having the human follow a “demonstration-by-expert” policy as opposed to a “best-response” policy. In the experiments, the authors used regret as a performance measure for learning the reward function with respect to a fully-observed setting where the robot knows the ground truth of the hidden reward function. Another performance measure used is the KL-divergence between the max-entropy trajectory distributions induced by the estimate of the reward parameters and the ground truth parameters. Finally, the L2-norm is used as a measure between the vector of rewards defined by the estimate of the reward parameters and the ground truth parameters. Comments  I believe that we’ll see some work coming out of OpenAI following this line of research in the near future (where you have AI and humans learning to colloborate safely). It is also interesting to note that the experiments conducted in this paper are on a grid-world and far from being applied in a real world setting (until we get much better POMDP solvers, that is).", "pdf_url": "http://arxiv.org/pdf/1606.03137v2.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/coop-inverse-rl.json"}
{"id": "53809041", "bin": "400_500", "summary_sentences": ["This paper is concerned with the problem of predicting a sequence at the output, e.g. using an RNN.", "It aims at addressing the issue it refers to as exposure bias, which here refers to the fact that while at training time the RNN producing the output sequence is being fed the ground truth previous tokens (words) when producing the next token (something sometimes referred to as teacher forcing, which really is just maximum likelihood), at test time this RNN makes predictions using recursive generation, i.e. it is instead recursively fed by its own predictions (which might be erroneous).", "Moreover, it also proposes a training procedure that can take into account a rich performance measure that can't easily be optimized directly, such as the BLEU score for text outputs.", "The key observation is that the REINFORCE algorithm could be used to optimize the expectation of such arbitrarily complicated performance measures, for outputs produced by (stochastic) recursive generation.", "However, REINFORCE is a notoriously unstable training algorithm, which can often work terribly (in fact, the authors mention that they have tried using REINFORCE only, without success).", "Thus, they instead propose to gradually go from training according to maximum likelihood / teacher forcing to training using the REINFORCE algorithm on the expected performance measure.", "The proposed procedure, dubbed MIXER (Mixed Incremental Cross-Entropy Reinforce), goes as follows: 1.", "Train model to optimize the likelihood of the target sequence, i.e. minimize the per time-step cross-entropy loss.", "2.", "Then, for a target sequence of size T, optimize the cross-entropy for the T-Δ first time steps of the sequence and use Reinforce to get a gradient on the expected loss (e.g. negative BLEU) for the recursive generation of the rest of the Δ time steps.", "3.", "Increase Δ and go back to 2., until Δ is equal to T.  Experiments on 3 text benchmarks (summarization, machine translation and image captioning) show that this approach yields models that produces much better outputs when not using beam search (i.e. using greedy recursive generation) to generate an output sequence, compared to other alternatives such as regular maximum likelihood and Data as Demonstrator (DaD).", "DaD is similar to the scheduled sampling method of Bengio et al. (see my note:  [ref] ), in that at training time, some of the previous tokens fed to the model are predicted tokens instead of ground truths.", "When using beam search, MIXER is only outperformed by DaD on the machine translation task."], "summary_text": "This paper is concerned with the problem of predicting a sequence at the output, e.g. using an RNN. It aims at addressing the issue it refers to as exposure bias, which here refers to the fact that while at training time the RNN producing the output sequence is being fed the ground truth previous tokens (words) when producing the next token (something sometimes referred to as teacher forcing, which really is just maximum likelihood), at test time this RNN makes predictions using recursive generation, i.e. it is instead recursively fed by its own predictions (which might be erroneous). Moreover, it also proposes a training procedure that can take into account a rich performance measure that can't easily be optimized directly, such as the BLEU score for text outputs. The key observation is that the REINFORCE algorithm could be used to optimize the expectation of such arbitrarily complicated performance measures, for outputs produced by (stochastic) recursive generation. However, REINFORCE is a notoriously unstable training algorithm, which can often work terribly (in fact, the authors mention that they have tried using REINFORCE only, without success). Thus, they instead propose to gradually go from training according to maximum likelihood / teacher forcing to training using the REINFORCE algorithm on the expected performance measure. The proposed procedure, dubbed MIXER (Mixed Incremental Cross-Entropy Reinforce), goes as follows: 1. Train model to optimize the likelihood of the target sequence, i.e. minimize the per time-step cross-entropy loss. 2. Then, for a target sequence of size T, optimize the cross-entropy for the T-Δ first time steps of the sequence and use Reinforce to get a gradient on the expected loss (e.g. negative BLEU) for the recursive generation of the rest of the Δ time steps. 3. Increase Δ and go back to 2., until Δ is equal to T.  Experiments on 3 text benchmarks (summarization, machine translation and image captioning) show that this approach yields models that produces much better outputs when not using beam search (i.e. using greedy recursive generation) to generate an output sequence, compared to other alternatives such as regular maximum likelihood and Data as Demonstrator (DaD). DaD is similar to the scheduled sampling method of Bengio et al. (see my note:  [ref] ), in that at training time, some of the previous tokens fed to the model are predicted tokens instead of ground truths. When using beam search, MIXER is only outperformed by DaD on the machine translation task.", "pdf_url": "http://arxiv.org/pdf/1511.06732", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/ranzatocaz15.json"}
{"id": "36278339", "bin": "400_500", "summary_sentences": ["The paper introduces Relation Network (RN) that refines the encoding representation of the given source document (or sentence).", "This refined source representation can then be used in Neural Machine Translation (NMT) systems to counter the problem of RNNs forgetting old information.", "Limitations of existing NMT models  The RNN encoder-decoder architecture is the standard choice for NMT systems.", "But the RNNs are prone to forgetting old information.", "In NMT models, the attention is modeled in the unit of words while the use of phrases (instead of words) would be a better choice.", "While NMT systems might be able to capture certain relationships between words, they are not explicitly designed to capture such information.", "Contributions of the paper  Learn the relationship between the source words using the context (neighboring words).", "Relation Networks (RNs) build pairwise relations between source words using the representations generated by the RNNs.", "The RN would sit between the encoder and the attention layer of the encoder-decoder framework thereby keeping the main architecture unaffected.", "Relation Network  Neural network which is desgined for relational reasoning.", "Given a set of inputs * O = o1, …, on *, RN is formed as a composition of inputs:    RN(O) = f(sum(g(oi, oj))), f and g are functions used to learn the relations (feed forward networks)  g learns how the objects are related hence the name “relation”.", "Components:  CNN Layer  Extract information from the words surrounding the given word (context).", "The final output of this layer is the sequence of vectors for different kernel width.", "Graph Propagation (GP) Layer  Connect all the words with each other in the form of a graph.", "Each output vector from the CNN corresponds to a node in the graph and there is an edge between all possible pair of nodes.", "The information flows between the nodes of the graph in a message passing sort of fashion (graph propagation) to obtain a new set of vectors for each node.", "Multi-Layer Perceptron (MLP) Layer  The representation from the GP Layer is fed to the MLP layer.", "The layer uses residual connections from previous layers in form of concatenation.", "Datasets  IWSLT Data - 44K sentences from tourism and travel domain.", "NIST Data - 1M Chinese-English parallel sentence pairs.", "Models  MOSES - Open source translation system -  [url]"], "summary_text": "The paper introduces Relation Network (RN) that refines the encoding representation of the given source document (or sentence). This refined source representation can then be used in Neural Machine Translation (NMT) systems to counter the problem of RNNs forgetting old information. Limitations of existing NMT models  The RNN encoder-decoder architecture is the standard choice for NMT systems. But the RNNs are prone to forgetting old information. In NMT models, the attention is modeled in the unit of words while the use of phrases (instead of words) would be a better choice. While NMT systems might be able to capture certain relationships between words, they are not explicitly designed to capture such information. Contributions of the paper  Learn the relationship between the source words using the context (neighboring words). Relation Networks (RNs) build pairwise relations between source words using the representations generated by the RNNs. The RN would sit between the encoder and the attention layer of the encoder-decoder framework thereby keeping the main architecture unaffected. Relation Network  Neural network which is desgined for relational reasoning. Given a set of inputs * O = o1, …, on *, RN is formed as a composition of inputs:    RN(O) = f(sum(g(oi, oj))), f and g are functions used to learn the relations (feed forward networks)  g learns how the objects are related hence the name “relation”. Components:  CNN Layer  Extract information from the words surrounding the given word (context). The final output of this layer is the sequence of vectors for different kernel width. Graph Propagation (GP) Layer  Connect all the words with each other in the form of a graph. Each output vector from the CNN corresponds to a node in the graph and there is an edge between all possible pair of nodes. The information flows between the nodes of the graph in a message passing sort of fashion (graph propagation) to obtain a new set of vectors for each node. Multi-Layer Perceptron (MLP) Layer  The representation from the GP Layer is fed to the MLP layer. The layer uses residual connections from previous layers in form of concatenation. Datasets  IWSLT Data - 44K sentences from tourism and travel domain. NIST Data - 1M Chinese-English parallel sentence pairs. Models  MOSES - Open source translation system -  [url]", "pdf_url": "https://arxiv.org/pdf/1805.11154.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/refining-source-representations-with-relation-networks-for-neural-machine-translation.json"}
{"id": "65062778", "bin": "400_500", "summary_sentences": ["The paper presents a database of ranked English and Spanish paraphrases derived by:  Extracting lexical, phrasal, and syntactic paraphrases from large bilingual parallel corpora.", "Computing the similarity scores for the pair of paraphrases using Google ngrams and the Annotated Gigaword corpus.", "Extracting Paraphrase from Bilingual Text  The basic idea is that if two English strings e1 and e2 translate to the same foreign string f (also called pivot), they should have the same meaning.", "Informally speaking, the input to the system is translation triplets of the form < f, e, φ >, where  f is a foreign string  e is an english string  φ is a vector of feature functions  The system can pivot over f to create paraphrase triplets < e1, e2, φp > where φp is computed using translation feature vectors φ1 and φ2  For example, conditional paraphrase probability p(e2|e1) can be computed by marginalizing over all shared foreign language translations f:  p(e2|e1) = Sum over all f, p(e2|f)p(e1|f)  Scoring Paraphrases Using Monolingual Distributional Similarity  Measure similarity of phrases using Distributional similarity.", "Can be used to rerank the paraphrases obtained from bilingual text or to obtain the paraphrases which could not be obtained from bilingual text alone.", "To describe a given phrase e1, collect contextual features like:  n-gram based features for words (to the left and right of the given phrase)  Lexical, lemma-based, POS and named entity unigrams and bigrams  Dependency link features  Syntactic features  Aggregate all the features, over all the occurences of e, to obtain distributional signature se.", "Define similarity between 2 phrases e1 and e2 as :  *sim(e1, e2) = dot(se1, s2)/(|se1||se2|)  Paper mentions two instances:  English paraphrases - 169.6 Million paraphrases  Spanish paraphrases - 161.6 Million paraphrases  Analysis  The paper performed tests to analyse the precision-recall tradeoff for coverage of Propbank predictions and predicate-argument tuples.", "Human evaluation was performed over a sample of 1900 paraphrases to establish the correlation of PPDB scores with human judgement.", "Areas of Improvement  Segregation of data by domain or topic  Support for more languages  Improving paraphrasing scores by using additional sources of information and better handling of paraphrases ambiguity.", "This comment has been minimized.", "Sign in to view  Copy link  Quote reply  sahilbadyal commented  Sep 21, 2018  Could you please explain the ALIGNMENT column in PPDB2.0"], "summary_text": "The paper presents a database of ranked English and Spanish paraphrases derived by:  Extracting lexical, phrasal, and syntactic paraphrases from large bilingual parallel corpora. Computing the similarity scores for the pair of paraphrases using Google ngrams and the Annotated Gigaword corpus. Extracting Paraphrase from Bilingual Text  The basic idea is that if two English strings e1 and e2 translate to the same foreign string f (also called pivot), they should have the same meaning. Informally speaking, the input to the system is translation triplets of the form < f, e, φ >, where  f is a foreign string  e is an english string  φ is a vector of feature functions  The system can pivot over f to create paraphrase triplets < e1, e2, φp > where φp is computed using translation feature vectors φ1 and φ2  For example, conditional paraphrase probability p(e2|e1) can be computed by marginalizing over all shared foreign language translations f:  p(e2|e1) = Sum over all f, p(e2|f)p(e1|f)  Scoring Paraphrases Using Monolingual Distributional Similarity  Measure similarity of phrases using Distributional similarity. Can be used to rerank the paraphrases obtained from bilingual text or to obtain the paraphrases which could not be obtained from bilingual text alone. To describe a given phrase e1, collect contextual features like:  n-gram based features for words (to the left and right of the given phrase)  Lexical, lemma-based, POS and named entity unigrams and bigrams  Dependency link features  Syntactic features  Aggregate all the features, over all the occurences of e, to obtain distributional signature se. Define similarity between 2 phrases e1 and e2 as :  *sim(e1, e2) = dot(se1, s2)/(|se1||se2|)  Paper mentions two instances:  English paraphrases - 169.6 Million paraphrases  Spanish paraphrases - 161.6 Million paraphrases  Analysis  The paper performed tests to analyse the precision-recall tradeoff for coverage of Propbank predictions and predicate-argument tuples. Human evaluation was performed over a sample of 1900 paraphrases to establish the correlation of PPDB scores with human judgement. Areas of Improvement  Segregation of data by domain or topic  Support for more languages  Improving paraphrasing scores by using additional sources of information and better handling of paraphrases ambiguity. This comment has been minimized. Sign in to view  Copy link  Quote reply  sahilbadyal commented  Sep 21, 2018  Could you please explain the ALIGNMENT column in PPDB2.0", "pdf_url": "http://www.cis.upenn.edu/~ccb/ppdb/pdf/ppdb-naacl-2013.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/1f387f084355dfafdf7550b1899af6.json"}
{"id": "87558652", "bin": "400_500", "summary_sentences": ["What  They suggest a method to estimate the number of visible people in images of crowds.", "Their method is based on a combination of CNNs and MRFs.", "How  They split each image of crowds into overlapping square patches.", "Each patch is resized to 224x224 and fed through ResNet-152.", "(Sounds like they don't fine-tune that model.)", "They extract the features from the last layer fc1000.", "(A bit weird considering those are the class probabilities.)", "They apply a few fully connected layers to that result in order to arrive at one value, which regresses the number of people visible in that patch.", "Those last fully connected layers are trained via a simple mean squared (or also absolute) error.", "The ground truth labels are computed from the total count of visible people in the image (sum of patch counts equals total number of people).", "They argue that count values should be similar between adjacent patches and smoothen them using a MRF:  p is a patch (same for q) and P are all patches  c_p/c_q is the count of patch p/q  D_p(c_p) is the cost of assigning count c_p to patch p, usually given by lambda * (ground_truth - c_p)^2, where lambda is a trained(?)", "weight (per patch?)", "V(c_p - c_q) is the smoothness cost, usually given by (c_p - c_q)^2  Training goal of the MRF formulation is to minimize the energy E(c), which is achieved by mostly assigning ground truth counts (D(.", ")), but also keeping the counts smooth to the neighbors (V(.)).", "The energy function is minimized using belief propagation.", "(But doesn't that have to be reexecuted for every single image?", "Only lambda is apparently trained, which is image-specific.", "That would probably be slow and requires the usage of ground truth annotation for all images, including validation/test?!)", "Visualization of the steps:  Results  UCF dataset  Contains 50 grayscale images with 63705 annotated people (each head is annotated).", "They achieve the best score among all competitors (about 10% lower in MAE compared to best alternative).", "Shanghaitech dataset  Contains 1198 images with 330165 annotated people (also head annotations).", "They achieve the best score among all competitors (about 30% lower in MAE compared to best alternative).", "Visualization of smoothing from MRF:"], "summary_text": "What  They suggest a method to estimate the number of visible people in images of crowds. Their method is based on a combination of CNNs and MRFs. How  They split each image of crowds into overlapping square patches. Each patch is resized to 224x224 and fed through ResNet-152. (Sounds like they don't fine-tune that model.) They extract the features from the last layer fc1000. (A bit weird considering those are the class probabilities.) They apply a few fully connected layers to that result in order to arrive at one value, which regresses the number of people visible in that patch. Those last fully connected layers are trained via a simple mean squared (or also absolute) error. The ground truth labels are computed from the total count of visible people in the image (sum of patch counts equals total number of people). They argue that count values should be similar between adjacent patches and smoothen them using a MRF:  p is a patch (same for q) and P are all patches  c_p/c_q is the count of patch p/q  D_p(c_p) is the cost of assigning count c_p to patch p, usually given by lambda * (ground_truth - c_p)^2, where lambda is a trained(?) weight (per patch?) V(c_p - c_q) is the smoothness cost, usually given by (c_p - c_q)^2  Training goal of the MRF formulation is to minimize the energy E(c), which is achieved by mostly assigning ground truth counts (D(. )), but also keeping the counts smooth to the neighbors (V(.)). The energy function is minimized using belief propagation. (But doesn't that have to be reexecuted for every single image? Only lambda is apparently trained, which is image-specific. That would probably be slow and requires the usage of ground truth annotation for all images, including validation/test?!) Visualization of the steps:  Results  UCF dataset  Contains 50 grayscale images with 63705 annotated people (each head is annotated). They achieve the best score among all competitors (about 10% lower in MAE compared to best alternative). Shanghaitech dataset  Contains 1198 images with 330165 annotated people (also head annotations). They achieve the best score among all competitors (about 30% lower in MAE compared to best alternative). Visualization of smoothing from MRF:", "pdf_url": "https://arxiv.org/pdf/1706.03686", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/image_crowd_counting_using_cnn_and_mrf.json"}
{"id": "34187605", "bin": "400_500", "summary_sentences": ["This paper presents a method for \"learning the learning rate\" of a stochastic gradient descent method, in the context of online learning.", "Indeed, variations on the chosen learning rate or learning rate schedule can have a large impact in observed performance of stochastic gradient descent.", "Moreover, in the context of online learning, where we are interested in achieving high performance not only at convergence but every step of the way, the \"choosing the learning rate\" problem is even more crucial.", "The authors present a method which attempts to train the learning rate itself by gradient descent.", "This is achieved by \"unrolling\" the parameter updates of our model across the time steps of online learning, which exposes the interaction between the learning rate and the sum of losses of the model across these time steps.", "The authors then propose a way to approximate the gradient of the sum of losses with respect to the learning rate, so that it can be used to perform gradient updates on the learning rate itself.", "The gradient on the learning rate has to be approximated, for essentially the same reason that gradients to train a recurrent neural network online must be approximated (see also my notes on another good paper by Yann Ollivier here:  [ref] ).", "Another approximation is introduced to avoid having to compute an Hessian matrix.", "Nevertheless, results suggest that the proposed approximation works well and can improve over a fixed learning with a reasonable rate decay schedule  #### My two cents  I think the authors are right on the money as to the challenges posed by online learning.", "I think these challenges are likely to be greater in the context of training neural networks online, for which little satisfactory solutions exist right now.", "So this is a direction of research I'm particularly excited about.", "At this points, the experiments consider fairly simple learning scenarios, but I don't see any obstacle in applying the same method to neural networks.", "One interesting observation from the results is that results are fairly robust to variations of \"the learning rate of the learning rate\", compared to varying and fixing the learning rate itself.", "Finally, I haven't had time to entirely digest one of their theoretical result, suggesting that their approximation actually corresponds to an exact gradient taken \"alongside the effective trajectory\" of gradient descent.", "However, that result seems quite interesting and would deserve more attention."], "summary_text": "This paper presents a method for \"learning the learning rate\" of a stochastic gradient descent method, in the context of online learning. Indeed, variations on the chosen learning rate or learning rate schedule can have a large impact in observed performance of stochastic gradient descent. Moreover, in the context of online learning, where we are interested in achieving high performance not only at convergence but every step of the way, the \"choosing the learning rate\" problem is even more crucial. The authors present a method which attempts to train the learning rate itself by gradient descent. This is achieved by \"unrolling\" the parameter updates of our model across the time steps of online learning, which exposes the interaction between the learning rate and the sum of losses of the model across these time steps. The authors then propose a way to approximate the gradient of the sum of losses with respect to the learning rate, so that it can be used to perform gradient updates on the learning rate itself. The gradient on the learning rate has to be approximated, for essentially the same reason that gradients to train a recurrent neural network online must be approximated (see also my notes on another good paper by Yann Ollivier here:  [ref] ). Another approximation is introduced to avoid having to compute an Hessian matrix. Nevertheless, results suggest that the proposed approximation works well and can improve over a fixed learning with a reasonable rate decay schedule  #### My two cents  I think the authors are right on the money as to the challenges posed by online learning. I think these challenges are likely to be greater in the context of training neural networks online, for which little satisfactory solutions exist right now. So this is a direction of research I'm particularly excited about. At this points, the experiments consider fairly simple learning scenarios, but I don't see any obstacle in applying the same method to neural networks. One interesting observation from the results is that results are fairly robust to variations of \"the learning rate of the learning rate\", compared to varying and fixing the learning rate itself. Finally, I haven't had time to entirely digest one of their theoretical result, suggesting that their approximation actually corresponds to an exact gradient taken \"alongside the effective trajectory\" of gradient descent. However, that result seems quite interesting and would deserve more attention.", "pdf_url": "http://arxiv.org/pdf/1511.02540", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/masseo15.json"}
{"id": "30877354", "bin": "400_500", "summary_sentences": ["What  They suggest a framework that can be used to test CNNs (for self-driving cars).", "The framework makes use of synthetically generated examples.", "How  Their framework is split into three parts: Image generator, sampling methods and visualization tools.", "Image generator  The image generator uses real images from a dataset as input.", "It may change these images via basic transformations (e.g. brightness, saturation) or by drawing objects on them (e.g.", "cars).", "Though they seem to only apply contrast changes and project cars (at x/y-positions).", "The car projection also makes use of the road location in order to draw cars smaller if they are further away.", "Visualization:  Sampling methods  Each modification applied by the image generator can be expressed as one or more components of a vector (e.g. 2.0 for \"increase brightness to 2x the initial value\").", "Such a generated example/vector is then a point in a space.", "The space contains all possible images that can be generated.", "Generating images and testing a CNN on them is expensive.", "So ideally one wants have a good search method to find images that confuse the CNN.", "They generate candidate points in the example space using Halton and lattice-based sequences.", "(Uniform sampling of candidate points would be possible, but would not cover the space optimally.)", "Using that method, they can generate example inputs and the corresponding CNN scores.", "They then train a gaussian process model on these (image, CNN score) pairs.", "This allows them to more efficiently generate images that probably cause the CNN to fail.", "Visualization tools  They generate plots from example/input vectors and measured scores (confidence, IoU).", "Results  Example results for SqueezeDet and Yolo bounding box detectors, applied to a single test image with a car projected onto various locations:  The size of each point indicates the IoU, i.e. small points indicate that the CNN missed the car or predicted a bad bounding box).", "Blue points indicate that the network predicted a low confidence value.", "SqueezeDet seems to have problems with cars at the center and right of the roud.", "Yolo's confidence value seem to follow the distance of the car.", "Its achieved IoUs seem to be lower in general with a blind spot on the right of the road.", "Corresponding 3d plots of points, organized by x/y position of the car, IoU and confidence value (:"], "summary_text": "What  They suggest a framework that can be used to test CNNs (for self-driving cars). The framework makes use of synthetically generated examples. How  Their framework is split into three parts: Image generator, sampling methods and visualization tools. Image generator  The image generator uses real images from a dataset as input. It may change these images via basic transformations (e.g. brightness, saturation) or by drawing objects on them (e.g. cars). Though they seem to only apply contrast changes and project cars (at x/y-positions). The car projection also makes use of the road location in order to draw cars smaller if they are further away. Visualization:  Sampling methods  Each modification applied by the image generator can be expressed as one or more components of a vector (e.g. 2.0 for \"increase brightness to 2x the initial value\"). Such a generated example/vector is then a point in a space. The space contains all possible images that can be generated. Generating images and testing a CNN on them is expensive. So ideally one wants have a good search method to find images that confuse the CNN. They generate candidate points in the example space using Halton and lattice-based sequences. (Uniform sampling of candidate points would be possible, but would not cover the space optimally.) Using that method, they can generate example inputs and the corresponding CNN scores. They then train a gaussian process model on these (image, CNN score) pairs. This allows them to more efficiently generate images that probably cause the CNN to fail. Visualization tools  They generate plots from example/input vectors and measured scores (confidence, IoU). Results  Example results for SqueezeDet and Yolo bounding box detectors, applied to a single test image with a car projected onto various locations:  The size of each point indicates the IoU, i.e. small points indicate that the CNN missed the car or predicted a bad bounding box). Blue points indicate that the network predicted a low confidence value. SqueezeDet seems to have problems with cars at the center and right of the roud. Yolo's confidence value seem to follow the distance of the car. Its achieved IoUs seem to be lower in general with a blind spot on the right of the road. Corresponding 3d plots of points, organized by x/y position of the car, IoU and confidence value (:", "pdf_url": "https://arxiv.org/pdf/1708.03309", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/systematic_testing_of_cnns_for_autonomous_driving.json"}
{"id": "58874274", "bin": "400_500", "summary_sentences": ["The rise of the citizen developer: assessing the security impact of online app generators Oltrogge et al., IEEE Security & Privacy 2018  “Low code”, “no code”, “citizen developers”, call it what you will, there’s been a big rise in platforms that seek to make it easy to develop applications for non-export developers.", "Today’s paper choice studies the online application generator (OAG) market for Android applications.", "When what used to be a web site (with many successful web site  templating and building options around) is often in many cases now also or instead a mobile app, so it makes sense that the same kind of templating and building approach should exist there too.", "For a brief period at the end of last year, Apple flirted with banning such apps from their app store , before back-tracking just a couple of weeks after the initial announcement.", "After reading today’s paper I can’t help but feel that perhaps they were on to something.", "Not that templated apps are bad per se, but when the generated apps contain widespread vulnerabilities and privacy issues, then that is bad.", "With the increasing use of OAGs the duty of generating secure code shifts away from the app developer to the generator service.", "This leaves the question of whether OAGs can provide save and privacy-preserving default implementations of common tasks to generate more secure apps at an unprecedented scale.", "Being an optimist by nature, my hope was that such app generation services would improve the state of security, because spending time and effort getting it right once would pay back across all of the generated apps.", "In theory that could still happen, but in practice it seems the opposite is occurring.", "It doesn’t seem to make a lot of difference whether you use a free online app generator (what are their incentives, and where does their revenue come from?", "Always good questions to ask), or a paid service, the situation is not good.", "The re-configuration attacks that the authors discover are particularly devastating.", "Online app generation services and penetration  Online application generators enable app development using wizard-like, point-and-click web interfaces in which developers only need to add and suitably interconnect UI elements that represent application components… There is no need and typically no option to write custom code.", "The authors started by searching the web for advertised online app generation platform, resulting in the set of services shown in the table below (the Como the authors refer to is I believe now called ‘swiftic’ –  [url]"], "summary_text": "The rise of the citizen developer: assessing the security impact of online app generators Oltrogge et al., IEEE Security & Privacy 2018  “Low code”, “no code”, “citizen developers”, call it what you will, there’s been a big rise in platforms that seek to make it easy to develop applications for non-export developers. Today’s paper choice studies the online application generator (OAG) market for Android applications. When what used to be a web site (with many successful web site  templating and building options around) is often in many cases now also or instead a mobile app, so it makes sense that the same kind of templating and building approach should exist there too. For a brief period at the end of last year, Apple flirted with banning such apps from their app store , before back-tracking just a couple of weeks after the initial announcement. After reading today’s paper I can’t help but feel that perhaps they were on to something. Not that templated apps are bad per se, but when the generated apps contain widespread vulnerabilities and privacy issues, then that is bad. With the increasing use of OAGs the duty of generating secure code shifts away from the app developer to the generator service. This leaves the question of whether OAGs can provide save and privacy-preserving default implementations of common tasks to generate more secure apps at an unprecedented scale. Being an optimist by nature, my hope was that such app generation services would improve the state of security, because spending time and effort getting it right once would pay back across all of the generated apps. In theory that could still happen, but in practice it seems the opposite is occurring. It doesn’t seem to make a lot of difference whether you use a free online app generator (what are their incentives, and where does their revenue come from? Always good questions to ask), or a paid service, the situation is not good. The re-configuration attacks that the authors discover are particularly devastating. Online app generation services and penetration  Online application generators enable app development using wizard-like, point-and-click web interfaces in which developers only need to add and suitably interconnect UI elements that represent application components… There is no need and typically no option to write custom code. The authors started by searching the web for advertised online app generation platform, resulting in the set of services shown in the table below (the Como the authors refer to is I believe now called ‘swiftic’ –  [url]", "pdf_url": "https://saschafahl.de/papers/appgens2018.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/the-rise-of-the-citizen-developer-assessing-the-security-impact-of-online-app-generators.json"}
{"id": "18027931", "bin": "400_500", "summary_sentences": ["Tenenbaum, et al. 2000  Isomap, seemingly named for “Isometric mapping”, seeks to provide a solution to the problem of non-linear dimensionality reduction.", "The method is especially suitable for high-dimensional manifolds that exhibit non-Euclidean geometry, such that the Euclidean distance between data points returns distances that are not actually realistic for the underlying low-dimensional manifold.", "The intuition for this approach lies in the use of the all-pairs shortest path algorithm to improve upon Multi-dimensional scaling.", "Under general conditions on the density and curvature of the points, a geodesic distance can be estimated between far away points on the high-dimensional manifold via the all-pairs shortest path that converges to the true distance in the limit.", "Then, similar to MDS, Isomap attempts to find coordinate vectors for a low-dimensional space within which the distances between points are preserved as much as possible.", "This essentially results in the selection of the largest p eigenvectors of the matrix of estimated distances on the high-dimensional manifold (transformed to inner products).", "To make the algorithm work, the first step consists of clustering the data points either using k-NN or $\\epsilon$-balls.", "Edges are placed between all points clustered together, to form the graph upon which all-pairs shortest path is run.", "In this paper, the authors present examples of applying Isomap to a dataset of faces, MNIST, and the “swiss roll” dataset.", "Interestingly, they are able to map the faces dataset to a 3-D space, capturing left-right poses, up-down poses, and variations in ambient lighting.", "They show that PCA and MDS converge (the residual loss goes to 0) but they are unable to recover the true dimensionality of the low-dimensional manifold.", "This seems to be troublesome, because if one naively applies PCA to a dataset and the residual loss goes to 0, it appears then that the user of this algorithm will mistakenly believe they have recovered the true low-dimensional manifold.", "It would be interesting to then run a classifier on this low-dimensional representation produced by PCA, and then check the performance against the same classifier using the low-dimensional representation learned by Isomap.", "I imagine that the Isomap classifier will have slightly better performance."], "summary_text": "Tenenbaum, et al. 2000  Isomap, seemingly named for “Isometric mapping”, seeks to provide a solution to the problem of non-linear dimensionality reduction. The method is especially suitable for high-dimensional manifolds that exhibit non-Euclidean geometry, such that the Euclidean distance between data points returns distances that are not actually realistic for the underlying low-dimensional manifold. The intuition for this approach lies in the use of the all-pairs shortest path algorithm to improve upon Multi-dimensional scaling. Under general conditions on the density and curvature of the points, a geodesic distance can be estimated between far away points on the high-dimensional manifold via the all-pairs shortest path that converges to the true distance in the limit. Then, similar to MDS, Isomap attempts to find coordinate vectors for a low-dimensional space within which the distances between points are preserved as much as possible. This essentially results in the selection of the largest p eigenvectors of the matrix of estimated distances on the high-dimensional manifold (transformed to inner products). To make the algorithm work, the first step consists of clustering the data points either using k-NN or $\\epsilon$-balls. Edges are placed between all points clustered together, to form the graph upon which all-pairs shortest path is run. In this paper, the authors present examples of applying Isomap to a dataset of faces, MNIST, and the “swiss roll” dataset. Interestingly, they are able to map the faces dataset to a 3-D space, capturing left-right poses, up-down poses, and variations in ambient lighting. They show that PCA and MDS converge (the residual loss goes to 0) but they are unable to recover the true dimensionality of the low-dimensional manifold. This seems to be troublesome, because if one naively applies PCA to a dataset and the residual loss goes to 0, it appears then that the user of this algorithm will mistakenly believe they have recovered the true low-dimensional manifold. It would be interesting to then run a classifier on this low-dimensional representation produced by PCA, and then check the performance against the same classifier using the low-dimensional representation learned by Isomap. I imagine that the Isomap classifier will have slightly better performance.", "pdf_url": "https://web.mit.edu/cocosci/Papers/sci_reprint.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/isomap.json"}
{"id": "36779622", "bin": "400_500", "summary_sentences": ["The paper proposes an attention based mechanism to decompose the problem of Natural Language Inference (NLI) into parallelizable subproblems.", "Further, it uses much fewer parameters as compared to any other model while obtaining state of the art results.", "The motivation behind the paper is that the tasks like NLI do not require deep modelling of the sentence structure and comparison of local text substructures followed by aggregation can also work very well  Approach  Given two sentences a and b, the model has to predict whether they have an “entailment” relationship, “neutral” relationship or “contradiction” relationship.", "Embed  All the words are mapped to their corresponding word vector representation.", "In subsequent steps, “word” refers to the word vector representation of the actual word.", "Attend  For each word i in a and j in b, obtain unnormalized attention weights *e(i, j)=F(i)TF(j) where F is a feed-forward neural network.", "For i, compute a βi by performing softmax-like normalization of j using e(i, j) as the weight and normalizing for all words j in b.  βi captures the subphrase in b that is softly aligned to a.", "Similarly compute αj for j.", "Compare  Create two set of comparison vectors, one for a and another for b  For a, v1, i = G(concatenate(i, βi)).", "Similarly for b, v2, j = G(concatenate(j, αj))  G is another feed-forward neural network.", "Aggregate  Aggregate over the two set of comparison vectors to obtain v1 and v2.", "Feed the aggregated results through the final classifier layer.", "Multi-class cross-entropy loss function.", "The paper also explains how this representation can be augmented using intra-sentence attention to the model compositional relationship between words.", "Computational Complexity  Computationally, the proposed model is asymptotically as good as LSTM with attention.", "Assuming that dimensionality of word vectors > length of the sentence (reasonable for the given SNLI dataset), the model is asymptotically as good as regular LSTM.", "Further, the model has the advantage of being parallelizable.", "Experiment  On Stanford Natural Language Inference (SNLI) dataset, the proposed model achieves the state of the art results even when it uses an order of magnitude lesser parameters than the next best model.", "Adding intra-sentence attention further improve the test accuracy by 0.5 percent.", "Notes  A similar approach could be tried on paraphrase detection problem as even that problem should not require very deep sentence representation.", "Quora Duplicate Question Detection Challenege would have been an ideal dataset but it has a lot of out-of-vocabulary information related to named entities which need to be accounted for."], "summary_text": "The paper proposes an attention based mechanism to decompose the problem of Natural Language Inference (NLI) into parallelizable subproblems. Further, it uses much fewer parameters as compared to any other model while obtaining state of the art results. The motivation behind the paper is that the tasks like NLI do not require deep modelling of the sentence structure and comparison of local text substructures followed by aggregation can also work very well  Approach  Given two sentences a and b, the model has to predict whether they have an “entailment” relationship, “neutral” relationship or “contradiction” relationship. Embed  All the words are mapped to their corresponding word vector representation. In subsequent steps, “word” refers to the word vector representation of the actual word. Attend  For each word i in a and j in b, obtain unnormalized attention weights *e(i, j)=F(i)TF(j) where F is a feed-forward neural network. For i, compute a βi by performing softmax-like normalization of j using e(i, j) as the weight and normalizing for all words j in b.  βi captures the subphrase in b that is softly aligned to a. Similarly compute αj for j. Compare  Create two set of comparison vectors, one for a and another for b  For a, v1, i = G(concatenate(i, βi)). Similarly for b, v2, j = G(concatenate(j, αj))  G is another feed-forward neural network. Aggregate  Aggregate over the two set of comparison vectors to obtain v1 and v2. Feed the aggregated results through the final classifier layer. Multi-class cross-entropy loss function. The paper also explains how this representation can be augmented using intra-sentence attention to the model compositional relationship between words. Computational Complexity  Computationally, the proposed model is asymptotically as good as LSTM with attention. Assuming that dimensionality of word vectors > length of the sentence (reasonable for the given SNLI dataset), the model is asymptotically as good as regular LSTM. Further, the model has the advantage of being parallelizable. Experiment  On Stanford Natural Language Inference (SNLI) dataset, the proposed model achieves the state of the art results even when it uses an order of magnitude lesser parameters than the next best model. Adding intra-sentence attention further improve the test accuracy by 0.5 percent. Notes  A similar approach could be tried on paraphrase detection problem as even that problem should not require very deep sentence representation. Quora Duplicate Question Detection Challenege would have been an ideal dataset but it has a lot of out-of-vocabulary information related to named entities which need to be accounted for.", "pdf_url": "https://arxiv.org/pdf/1606.01933", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/a-decomposable-attention-model-for-natural-language-inference.json"}
{"id": "93980002", "bin": "400_500", "summary_sentences": ["This paper presents an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop policies.", "Only a finite set of a priori known policies are considered.", "Bayesian changepoint detection is used to estimate which policy a given vehicle was executing at each point in its history of actions, then infer the likelihood of each potential intention of the vehicle.", "A statistical test is proposed based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes.", "Evidence  Anomaly detection was explored by recording three trajectories corresponding to two bikes and a bus.", "The bikes crossed an intersection from a sidewalk, while the bus made a significantly wide turn.", "System was able to detect these trajectories as anomalous (Not within the set of known policies)  Evaluated in simulated driving environment  Notes  Bayesian Changepoint detection infers the points in the history of observations where the underlying policy that generated the observations changed.", "Then, the likelihood of all available policies for the target car given the distribution over the car’s potential policies at the current timestep can be computed (sounds like HMM).", "The CHAMP algorithm infers the maximum a posteriori set of times at which changepoints between policies have occurred, yielding a set of segments.", "Given a segment from time s to t and a policy pi, CHAMP approx the log of the policy-evidence for that segment via the (Bae)yesian information criterion (BIC)  Viterbi path is found for the most likely sequence of latent policies  For decision-making, a set of samples are drawn from the distribution over policies of other cars where each sample assigns a policy to each nearby vehicle, excluding the ego car.", "For each policy available to the ego car (not all policies are available in every scenario e.g. intersection handling policy is not applicable when driving on a highway), and for each sample s, the process is rolled out forward in time until the decision horizon.", "This yields a set of simulated trajectories.", "The reward is evaluated for each element of the set of simulated trajectories and the maximal policy for the ego vehicle is chosen.", "This repeats continuously in a receding horizon manner.", "Reward function  distance to the goal at the end of the evaluation horizon  minimum distance to obstacles to evaluate safety  lane choice bias to add a preference for the right lane  maximum yaw rate and longitudinal jerk to measure passenger comfort"], "summary_text": "This paper presents an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop policies. Only a finite set of a priori known policies are considered. Bayesian changepoint detection is used to estimate which policy a given vehicle was executing at each point in its history of actions, then infer the likelihood of each potential intention of the vehicle. A statistical test is proposed based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes. Evidence  Anomaly detection was explored by recording three trajectories corresponding to two bikes and a bus. The bikes crossed an intersection from a sidewalk, while the bus made a significantly wide turn. System was able to detect these trajectories as anomalous (Not within the set of known policies)  Evaluated in simulated driving environment  Notes  Bayesian Changepoint detection infers the points in the history of observations where the underlying policy that generated the observations changed. Then, the likelihood of all available policies for the target car given the distribution over the car’s potential policies at the current timestep can be computed (sounds like HMM). The CHAMP algorithm infers the maximum a posteriori set of times at which changepoints between policies have occurred, yielding a set of segments. Given a segment from time s to t and a policy pi, CHAMP approx the log of the policy-evidence for that segment via the (Bae)yesian information criterion (BIC)  Viterbi path is found for the most likely sequence of latent policies  For decision-making, a set of samples are drawn from the distribution over policies of other cars where each sample assigns a policy to each nearby vehicle, excluding the ego car. For each policy available to the ego car (not all policies are available in every scenario e.g. intersection handling policy is not applicable when driving on a highway), and for each sample s, the process is rolled out forward in time until the decision horizon. This yields a set of simulated trajectories. The reward is evaluated for each element of the set of simulated trajectories and the maximal policy for the ego vehicle is chosen. This repeats continuously in a receding horizon manner. Reward function  distance to the goal at the end of the evaluation horizon  minimum distance to obstacles to evaluate safety  lane choice bias to add a preference for the right lane  maximum yaw rate and longitudinal jerk to measure passenger comfort", "pdf_url": "http://www.roboticsproceedings.org/rss11/p43.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/multipolicy-decision-making.json"}
{"id": "7590139", "bin": "400_500", "summary_sentences": ["What  They combine several previous improvements for reinforcement learning to one algorithm.", "The combination beats previous methods by a good margin.", "They analyze which of the used improvements has most influence on the result.", "How  They use the following improvements:  Double Q-learning  Uses two networks during training.", "One predicts Q-values, the other is updated.", "Usually done by using a copy with freezed parameters.", "Prioritized Replay  Samples experiences from the replay memory based on the difference between predicted and real Q-values.", "Recent experiences also get higher priority.", "Dueling Networks  Splits the Q-value prediction into a value stream (mean value over all values) and an advantage stream (advantage of specific actions over others).", "Multi-Step Learning  Splits direct reward + Q(next state, next action) into direct reward + direct reward of next N actions + Q(next Nth state, next Nth action) (weighted with discrount factor).", "I.e. per training example, some experiences from the future are directly used to compute the rewards and only after some point the Q-function is used.", "Distributional RL  Seems to be nothing else but switching the regression (of Q-values) to classification in order to avoid standard mode problems with regression.", "The range of possible reward values is partitioned into N bins.", "Noisy Nets  Gets rid of the exploration factor in epsilon-greedy strategies.", "Instead it uses a noisy fully connected layer where the noise weights are learned by the network.", "As the network becomes more accurate at predicting good Q-values, it automatically decreases the noise.", "They combine all of the mentioned methods.", "They use a KL term for weighting in Prioritized Replay (to account for the Distributional RL).", "Training  They start training after 80k frames.", "They use Adam.", "When using epsilon-greedy instead of noise nets, they anneal epsilon to 0.01 at 250k frames.", "For multi-step learning they use n=3 future experiences.", "Results  They evaluate Rainbow 57 Atari games.", "Rainbow beats all other methods used on their own, both in learning speed and maximum skill level.", "It performs far better than the classic DQN approach.", "Average performance:  Ablation  Removing the priority replay, multi-step learning or distributional RL significantly worsens the performance.", "Removing noise nets also harms the performance, although a bit less.", "Removing double Q-learning or dueling networks seems to have no significiant effect.", "Visualization:  Learning curves by game:"], "summary_text": "What  They combine several previous improvements for reinforcement learning to one algorithm. The combination beats previous methods by a good margin. They analyze which of the used improvements has most influence on the result. How  They use the following improvements:  Double Q-learning  Uses two networks during training. One predicts Q-values, the other is updated. Usually done by using a copy with freezed parameters. Prioritized Replay  Samples experiences from the replay memory based on the difference between predicted and real Q-values. Recent experiences also get higher priority. Dueling Networks  Splits the Q-value prediction into a value stream (mean value over all values) and an advantage stream (advantage of specific actions over others). Multi-Step Learning  Splits direct reward + Q(next state, next action) into direct reward + direct reward of next N actions + Q(next Nth state, next Nth action) (weighted with discrount factor). I.e. per training example, some experiences from the future are directly used to compute the rewards and only after some point the Q-function is used. Distributional RL  Seems to be nothing else but switching the regression (of Q-values) to classification in order to avoid standard mode problems with regression. The range of possible reward values is partitioned into N bins. Noisy Nets  Gets rid of the exploration factor in epsilon-greedy strategies. Instead it uses a noisy fully connected layer where the noise weights are learned by the network. As the network becomes more accurate at predicting good Q-values, it automatically decreases the noise. They combine all of the mentioned methods. They use a KL term for weighting in Prioritized Replay (to account for the Distributional RL). Training  They start training after 80k frames. They use Adam. When using epsilon-greedy instead of noise nets, they anneal epsilon to 0.01 at 250k frames. For multi-step learning they use n=3 future experiences. Results  They evaluate Rainbow 57 Atari games. Rainbow beats all other methods used on their own, both in learning speed and maximum skill level. It performs far better than the classic DQN approach. Average performance:  Ablation  Removing the priority replay, multi-step learning or distributional RL significantly worsens the performance. Removing noise nets also harms the performance, although a bit less. Removing double Q-learning or dueling networks seems to have no significiant effect. Visualization:  Learning curves by game:", "pdf_url": "https://arxiv.org/pdf/1710.02298", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/rainbow.json"}
{"id": "47397989", "bin": "400_500", "summary_sentences": ["What  They present a method to detect hard edge cases in datasets of self-driving cars.", "These are cases that might lead to accidents.", "The method is based on running two models simultaneously and measuring their disagreement with each other.", "How  They use two different models:  Tesla-AD: Proprietary autopilot from Tesla.", "CNN: Their own CNN, trained on a Tesla dataset (images from car's perspective + steering & acceleration annotation; 420 hours of driving).", "The network's architecture is similar to the one in the NVIDIA paper.", "Their considered five different inputs for their own CNN:  M5: RGB images (same as in the NVIDIA paper)  M4: Images of edges in each color channel (i.e. three edge maps).", "(No detailed explanation on how the edges were detected.)", "M3: Grayscale images, from t-20, t-10 and t (where t is the current frame).", "M2: Grayscale difference images t - t-10 (i.e. difference between the current frame and the one 10 frames ago), t - t-5, t - t-1.", "M1: Grayscale difference images t-20 - t-30, t-10 - t-20, t - t-10.", "Visualization:  Training  They split the data into subgroups, each being defined up by the steering wheel angle.", "E.g. all example with an angle in the range [-10, 10] end in one group.", "They select equally from all groups and get 100k training and 50k validation images.", "This prevents the model from only training on examples showing straight driving.", "Disagreement  Their intention is to find hard example / edge cases.", "They do this by measuring the disagreement between the models (Tesla-AD and CNN).", "To do that, they first clip the predicted angles to the range [-10, 10].", "Then they normalize the result to the range [-1, 1].", "They sum the differences of these predictions over a one second window (30 examples).", "If the sum exceeds a threshold delta, they consider the models to be in disagreement and the example to be an edge case.", "They use delta=10.", "Results  Input M1 performed best, followed by M2, M3, M4 and M5 (in that order).", "(Measured via MAE of CNN predictions vs. ground truth annotations.)", "They can predict with 90% accuracy whether there will be a disengagement in the next 5 seconds (i.e. human driver took control in the dataset)."], "summary_text": "What  They present a method to detect hard edge cases in datasets of self-driving cars. These are cases that might lead to accidents. The method is based on running two models simultaneously and measuring their disagreement with each other. How  They use two different models:  Tesla-AD: Proprietary autopilot from Tesla. CNN: Their own CNN, trained on a Tesla dataset (images from car's perspective + steering & acceleration annotation; 420 hours of driving). The network's architecture is similar to the one in the NVIDIA paper. Their considered five different inputs for their own CNN:  M5: RGB images (same as in the NVIDIA paper)  M4: Images of edges in each color channel (i.e. three edge maps). (No detailed explanation on how the edges were detected.) M3: Grayscale images, from t-20, t-10 and t (where t is the current frame). M2: Grayscale difference images t - t-10 (i.e. difference between the current frame and the one 10 frames ago), t - t-5, t - t-1. M1: Grayscale difference images t-20 - t-30, t-10 - t-20, t - t-10. Visualization:  Training  They split the data into subgroups, each being defined up by the steering wheel angle. E.g. all example with an angle in the range [-10, 10] end in one group. They select equally from all groups and get 100k training and 50k validation images. This prevents the model from only training on examples showing straight driving. Disagreement  Their intention is to find hard example / edge cases. They do this by measuring the disagreement between the models (Tesla-AD and CNN). To do that, they first clip the predicted angles to the range [-10, 10]. Then they normalize the result to the range [-1, 1]. They sum the differences of these predictions over a one second window (30 examples). If the sum exceeds a threshold delta, they consider the models to be in disagreement and the example to be an edge case. They use delta=10. Results  Input M1 performed best, followed by M2, M3, M4 and M5 (in that order). (Measured via MAE of CNN predictions vs. ground truth annotations.) They can predict with 90% accuracy whether there will be a disengagement in the next 5 seconds (i.e. human driver took control in the dataset).", "pdf_url": "https://arxiv.org/pdf/1710.04459", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/arguing_machines.json"}
{"id": "88532041", "bin": "500_600", "summary_sentences": ["The paper presents persona-based models which add coherency to the response generated by sequence-to-senquence models like Neural Conversational Model .", "Persona  Defined as the character that an agent performs during conversational interactions.", "Combination of identity, language, behaviour and interaction style.", "May be adapted during the conversation itself.", "Neural Conversational Model fails to maintain a consistent persona throughout the conversation resulting in incoherent responses.", "Persona Based Models  Speaker Model  Models the personality of only the respondent.", "Represents each speaker as a vector (embedding) which encodes speaker-specific information (eg age, gender, etc)  The model learns to cluster users along these traits using the data alone.", "The vector v, corresponding to the given speaker, is used along with the context vector and the target representation generated in the previous timestamp to generate the current output representation.", "v is learnt along with other model parameters.", "Since the model learns the representation for each speaker, it can infer answers to certain questions about a given speaker even if the question has not been explicitly answered in the context of the given user (using the answers for similar users).", "Speaker-Addressee Model  Models the personality of both the speaker and addressee.", "Associate a representation Vi, j to capture the style of user i towards user j.  Vi, j = tanh(W1 · vi + W2 · v2)  Use Vi, j as we used v in the speaker model.", "Speaker-Addressee model can derive generalization capabilities from speaker embeddings.", "For example, even if two speakers have never engaged in a conversation, the conversations between speakers similar to the two given speakers can be to capture the associated representation.", "Decoding and Reranking  Generate N-best lists with beam size B = 200 and Max length = 20 (for generated candidates).", "At each time step, examine all B × B possible next-word candidates, and add all hypothesis ending with an EOS token to the N-best list.", "Rerank the generated N-best list using the scoring function from Li et al to avoid generic and commonplace responses.", "Datasets  Twitter Persona Dataset  Dataset of tweet sequences having frequent (at least 60) engagements from Twitter FireHose.", "Twitter Sordoni Dataset  Similar to Twitter Persona Dataset but with more references per message (up to 10).", "Television Transcripts  Since this dataset alone was very small to train an open domain dialogue model, a standard SEQ2SEQ model is first trained using OpenSubtitles dataset and further tuned to the transcripts dataset.", "Experiments  The proposed models yields performance improvements in both perplexity and BLEU scores over baseline SEQ2SEQ models.", "Similar gains observed in speaker consistency as measured by human judges.", "Open Questions  There is no evaluation of what does the speaker embeddings map to.", "The paper mentions that the embeddings should be able to capture the aspects like age, gender etc but these embeddings have not been explored in the paper."], "summary_text": "The paper presents persona-based models which add coherency to the response generated by sequence-to-senquence models like Neural Conversational Model . Persona  Defined as the character that an agent performs during conversational interactions. Combination of identity, language, behaviour and interaction style. May be adapted during the conversation itself. Neural Conversational Model fails to maintain a consistent persona throughout the conversation resulting in incoherent responses. Persona Based Models  Speaker Model  Models the personality of only the respondent. Represents each speaker as a vector (embedding) which encodes speaker-specific information (eg age, gender, etc)  The model learns to cluster users along these traits using the data alone. The vector v, corresponding to the given speaker, is used along with the context vector and the target representation generated in the previous timestamp to generate the current output representation. v is learnt along with other model parameters. Since the model learns the representation for each speaker, it can infer answers to certain questions about a given speaker even if the question has not been explicitly answered in the context of the given user (using the answers for similar users). Speaker-Addressee Model  Models the personality of both the speaker and addressee. Associate a representation Vi, j to capture the style of user i towards user j.  Vi, j = tanh(W1 · vi + W2 · v2)  Use Vi, j as we used v in the speaker model. Speaker-Addressee model can derive generalization capabilities from speaker embeddings. For example, even if two speakers have never engaged in a conversation, the conversations between speakers similar to the two given speakers can be to capture the associated representation. Decoding and Reranking  Generate N-best lists with beam size B = 200 and Max length = 20 (for generated candidates). At each time step, examine all B × B possible next-word candidates, and add all hypothesis ending with an EOS token to the N-best list. Rerank the generated N-best list using the scoring function from Li et al to avoid generic and commonplace responses. Datasets  Twitter Persona Dataset  Dataset of tweet sequences having frequent (at least 60) engagements from Twitter FireHose. Twitter Sordoni Dataset  Similar to Twitter Persona Dataset but with more references per message (up to 10). Television Transcripts  Since this dataset alone was very small to train an open domain dialogue model, a standard SEQ2SEQ model is first trained using OpenSubtitles dataset and further tuned to the transcripts dataset. Experiments  The proposed models yields performance improvements in both perplexity and BLEU scores over baseline SEQ2SEQ models. Similar gains observed in speaker consistency as measured by human judges. Open Questions  There is no evaluation of what does the speaker embeddings map to. The paper mentions that the embeddings should be able to capture the aspects like age, gender etc but these embeddings have not been explored in the paper.", "pdf_url": "https://arxiv.org/pdf/1603.06155", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/d464e7d0ea4c7c6ed5189ac4e44095.json"}
{"id": "39382584", "bin": "500_600", "summary_sentences": ["What  They compare the results of various models for pedestrian detection.", "The various models were developed over the course of ~10 years (2003-2014).", "They analyze which factors seemed to improve the results.", "They derive new models for pedestrian detection from that.", "Comparison: Datasets  Available datasets  INRIA: Small dataset.", "Diverse images.", "ETH: Video dataset.", "Stereo images.", "TUD-Brussels: Video dataset.", "Daimler: No color channel.", "Daimler stereo: Stereo images.", "Caltech-USA: Most often used.", "Large dataset.", "KITTI: Often used.", "Large dataset.", "Stereo images.", "All datasets except KITTI are part of the \"unified evaluation toolbox\" that allows authors to easily test on all of these datasets.", "The evaluation started initially with per-window (FPPW) and later changed to per-image (FPPI), because per-window skewed the results.", "Common evaluation metrics:  MR: Log-average miss-rate (lower is better)  AUC: Area under the precision-recall curve (higher is better)  Comparison: Methods  Families  They identified three families of methods: Deformable Parts Models, Deep Neural Networks, Decision Forests.", "Decision Forests was the most popular family.", "No specific family seemed to perform better than other families.", "There was no evidence that non-linearity in kernels was needed (given sophisticated features).", "Additional data  Adding (coarse) optical flow data to each image seemed to consistently improve results.", "There was some indication that adding stereo data to each image improves the results.", "Context  For sliding window detectors, adding context from around the window seemed to improve the results.", "E.g. context can indicate whether there were detections next to the window as people tend to walk in groups.", "Deformable parts  They saw no evidence that deformable part models outperformed other models.", "Multi-Scale models  Training separate models for each sliding window scale seemed to improve results slightly.", "Deep architectures  They saw no evidence that deep neural networks outperformed other models.", "(Note: Paper is from 2014, might have changed already?)", "Features  Best performance was usually achieved with simple HOG+LUV features, i.e. by converting each window into:  6 channels of gradient orientations  1 channel of gradient magnitude  3 channels of LUV color space  Some models use significantly more channels for gradient orientations, but there was no evidence that this was necessary to achieve good accuracy.", "However, using more different features (and more sophisticated ones) seemed to improve results.", "Their new model:  They choose Decisions Forests as their model framework (2048 level-2 trees, i.e. 3 thresholds per tree).", "They use features from the Integral Channels Features framework .", "(Basically just a mixture of common/simple features per window.)", "They add optical flow as a feature.", "They add context around the window as a feature.", "(A second detector that detects windows containing two persons.)", "Their model significantly improves upon the state of the art (from 34 to 22% MR on Caltech dataset).", "Overview of models developed over the years, starting with Viola Jones (VJ) and ending with their suggested model (Katamari-v1).", "(DF = Decision Forest, DPM = Deformable Parts Model, DN = Deep Neural Network; I = Inria Dataset, C = Caltech Dataset)"], "summary_text": "What  They compare the results of various models for pedestrian detection. The various models were developed over the course of ~10 years (2003-2014). They analyze which factors seemed to improve the results. They derive new models for pedestrian detection from that. Comparison: Datasets  Available datasets  INRIA: Small dataset. Diverse images. ETH: Video dataset. Stereo images. TUD-Brussels: Video dataset. Daimler: No color channel. Daimler stereo: Stereo images. Caltech-USA: Most often used. Large dataset. KITTI: Often used. Large dataset. Stereo images. All datasets except KITTI are part of the \"unified evaluation toolbox\" that allows authors to easily test on all of these datasets. The evaluation started initially with per-window (FPPW) and later changed to per-image (FPPI), because per-window skewed the results. Common evaluation metrics:  MR: Log-average miss-rate (lower is better)  AUC: Area under the precision-recall curve (higher is better)  Comparison: Methods  Families  They identified three families of methods: Deformable Parts Models, Deep Neural Networks, Decision Forests. Decision Forests was the most popular family. No specific family seemed to perform better than other families. There was no evidence that non-linearity in kernels was needed (given sophisticated features). Additional data  Adding (coarse) optical flow data to each image seemed to consistently improve results. There was some indication that adding stereo data to each image improves the results. Context  For sliding window detectors, adding context from around the window seemed to improve the results. E.g. context can indicate whether there were detections next to the window as people tend to walk in groups. Deformable parts  They saw no evidence that deformable part models outperformed other models. Multi-Scale models  Training separate models for each sliding window scale seemed to improve results slightly. Deep architectures  They saw no evidence that deep neural networks outperformed other models. (Note: Paper is from 2014, might have changed already?) Features  Best performance was usually achieved with simple HOG+LUV features, i.e. by converting each window into:  6 channels of gradient orientations  1 channel of gradient magnitude  3 channels of LUV color space  Some models use significantly more channels for gradient orientations, but there was no evidence that this was necessary to achieve good accuracy. However, using more different features (and more sophisticated ones) seemed to improve results. Their new model:  They choose Decisions Forests as their model framework (2048 level-2 trees, i.e. 3 thresholds per tree). They use features from the Integral Channels Features framework . (Basically just a mixture of common/simple features per window.) They add optical flow as a feature. They add context around the window as a feature. (A second detector that detects windows containing two persons.) Their model significantly improves upon the state of the art (from 34 to 22% MR on Caltech dataset). Overview of models developed over the years, starting with Viola Jones (VJ) and ending with their suggested model (Katamari-v1). (DF = Decision Forest, DPM = Deformable Parts Model, DN = Deep Neural Network; I = Inria Dataset, C = Caltech Dataset)", "pdf_url": "https://arxiv.org/pdf/1411.4304", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/ten_years_of_pedestrian_detection_what_have_we_learned.json"}
{"id": "33427271", "bin": "500_600", "summary_sentences": ["For many states, it is unnecessary to estimate the action value for each action.", "This is a problem with methods that attempt to favor exploration over exploitation too much, because often times there will be a large number of actions that have little to no value for a given state.", "The Q-Network in this novel architecture is decomposed into two separate streams; a value stream and an advantage stream.", "Feature learning is carried out by a number of convolutional and pooling layers.", "The activations of the last of these layers are sent to both separate streams.", "Each stream contains a number of fully-connected layers.", "The final layer combines the output of the two streams, and the output of the network is a set of Q values, one for each action.", "The aggregator for the two outputs of the advantage and value streams is:  $\\beta$ refers to the parameters specific to the value network, and the alpha refers to the parameters specific to the advantage network.", "The advantage of the dueling network over standard Q-Networks is especially prominent when the number of actions is large.", "For standard Q-Networks, when the variation between actions is small, the Q-Network effectively has to learn the same value for all actions while each update only modifies the Q value of one action.", "Evidence  The dueling network architecture outperformed the Double-DQN results in 50 out of 57 learned Atari games.", "Strengths  The paper does a good job of making its main contribution (a novel neural network architecture) clear at the beginning.", "The experimental algorithm for Dueling Networks employs other state-of-the-art advances in DRL (such as Double-DQN) which helps show the correlation between research being carried on in this field.", "Interesting related works  Increasing the action gap (Bellemare et al., 2016)  Notes  The value function V measures the importance of being in a particular state s. The Q-function measures the importance about the value of choosing each possible action when in this state.", "The advantage function, $A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)$, subtracts the value of the state from the Q-function to obtain a relative measure of the importance of each action  A deep Q-network is a non-linear function approximator for the Q function having the form $Q(s,a;\\theta)$ with parameters theta  We optimize the following sequence of loss functions:  with the target of the loss function, $y_i^{DQN}$, given by the reward signal plus the discounted maximal Q value provided by the target Q-Network  A target Q-Network that uses parameter freezing for a certain number of iterations is used to stabilize the algorithm  The specific gradient is  Experience replay is used; use prioritized experience replay instead!", "(Don’t just sample uniformly from memory)  To avoid over-optimistic value estimates (van Hasselt, 2010), use Double Q-Learning.", "Originally, the max operator uses the same values to both select and evaluate an action.", "Instead, use the following target:"], "summary_text": "For many states, it is unnecessary to estimate the action value for each action. This is a problem with methods that attempt to favor exploration over exploitation too much, because often times there will be a large number of actions that have little to no value for a given state. The Q-Network in this novel architecture is decomposed into two separate streams; a value stream and an advantage stream. Feature learning is carried out by a number of convolutional and pooling layers. The activations of the last of these layers are sent to both separate streams. Each stream contains a number of fully-connected layers. The final layer combines the output of the two streams, and the output of the network is a set of Q values, one for each action. The aggregator for the two outputs of the advantage and value streams is:  $\\beta$ refers to the parameters specific to the value network, and the alpha refers to the parameters specific to the advantage network. The advantage of the dueling network over standard Q-Networks is especially prominent when the number of actions is large. For standard Q-Networks, when the variation between actions is small, the Q-Network effectively has to learn the same value for all actions while each update only modifies the Q value of one action. Evidence  The dueling network architecture outperformed the Double-DQN results in 50 out of 57 learned Atari games. Strengths  The paper does a good job of making its main contribution (a novel neural network architecture) clear at the beginning. The experimental algorithm for Dueling Networks employs other state-of-the-art advances in DRL (such as Double-DQN) which helps show the correlation between research being carried on in this field. Interesting related works  Increasing the action gap (Bellemare et al., 2016)  Notes  The value function V measures the importance of being in a particular state s. The Q-function measures the importance about the value of choosing each possible action when in this state. The advantage function, $A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)$, subtracts the value of the state from the Q-function to obtain a relative measure of the importance of each action  A deep Q-network is a non-linear function approximator for the Q function having the form $Q(s,a;\\theta)$ with parameters theta  We optimize the following sequence of loss functions:  with the target of the loss function, $y_i^{DQN}$, given by the reward signal plus the discounted maximal Q value provided by the target Q-Network  A target Q-Network that uses parameter freezing for a certain number of iterations is used to stabilize the algorithm  The specific gradient is  Experience replay is used; use prioritized experience replay instead! (Don’t just sample uniformly from memory)  To avoid over-optimistic value estimates (van Hasselt, 2010), use Double Q-Learning. Originally, the max operator uses the same values to both select and evaluate an action. Instead, use the following target:", "pdf_url": "http://arxiv.org/pdf/1511.06581", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/dueling-networks.json"}
{"id": "66099843", "bin": "500_600", "summary_sentences": ["Information Extraction  - Given a query to be answered and an external search engine, information extraction entails the task of issuing search queries, extracting information from new sources and reconciling the extracted values till we are sufficiently confident about the extracted values.", "The paper proposes the use of Reinforcement Learning (RL) to solve this task.", "Implementation  Key Aspect  Use of Reinforcement Learning to resolve the ambiguity inherent in the textual documents.", "Given a query, the RL agent would use template statement to formulate the queries (to be performed on the black box search engine).", "It would further resolve and combine the result for the query from the set of retrieved documents.", "Datasets  Database of Mass Shootings in the United States.", "Food Shield database of illegal food adulteration.", "Framework  Information extraction task is modelled as a Markov Decision Process (MDP) <S, A, T, R>  S - Set of all possible states  The state consists of:  Extractor’s confidence in predicted entity values.", "Context from which values are extracted.", "Similarity between the new document (extracted just now from the search engine) and the original document accompanying the given query.", "A - Set of all possible actions  Reconciliation decision - d  Accept all entities values.", "Reject all entities values.", "Stop the current episode.", "Query choice - q  Choose the next query from a set of automatically generated alternatives.", "R - Rewards  Maximise the final extraction accuracy while minimising the number of queries.", "Q - Queries  Generated using a template.", "The query is searched on a search engine and the top k links are retrieved.", "Transition  Start with a single source article xi and extract the initial set of entities.", "At each timestep, the agent is given the state (s) on basis of which it chooses the action (d, q).", "The episode stops whenever the action is a stop action.", "Deep Q Network is used.", "Parameters are learned using SGD and RMSProp.", "Experimental Setup  Extraction Model  Max Entropy Classifier is used as the base extraction system.", "First, all the words in the document are tagged as one of the entity types and the mode of these values is used to obtain the set of extracted entities.", "Baseline  Basic Extractors  Aggregation System which either chooses the entity value with the highest confidence or takes a majority vote over all extracted values.", "Meta-Classifier which operates over the same input state space and produces the same set of reconciliation decisions as the DQN.", "Oracle Extractor which is computed assuming perfect reconciliation and query decisions on the top of the Maxnet base extractor.", "RL Models  RL Basic - Only reconciliation decision.", "RL Query - Only query decision with a fixed reconciliation strategy.", "RL Extract - the full system with both reconciliation and query decision.", "Result  RL Extract obtains substantial gains eg up to 11% over Maxnet.", "Simple aggregation schemes do not handle the task well.", "In terms of reward structure, providing rewards after each step works better than a single delayed reward."], "summary_text": "Information Extraction  - Given a query to be answered and an external search engine, information extraction entails the task of issuing search queries, extracting information from new sources and reconciling the extracted values till we are sufficiently confident about the extracted values. The paper proposes the use of Reinforcement Learning (RL) to solve this task. Implementation  Key Aspect  Use of Reinforcement Learning to resolve the ambiguity inherent in the textual documents. Given a query, the RL agent would use template statement to formulate the queries (to be performed on the black box search engine). It would further resolve and combine the result for the query from the set of retrieved documents. Datasets  Database of Mass Shootings in the United States. Food Shield database of illegal food adulteration. Framework  Information extraction task is modelled as a Markov Decision Process (MDP) <S, A, T, R>  S - Set of all possible states  The state consists of:  Extractor’s confidence in predicted entity values. Context from which values are extracted. Similarity between the new document (extracted just now from the search engine) and the original document accompanying the given query. A - Set of all possible actions  Reconciliation decision - d  Accept all entities values. Reject all entities values. Stop the current episode. Query choice - q  Choose the next query from a set of automatically generated alternatives. R - Rewards  Maximise the final extraction accuracy while minimising the number of queries. Q - Queries  Generated using a template. The query is searched on a search engine and the top k links are retrieved. Transition  Start with a single source article xi and extract the initial set of entities. At each timestep, the agent is given the state (s) on basis of which it chooses the action (d, q). The episode stops whenever the action is a stop action. Deep Q Network is used. Parameters are learned using SGD and RMSProp. Experimental Setup  Extraction Model  Max Entropy Classifier is used as the base extraction system. First, all the words in the document are tagged as one of the entity types and the mode of these values is used to obtain the set of extracted entities. Baseline  Basic Extractors  Aggregation System which either chooses the entity value with the highest confidence or takes a majority vote over all extracted values. Meta-Classifier which operates over the same input state space and produces the same set of reconciliation decisions as the DQN. Oracle Extractor which is computed assuming perfect reconciliation and query decisions on the top of the Maxnet base extractor. RL Models  RL Basic - Only reconciliation decision. RL Query - Only query decision with a fixed reconciliation strategy. RL Extract - the full system with both reconciliation and query decision. Result  RL Extract obtains substantial gains eg up to 11% over Maxnet. Simple aggregation schemes do not handle the task well. In terms of reward structure, providing rewards after each step works better than a single delayed reward.", "pdf_url": "https://arxiv.org/pdf/1603.07954", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/improving-information-extraction-by-acquiring-external-evidence-with-reinforcement-learning.json"}
{"id": "45186835", "bin": "500_600", "summary_sentences": ["Large-scale analysis of style injection by relative path overwrite Arshad et al., WWW’18  (If you don’t have ACM Digital Library access, the paper can be accessed either by following the link above directly from The Morning Paper blog site, or from the WWW 2018 proceedings page).", "We’ve all been fairly well trained to have good awareness of cross-site scripting (XSS) attacks.", "Less obvious, and also less well known, is that a similar attack is possible using style sheet injection.", "A good name for these attacks might be SSS: same-site style attacks.", "Even though style injection may appear less serious a threat than script injection, it has been shown that it enables a range of attacks, including secret exfiltration… Our work shows that around 9% of the sites in the Alexa top 10,000 contain at least one vulnerable page, out of which more than one third can be exploited.", "I’m going to break today’s write-up down into four parts:  How on earth do you do secret exfiltration with a stylesheet?", "Injecting stylesheet content using Relative Path Overwite (RPO)  Finding RPO vulnerabilities in the wild  How can you defend against RPO attacks?", "Secret exfiltration via stylesheets  Style sheet injection belongs to a family of attacks known as ‘scriptless’ attacks.", "While CSS is intended for controlling styling and layout, it does also contain some context-sensitive features that can be used to extract and exfiltrate data.", "Suppose a page contains some sensitive data you’d like to get your hands on.", "The first thing you need to do is make that visible if it was otherwise hidden (CSS attribute accessors and content properties will help with this).", "Once the content is visible, you can apply style directives to it, such as fonts…  Custom attacker-supplied fonts can change the size of the secret text depending on its value.", "Animation features can be used to cycle through a number of fonts in order to test different combinations.", "Media queries or the appearance of scrollbars can be used to implement conditional style, and data exfiltration by loading a different URL for each condition from the attacker’s server.", "Taken together, Heiderich et al. demonstrate that these techniques allow an attacker to steal credit card numbers or CSRF tokens without script execution.", "There are other attacks too, this is just one example.", "Helping the attacker is the fact that the CSS standard mandates browsers be forgiving when parsing CSS, skipping over parts they don’t understand.", "Against the attacker though, is the fact that modern browsers won’t load documents with non-CSS content types or syntax errors as stylesheets, if they come from a different domain than the including page.", "The Relative Path Overwrite (RPO) attack vector  If both the including page and the included stylesheet come from the same domain though, it’s game on.", "Relative Path Overwrite vulnerabilities allow an attacker to engineer this scenario.", "Consider a web page hosted at  [url]"], "summary_text": "Large-scale analysis of style injection by relative path overwrite Arshad et al., WWW’18  (If you don’t have ACM Digital Library access, the paper can be accessed either by following the link above directly from The Morning Paper blog site, or from the WWW 2018 proceedings page). We’ve all been fairly well trained to have good awareness of cross-site scripting (XSS) attacks. Less obvious, and also less well known, is that a similar attack is possible using style sheet injection. A good name for these attacks might be SSS: same-site style attacks. Even though style injection may appear less serious a threat than script injection, it has been shown that it enables a range of attacks, including secret exfiltration… Our work shows that around 9% of the sites in the Alexa top 10,000 contain at least one vulnerable page, out of which more than one third can be exploited. I’m going to break today’s write-up down into four parts:  How on earth do you do secret exfiltration with a stylesheet? Injecting stylesheet content using Relative Path Overwite (RPO)  Finding RPO vulnerabilities in the wild  How can you defend against RPO attacks? Secret exfiltration via stylesheets  Style sheet injection belongs to a family of attacks known as ‘scriptless’ attacks. While CSS is intended for controlling styling and layout, it does also contain some context-sensitive features that can be used to extract and exfiltrate data. Suppose a page contains some sensitive data you’d like to get your hands on. The first thing you need to do is make that visible if it was otherwise hidden (CSS attribute accessors and content properties will help with this). Once the content is visible, you can apply style directives to it, such as fonts…  Custom attacker-supplied fonts can change the size of the secret text depending on its value. Animation features can be used to cycle through a number of fonts in order to test different combinations. Media queries or the appearance of scrollbars can be used to implement conditional style, and data exfiltration by loading a different URL for each condition from the attacker’s server. Taken together, Heiderich et al. demonstrate that these techniques allow an attacker to steal credit card numbers or CSRF tokens without script execution. There are other attacks too, this is just one example. Helping the attacker is the fact that the CSS standard mandates browsers be forgiving when parsing CSS, skipping over parts they don’t understand. Against the attacker though, is the fact that modern browsers won’t load documents with non-CSS content types or syntax errors as stylesheets, if they come from a different domain than the including page. The Relative Path Overwrite (RPO) attack vector  If both the including page and the included stylesheet come from the same domain though, it’s game on. Relative Path Overwrite vulnerabilities allow an attacker to engineer this scenario. Consider a web page hosted at  [url]", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3178876.3186090?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/large-scale-analysis-of-style-injection-by-relative-path-overwrite.json"}
{"id": "63848470", "bin": "500_600", "summary_sentences": ["This paper presents a variational approach to the maximisation of mutual information in the context of a reinforcement learning agent.", "Mutual information in this context can provide a learning signal to the agent that is \"intrinsically motivated\", because it relies solely on the agent's state/beliefs and does not require from the (\"outside\") user an explicit definition of rewards.", "Specifically, the learning objective, for a current state s, is the mutual information between the sequence of K actions a proposed by an exploration distribution $w(a|s)$ and the final state s' of the agent after performing these actions.", "To understand what the properties of this objective, it is useful to consider the form of this mutual information as a difference of conditional entropies:  $$I(a,s'|s) = H(a|s) - H(a|s',s)$$  Where $I(.|.", ")$ is the (conditional) mutual information and $H(.|.", ")$ is the (conditional) entropy.", "This objective thus asks that the agent find an exploration distribution that explores as much as possible (i.e. has high $H(a|s)$ entropy) but is such that these actions have predictable consequences (i.e.", "lead to predictable state s' so that $H(a|s',s)$ is low).", "So one could think of the agent as trying to learn to have control of as much of the environment as possible, thus this objective has also been coined as \"empowerment\".", "The main contribution of this work is to show how to train, on a large scale (i.e. larger state space and action space) with this objective, using neural networks.", "They build on a variational lower bound on the mutual information and then derive from it a stochastic variational training algorithm for it.", "The procedure has 3 components: the exploration distribution $w(a|s)$, the environment $p(s'|s,a)$ (can be thought as an encoder, but which isn't modeled and is only interacted with/sampled from) and the planning model $p(a|s',s)$ (which is modeled and can be thought of as a decoder).", "The main technical contribution is in how to update the exploration distribution (see section 4.2.2 for the technical details).", "This approach exploits neural networks of various forms.", "Neural autoregressive generative models are also used as models for the exploration distribution as well as the decoder or planning distribution.", "Interestingly, the framework allows to also learn the state representation s as a function of some \"raw\" representation x of states.", "For raw states corresponding to images (e.g. the pixels of the screen image in a game), CNNs are used."], "summary_text": "This paper presents a variational approach to the maximisation of mutual information in the context of a reinforcement learning agent. Mutual information in this context can provide a learning signal to the agent that is \"intrinsically motivated\", because it relies solely on the agent's state/beliefs and does not require from the (\"outside\") user an explicit definition of rewards. Specifically, the learning objective, for a current state s, is the mutual information between the sequence of K actions a proposed by an exploration distribution $w(a|s)$ and the final state s' of the agent after performing these actions. To understand what the properties of this objective, it is useful to consider the form of this mutual information as a difference of conditional entropies:  $$I(a,s'|s) = H(a|s) - H(a|s',s)$$  Where $I(.|. )$ is the (conditional) mutual information and $H(.|. )$ is the (conditional) entropy. This objective thus asks that the agent find an exploration distribution that explores as much as possible (i.e. has high $H(a|s)$ entropy) but is such that these actions have predictable consequences (i.e. lead to predictable state s' so that $H(a|s',s)$ is low). So one could think of the agent as trying to learn to have control of as much of the environment as possible, thus this objective has also been coined as \"empowerment\". The main contribution of this work is to show how to train, on a large scale (i.e. larger state space and action space) with this objective, using neural networks. They build on a variational lower bound on the mutual information and then derive from it a stochastic variational training algorithm for it. The procedure has 3 components: the exploration distribution $w(a|s)$, the environment $p(s'|s,a)$ (can be thought as an encoder, but which isn't modeled and is only interacted with/sampled from) and the planning model $p(a|s',s)$ (which is modeled and can be thought of as a decoder). The main technical contribution is in how to update the exploration distribution (see section 4.2.2 for the technical details). This approach exploits neural networks of various forms. Neural autoregressive generative models are also used as models for the exploration distribution as well as the decoder or planning distribution. Interestingly, the framework allows to also learn the state representation s as a function of some \"raw\" representation x of states. For raw states corresponding to images (e.g. the pixels of the screen image in a game), CNNs are used.", "pdf_url": "http://papers.nips.cc/paper/5668-variational-information-maximisation-for-intrinsically-motivated-reinforcement-learning.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/mohamedr15.json"}
{"id": "30789903", "bin": "500_600", "summary_sentences": ["The paper proposes a general and end-to-end approach for sequence learning that uses two deep LSTMs, one to map input sequence to vector space and another to map vector to the output sequence.", "For sequence learning, Deep Neural Networks (DNNs) requires the dimensionality of input and output sequences be known and fixed.", "This limitation is overcome by using the two LSTMs.", "Model  Recurrent Neural Networks (RNNs) generalizes feed forward neural networks to sequences.", "Given a sequence of inputs (x1, x2...xt), RNN computes a sequence of outputs (y1, y2...yt') by iterating over the following equation:  ht = sigm(Whxxt + Whhht-1) yt = Wyhht  To map variable length sequences, the input is mapped to a fixed size vector using an RNN and this fixed size vector is mapped to output sequence using another RNN.", "Given the long-term dependencies between the two sequences, LSTMs are preferred over RNNs.", "LSTMs estimate the conditional probability p(output sequence | input sequence) by first mapping the input sequence to a fixed dimensional representation and then computing the probability of output with a standard LST-LM formulation.", "Differences between the model and standard LSTMs  The model uses two LSTMs (one for input sequence and another for output sequence), thereby increasing the number of model parameters at negligible computing cost.", "Model uses Deep LSTMs (4 layers).", "The words in the input sequences are reversed to introduce short-term dependencies and to reduce the \"minimal time lag\".", "By reversing the word order, the first few words in the source sentence (input sentence) are much closer to first few words in the target sentence (output sentence) thereby making it easier for LSTM to \"establish\" communication between input and output sentences.", "Experiments  WMT'14 English to French dataset containing 12 million sentences consisting of 348 million French words and 304 million English words.", "Model tested on translation task and on the task of re-scoring the n-best results of baseline approach.", "Deep LSTMs trained in sentence pairs by maximizing the log probability of a correct translation T, given the source sentence S  The training objective is to maximize this log probability, averaged over all the pairs in the training set.", "Most likely translation is found by performing a simple, left-to-right beam search for translation.", "A hard constraint is enforced on the norm of the gradient to avoid the exploding gradient problem.", "Min batches are selected to have sentences of similar lengths to reduce training time.", "Model performs better when reversed sentences are used for training.", "While the model does not beat the state-of-the-art, it is the first pure neural translation system to outperform a phrase-based SMT baseline.", "The model performs well on long sentences as well with only a minor degradation for the largest sentences.", "The paper prepares ground for the application of sequence-to-sequence based learning models in other domains by demonstrating how a simple and relatively unoptimised neural model could outperform a mature SMT system on translation tasks."], "summary_text": "The paper proposes a general and end-to-end approach for sequence learning that uses two deep LSTMs, one to map input sequence to vector space and another to map vector to the output sequence. For sequence learning, Deep Neural Networks (DNNs) requires the dimensionality of input and output sequences be known and fixed. This limitation is overcome by using the two LSTMs. Model  Recurrent Neural Networks (RNNs) generalizes feed forward neural networks to sequences. Given a sequence of inputs (x1, x2...xt), RNN computes a sequence of outputs (y1, y2...yt') by iterating over the following equation:  ht = sigm(Whxxt + Whhht-1) yt = Wyhht  To map variable length sequences, the input is mapped to a fixed size vector using an RNN and this fixed size vector is mapped to output sequence using another RNN. Given the long-term dependencies between the two sequences, LSTMs are preferred over RNNs. LSTMs estimate the conditional probability p(output sequence | input sequence) by first mapping the input sequence to a fixed dimensional representation and then computing the probability of output with a standard LST-LM formulation. Differences between the model and standard LSTMs  The model uses two LSTMs (one for input sequence and another for output sequence), thereby increasing the number of model parameters at negligible computing cost. Model uses Deep LSTMs (4 layers). The words in the input sequences are reversed to introduce short-term dependencies and to reduce the \"minimal time lag\". By reversing the word order, the first few words in the source sentence (input sentence) are much closer to first few words in the target sentence (output sentence) thereby making it easier for LSTM to \"establish\" communication between input and output sentences. Experiments  WMT'14 English to French dataset containing 12 million sentences consisting of 348 million French words and 304 million English words. Model tested on translation task and on the task of re-scoring the n-best results of baseline approach. Deep LSTMs trained in sentence pairs by maximizing the log probability of a correct translation T, given the source sentence S  The training objective is to maximize this log probability, averaged over all the pairs in the training set. Most likely translation is found by performing a simple, left-to-right beam search for translation. A hard constraint is enforced on the norm of the gradient to avoid the exploding gradient problem. Min batches are selected to have sentences of similar lengths to reduce training time. Model performs better when reversed sentences are used for training. While the model does not beat the state-of-the-art, it is the first pure neural translation system to outperform a phrase-based SMT baseline. The model performs well on long sentences as well with only a minor degradation for the largest sentences. The paper prepares ground for the application of sequence-to-sequence based learning models in other domains by demonstrating how a simple and relatively unoptimised neural model could outperform a mature SMT system on translation tasks.", "pdf_url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/915921d7d0ac5cfd0e379025acfb9f.json"}
{"id": "86423785", "bin": "500_600", "summary_sentences": ["Given a video, can a machine learning system detect the arrow of time and distinguish whether the video is running forward or backwards.", "Datasets  Youtube Dataset  180 short videos (6-10 seconds) are manually selected using keywords like \"dance\", \"stream trains\" etc.", "155 forward videos and 25 reverse videos - highly imbalanced dataset!!", "Tennis Ball Dataset  Recorded 13 HD videos of tennis ball rolling and colliding on the floor.", "Methods  Baseline  Spatial-temporal Oriented Energy (SOE) as off the shelf feature extractor.", "Split videos into 2x2 spatial subregions and concatenate SOE features to obtain final features.", "These final features are fed to linear SVM classifier and the performance varies from 48% to 60%.", "One reason for poor performance could be the difficulty in generalising motion over different sub-regions.", "Statistical Flow Method  Idea is to capture local regions of motion in a video to examine what type of motion is a good feature for detecting the arrow of time.", "Flow Words are object-motion descriptors based on SIFTlike descriptors and capture motion occurring in small patches of videos.", "These descriptors are motion quantized to obtain a discrete set of flow words.", "The entire video sequence can be encoded as a bag of flow-word descriptors which becomes the features for the learning system.", "Training  For each video, 4 descriptor histograms were extracted:  (A): the native direction of the video  (B): this video mirrored in the left-right direction  (C): the original video time-flipped  (D): the time-flipped left-right-mirrored version  Train an SVM using the 4 histograms and combine their scores as A + B - C - D expecting a positive result for forwarding clips and negative for backwards clips.", "Result  Performance varies from 75% to 90%  Motion-Causation Method  Idea is to capture motion causing other motions as it is more common for one motion to cause multiple motions instead of multiple motions collapsing into one motion.", "The system looks at the regions in the video from frame to frame with the expectation that, in the forwards-time direction, there would be more occurrences of one region splitting in two than of two regions joining to become one.", "Result  Performance varies from 70% to 73%.", "Though it underperforms as compared to the flow-word method, it can complement that method as Motion-causation considers the spatial location of motions while flow-word method considers motion in each frame separately.", "AR Method  Idea is to model the problem as that of inferring casual direction in cause-effect models.", "The assumption is that some image motions will be modelled as AR models with additive non-Gaussian noise.", "In such a scenario, noise added at some point in time, is independent of the past values of the time series but not of future values.", "This allows independence tests to be performed for determining the direction of time.", "Result  There is a tradeoff between the accuracy achieved by the system versus the number of videos it can classify (depending on the value of delta for p-test).", "Comment  The paper poses a new and interesting research problem but uses a very small dataset which makes the results inconclusive in my opinion."], "summary_text": "Given a video, can a machine learning system detect the arrow of time and distinguish whether the video is running forward or backwards. Datasets  Youtube Dataset  180 short videos (6-10 seconds) are manually selected using keywords like \"dance\", \"stream trains\" etc. 155 forward videos and 25 reverse videos - highly imbalanced dataset!! Tennis Ball Dataset  Recorded 13 HD videos of tennis ball rolling and colliding on the floor. Methods  Baseline  Spatial-temporal Oriented Energy (SOE) as off the shelf feature extractor. Split videos into 2x2 spatial subregions and concatenate SOE features to obtain final features. These final features are fed to linear SVM classifier and the performance varies from 48% to 60%. One reason for poor performance could be the difficulty in generalising motion over different sub-regions. Statistical Flow Method  Idea is to capture local regions of motion in a video to examine what type of motion is a good feature for detecting the arrow of time. Flow Words are object-motion descriptors based on SIFTlike descriptors and capture motion occurring in small patches of videos. These descriptors are motion quantized to obtain a discrete set of flow words. The entire video sequence can be encoded as a bag of flow-word descriptors which becomes the features for the learning system. Training  For each video, 4 descriptor histograms were extracted:  (A): the native direction of the video  (B): this video mirrored in the left-right direction  (C): the original video time-flipped  (D): the time-flipped left-right-mirrored version  Train an SVM using the 4 histograms and combine their scores as A + B - C - D expecting a positive result for forwarding clips and negative for backwards clips. Result  Performance varies from 75% to 90%  Motion-Causation Method  Idea is to capture motion causing other motions as it is more common for one motion to cause multiple motions instead of multiple motions collapsing into one motion. The system looks at the regions in the video from frame to frame with the expectation that, in the forwards-time direction, there would be more occurrences of one region splitting in two than of two regions joining to become one. Result  Performance varies from 70% to 73%. Though it underperforms as compared to the flow-word method, it can complement that method as Motion-causation considers the spatial location of motions while flow-word method considers motion in each frame separately. AR Method  Idea is to model the problem as that of inferring casual direction in cause-effect models. The assumption is that some image motions will be modelled as AR models with additive non-Gaussian noise. In such a scenario, noise added at some point in time, is independent of the past values of the time series but not of future values. This allows independence tests to be performed for determining the direction of time. Result  There is a tradeoff between the accuracy achieved by the system versus the number of videos it can classify (depending on the value of delta for p-test). Comment  The paper poses a new and interesting research problem but uses a very small dataset which makes the results inconclusive in my opinion.", "pdf_url": "https://www.robots.ox.ac.uk/~vgg/publications/2014/Pickup14/pickup14.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/8d8de0034a350d97738bbedadc9373.json"}
{"id": "69976012", "bin": "500_600", "summary_sentences": ["Consider problems where the input to the model is a set.", "In such problems (referred to as the set-input problems), the model should be invariant to the permutation of the data points.", "In “set pooling” methods ( 1 , 2 ), each data point (in the input set) is encoded using a feed-forward network and the resulting set of encoded representations are pooled using the “sum” operator.", "This approach can be shown to be bot permutation-invariant and a universal function approximator.", "The paper proposes an attention-based network module, called as the Set Transformer, which can model the interactions between the elements of an input set while being permutation invariant.", "Transformer  An attention function Attn(Q, K, V) = (QKT)V is used to map queries Q to output using key-value pairs K, V.  In case of multi-head attention, the key, query, and value are projected into h different vectors and attention is applied on all these vectors.", "The output is a linear transformation of the concatenation of all the vectors.", "Set Transformer  3 modules are introduced: MAB, SAB and ISAB.", "Multihead Attention Block (MAB) is a module very similar to to the encoder in the Transformer, without the positional encoding and dropout.", "Set Attention Block (SAB) is a module that takes as input a set and performs self-attention between the elements of the set to produce another set of the same size ie SAB(X) = MAB(X, X).", "The time complexity of the SAB operation is O(n2) where n is the number of elements in the set.", "It can be reduced to O(m*n) by using Induced Set Attention Blocks (ISAB) with m induced point vectors (denoted as I).", "ISABm = MAB(X, MAB(I, X)).", "ISAB can be seen as performing a low-rank projection of inputs.", "These modules can be used to model the interactions between data points in any given set.", "Pooling by Multihead Attention (PMA)  Aggregation is performed by applying multi-head attention on a set of k seed vectors.", "The interaction between the k outputs (from PMA) can be modeled by applying another SAB.", "Thus the entire network is a stack of SABs and ISABs.", "Both the modules are permutation invariant and so is any network obtained by stacking them.", "Experiments  Datasets include:  Predicting the maximum value from a set.", "Counting unique (Omniglot) characters from an image.", "Clustering with a mixture of Gaussians (synthetic points and CIFAR 100).", "Set Anomaly detection (celebA).", "Generally, increasing m (the number of inducing datapoints) improve performance, to some extent.", "This is somewhat expected.", "The paper considers various ablations of the proposed approach (like disabling attention in the encoder or pooling layer) and shows that attention mechanism is needed during both the stages.", "The work has two main benefits over prior work:  Reducing the O(n2) complexity to O(m*n) complexity.", "Using self-attention mechanism for both encodings the inputs and for aggregating the encoded representations."], "summary_text": "Consider problems where the input to the model is a set. In such problems (referred to as the set-input problems), the model should be invariant to the permutation of the data points. In “set pooling” methods ( 1 , 2 ), each data point (in the input set) is encoded using a feed-forward network and the resulting set of encoded representations are pooled using the “sum” operator. This approach can be shown to be bot permutation-invariant and a universal function approximator. The paper proposes an attention-based network module, called as the Set Transformer, which can model the interactions between the elements of an input set while being permutation invariant. Transformer  An attention function Attn(Q, K, V) = (QKT)V is used to map queries Q to output using key-value pairs K, V.  In case of multi-head attention, the key, query, and value are projected into h different vectors and attention is applied on all these vectors. The output is a linear transformation of the concatenation of all the vectors. Set Transformer  3 modules are introduced: MAB, SAB and ISAB. Multihead Attention Block (MAB) is a module very similar to to the encoder in the Transformer, without the positional encoding and dropout. Set Attention Block (SAB) is a module that takes as input a set and performs self-attention between the elements of the set to produce another set of the same size ie SAB(X) = MAB(X, X). The time complexity of the SAB operation is O(n2) where n is the number of elements in the set. It can be reduced to O(m*n) by using Induced Set Attention Blocks (ISAB) with m induced point vectors (denoted as I). ISABm = MAB(X, MAB(I, X)). ISAB can be seen as performing a low-rank projection of inputs. These modules can be used to model the interactions between data points in any given set. Pooling by Multihead Attention (PMA)  Aggregation is performed by applying multi-head attention on a set of k seed vectors. The interaction between the k outputs (from PMA) can be modeled by applying another SAB. Thus the entire network is a stack of SABs and ISABs. Both the modules are permutation invariant and so is any network obtained by stacking them. Experiments  Datasets include:  Predicting the maximum value from a set. Counting unique (Omniglot) characters from an image. Clustering with a mixture of Gaussians (synthetic points and CIFAR 100). Set Anomaly detection (celebA). Generally, increasing m (the number of inducing datapoints) improve performance, to some extent. This is somewhat expected. The paper considers various ablations of the proposed approach (like disabling attention in the encoder or pooling layer) and shows that attention mechanism is needed during both the stages. The work has two main benefits over prior work:  Reducing the O(n2) complexity to O(m*n) complexity. Using self-attention mechanism for both encodings the inputs and for aggregating the encoded representations.", "pdf_url": "https://arxiv.org/pdf/1810.00825", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/set-transformer-a-framework-for-attention-based-permutation-invariant-neural-networks.json"}
{"id": "11684235", "bin": "500_600", "summary_sentences": ["Problem Statement  Standard VQA models benefit from the inherent bias in the structure of the world and the language of the question.", "For example, if the question starts with “Do you see a …”, it is more likely to be “yes” than “no”.", "To truly assess the capability of any VQA system, we need to have evaluation tasks that require the use of both the visual and the language modality.", "The authors present a balanced version of VQA dataset where each question in the dataset is associated with a pair of similar images such that the same question would give different answers on the two images.", "The proposed data collection procedure enables the authors to develop a novel interpretable model which, given an image and a question, identifies an image that is similar to the original image but has a different answer to the same question thereby building trust for the system.", "Dataset Collection  Given an (image, question, answer) triplet (I, Q, A) from the VQA dataset, a human worker (on AMT) is asked to identify an image I’ which is similar to I but for which the answer to question Q is A’ (different from A).", "To facilitate the search for I’, the worker is shown 24 nearest-neighbor images of I (based on VGGNet features) and is asked to choose the most similar image to I, for which Q makes sense and answer for Q is different than A.", "In case none of the 24 images qualifies, the worker may select “not possible”.", "In the second round, the workers were asked to answer Q for I’.", "This 2-stage protocol results in a significantly more balanced dataset than the previous dataset.", "Observation  State-of-the-art models trained on unbalanced VQA dataset perform significantly worse on the new, balanced dataset indicating that those models benefitted from the language bias in the older dataset.", "Training on balanced dataset improves performance on the unbalanced dataset.", "Further, the VQA model, trained on the balanced dataset, learns to differentiate between otherwise similar images.", "Counter-example Explanations  Given an image and a question, the model not only answers the question, it also provides an image (from the k nearest neighbours of I, based on VGGNet features) which is similar to the input image but for which the model would have given different answer for the same image.", "Supervising signal is provided by the data collection procedure where humans pick the image I’ from the same set of candidate images.", "For each image in the candidate set, compute the inner product of question-image embedding and answer embedding.", "The K inner product values are passed through a fully connected layer to generate K scores.", "Trained with pairwise hinge ranking loss so that the score of the human picked image is higher than the score of all other images by a margin of M (hyperparameter).", "The proposed explanation model achieves a recall@5 of 43.49%"], "summary_text": "Problem Statement  Standard VQA models benefit from the inherent bias in the structure of the world and the language of the question. For example, if the question starts with “Do you see a …”, it is more likely to be “yes” than “no”. To truly assess the capability of any VQA system, we need to have evaluation tasks that require the use of both the visual and the language modality. The authors present a balanced version of VQA dataset where each question in the dataset is associated with a pair of similar images such that the same question would give different answers on the two images. The proposed data collection procedure enables the authors to develop a novel interpretable model which, given an image and a question, identifies an image that is similar to the original image but has a different answer to the same question thereby building trust for the system. Dataset Collection  Given an (image, question, answer) triplet (I, Q, A) from the VQA dataset, a human worker (on AMT) is asked to identify an image I’ which is similar to I but for which the answer to question Q is A’ (different from A). To facilitate the search for I’, the worker is shown 24 nearest-neighbor images of I (based on VGGNet features) and is asked to choose the most similar image to I, for which Q makes sense and answer for Q is different than A. In case none of the 24 images qualifies, the worker may select “not possible”. In the second round, the workers were asked to answer Q for I’. This 2-stage protocol results in a significantly more balanced dataset than the previous dataset. Observation  State-of-the-art models trained on unbalanced VQA dataset perform significantly worse on the new, balanced dataset indicating that those models benefitted from the language bias in the older dataset. Training on balanced dataset improves performance on the unbalanced dataset. Further, the VQA model, trained on the balanced dataset, learns to differentiate between otherwise similar images. Counter-example Explanations  Given an image and a question, the model not only answers the question, it also provides an image (from the k nearest neighbours of I, based on VGGNet features) which is similar to the input image but for which the model would have given different answer for the same image. Supervising signal is provided by the data collection procedure where humans pick the image I’ from the same set of candidate images. For each image in the candidate set, compute the inner product of question-image embedding and answer embedding. The K inner product values are passed through a fully connected layer to generate K scores. Trained with pairwise hinge ranking loss so that the score of the human picked image is higher than the score of all other images by a margin of M (hyperparameter). The proposed explanation model achieves a recall@5 of 43.49%", "pdf_url": "https://arxiv.org/pdf/1612.00837", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering.json"}
{"id": "16973018", "bin": "500_600", "summary_sentences": ["   The paper provides useful empirical advice for adapting pretrained language models for a given target task.", "Pre-trained models considered  ELMo  BERT  Tasks considered  Named Entity Recognition (NER) - CoNLL 2003 dataset  Sentiment Analysis (SA) - Stanford Sentiment Treebank (SST-2) dataset  Natural Language Inference (NLI) - MultiNLI and Sentences Involving Compositional Knowledge (SICK-E) dataset  Paraphrase Detection (PD) - Microsoft Research Paraphrase Corpus (MRPC)  Semantic Textual Similarity (STS) - Semantic Textual Similarity Benchmark (STS-B) and SICK-R  The last 3 tasks (NLI, PD, STS) are defined for sentence pairs.", "Adaptation Strategies  Feature Extraction  The pretrained model is only used for extracting features and its weights are kept fixed.", "For both ELMo and BERT, the contextual representation of the words from all the layers are extracted.", "A weighted combination of these layers is used as an input to the task-specific model.", "Task-specific models  NER - BiLSTM with CRF layer  SA - bi-attentive classification network  NLI, PD, STS - Enhanced Sequential Inference Model (ESIM)  Fine-tuning  The pretrained model is finetuned on the target task.", "Task-specific models for ELMO  NER - CRF on top of LSTM states  SA - Max-pool over the language model states followed by a softmax layer  NLI, PD, STS - cross sentence bi-attention between the language model states followed by pooling and softmax layer.", "Task-specific models for BERT  NER - Extract representation of the first-word piece of each token followed by the softmax layer  SA, NLI, PD, STS - standard BERT training  Main observations  Feature extraction and fine-tuning have comparable performance in most cases unless the two tasks are highly similar(fine-tuning is better) or highly dissimilar (feature extraction is better).", "For ELMo, feature extraction consistently outperforms fine-tuning for the sentence pair tasks (NLI, PD, STS).", "The reverse trend is observed for BERT with fine-tuning being better on sentence pair tasks.", "Adding extra parameters is helpful for feature extraction but not fine-tuning.", "ELMo fine-tuning requires careful tuning and other tricks like triangular learning rates, gradual unfreezing and discriminative fine-tuning.", "For the tasks considered, there is no correlation observed between the distance of the source and target domains and adaptation performance.", "Training a diagnostic classifier (on the intermediate representations) suggests that fine-tuning improves the performance of the classifier at all the intermediate layers (which is sort of expected).", "In terms of mutual information estimates, fine-tuned representations have a much higher mutual information as compared to the feature extraction based representations.", "Knowledge for single sentence tasks seems to be mostly concentrated in the last layers while for pair classification tasks, the knowledge seems gradually build un in the intermediate layers, all the way up to the last layer."], "summary_text": "The paper provides useful empirical advice for adapting pretrained language models for a given target task. Pre-trained models considered  ELMo  BERT  Tasks considered  Named Entity Recognition (NER) - CoNLL 2003 dataset  Sentiment Analysis (SA) - Stanford Sentiment Treebank (SST-2) dataset  Natural Language Inference (NLI) - MultiNLI and Sentences Involving Compositional Knowledge (SICK-E) dataset  Paraphrase Detection (PD) - Microsoft Research Paraphrase Corpus (MRPC)  Semantic Textual Similarity (STS) - Semantic Textual Similarity Benchmark (STS-B) and SICK-R  The last 3 tasks (NLI, PD, STS) are defined for sentence pairs. Adaptation Strategies  Feature Extraction  The pretrained model is only used for extracting features and its weights are kept fixed. For both ELMo and BERT, the contextual representation of the words from all the layers are extracted. A weighted combination of these layers is used as an input to the task-specific model. Task-specific models  NER - BiLSTM with CRF layer  SA - bi-attentive classification network  NLI, PD, STS - Enhanced Sequential Inference Model (ESIM)  Fine-tuning  The pretrained model is finetuned on the target task. Task-specific models for ELMO  NER - CRF on top of LSTM states  SA - Max-pool over the language model states followed by a softmax layer  NLI, PD, STS - cross sentence bi-attention between the language model states followed by pooling and softmax layer. Task-specific models for BERT  NER - Extract representation of the first-word piece of each token followed by the softmax layer  SA, NLI, PD, STS - standard BERT training  Main observations  Feature extraction and fine-tuning have comparable performance in most cases unless the two tasks are highly similar(fine-tuning is better) or highly dissimilar (feature extraction is better). For ELMo, feature extraction consistently outperforms fine-tuning for the sentence pair tasks (NLI, PD, STS). The reverse trend is observed for BERT with fine-tuning being better on sentence pair tasks. Adding extra parameters is helpful for feature extraction but not fine-tuning. ELMo fine-tuning requires careful tuning and other tricks like triangular learning rates, gradual unfreezing and discriminative fine-tuning. For the tasks considered, there is no correlation observed between the distance of the source and target domains and adaptation performance. Training a diagnostic classifier (on the intermediate representations) suggests that fine-tuning improves the performance of the classifier at all the intermediate layers (which is sort of expected). In terms of mutual information estimates, fine-tuned representations have a much higher mutual information as compared to the feature extraction based representations. Knowledge for single sentence tasks seems to be mostly concentrated in the last layers while for pair classification tasks, the knowledge seems gradually build un in the intermediate layers, all the way up to the last layer.", "pdf_url": "https://arxiv.org/pdf/1903.05987", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/to-tune-or-not-to-tune-adapting-pretrained-representations-to-diverse-tasks.json"}
{"id": "72788236", "bin": "500_600", "summary_sentences": ["This is another \"learning the learning rate\" paper, which predates (and might have inspired) the \"Speed learning on the fly\" paper I recently wrote notes about (see  [ref] ).", "In this paper, they consider the off-line training scenario, and propose to do gradient descent on the learning rate by unrolling the *complete* training procedure and treating it all as a function to optimize, with respect to the learning rate.", "This way, they can optimize directly the validation set loss.", "The paper in fact goes much further and can tune many other hyper-parameters of the gradient descent procedure: momentum, weight initialization distribution parameters, regularization and input preprocessing.", "#### My two cents  This is one of my favorite papers of this year.", "While the method of unrolling several steps of gradient descent (100 iterations in the paper) makes it somewhat impractical for large networks (which is probably why they considered 3-layer networks with only 50 hidden units per layer), it provides an incredibly interesting window on what are good hyper-parameter choices for neural networks.", "Note that, to substantially reduce the memory requirements of the method, the authors had to be quite creative and smart about how to encode changes in the network's weight changes.", "There are tons of interesting experiments, which I encourage the reader to go check out (see section 3).", "One experiment on training the learning rates, separately for each iteration (i.e. learning a learning rate schedule), for each layer and for either weights or biases (800 hyper-parameters total) shows that a good schedule is one where the top layer first learns quickly (large learning), then the bottom layer starts training faster, and finally the learning rates of all layers is decayed towards zero.", "Note that some of the experiments presented actually optimized the training error, instead of the validation set error.", "Another looked at finding optimal scales for the weight initialization.", "Interestingly, the values found weren't that far from an often prescribed scale of $1 / \\sqrt{N}$, where $N$ is the number of units in the previous layer.", "The experiment on \"training the training set\", i.e. generating the 10 examples (one per class) that would minimize the validation set loss of a network trained on these examples is a pretty cool idea (it essentially learns prototypical images of the digits from 0 to 9 on MNIST).", "Another experiment tried to optimize a multitask regularization matrix, in order to encourage forms of soft-weight-tying across tasks.", "Note that approaches like the one in this paper make tools for automatic differentiation incredibly valuable.", "Python autograd, the author's automatic differentiation Python library  [url]"], "summary_text": "This is another \"learning the learning rate\" paper, which predates (and might have inspired) the \"Speed learning on the fly\" paper I recently wrote notes about (see  [ref] ). In this paper, they consider the off-line training scenario, and propose to do gradient descent on the learning rate by unrolling the *complete* training procedure and treating it all as a function to optimize, with respect to the learning rate. This way, they can optimize directly the validation set loss. The paper in fact goes much further and can tune many other hyper-parameters of the gradient descent procedure: momentum, weight initialization distribution parameters, regularization and input preprocessing. #### My two cents  This is one of my favorite papers of this year. While the method of unrolling several steps of gradient descent (100 iterations in the paper) makes it somewhat impractical for large networks (which is probably why they considered 3-layer networks with only 50 hidden units per layer), it provides an incredibly interesting window on what are good hyper-parameter choices for neural networks. Note that, to substantially reduce the memory requirements of the method, the authors had to be quite creative and smart about how to encode changes in the network's weight changes. There are tons of interesting experiments, which I encourage the reader to go check out (see section 3). One experiment on training the learning rates, separately for each iteration (i.e. learning a learning rate schedule), for each layer and for either weights or biases (800 hyper-parameters total) shows that a good schedule is one where the top layer first learns quickly (large learning), then the bottom layer starts training faster, and finally the learning rates of all layers is decayed towards zero. Note that some of the experiments presented actually optimized the training error, instead of the validation set error. Another looked at finding optimal scales for the weight initialization. Interestingly, the values found weren't that far from an often prescribed scale of $1 / \\sqrt{N}$, where $N$ is the number of units in the previous layer. The experiment on \"training the training set\", i.e. generating the 10 examples (one per class) that would minimize the validation set loss of a network trained on these examples is a pretty cool idea (it essentially learns prototypical images of the digits from 0 to 9 on MNIST). Another experiment tried to optimize a multitask regularization matrix, in order to encourage forms of soft-weight-tying across tasks. Note that approaches like the one in this paper make tools for automatic differentiation incredibly valuable. Python autograd, the author's automatic differentiation Python library  [url]", "pdf_url": "http://proceedings.mlr.press/v37/maclaurin15.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/maclaurinda15.json"}
{"id": "8087311", "bin": "500_600", "summary_sentences": ["The paper presents NewsQA, a machine comprehension dataset of 119,633 natural language questions obtained from 12,744 CNN articles.", "Issues With Existing Datasets  Too small - eg MCTest  USe synthetically generated questions - eg BookTest Dataset  SQuAD is similar to NewsQA but is not as challenging and diverse as NewsQA.", "Desired Characteristics Of A Machine Comprehension Dataset  Answers of arbitrary length instead of candidate answers to choose from.", "Some questions should have no correct answer in the document.", "Lexical and syntactic divergence between questions and answers.", "Questions should require reasoning beyond simple word and context matching.", "Collection Methodology  Article Curation  Retrieve and sample articles from CNN.", "Partition data into a training set (90%), a development set (5%), and a test set (5%).", "Question Sourcing  Questioners see a news article's headline and its summary and use that to formulate questions about the article.", "Answer Sourcing  Answerers receive the questions along with the full article.", "They either mark the answer in the article or reject the question as nonsensical or select null answer if the article contains insufficient information.", "Validation  Another set of crowd workers sees the full article, a question and the set of unique answers to that question.", "They either choose the best answer among the candidates or reject all the answers.", "Data Analysis  Answer Types  Linguistically diverse answer set with following distribution:  common noun phrases (22.2%), clause phrase (18.3%), person (14.8%), numeric (9.8%), and other (11.2%) types.", "Reasoning Types  Type of reasoning required, in ascending order of difficulty, along with approx.", "percentage of questions:  Word Matching (32.7%)  Paraphrasing (27%)  Inference (13.2%)  Synthesis (20.7%)  Ambiguous/Insufficient (6.4%)  Baseline Models  match-LSTM  LSTM network encodes the article and the question as sequences of hidden states.", "mLSTM network compares the article encodings with the question encodings.", "A Pointer Network uses the hidden states of the mLSTM to select the boundaries of the answer span.", "Bilinear Annotation Re-encoding Boundary (BARB) Model  Encode all the words in the articles and the question using GloVe embeddings and further into contextual states using GRU.", "Compare the document and the question encodings using C bilinear transformations to obtain the tensor of annotation scores.", "Take the maximum over the question-token dimension to obtain annotation over document word dimension.", "For each document word, input the document encodings, annotation vector and binary feature (indicating whether the document appears in the question) to the re-encoding RNN and obtain encodings for the boundary-pointing stage.", "Use convolutional networks to determine the boundaries of answer span (similar to edge detection).", ".", "Observations  Gap between human and machine performance on NewsQA is much higher than that for SQuAD probably because of longer sentences in NewsQA.", "This suggests that NewsQA is a far more challenging dataset than SQuAD and presents a large scope for improvement for machine comprehension tasks.", "Questions requiring inference and synthesis are more challenging for the model as compared to other kinds of questions.", "Interestingly, BARB outperforms human annotators on SQuAD in terms of answering ambiguous questions or those with incomplete information."], "summary_text": "The paper presents NewsQA, a machine comprehension dataset of 119,633 natural language questions obtained from 12,744 CNN articles. Issues With Existing Datasets  Too small - eg MCTest  USe synthetically generated questions - eg BookTest Dataset  SQuAD is similar to NewsQA but is not as challenging and diverse as NewsQA. Desired Characteristics Of A Machine Comprehension Dataset  Answers of arbitrary length instead of candidate answers to choose from. Some questions should have no correct answer in the document. Lexical and syntactic divergence between questions and answers. Questions should require reasoning beyond simple word and context matching. Collection Methodology  Article Curation  Retrieve and sample articles from CNN. Partition data into a training set (90%), a development set (5%), and a test set (5%). Question Sourcing  Questioners see a news article's headline and its summary and use that to formulate questions about the article. Answer Sourcing  Answerers receive the questions along with the full article. They either mark the answer in the article or reject the question as nonsensical or select null answer if the article contains insufficient information. Validation  Another set of crowd workers sees the full article, a question and the set of unique answers to that question. They either choose the best answer among the candidates or reject all the answers. Data Analysis  Answer Types  Linguistically diverse answer set with following distribution:  common noun phrases (22.2%), clause phrase (18.3%), person (14.8%), numeric (9.8%), and other (11.2%) types. Reasoning Types  Type of reasoning required, in ascending order of difficulty, along with approx. percentage of questions:  Word Matching (32.7%)  Paraphrasing (27%)  Inference (13.2%)  Synthesis (20.7%)  Ambiguous/Insufficient (6.4%)  Baseline Models  match-LSTM  LSTM network encodes the article and the question as sequences of hidden states. mLSTM network compares the article encodings with the question encodings. A Pointer Network uses the hidden states of the mLSTM to select the boundaries of the answer span. Bilinear Annotation Re-encoding Boundary (BARB) Model  Encode all the words in the articles and the question using GloVe embeddings and further into contextual states using GRU. Compare the document and the question encodings using C bilinear transformations to obtain the tensor of annotation scores. Take the maximum over the question-token dimension to obtain annotation over document word dimension. For each document word, input the document encodings, annotation vector and binary feature (indicating whether the document appears in the question) to the re-encoding RNN and obtain encodings for the boundary-pointing stage. Use convolutional networks to determine the boundaries of answer span (similar to edge detection). . Observations  Gap between human and machine performance on NewsQA is much higher than that for SQuAD probably because of longer sentences in NewsQA. This suggests that NewsQA is a far more challenging dataset than SQuAD and presents a large scope for improvement for machine comprehension tasks. Questions requiring inference and synthesis are more challenging for the model as compared to other kinds of questions. Interestingly, BARB outperforms human annotators on SQuAD in terms of answering ambiguous questions or those with incomplete information.", "pdf_url": "https://arxiv.org/pdf/1611.09830", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/7f0d5c1dfe60ce5da0dd8241e506ea.json"}
{"id": "43280550", "bin": "500_600", "summary_sentences": ["The paper introduces a new, procedurally generated environment called as CoinRun that is designed to benchmark the generalization capabilities of RL algorithms.", "The paper reports that deep convolutional architectures and techniques like L2 regularization, batch norm, etc (which were proposed in the context of generalization in supervised learning) are also useful for RL.", "CoinRun Environment  CoinRun is made of multiple levels.", "In each level, the agent spawns on the far left side and needs to collect a single coin that lies on the far right side.", "There are many obstacles in between and colliding with an obstacle leads to agent’s death.", "Each episode extends for a maximum for 1000 steps.", "CoinRun is designed such that given sufficient training time and levels, a near-optimal policy can be learned for all the levels.", "Generalization  Generalization can be measure by training an agent on a given set of training tasks and evaluating on an unseen set of test tasks.", "9 agents are trained to play CoinRun, on different training sets (each with a different number of levels).", "The first 8 agents are trained on sets of size 100 to 16000 levels while the last agent is trained on an unbounded set of levels.", "Training a model on an unbounded set of levels provides a good proxy for the train-to-test generalization performance.", "Evaluating Architectures  Two convolutional architectures (of different sizes) are compared:  Nature-CNN: The CNN architecture used in the Deep Q Network .", "This is the smaller network among the two models.", "IMPALA-CNN: The CNN architecture used in the Imapla architecture .", "IMPALA-CNN agent always outperforms the Nature-CNN agent indicating that larger architecture has more capacity for generalization.", "But increasing the network size beyond a limit gives diminishing returns.", "Evaluating Regularization  While both L2 regularization and Dropout helps to improve generalization, L2 regularization is more impactful.", "A domain randomization/data augmentation approach is tested where rectangular regions of different sizes are masked and assigned a random color.", "This approach seems to improve performance.", "Batch Normalization helps to improve performance as well.", "Environment stochasticity is introduced by using sticky actions while policy stochasticity is introduced by controlling the entropy bonus.", "Both these forms of stochasticity boost performance.", "While combining different regularization methods help, the gains are only marginally better than using just 1 regularization approach.", "This suggests that these different approaches induce similar generalization properties.", "Additional Environments  Two additional environments are also considered to verify the high degree of overfitting observed in the CoinRun environment:  CoinRun-Platforms:  Unlike CoinRun, each episode can have multiple coins and the time limit is 0increased to 1000 steps.", "Levels are larger as well so the agent might need to backtrack their steps.", "RandomMazes:  Partially observed environment with square mazes of dimensions 3x3 to 25x25.", "Timelimit of 500 steps  Overfitting is observed for both these environments as well."], "summary_text": "The paper introduces a new, procedurally generated environment called as CoinRun that is designed to benchmark the generalization capabilities of RL algorithms. The paper reports that deep convolutional architectures and techniques like L2 regularization, batch norm, etc (which were proposed in the context of generalization in supervised learning) are also useful for RL. CoinRun Environment  CoinRun is made of multiple levels. In each level, the agent spawns on the far left side and needs to collect a single coin that lies on the far right side. There are many obstacles in between and colliding with an obstacle leads to agent’s death. Each episode extends for a maximum for 1000 steps. CoinRun is designed such that given sufficient training time and levels, a near-optimal policy can be learned for all the levels. Generalization  Generalization can be measure by training an agent on a given set of training tasks and evaluating on an unseen set of test tasks. 9 agents are trained to play CoinRun, on different training sets (each with a different number of levels). The first 8 agents are trained on sets of size 100 to 16000 levels while the last agent is trained on an unbounded set of levels. Training a model on an unbounded set of levels provides a good proxy for the train-to-test generalization performance. Evaluating Architectures  Two convolutional architectures (of different sizes) are compared:  Nature-CNN: The CNN architecture used in the Deep Q Network . This is the smaller network among the two models. IMPALA-CNN: The CNN architecture used in the Imapla architecture . IMPALA-CNN agent always outperforms the Nature-CNN agent indicating that larger architecture has more capacity for generalization. But increasing the network size beyond a limit gives diminishing returns. Evaluating Regularization  While both L2 regularization and Dropout helps to improve generalization, L2 regularization is more impactful. A domain randomization/data augmentation approach is tested where rectangular regions of different sizes are masked and assigned a random color. This approach seems to improve performance. Batch Normalization helps to improve performance as well. Environment stochasticity is introduced by using sticky actions while policy stochasticity is introduced by controlling the entropy bonus. Both these forms of stochasticity boost performance. While combining different regularization methods help, the gains are only marginally better than using just 1 regularization approach. This suggests that these different approaches induce similar generalization properties. Additional Environments  Two additional environments are also considered to verify the high degree of overfitting observed in the CoinRun environment:  CoinRun-Platforms:  Unlike CoinRun, each episode can have multiple coins and the time limit is 0increased to 1000 steps. Levels are larger as well so the agent might need to backtrack their steps. RandomMazes:  Partially observed environment with square mazes of dimensions 3x3 to 25x25. Timelimit of 500 steps  Overfitting is observed for both these environments as well.", "pdf_url": "https://arxiv.org/pdf/1812.02341", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/quantifying-generalization-in-reinforcement-learning.json"}
{"id": "85853580", "bin": "500_600", "summary_sentences": ["This paper starts by introducing a trick to reduce the variance of stochastic gradient variational Bayes (SGVB) estimators.", "In neural networks, SGVB consists in learning a variational (e.g. diagonal Gaussian) posterior over the weights and biases of neural networks, through a procedure that (for the most part) alternates between adding (Gaussian) noise to the model's parameters and then performing a model update with backprop.", "The authors present a local reparameterization trick, which exploits the fact that the Gaussian noise added into the weights could instead be added directly into the pre-activation (i.e. before the activation fonction) vectors during forward propagation.", "This is due to the fact that computing the pre-activation is a linear operation, thus noise at that level is also Gaussian.", "The advantage of doing so is that, in the context of minibatch training, one can efficiently then add independent noise to the pre-activation vectors for each example of the minibatch.", "The nature of the local reparameterization trick implies that this is equivalent to using one corrupted version of the weights for each example in the minibatch, something that wouldn't be practical computationally otherwise.", "This is in fact why, in normal SGVB, previous work would normally use a single corrupted version of the weights for all the minibatch.", "The authors demonstrate that using the local reparameterization trick yields stochastic gradients with lower variance, which should improve the speed of convergence.", "Then, the authors demonstrate that the Gaussian version of dropout (one that uses multiplicative Gaussian noise, instead of 0-1 masking noise) can be seen as the local reparameterization trick version of a SGVB objective, with some specific prior and variational posterior.", "In this SGVB view of Gaussian dropout, the dropout rate is an hyper-parameter of this prior, which can now be tuned by optimizing the variational lower bound of SGVB.", "In other words, we now have a method to also train the dropout rate!", "Moreover, it becomes possible to tune an individual dropout rate parameter for each layer, or even each parameter of the model.", "Experiments on MNIST confirm that tuning that parameter works and allows to reach good performance of various network sizes, compared to using a default dropout rate.", "##### My two cents  This is another thought provoking connection between Bayesian learning and dropout.", "Indeed, while Deep GPs have allowed to make a Bayesian connection with regular (binary) dropout learning  [ref] , this paper sheds light on a neat Bayesian connection for the Gaussian version of dropout.", "This is great, because it suggests that Gaussian dropout training is another legit way of modeling uncertainty in the parameters of neural networks.", "It's also nice that that connection also yielded a method for tuning the dropout rate automatically.", "I hope future work (by the authors or by others) can evaluate the quality of the corresponding variational posterior in terms of estimating uncertainty in the network and, in particular, in obtaining calibrated output probabilities.", "Little detail: I couldn't figure out whether the authors tuned a single dropout rate for the whole network, or used many rates, for instance one per parameter, as they suggest can be done."], "summary_text": "This paper starts by introducing a trick to reduce the variance of stochastic gradient variational Bayes (SGVB) estimators. In neural networks, SGVB consists in learning a variational (e.g. diagonal Gaussian) posterior over the weights and biases of neural networks, through a procedure that (for the most part) alternates between adding (Gaussian) noise to the model's parameters and then performing a model update with backprop. The authors present a local reparameterization trick, which exploits the fact that the Gaussian noise added into the weights could instead be added directly into the pre-activation (i.e. before the activation fonction) vectors during forward propagation. This is due to the fact that computing the pre-activation is a linear operation, thus noise at that level is also Gaussian. The advantage of doing so is that, in the context of minibatch training, one can efficiently then add independent noise to the pre-activation vectors for each example of the minibatch. The nature of the local reparameterization trick implies that this is equivalent to using one corrupted version of the weights for each example in the minibatch, something that wouldn't be practical computationally otherwise. This is in fact why, in normal SGVB, previous work would normally use a single corrupted version of the weights for all the minibatch. The authors demonstrate that using the local reparameterization trick yields stochastic gradients with lower variance, which should improve the speed of convergence. Then, the authors demonstrate that the Gaussian version of dropout (one that uses multiplicative Gaussian noise, instead of 0-1 masking noise) can be seen as the local reparameterization trick version of a SGVB objective, with some specific prior and variational posterior. In this SGVB view of Gaussian dropout, the dropout rate is an hyper-parameter of this prior, which can now be tuned by optimizing the variational lower bound of SGVB. In other words, we now have a method to also train the dropout rate! Moreover, it becomes possible to tune an individual dropout rate parameter for each layer, or even each parameter of the model. Experiments on MNIST confirm that tuning that parameter works and allows to reach good performance of various network sizes, compared to using a default dropout rate. ##### My two cents  This is another thought provoking connection between Bayesian learning and dropout. Indeed, while Deep GPs have allowed to make a Bayesian connection with regular (binary) dropout learning  [ref] , this paper sheds light on a neat Bayesian connection for the Gaussian version of dropout. This is great, because it suggests that Gaussian dropout training is another legit way of modeling uncertainty in the parameters of neural networks. It's also nice that that connection also yielded a method for tuning the dropout rate automatically. I hope future work (by the authors or by others) can evaluate the quality of the corresponding variational posterior in terms of estimating uncertainty in the network and, in particular, in obtaining calibrated output probabilities. Little detail: I couldn't figure out whether the authors tuned a single dropout rate for the whole network, or used many rates, for instance one per parameter, as they suggest can be done.", "pdf_url": "http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/blumhp15.json"}
{"id": "89767636", "bin": "500_600", "summary_sentences": ["LegoOS: a disseminated, distributed OS for hardware resource disaggregation Shan et al., OSDI’18  One of the interesting trends in hardware is the proliferation and importance of dedicated accelerators as general purposes CPUs stopped benefitting from Moore’s law.", "At the same time we’ve seen networking getting faster and faster, causing us to rethink some of the trade-offs between local I/O and network access.", "The monolithic server as the unit of packaging for collections of such devices is starting to look less attractive:  It leads to inefficient resource utilisation, since CPU and memory for a job have to be allocated from the same machine.", "This can lead to eviction even when utilisation is overall low (e.g. 50%).", "It is difficult to add, move, remove, or reconfigure hardware components after they have been installed in a server, leading to long up-front planning cycles for hardware rollouts at odds with the fast-moving rate of change in requirements.", "It creates a coarse failure domain – when any hardware component within a server fails, the whole server is often unusable.", "It doesn’t work well with heterogeneous devices and their rollout: e.g. GPGPUs, TPUs, DPUs, FGPAs, NVM, and NVMe-based SSDs.", "To fully support the growing heterogeneity in hardware and to provide elasticity and flexibility at the hardware level, we should break the monolithic server model.", "We envision a hardware resource disaggregation model where hardware resources in traditional servers are disseminated into network-attached hardware components.", "Each component has a controller and a network interface, can operate on its own, and is an independent, failure-isolated entity.", "Hardware resource disaggregation is enabled by three hardware trends:  Rapidly growing networks speeds (e.g. 200 Gbps InfiniBand will be only 2-4x slower than the main memory bus in bandwidth).", "“With the main memory bus facing a bandwidth wall, future network bandwith (at line rate) is even projected to exceed local DRAM bandwidth” (just think about how many design assumptions that turns on their heads!).", "Network technologies such as Intel OmniPath, RDMA, and NVMe over Fabrics enable hardware devices to access the network directly without the need to attached any process.", "Hardware devices are incorporating more processing power, making it possible to offload more application and OS functionality to them.", "From a hardware perspective this seems to open up a bunch of exciting possibilities.", "But what on earth does an operating system look like in such a world?", "That’s the question this paper sets out to address, with the design of LegoOS.", "LegoOS distributes operating system functions across a collection of loosely-coupled monitors, each of which runs and manages a hardware component.", "The initial implementation goes after the big three: processing, memory, and storage.", "Yes, that does mean that processor and memory are separated over the network!", "The biggest challenge and our focus in building LegoOS is the separation of processor and memory and their management.", "Modern processors and OSes assume all hardware memory units including main memory, page tables, and TLB are local.", "Simply moving all memory hardware and memory management software to across the network will not work.", "LegoOS is available at  [url]"], "summary_text": "LegoOS: a disseminated, distributed OS for hardware resource disaggregation Shan et al., OSDI’18  One of the interesting trends in hardware is the proliferation and importance of dedicated accelerators as general purposes CPUs stopped benefitting from Moore’s law. At the same time we’ve seen networking getting faster and faster, causing us to rethink some of the trade-offs between local I/O and network access. The monolithic server as the unit of packaging for collections of such devices is starting to look less attractive:  It leads to inefficient resource utilisation, since CPU and memory for a job have to be allocated from the same machine. This can lead to eviction even when utilisation is overall low (e.g. 50%). It is difficult to add, move, remove, or reconfigure hardware components after they have been installed in a server, leading to long up-front planning cycles for hardware rollouts at odds with the fast-moving rate of change in requirements. It creates a coarse failure domain – when any hardware component within a server fails, the whole server is often unusable. It doesn’t work well with heterogeneous devices and their rollout: e.g. GPGPUs, TPUs, DPUs, FGPAs, NVM, and NVMe-based SSDs. To fully support the growing heterogeneity in hardware and to provide elasticity and flexibility at the hardware level, we should break the monolithic server model. We envision a hardware resource disaggregation model where hardware resources in traditional servers are disseminated into network-attached hardware components. Each component has a controller and a network interface, can operate on its own, and is an independent, failure-isolated entity. Hardware resource disaggregation is enabled by three hardware trends:  Rapidly growing networks speeds (e.g. 200 Gbps InfiniBand will be only 2-4x slower than the main memory bus in bandwidth). “With the main memory bus facing a bandwidth wall, future network bandwith (at line rate) is even projected to exceed local DRAM bandwidth” (just think about how many design assumptions that turns on their heads!). Network technologies such as Intel OmniPath, RDMA, and NVMe over Fabrics enable hardware devices to access the network directly without the need to attached any process. Hardware devices are incorporating more processing power, making it possible to offload more application and OS functionality to them. From a hardware perspective this seems to open up a bunch of exciting possibilities. But what on earth does an operating system look like in such a world? That’s the question this paper sets out to address, with the design of LegoOS. LegoOS distributes operating system functions across a collection of loosely-coupled monitors, each of which runs and manages a hardware component. The initial implementation goes after the big three: processing, memory, and storage. Yes, that does mean that processor and memory are separated over the network! The biggest challenge and our focus in building LegoOS is the separation of processor and memory and their management. Modern processors and OSes assume all hardware memory units including main memory, page tables, and TLB are local. Simply moving all memory hardware and memory management software to across the network will not work. LegoOS is available at  [url]", "pdf_url": "https://www.usenix.org/system/files/osdi18-shan.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/legoos-a-disseminated-distributed-os-for-hardware-resource-disaggregation.json"}
{"id": "4728464", "bin": "500_600", "summary_sentences": ["What  They describe a method that applies the style of a source image to a target image.", "Example: Let a normal photo look like a van Gogh painting.", "Example: Let a normal car look more like a specific luxury car.", "Their method builds upon the well known artistic style paper and uses a new MRF prior.", "The prior leads to locally more plausible patterns (e.g. less artifacts).", "How  They reuse the content loss from the artistic style paper .", "The content loss was calculated by feed the source and target image through a network (here: VGG19) and then estimating the squared error of the euclidean distance between one or more hidden layer activations.", "They use layer relu4_2 for the distance measurement.", "They replace the original style loss with a MRF based style loss.", "Step 1: Extract from the source image k x k sized overlapping patches.", "Step 2: Perform step (1) analogously for the target image.", "Step 3: Feed the source image patches through a pretrained network (here: VGG19) and select the representations r_s from specific hidden layers (here: relu3_1, relu4_1).", "Step 4: Perform step (3) analogously for the target image.", "(Result: r_t)  Step 5: For each patch of r_s find the best matching patch in r_t (based on normalized cross correlation).", "Step 6: Calculate the sum of squared errors (based on euclidean distances) of each patch in r_s and its best match (according to step 5).", "They add a regularizer loss.", "The loss encourages smooth transitions in the synthesized image (i.e. few edges, corners).", "It is based on the raw pixel values of the last synthesized image.", "For each pixel in the synthesized image, they calculate the squared x-gradient and the squared y-gradient and then add both.", "They use the sum of all those values as their loss (i.e. regularizer loss = <sum over all pixels> x-gradient^2 + y-gradient^2).", "Their whole optimization problem is then roughly image = argmin_image MRF-style-loss + alpha1 * content-loss + alpha2 * regularizer-loss.", "In practice, they start their synthesis with a low resolution image and then progressively increase the resolution (each time performing some iterations of optimization).", "In practice, they sample patches from the style image under several different rotations and scalings.", "Results  In comparison to the original artistic style paper:  Less artifacts.", "Their method tends to preserve style better, but content worse.", "Can handle photorealistic style transfer better, so long as the images are similar enough.", "If no good matches between patches can be found, their method performs worse.", "Non-photorealistic example images.", "Their method vs. the one from the original artistic style paper.", "Photorealistic example images.", "Their method vs. the one from the original artistic style paper."], "summary_text": "What  They describe a method that applies the style of a source image to a target image. Example: Let a normal photo look like a van Gogh painting. Example: Let a normal car look more like a specific luxury car. Their method builds upon the well known artistic style paper and uses a new MRF prior. The prior leads to locally more plausible patterns (e.g. less artifacts). How  They reuse the content loss from the artistic style paper . The content loss was calculated by feed the source and target image through a network (here: VGG19) and then estimating the squared error of the euclidean distance between one or more hidden layer activations. They use layer relu4_2 for the distance measurement. They replace the original style loss with a MRF based style loss. Step 1: Extract from the source image k x k sized overlapping patches. Step 2: Perform step (1) analogously for the target image. Step 3: Feed the source image patches through a pretrained network (here: VGG19) and select the representations r_s from specific hidden layers (here: relu3_1, relu4_1). Step 4: Perform step (3) analogously for the target image. (Result: r_t)  Step 5: For each patch of r_s find the best matching patch in r_t (based on normalized cross correlation). Step 6: Calculate the sum of squared errors (based on euclidean distances) of each patch in r_s and its best match (according to step 5). They add a regularizer loss. The loss encourages smooth transitions in the synthesized image (i.e. few edges, corners). It is based on the raw pixel values of the last synthesized image. For each pixel in the synthesized image, they calculate the squared x-gradient and the squared y-gradient and then add both. They use the sum of all those values as their loss (i.e. regularizer loss = <sum over all pixels> x-gradient^2 + y-gradient^2). Their whole optimization problem is then roughly image = argmin_image MRF-style-loss + alpha1 * content-loss + alpha2 * regularizer-loss. In practice, they start their synthesis with a low resolution image and then progressively increase the resolution (each time performing some iterations of optimization). In practice, they sample patches from the style image under several different rotations and scalings. Results  In comparison to the original artistic style paper:  Less artifacts. Their method tends to preserve style better, but content worse. Can handle photorealistic style transfer better, so long as the images are similar enough. If no good matches between patches can be found, their method performs worse. Non-photorealistic example images. Their method vs. the one from the original artistic style paper. Photorealistic example images. Their method vs. the one from the original artistic style paper.", "pdf_url": "http://arxiv.org/pdf/1601.04589", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/combining_mrfs_and_cnns_for_image_synthesis.json"}
{"id": "4784102", "bin": "500_600", "summary_sentences": ["Moment-based quantile sketches for efficient high cardinality aggregation queries Gan et al., VLDB’18  Today we’re temporarily pausing our tour through some of the OSDI’18 papers in order to look at a great sketch-based data structure for quantile queries over high-cardinality aggregates.", "That’s a bit of a mouthful so let’s jump straight into an example of the problem at hand.", "Say you have telemetry data from millions of heterogenous mobile devices running your app.", "Each device tracks multiple metrics such as request latency and memory usage, and is associated with dimensional metadata (categorical variables) such as application version and hardware model.", "In applications such as A/B testing, exploratory data analysis, and operations monitoring, analysts perform aggregation queries to understand how specific user cohorts, device types, and feature flags are behaving.", "We want to be able to ask questions like “what’s the 99%-ile latency over the last two weeks for v8.2 of the app?”  SELECT percentile(latency, 99) FROM requests WHERE time > date_sub(curdate(), 2 WEEK) AND app_version = \"v8.2\"  As well as threshold queries such as “what combinations of app version and hardware platform have a 99th percentile latency exceeding 100ms?”  SELECT app_version, hw_model, PERCENTILE(latency, 99) as p99 FROM requests GROUP BY app_version, hw_model HAVING p99 > 100  Instead of starting from raw data every time when answering this type of query, OLAP engines can reduce query time and memory usage by maintaining a data cube of pre-aggregated summaries for each tuple of dimension values.", "The ultimate query performance then depends on just how quickly we can merge those summaries to compute quantile roll-ups over the requested dimensions.", "Let’s take a very simple example.", "Suppose I have two dimensions, letter (with values A and B), and colour (with values red and green), and I have request latency data from log messages including these attributes.", "Then I will have four summary sketches, one accumulating latency values for (A, red) one for (A, green), one for (B, red) and one for (B, green).", "If a query wants to know the P99 latency for ‘red’ requests, we can add together the (A, red) and (B, red) sketches to get a complete sketch for red.", "In this paper, we enable interactive quantile queries over high-cardinality aggregates by introducing a compact and efficiently mergeable quantile sketch and associated quantile estimation routines.", "The data structure than makes all this possible is called a moments sketch (named after the method of moments statistical technique).", "It’s easy to construct, but a bit more difficult to interpret.", "It’s worth the effort though, as the evaluation shows:  The moments sketch supports 15-50x faster query times that comparably accurate summaries on quantile aggregations  The moments sketch gives good accuracy across a range of real-world datasets using less than 200 bytes of storage  Integration of the moments sketch in Druid provides 7x faster quantile queries than the default quantile summary in Druid workloads.", "There’s a Java implementation available at  [url]"], "summary_text": "Moment-based quantile sketches for efficient high cardinality aggregation queries Gan et al., VLDB’18  Today we’re temporarily pausing our tour through some of the OSDI’18 papers in order to look at a great sketch-based data structure for quantile queries over high-cardinality aggregates. That’s a bit of a mouthful so let’s jump straight into an example of the problem at hand. Say you have telemetry data from millions of heterogenous mobile devices running your app. Each device tracks multiple metrics such as request latency and memory usage, and is associated with dimensional metadata (categorical variables) such as application version and hardware model. In applications such as A/B testing, exploratory data analysis, and operations monitoring, analysts perform aggregation queries to understand how specific user cohorts, device types, and feature flags are behaving. We want to be able to ask questions like “what’s the 99%-ile latency over the last two weeks for v8.2 of the app?”  SELECT percentile(latency, 99) FROM requests WHERE time > date_sub(curdate(), 2 WEEK) AND app_version = \"v8.2\"  As well as threshold queries such as “what combinations of app version and hardware platform have a 99th percentile latency exceeding 100ms?”  SELECT app_version, hw_model, PERCENTILE(latency, 99) as p99 FROM requests GROUP BY app_version, hw_model HAVING p99 > 100  Instead of starting from raw data every time when answering this type of query, OLAP engines can reduce query time and memory usage by maintaining a data cube of pre-aggregated summaries for each tuple of dimension values. The ultimate query performance then depends on just how quickly we can merge those summaries to compute quantile roll-ups over the requested dimensions. Let’s take a very simple example. Suppose I have two dimensions, letter (with values A and B), and colour (with values red and green), and I have request latency data from log messages including these attributes. Then I will have four summary sketches, one accumulating latency values for (A, red) one for (A, green), one for (B, red) and one for (B, green). If a query wants to know the P99 latency for ‘red’ requests, we can add together the (A, red) and (B, red) sketches to get a complete sketch for red. In this paper, we enable interactive quantile queries over high-cardinality aggregates by introducing a compact and efficiently mergeable quantile sketch and associated quantile estimation routines. The data structure than makes all this possible is called a moments sketch (named after the method of moments statistical technique). It’s easy to construct, but a bit more difficult to interpret. It’s worth the effort though, as the evaluation shows:  The moments sketch supports 15-50x faster query times that comparably accurate summaries on quantile aggregations  The moments sketch gives good accuracy across a range of real-world datasets using less than 200 bytes of storage  Integration of the moments sketch in Druid provides 7x faster quantile queries than the default quantile summary in Druid workloads. There’s a Java implementation available at  [url]", "pdf_url": "http://www.bailis.org/papers/moments-vldb2018.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/moment-based-quantile-sketches-for-efficient-high-cardinality-aggregation-queries.json"}
{"id": "76817909", "bin": "500_600", "summary_sentences": ["BabyAI is a research platform to investigate and support the feasibility of including humans in the loop for grounded language learning.", "The setup is a series of levels (of increasing difficulty) to train the agent to acquire a synthetic language (Baby Language) which is a proper subset of English language.", "Motivation  BabyAI platform provides support for curriculum learning and interactive learning as part of its human-in-the-loop training setup.", "Curriculum learning is incorporated by having a curriculum of levels of increasing difficulty.", "Interactive learning is supported by including a heuristic expert which can provide new demonstrations on the fly to the learning agent.", "The heuristic expert can be thought of as the human-in-the-loop which can guide the agent through the learning process.", "One downside of human-in-the-loop is the poor sample complexity of the learning agent.", "The heuristic agent can be used to estimate the sample  efficiency.", "Contribution  BabyAI research platform for grounded language learning with a simulated human-in-the-loop.", "Baseline results for performance and sample efficiency for the different tasks.", "BabyAI Platform  Environment  MiniGrid - A partially observable 2D grid-world environment.", "Entities - Agent, ball, box, door, keys  Actions - pick, drop or move objects, unlock doors etc.", "Baby Language  Synthetic Language (a proper subset of English) - Used to give instructions to the agent  Support for verifying if the task (and the subtasks) are completed or not  Levels  A level is an instruction-following task.", "Formally, a level is a distribution of missions - a combination of initial state of the environment and an instruction (in Baby Language)  Motivated by curriculum learning, the authors create a series of tasks (with increasing difficulty).", "A subset of skills (competencies) is required for solving each task.", "The platform takes into account this constraint when creating a level.", "Heuristic Expert  The platform supports a Heuristic expert that simulates the role of a human teacher and knows how to solve each task.", "For any level, it can suggest actions or generate demonstrations (given the state of the environment).", "Experiment  An imitation learning baseline is trained for each level.", "Data requirement for each level and the benefits of curriculum learning and imitation learning are investigated (in terms of sample efficiency).", "Model Architecture  GRU to encode the sentence, CNN to encode the input observation  FiLM layer to combine the two representations  LSTM to encode the per-timestep FiLM encoding (timesteps in the environment)  Two model variants are considered:  Large Model - Bidirectional GRU + attention + large hidden state  Small Model - Unidirectional GRU + No attention + small hidden state  Heuristic expert used to generate trajectory and the models are trained by imitation learning (to be used as baselines)  Results  The key takeaway is that the current deep learning approaches are extremely sample inefficient when learning a compositional language.", "Data efficiency of RL methods is much worse than that of imitation learning methods showing that the current imitation learning and reinforcement learning methods scale and generalize poorly.", "Curriculum-based pretraining and interactive learning was found to be useful in only some cases."], "summary_text": "BabyAI is a research platform to investigate and support the feasibility of including humans in the loop for grounded language learning. The setup is a series of levels (of increasing difficulty) to train the agent to acquire a synthetic language (Baby Language) which is a proper subset of English language. Motivation  BabyAI platform provides support for curriculum learning and interactive learning as part of its human-in-the-loop training setup. Curriculum learning is incorporated by having a curriculum of levels of increasing difficulty. Interactive learning is supported by including a heuristic expert which can provide new demonstrations on the fly to the learning agent. The heuristic expert can be thought of as the human-in-the-loop which can guide the agent through the learning process. One downside of human-in-the-loop is the poor sample complexity of the learning agent. The heuristic agent can be used to estimate the sample  efficiency. Contribution  BabyAI research platform for grounded language learning with a simulated human-in-the-loop. Baseline results for performance and sample efficiency for the different tasks. BabyAI Platform  Environment  MiniGrid - A partially observable 2D grid-world environment. Entities - Agent, ball, box, door, keys  Actions - pick, drop or move objects, unlock doors etc. Baby Language  Synthetic Language (a proper subset of English) - Used to give instructions to the agent  Support for verifying if the task (and the subtasks) are completed or not  Levels  A level is an instruction-following task. Formally, a level is a distribution of missions - a combination of initial state of the environment and an instruction (in Baby Language)  Motivated by curriculum learning, the authors create a series of tasks (with increasing difficulty). A subset of skills (competencies) is required for solving each task. The platform takes into account this constraint when creating a level. Heuristic Expert  The platform supports a Heuristic expert that simulates the role of a human teacher and knows how to solve each task. For any level, it can suggest actions or generate demonstrations (given the state of the environment). Experiment  An imitation learning baseline is trained for each level. Data requirement for each level and the benefits of curriculum learning and imitation learning are investigated (in terms of sample efficiency). Model Architecture  GRU to encode the sentence, CNN to encode the input observation  FiLM layer to combine the two representations  LSTM to encode the per-timestep FiLM encoding (timesteps in the environment)  Two model variants are considered:  Large Model - Bidirectional GRU + attention + large hidden state  Small Model - Unidirectional GRU + No attention + small hidden state  Heuristic expert used to generate trajectory and the models are trained by imitation learning (to be used as baselines)  Results  The key takeaway is that the current deep learning approaches are extremely sample inefficient when learning a compositional language. Data efficiency of RL methods is much worse than that of imitation learning methods showing that the current imitation learning and reinforcement learning methods scale and generalize poorly. Curriculum-based pretraining and interactive learning was found to be useful in only some cases.", "pdf_url": "https://arxiv.org/pdf/1810.08272", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/babyai-first-steps-towards-grounded-language-learning-with-a-human-in-the-loop.json"}
{"id": "32248697", "bin": "500_600", "summary_sentences": ["Much of the work in representation leaning uses Euclidean vector spaces to embed datapoints (like words, nodes, entities etc).", "This approach is not effective when data has a (latent) hierarchical structure.", "The paper proposes to compute the embeddings in the hyperbolic space so as to preserve both the similarity and structure information.", "Hyperbolic Geometry  Hyperbolic spaces are spaces with a constant negative curvature while Euclidean spaces have zero curvature.", "The hyperbolic disc area and circle length increase exponentially with the radius r while in Euclidean space, it increases quadratically and linearly respectively.", "This makes the hyperbolic space more suitable for embedding tree-like structures where the number of nodes increases as we move away from the root.", "Hyperbolic spaces can be thought of as the continuous version of trees and trees can be thought of as the discrete version of hyperbolic spaces.", "Poincare Embeddings  Poincare model is one of the several possible models of the hyperbolic space and is considered here as it is more amenable to gradient-based optimisation.", "Distance between 2 pints change smoothly and is symmetric.", "Thus the hierarchical organisation only depends on the distance from the origin which makes the model applicable in settings where the hierarchical structure needs to be inferred from the data.", "Eventually the norm of a point represents its hierarchy and distance between the points represents similarity.", "Optimization  RSGD (Riemannian SGD) method is used.", "Riemannian gradients can be computed from the Euclidean gradients by rescaling with the inverse of the Poincare ball metric tensor.", "The embeddings are constrained to be within the Poincare ball by projection operation which normalizes the magnitude of embeddings to be 1.", "Training Details  Initializing the embeddings close to 0 (by sampling uniformly from (-0.001, 0.001)) helps.", "The model is trained for an initial burn-out period of 10 epochs with 0.1 times the learning rate so as to find a better initial angular layout.", "Evaluation  Embedding taxonomy for wordnet task  Setup  Reconstruction  Link Prediction  The input data is a collection of a pair of words (u, v) which are related to each other.", "For each word pair, 10 negative samples of the form (u, v’) are sampled and the training procedure uses a soft ranking loss that aims to bring the related objects closer together.", "Network Embedding  Baselines  Euclidean Embeddings  Translational Embedding where a relation vector corresponding to the edge type is also learnt.", "Datasets  ASTROPH  CONDMAT  GRQC  HEPPH  Lexical Entailment  * Hyperlex - Gold standard to evaluate how well the semantics models capture lexical entailment on a scale of [0, 10].", "* The key takeaway is that for all the datasets/setups, hyperbolic embeddings give a performance benefit when the embedding dimension is small.", "Challenges  Hyperbolic embeddings are not suitable for all the datasets.", "Eg if the dataset is not tree-like or has cycles.", "Hyperbolic embeddings are difficult to optimize as each operation needs to be modified to be usable in the hyperbolic space."], "summary_text": "Much of the work in representation leaning uses Euclidean vector spaces to embed datapoints (like words, nodes, entities etc). This approach is not effective when data has a (latent) hierarchical structure. The paper proposes to compute the embeddings in the hyperbolic space so as to preserve both the similarity and structure information. Hyperbolic Geometry  Hyperbolic spaces are spaces with a constant negative curvature while Euclidean spaces have zero curvature. The hyperbolic disc area and circle length increase exponentially with the radius r while in Euclidean space, it increases quadratically and linearly respectively. This makes the hyperbolic space more suitable for embedding tree-like structures where the number of nodes increases as we move away from the root. Hyperbolic spaces can be thought of as the continuous version of trees and trees can be thought of as the discrete version of hyperbolic spaces. Poincare Embeddings  Poincare model is one of the several possible models of the hyperbolic space and is considered here as it is more amenable to gradient-based optimisation. Distance between 2 pints change smoothly and is symmetric. Thus the hierarchical organisation only depends on the distance from the origin which makes the model applicable in settings where the hierarchical structure needs to be inferred from the data. Eventually the norm of a point represents its hierarchy and distance between the points represents similarity. Optimization  RSGD (Riemannian SGD) method is used. Riemannian gradients can be computed from the Euclidean gradients by rescaling with the inverse of the Poincare ball metric tensor. The embeddings are constrained to be within the Poincare ball by projection operation which normalizes the magnitude of embeddings to be 1. Training Details  Initializing the embeddings close to 0 (by sampling uniformly from (-0.001, 0.001)) helps. The model is trained for an initial burn-out period of 10 epochs with 0.1 times the learning rate so as to find a better initial angular layout. Evaluation  Embedding taxonomy for wordnet task  Setup  Reconstruction  Link Prediction  The input data is a collection of a pair of words (u, v) which are related to each other. For each word pair, 10 negative samples of the form (u, v’) are sampled and the training procedure uses a soft ranking loss that aims to bring the related objects closer together. Network Embedding  Baselines  Euclidean Embeddings  Translational Embedding where a relation vector corresponding to the edge type is also learnt. Datasets  ASTROPH  CONDMAT  GRQC  HEPPH  Lexical Entailment  * Hyperlex - Gold standard to evaluate how well the semantics models capture lexical entailment on a scale of [0, 10]. * The key takeaway is that for all the datasets/setups, hyperbolic embeddings give a performance benefit when the embedding dimension is small. Challenges  Hyperbolic embeddings are not suitable for all the datasets. Eg if the dataset is not tree-like or has cycles. Hyperbolic embeddings are difficult to optimize as each operation needs to be modified to be usable in the hyperbolic space.", "pdf_url": "https://arxiv.org/pdf/1705.08039.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/poincare-embeddings-for-learning-hierarchical-representations.json"}
{"id": "17409856", "bin": "500_600", "summary_sentences": ["The paper presents a novel open vocabulary NMT(Neural Machine Translation) system that translates mostly at word level and falls back to character level models for rare words.", "Advantages:  Faster and easier to train as compared to character models.", "Does not produce unknown words in the translations which need to be removed using unk replacement techniques.", "Unk Replacement Technique  Most NMT operate on constrained vocabulary and represent unknown words with unk token.", "A post-processing step replaces unk tokens with actual words using alignment information.", "Disadvantages:  These systems treat words as independent entities while they are morphologically related.", "Difficult to capture things like name translation.", "Proposed Architecture  Word-level NMT  Deep LSTM encoder-decoder.", "Global attention mechanism and bilinear attention scoring function.", "Similar to regular NMT system except in the way unknown words are handled.", "Character-level NMT  Deep LSTM model used to generate on-the-fly representation of rare words (using final hidden state from the top layer).", "Advantages:  Simplified architecture.", "Efficiency through precomputation - representations for rare sources words can be computed at once before each mini-batch.", "The model can be trained easily in an end-to-end fashion.", "Hidden-state Initialization  For source representation, layers of the LSTM are initialized with zero hidden states and cell values.", "For target representation, the same strategy is followed except for the hidden state of the first layer where one of the following approaches are used:  same-path target generation approach  Use the context vector just before softmax (of word-level NMT).", "seperate-path target generation approach  Learn a new weight matrix W that will be used to generate the context vector.", "Training Objective  J = Jw + αJc  J - total loss  Jw - loss in a regular word-level NMT  αJc - loss in the character-level NMT  Word Character Generation Strategy  The final hidden state from character-level decoder could be interpreted as the representation of unk token but this approach would not be efficient.", "Instead, unk is fed to the word-level decoder as it is so as to decouple the execution for the character-level model as soon the word-level model finishes.", "During testing, a beam search decoder is run at the word level to find the best translation using the word NMT alone.", "Next, a character-level encoder is used to generate the words in place of unk to minimise the combined loss.", "Experiments  Data  WMT’15 translation task from English into Czech with newstest2013 (3000 sentences) as dev set and newstest2015 (2656 sentences) as a test set.", "Metrics  Case-sensitive NIST BLEU.", "chrF3  Models  Purely word based  Purely character based  Hybrid (proposed model)  Observations  Hybrid model surpasses all the other systems (neural/non-neural) and establishes a new state-of-the-art result for English-Czech translation in WMT’15 with 19.9 BLEU.", "Character-level models, when used as a replacement for the standard unk replacement technique in NMT, yields an improvement of up to +7.9 BLEU points.", "Attention is very important for character-based models as the non-attentional character models perform poorly.", "Character models with shorter time-step backpropagation perform inferior as compared to ones with longer backpropagation.", "Separate-path strategy outperforms same-path strategy.", "Rare word embeddings  Obtain representations for rare words.", "Compare the Spearman correlation between similarity scores assigned by humans and by the model.", "Outperforms the recursive neural network model (which also uses a morphological analyser) on this task."], "summary_text": "The paper presents a novel open vocabulary NMT(Neural Machine Translation) system that translates mostly at word level and falls back to character level models for rare words. Advantages:  Faster and easier to train as compared to character models. Does not produce unknown words in the translations which need to be removed using unk replacement techniques. Unk Replacement Technique  Most NMT operate on constrained vocabulary and represent unknown words with unk token. A post-processing step replaces unk tokens with actual words using alignment information. Disadvantages:  These systems treat words as independent entities while they are morphologically related. Difficult to capture things like name translation. Proposed Architecture  Word-level NMT  Deep LSTM encoder-decoder. Global attention mechanism and bilinear attention scoring function. Similar to regular NMT system except in the way unknown words are handled. Character-level NMT  Deep LSTM model used to generate on-the-fly representation of rare words (using final hidden state from the top layer). Advantages:  Simplified architecture. Efficiency through precomputation - representations for rare sources words can be computed at once before each mini-batch. The model can be trained easily in an end-to-end fashion. Hidden-state Initialization  For source representation, layers of the LSTM are initialized with zero hidden states and cell values. For target representation, the same strategy is followed except for the hidden state of the first layer where one of the following approaches are used:  same-path target generation approach  Use the context vector just before softmax (of word-level NMT). seperate-path target generation approach  Learn a new weight matrix W that will be used to generate the context vector. Training Objective  J = Jw + αJc  J - total loss  Jw - loss in a regular word-level NMT  αJc - loss in the character-level NMT  Word Character Generation Strategy  The final hidden state from character-level decoder could be interpreted as the representation of unk token but this approach would not be efficient. Instead, unk is fed to the word-level decoder as it is so as to decouple the execution for the character-level model as soon the word-level model finishes. During testing, a beam search decoder is run at the word level to find the best translation using the word NMT alone. Next, a character-level encoder is used to generate the words in place of unk to minimise the combined loss. Experiments  Data  WMT’15 translation task from English into Czech with newstest2013 (3000 sentences) as dev set and newstest2015 (2656 sentences) as a test set. Metrics  Case-sensitive NIST BLEU. chrF3  Models  Purely word based  Purely character based  Hybrid (proposed model)  Observations  Hybrid model surpasses all the other systems (neural/non-neural) and establishes a new state-of-the-art result for English-Czech translation in WMT’15 with 19.9 BLEU. Character-level models, when used as a replacement for the standard unk replacement technique in NMT, yields an improvement of up to +7.9 BLEU points. Attention is very important for character-based models as the non-attentional character models perform poorly. Character models with shorter time-step backpropagation perform inferior as compared to ones with longer backpropagation. Separate-path strategy outperforms same-path strategy. Rare word embeddings  Obtain representations for rare words. Compare the Spearman correlation between similarity scores assigned by humans and by the model. Outperforms the recursive neural network model (which also uses a morphological analyser) on this task.", "pdf_url": "https://arxiv.org/pdf/1604.00788", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/2e665b27696ce0436c79174a136410.json"}
{"id": "47469397", "bin": "500_600", "summary_sentences": ["The paper introduces:  Sentiment Sentiment Treebank - A dataset containing 215,154 phrases with fine-grained sentiment labels (5 classes).", "Recursive Neural Tensor Network - Model to learn these fine-grained sentiment labels.", "Sentiment Sentiment Treebank  Corpus of 11,855 sentences with fully labelled parse trees.", "Can be used to analyse the compositional effects of sentiment in language.", "Start with movie reviews dataset , normalise and parse sentences, and label using crowdsourcing on Amazon Mechanical Turk.", "Observations  Majority of shorter phrases are neutral while longer phrases have stronger sentiments.", "A 5-class classification is sufficient to capture the variability of sentiments.", "Recursive Neural Models  Parse a given n-gram into a binary tree and represent each word (corresponding to leaves in the tree) using a d-dimensional vector.", "Compute parent vectors using a bottom-up approach using different composition functions.", "To start with, word vectors are initialized randomly from a uniform distribution.", "For classification task, use the compositions word vectors as input for the softmax.", "Different models differ in terms of how word vectors are combined together as shown in the figure.", "RNN: Recursive Neural Network  Uses the equation shown in the figure  f = tanh  W is the weight matrix to be learnt.", "Con  Input vectors interact only implicitly, via the nonlinear tanh function.", "MV-RNN: Matrix-Vector RNN  Represent every word and phrase as both a vector and a matrix.", "Matrix for each word is initialized as identity matrix plus a small Gaussian noise.", "For the parse tree as  the equation used is  W and WM are both learnt.", "Con  Number of parameters depend on the size of vocabulary and could be very large.", "RNTN: Recursive Neural Tensor Network  Equations  V is the tensor that defines multiple bilinear forms.", "Each slice of the tensor V can be interpreted as capturing a specific type of composition.", "Observations  Models compared with  Naive Bayes - NB  SVMs  Naive Bayes with bag of bigram features - biNB  Average neural word vectors (ignoring word order) - vecAvg  Task  Fine-grained Sentiment For All Phrases  RNTN > MV-RNN > RNN > other models.", "Full Sentence Binary Sentiment  RNTN pushes the state of the art on short phrases to 85.4%  Contrastive Conjunction  Sentences of the form X but Y  RNTN > MV-RNN > RNN > SVM  RNTN outperforms other models in special cases like where a positive sentence is negated or where a negative sentence is negated to make it less negative (not positive though).", "This suggests that RNTN could capture the effect of negative words in both positive and negative sentiment sentences.", "Notes  The optimal word vector size reported by the paper was between 25 and 35 and these word vectors were trained as part of sentiment tagging process.", "It would be interesting to see how are these results affected by using word vectors from say Glove which may or may not be fine tuned for the sentiment labelling."], "summary_text": "The paper introduces:  Sentiment Sentiment Treebank - A dataset containing 215,154 phrases with fine-grained sentiment labels (5 classes). Recursive Neural Tensor Network - Model to learn these fine-grained sentiment labels. Sentiment Sentiment Treebank  Corpus of 11,855 sentences with fully labelled parse trees. Can be used to analyse the compositional effects of sentiment in language. Start with movie reviews dataset , normalise and parse sentences, and label using crowdsourcing on Amazon Mechanical Turk. Observations  Majority of shorter phrases are neutral while longer phrases have stronger sentiments. A 5-class classification is sufficient to capture the variability of sentiments. Recursive Neural Models  Parse a given n-gram into a binary tree and represent each word (corresponding to leaves in the tree) using a d-dimensional vector. Compute parent vectors using a bottom-up approach using different composition functions. To start with, word vectors are initialized randomly from a uniform distribution. For classification task, use the compositions word vectors as input for the softmax. Different models differ in terms of how word vectors are combined together as shown in the figure. RNN: Recursive Neural Network  Uses the equation shown in the figure  f = tanh  W is the weight matrix to be learnt. Con  Input vectors interact only implicitly, via the nonlinear tanh function. MV-RNN: Matrix-Vector RNN  Represent every word and phrase as both a vector and a matrix. Matrix for each word is initialized as identity matrix plus a small Gaussian noise. For the parse tree as  the equation used is  W and WM are both learnt. Con  Number of parameters depend on the size of vocabulary and could be very large. RNTN: Recursive Neural Tensor Network  Equations  V is the tensor that defines multiple bilinear forms. Each slice of the tensor V can be interpreted as capturing a specific type of composition. Observations  Models compared with  Naive Bayes - NB  SVMs  Naive Bayes with bag of bigram features - biNB  Average neural word vectors (ignoring word order) - vecAvg  Task  Fine-grained Sentiment For All Phrases  RNTN > MV-RNN > RNN > other models. Full Sentence Binary Sentiment  RNTN pushes the state of the art on short phrases to 85.4%  Contrastive Conjunction  Sentences of the form X but Y  RNTN > MV-RNN > RNN > SVM  RNTN outperforms other models in special cases like where a positive sentence is negated or where a negative sentence is negated to make it less negative (not positive though). This suggests that RNTN could capture the effect of negative words in both positive and negative sentiment sentences. Notes  The optimal word vector size reported by the paper was between 25 and 35 and these word vectors were trained as part of sentiment tagging process. It would be interesting to see how are these results affected by using word vectors from say Glove which may or may not be fine tuned for the sentiment labelling.", "pdf_url": "http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/a136088f58d24f7b08056ec8b97595.json"}
{"id": "46416317", "bin": "500_600", "summary_sentences": ["What  They describe a model based on the A3C algorithm.", "The model is optimized so that it learns fast to navigate through mazes/games.", "Most of the modification is based on adding auxiliary losses (depth prediction and \"have I been here before?\").", "How  Their model is based on the A3C algorithm and works mostly in the same way.", "They describe four variations of the model:  FF A3C: Standard A3C based agent with a simple CNN.", "LSTM A3C: Standard A3C based agent with a simple CNN+LSTM.", "Nav A3C: Like \"LSTM A3C\", but uses two stacked LSTMs and gets three additional inputs: last received reward, last performed action, current velocity vector (lateral + rotation).", "Nav A3C +D1D2L: Like \"Nav A3C\", but additionally predicts depth information and loop information (see below).", "Visualizations:  All of these models predict (at least) a policy (pi(a_t|s_t)) and value function (V(s_t)).", "Depth prediction  They let the model predict the depth map of the currently visible scene/screen.", "They could also feed the depth map as an additional input into the model, but argue that it helps more with understanding and learning the navigation task to let the model predict it.", "They try prediction both via regression and classification.", "(Where classification uses non-uniform bins, so that there are more bins for far away depths.)", "Loop closure prediction  They let the model predict whether it  has been at (roughly) the current position t timesteps in the past  AND also moved sufficiently far away from that position between now and t timesteps in the past (in order to not make the loss too simple)  Results  They use a maze game from DeepMind Labs for training and testing.", "In that maze, the agent can accelerate forward/backward/sideways/rotational.", "The agent must find a goal within a short amount of time to gain 10 reward.", "Occasionally there are other reward objects giving +1 or +2.", "They use the following maze types:  Static maze: Goal locations are always the same, but the agent is placed at a random location after finding them.", "Dynamic maze / random goal maze: Goal locations vary per episode.", "I-Maze: Static maze layout.", "Goal is at a random one of four locations per episode.", "Observations:  Predicting depth via regression works significantly worse than via classification.", "FF A3C (raw feed-forward CNN without RNN-memory) can sometimes still navigate quite well.", "Memory is apparently not always required for navigation, despite ambiguity.", "Adding the last action, last reward and velocity as an input seems to usually help the agent, but not always.", "Adding the auxiliary losses helps significantly with learning.", "Learning curves:  Legend: -L = with loop closure prediction, -D1 = with depth prediction after first LSTM layer, -D2 = with depth prediction after second LSTM layer."], "summary_text": "What  They describe a model based on the A3C algorithm. The model is optimized so that it learns fast to navigate through mazes/games. Most of the modification is based on adding auxiliary losses (depth prediction and \"have I been here before?\"). How  Their model is based on the A3C algorithm and works mostly in the same way. They describe four variations of the model:  FF A3C: Standard A3C based agent with a simple CNN. LSTM A3C: Standard A3C based agent with a simple CNN+LSTM. Nav A3C: Like \"LSTM A3C\", but uses two stacked LSTMs and gets three additional inputs: last received reward, last performed action, current velocity vector (lateral + rotation). Nav A3C +D1D2L: Like \"Nav A3C\", but additionally predicts depth information and loop information (see below). Visualizations:  All of these models predict (at least) a policy (pi(a_t|s_t)) and value function (V(s_t)). Depth prediction  They let the model predict the depth map of the currently visible scene/screen. They could also feed the depth map as an additional input into the model, but argue that it helps more with understanding and learning the navigation task to let the model predict it. They try prediction both via regression and classification. (Where classification uses non-uniform bins, so that there are more bins for far away depths.) Loop closure prediction  They let the model predict whether it  has been at (roughly) the current position t timesteps in the past  AND also moved sufficiently far away from that position between now and t timesteps in the past (in order to not make the loss too simple)  Results  They use a maze game from DeepMind Labs for training and testing. In that maze, the agent can accelerate forward/backward/sideways/rotational. The agent must find a goal within a short amount of time to gain 10 reward. Occasionally there are other reward objects giving +1 or +2. They use the following maze types:  Static maze: Goal locations are always the same, but the agent is placed at a random location after finding them. Dynamic maze / random goal maze: Goal locations vary per episode. I-Maze: Static maze layout. Goal is at a random one of four locations per episode. Observations:  Predicting depth via regression works significantly worse than via classification. FF A3C (raw feed-forward CNN without RNN-memory) can sometimes still navigate quite well. Memory is apparently not always required for navigation, despite ambiguity. Adding the last action, last reward and velocity as an input seems to usually help the agent, but not always. Adding the auxiliary losses helps significantly with learning. Learning curves:  Legend: -L = with loop closure prediction, -D1 = with depth prediction after first LSTM layer, -D2 = with depth prediction after second LSTM layer.", "pdf_url": "https://arxiv.org/pdf/1611.03673", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/learning_to_navigate_in_complex_environments.json"}
{"id": "1540690", "bin": "500_600", "summary_sentences": ["Notes  The paper presents a simple yet effective approach for transferring knowledge from a trained neural network (referred to as the teacher network) to a large, untrained neural network (referred to as the student network).", "The key idea is to use a function-preserving transformation that guarantees that for any given input, the output from the teacher network and the newly created student network would be the same.", "Link to an implementation  The approach works as follows - Let us say that the teacher network was represented by the transformation y = f(x, θ) where θ refer to the parameters of the network.", "The task is to choose a new set of parameters θ’ for the student network g(x, θ’) such that for all x, f(x, θ) = g(x, θ’)  To start, we can assume that f and g are composed of standard linear layers.", "Layer i and i+1 are represented by weights Wmxni and Wnxpi+1  We want to grow layer i to have q output units (where q > n) and layer i+1 to have q input units.", "The new weight matrix would be Umxqi and Uqxpi+1  The first q columns (rows) of Wi (Wi+1) would be copied as it is into Ui(Ui+1).", "For filling the remaining n-q slots, columns (rows) would be sampled randomly from Wi (Wi+1).", "Finally, each layer in Ui is scaled by dividing by the corresponding replication factor to ensure that the output value of function remains unchanged by the operation.", "Since convolutions can be seen as multiplication by a double block circulant matrix, the approach can be readily extended for convolutional networks.", "The benefits of using this approach are the following:  The newly created student network performs at least as good as the teacher network.", "Any changes to the network are guaranteed to be an improvement.", "It is safe to optimize all the parameters in the network.", "The variant discussed above is called the Net2WiderNet variant.", "There is another variant calledNet2DeeperNet that enables the network to grow in depth.", "In that case, a new matrix, U, initialized as the identity matrix, is added to the network.", "Note that unlike the Net2WiderNet, this approach would not work with arbitrary activation function between the layers.", "Strengths  The model can accelerate the training of neural networks, especially during development cycle when the designers try out different models.", "The approach could potentially be used in life-long learning systems where the model is trained over a stream of data and needs to grow over time.", "Limitations  The function preserving transformations need to be worked out manually.", "Extra care needs to be taken when operations like concatenation or batch norm are present."], "summary_text": "Notes  The paper presents a simple yet effective approach for transferring knowledge from a trained neural network (referred to as the teacher network) to a large, untrained neural network (referred to as the student network). The key idea is to use a function-preserving transformation that guarantees that for any given input, the output from the teacher network and the newly created student network would be the same. Link to an implementation  The approach works as follows - Let us say that the teacher network was represented by the transformation y = f(x, θ) where θ refer to the parameters of the network. The task is to choose a new set of parameters θ’ for the student network g(x, θ’) such that for all x, f(x, θ) = g(x, θ’)  To start, we can assume that f and g are composed of standard linear layers. Layer i and i+1 are represented by weights Wmxni and Wnxpi+1  We want to grow layer i to have q output units (where q > n) and layer i+1 to have q input units. The new weight matrix would be Umxqi and Uqxpi+1  The first q columns (rows) of Wi (Wi+1) would be copied as it is into Ui(Ui+1). For filling the remaining n-q slots, columns (rows) would be sampled randomly from Wi (Wi+1). Finally, each layer in Ui is scaled by dividing by the corresponding replication factor to ensure that the output value of function remains unchanged by the operation. Since convolutions can be seen as multiplication by a double block circulant matrix, the approach can be readily extended for convolutional networks. The benefits of using this approach are the following:  The newly created student network performs at least as good as the teacher network. Any changes to the network are guaranteed to be an improvement. It is safe to optimize all the parameters in the network. The variant discussed above is called the Net2WiderNet variant. There is another variant calledNet2DeeperNet that enables the network to grow in depth. In that case, a new matrix, U, initialized as the identity matrix, is added to the network. Note that unlike the Net2WiderNet, this approach would not work with arbitrary activation function between the layers. Strengths  The model can accelerate the training of neural networks, especially during development cycle when the designers try out different models. The approach could potentially be used in life-long learning systems where the model is trained over a stream of data and needs to grow over time. Limitations  The function preserving transformations need to be worked out manually. Extra care needs to be taken when operations like concatenation or batch norm are present.", "pdf_url": "https://arxiv.org/pdf/1511.05641", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/net2net-accelerating-learning-via-knowledge-transfer.json"}
{"id": "65123124", "bin": "500_600", "summary_sentences": ["Recurrent Neural Networks have two key issues:  Over parameterization which increases the time for training and inference.", "Ill conditioned recurrent weight matrix which makes training difficult due to vanishing or exploding gradients.", "The paper presents a flexible RNN model called as KRU (Kronecker Recurrent Units) which overcomes the above problems by using a Kronecker factored recurrent matrix and soft unitary constraints on the factors.", "Related Work  Existing solutions for overparameterization  Low-rank decomposition.", "Training a neural network on the soft targets predicted by a big pre-trained network.", "Low-bit precision training.", "Hashing.", "Existing solutions for vanishing and exploding gradients  Gating mechanism like in LSTMs.", "Gradient Clipping.", "Orthogonal Weight Initialization.", "Parameterizing recurrent weight matrix.", "KRU  Uses a Kronecker factored recurrent matrix which enables controlling the number of parameters and number of factor matrices.", "Vanishing and exploding gradients are taken care of by using a soft unitary constraint.", "Why not use strict unitary constraint:  Restricts the search space and makes learning process unstable.", "Makes forgetting (irrelevant) information difficult.", "Relaxing the strict constraint has shown to improve the convergence speed and generalization performance.", "KRU can be easily plugged into RNNs, LSTMs and other variants.", "The recurrent matrix W is paramterized as a kronecker product of F matrices W0, …, WF-1 where each Wf is a complex matrix of shape Pf x Qf and the product of all Pf and producto of all Qf are both equal to N.  Why is W a complex matrix?", "In the real space, the set of all unitary matrices have the determinant as 1 or -1.", "Given that determinant is a continuous function, the unitary set in the real space is disconnected.", "The unitary set in the complex space is connected as its determinants are points on the unit circle.", "Soft Unitary Constraint  A soft unitary constraint is introduced in the form of regularization term     WfHWf - I     2 (per kronecker factored recurrent matrix).", "If each of the Kronecker factors is unitary, the resulting matrix W would also be unitary.", "It is computationally inefficient to apply this constraint over the recurrent matrix W itself as the complexity of the regularizer is given as O(N3).", "Use of Kronecker factorisation makes it computationally feasible to use this regulariser.", "Experiment  The Kronecker recurrent model is compared against the existing recurrent models for multiple tasks including copy memory, adding memory, pixel-by-pixel MNIST, char level language models, polyphonic music modelling, and framewise phoneme classification.", "For most of the task, KRU model produces results comparable to the best performing models despite using fewer parameters.", "Using soft unitary constraints in KRU provides a principled alternative to gradient clipping (a common heuristic to avoid exploding gradients).", "Further, recent theoretical results suggest the gradient descent converges to a global optimizer of linear recurrent networks even if the learning problem is non-convex provided that the spectral norm of the recurrent matrix is bound by 1.", "The key take away from the paper is that state should be high dimensional so that high capacity network can be used for encoding and decoding the input and output.", "The recurrent dynamics should be implemented via a low capacity model.s per task."], "summary_text": "Recurrent Neural Networks have two key issues:  Over parameterization which increases the time for training and inference. Ill conditioned recurrent weight matrix which makes training difficult due to vanishing or exploding gradients. The paper presents a flexible RNN model called as KRU (Kronecker Recurrent Units) which overcomes the above problems by using a Kronecker factored recurrent matrix and soft unitary constraints on the factors. Related Work  Existing solutions for overparameterization  Low-rank decomposition. Training a neural network on the soft targets predicted by a big pre-trained network. Low-bit precision training. Hashing. Existing solutions for vanishing and exploding gradients  Gating mechanism like in LSTMs. Gradient Clipping. Orthogonal Weight Initialization. Parameterizing recurrent weight matrix. KRU  Uses a Kronecker factored recurrent matrix which enables controlling the number of parameters and number of factor matrices. Vanishing and exploding gradients are taken care of by using a soft unitary constraint. Why not use strict unitary constraint:  Restricts the search space and makes learning process unstable. Makes forgetting (irrelevant) information difficult. Relaxing the strict constraint has shown to improve the convergence speed and generalization performance. KRU can be easily plugged into RNNs, LSTMs and other variants. The recurrent matrix W is paramterized as a kronecker product of F matrices W0, …, WF-1 where each Wf is a complex matrix of shape Pf x Qf and the product of all Pf and producto of all Qf are both equal to N.  Why is W a complex matrix? In the real space, the set of all unitary matrices have the determinant as 1 or -1. Given that determinant is a continuous function, the unitary set in the real space is disconnected. The unitary set in the complex space is connected as its determinants are points on the unit circle. Soft Unitary Constraint  A soft unitary constraint is introduced in the form of regularization term     WfHWf - I     2 (per kronecker factored recurrent matrix). If each of the Kronecker factors is unitary, the resulting matrix W would also be unitary. It is computationally inefficient to apply this constraint over the recurrent matrix W itself as the complexity of the regularizer is given as O(N3). Use of Kronecker factorisation makes it computationally feasible to use this regulariser. Experiment  The Kronecker recurrent model is compared against the existing recurrent models for multiple tasks including copy memory, adding memory, pixel-by-pixel MNIST, char level language models, polyphonic music modelling, and framewise phoneme classification. For most of the task, KRU model produces results comparable to the best performing models despite using fewer parameters. Using soft unitary constraints in KRU provides a principled alternative to gradient clipping (a common heuristic to avoid exploding gradients). Further, recent theoretical results suggest the gradient descent converges to a global optimizer of linear recurrent networks even if the learning problem is non-convex provided that the spectral norm of the recurrent matrix is bound by 1. The key take away from the paper is that state should be high dimensional so that high capacity network can be used for encoding and decoding the input and output. The recurrent dynamics should be implemented via a low capacity model.s per task.", "pdf_url": "https://arxiv.org/pdf/1705.10142", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/kronecker-recurrent-units.json"}
{"id": "15765418", "bin": "500_600", "summary_sentences": ["This paper proposes to train a neural network generative model by optimizing an importance sampling (IS) weighted estimate of the log probability under the model.", "The authors show that the case of an estimate based on a single sample actually corresponds to the learning objective of variational autoencoders (VAE).", "Importantly, they exploit this connection by showing that, similarly to VAE, a gradient can be passed through the approximate posterior (the IS proposal) samples, thus yielding an importance weighted autoencoder (IWAE).", "The authors also show that, by using more samples, this objective, which is a lower bound of the actual log-likelihood, becomes an increasingly tighter approximation to the log-likelihood.", "In other words, the IWAE is expected to better optimize the real log-likelihood of the neural network, compared to VAE.", "The experiments presented show that the model achieves competitive performance on a version of the binarized MNIST benchmark and on the Omniglot dataset.", "#### My two cents  This is a really neat contribution!", "While simple (both conceptually and algorithmically), it really seems to be an important step forward for the VAE framework.", "I really like the theoretical result showing that IWAE provides a better approximation to the real log-likelihood, it's quite neat and provides an excellent motivation for the method.", "The results on binarized MNIST are certainly impressive.", "Unfortunately, it appears that the training setup isn't actually comparable to the majority of published results on this dataset.", "Indeed, it seems that they didn't use the stochastic but *fixed* binarization of the inputs that other publications on this benchmark have used (since my paper on NADE with Iain Murray, we've made available that fixed training set for everyone to use, along with fixed validation and test sets as well).", "I believe instead they've re-sampled the binarization for each minibatch, effectively creating a setup with a somewhat larger training set than usual.", "It's unfortunate that this is the case, since it makes this result effectively impossible to compare directly with previous work.", "I'm being picky on this issue only because I'm super interested in this problem (that is of generative modeling with neural networks) and this little issue is pretty much the only thing that stops this paper from being a slam dunk.", "Hopefully the authors (or perhaps someone interested in reimplementing IWAE) can clarify this question eventually.", "Otherwise, it seems quite clear to me that IWAE is an improvement over VAE.", "The experiments of section 5.2, showing that fine-tuning a VAE model with IWAE training improves performance, while fine-tuning a IWAE model using VAE actually makes things worse, is further demonstration that IWAE is indeed a good idea."], "summary_text": "This paper proposes to train a neural network generative model by optimizing an importance sampling (IS) weighted estimate of the log probability under the model. The authors show that the case of an estimate based on a single sample actually corresponds to the learning objective of variational autoencoders (VAE). Importantly, they exploit this connection by showing that, similarly to VAE, a gradient can be passed through the approximate posterior (the IS proposal) samples, thus yielding an importance weighted autoencoder (IWAE). The authors also show that, by using more samples, this objective, which is a lower bound of the actual log-likelihood, becomes an increasingly tighter approximation to the log-likelihood. In other words, the IWAE is expected to better optimize the real log-likelihood of the neural network, compared to VAE. The experiments presented show that the model achieves competitive performance on a version of the binarized MNIST benchmark and on the Omniglot dataset. #### My two cents  This is a really neat contribution! While simple (both conceptually and algorithmically), it really seems to be an important step forward for the VAE framework. I really like the theoretical result showing that IWAE provides a better approximation to the real log-likelihood, it's quite neat and provides an excellent motivation for the method. The results on binarized MNIST are certainly impressive. Unfortunately, it appears that the training setup isn't actually comparable to the majority of published results on this dataset. Indeed, it seems that they didn't use the stochastic but *fixed* binarization of the inputs that other publications on this benchmark have used (since my paper on NADE with Iain Murray, we've made available that fixed training set for everyone to use, along with fixed validation and test sets as well). I believe instead they've re-sampled the binarization for each minibatch, effectively creating a setup with a somewhat larger training set than usual. It's unfortunate that this is the case, since it makes this result effectively impossible to compare directly with previous work. I'm being picky on this issue only because I'm super interested in this problem (that is of generative modeling with neural networks) and this little issue is pretty much the only thing that stops this paper from being a slam dunk. Hopefully the authors (or perhaps someone interested in reimplementing IWAE) can clarify this question eventually. Otherwise, it seems quite clear to me that IWAE is an improvement over VAE. The experiments of section 5.2, showing that fine-tuning a VAE model with IWAE training improves performance, while fine-tuning a IWAE model using VAE actually makes things worse, is further demonstration that IWAE is indeed a good idea.", "pdf_url": "http://arxiv.org/pdf/1509.00519", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/burdags15.json"}
{"id": "26955055", "bin": "500_600", "summary_sentences": ["For top-k classification tasks, cross entropy is widely used as the learning objective even though it is the optimal metric only in the limit of infinite data.", "The paper introduces a family of smoothed loss functions that are specially designed for top-k optimization.", "Paper  Code  Idea  Inspired by the multi-loss SVMs, a surrogate loss (lk) is introduced that creates a margin between the ground truth and the kth largest score.", "Here s denotes the output of the classifier model to be learnt, y is the ground truth label, s[p] denotes the kth largest element of s and s\\p denotes the vector s without pth element.", "This lk loss has two limitations:  It is continous but not differentiable in s.  Its weak derivatives have at most 2-nonzero elements.", "The loss can be reformulated by adding and subtracting the k-1 largest scores of s\\y and sy and by introducing a temperature parameter τ.", "Properties of Lkτ  For any τ > 0, Lkτ is infinite-differentiable and has non-sparse gradients.", "Under mild conditions, Lkτ apporachs lk (in a pointwise sense) as τ approaches to 0++.", "It is an upper bound on the actual loss (up to a constant factor).", "It is a generalization of the cross-entropy loss for different values of k, and τ and higher margins.", "Computational Challenges  nCk number of terms needs to be evaluated for computing the loss for one sample (n is number of classes).", "Loss Lkτ can be expressed in terms of elementary symmetric polynomials σi(e) (sum of all products of i distinct elements of vector e).", "Thus the challenge is to compute σk efficiently.", "Forward Computation  Compute σk(e) where e is a n-dimensional vector and k« n and e[i]!=0 for all i.  σi(e) can be computed using the coefficients of the polynomial (X+e1)(X+e2)…(X+en) by divide and conquer approach with polynomial multiplication.", "With some more optimizations (eg log(n) levels of recursion and each level being parallelized on a GPU), the resulting algorithms scale well with n on a GPU.", "Operations are performed in the log-space using the log-sum-exp trick to achieve numerical stability in single floating point precision.", "Backward computation  The backward pass uses optimizations like computing derivative of σj with respect to ei in a recursive manner.", "Appendix of the paper describes these techniques in detail.", "Experiments  Experiments are performed on CIFAR-100 (with noise) and Imagenet.", "For CIFAR-100 with noise, the labels are randomized with probability p (within the same top-level class).", "The proposed loss function is very robust to both noise and reduction in the amount of training dataset as compared to cross-entropy loss function for both top-k and top-1 performance."], "summary_text": "For top-k classification tasks, cross entropy is widely used as the learning objective even though it is the optimal metric only in the limit of infinite data. The paper introduces a family of smoothed loss functions that are specially designed for top-k optimization. Paper  Code  Idea  Inspired by the multi-loss SVMs, a surrogate loss (lk) is introduced that creates a margin between the ground truth and the kth largest score. Here s denotes the output of the classifier model to be learnt, y is the ground truth label, s[p] denotes the kth largest element of s and s\\p denotes the vector s without pth element. This lk loss has two limitations:  It is continous but not differentiable in s.  Its weak derivatives have at most 2-nonzero elements. The loss can be reformulated by adding and subtracting the k-1 largest scores of s\\y and sy and by introducing a temperature parameter τ. Properties of Lkτ  For any τ > 0, Lkτ is infinite-differentiable and has non-sparse gradients. Under mild conditions, Lkτ apporachs lk (in a pointwise sense) as τ approaches to 0++. It is an upper bound on the actual loss (up to a constant factor). It is a generalization of the cross-entropy loss for different values of k, and τ and higher margins. Computational Challenges  nCk number of terms needs to be evaluated for computing the loss for one sample (n is number of classes). Loss Lkτ can be expressed in terms of elementary symmetric polynomials σi(e) (sum of all products of i distinct elements of vector e). Thus the challenge is to compute σk efficiently. Forward Computation  Compute σk(e) where e is a n-dimensional vector and k« n and e[i]!=0 for all i.  σi(e) can be computed using the coefficients of the polynomial (X+e1)(X+e2)…(X+en) by divide and conquer approach with polynomial multiplication. With some more optimizations (eg log(n) levels of recursion and each level being parallelized on a GPU), the resulting algorithms scale well with n on a GPU. Operations are performed in the log-space using the log-sum-exp trick to achieve numerical stability in single floating point precision. Backward computation  The backward pass uses optimizations like computing derivative of σj with respect to ei in a recursive manner. Appendix of the paper describes these techniques in detail. Experiments  Experiments are performed on CIFAR-100 (with noise) and Imagenet. For CIFAR-100 with noise, the labels are randomized with probability p (within the same top-level class). The proposed loss function is very robust to both noise and reduction in the amount of training dataset as compared to cross-entropy loss function for both top-k and top-1 performance.", "pdf_url": "https://arxiv.org/pdf/1812.00420", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/smooth-loss-functions-for-deep-top-k-classification.json"}
{"id": "7415123", "bin": "500_600", "summary_sentences": ["In machine learning, it is common to train a single large model (with a large number of parameters) or ensemble of multiple smaller models using the same dataset.", "While such large models help to improve the performance of the system, they also make it difficult and computationally expensive to deploy the system.", "The paper proposes to transfer the knowledge from such “cumbersome” models into a single, “simpler” model which is more suitable for deployment.", "This transfer of knowledge is referred to as “distillation”.", "Idea  Train the cumbersome model using the given training data in the usual way.", "Train the simpler, distilled model using the class probabilities (from the cumbersome model) as the soft target.", "Thus, the simpler model is trained to generalise the same way as the cumbersome model.", "If the soft targets have high entropy, they provide much more information than the hard targets and the gradient (between training examples) would vary lesser.", "One approach is to minimise the L2 difference between logits produced by the cumbersome model and the simpler model.", "This approach was pursued by Buciluǎ et al. The paper proposes a more general solution which they name “distillation”.", "The temperature of the final softmax is increased till the cumbersome model produces a set of soft targets (from the final softmax layer).", "These soft targets are then used to train the simpler model.", "It also shows that the proposed approach is, in fact, a more general case of the first approach.", "Approach  In the simplest setting, the cumbersome model is first trained with a high value of temperature and then the same temperature value is used to train the simpler model.", "The temperature is set to 1 when making predictions using the simpler model.", "It helps to add an auxiliary objective function which corresponds to the cross-entropy loss with the correct labels.", "The second objective function should be given a much lower weight though.", "Further, the magnitude of the soft targets needs to be scaled by multiplying with the square of temperature.", "Experiment  The paper reports favourable results for distillation task for the following domains:  Image Classification (on MNIST dataset)  An extra experiment is performed where the simpler model is not shown any images of “3” but the model fails for only 133 cases out of 1010 cases involving “3”.", "Automatic Speech Recognition (ASR)  An extra experiment is performed where the baseline model is trained using both hard targets and soft targets alternatively.", "Further, only 3% of the total dataset is used.", "The model using hard targets overfits and has poor test accuracy while the model using soft targets does not overfit and gets much better test accuracy.", "This shows the regularizing effect of soft targets.", "Training ensemble specialists for very large datasets (JFT dataset - an internal dataset at Google)  The experiment shows that while training a single large model would take a lot of time, the performance of the model can be improved by learning a small number of specialised networks (which are faster to train).", "Though it is yet to be shown that the knowledge of such specialist models can be distilled back into a single model."], "summary_text": "In machine learning, it is common to train a single large model (with a large number of parameters) or ensemble of multiple smaller models using the same dataset. While such large models help to improve the performance of the system, they also make it difficult and computationally expensive to deploy the system. The paper proposes to transfer the knowledge from such “cumbersome” models into a single, “simpler” model which is more suitable for deployment. This transfer of knowledge is referred to as “distillation”. Idea  Train the cumbersome model using the given training data in the usual way. Train the simpler, distilled model using the class probabilities (from the cumbersome model) as the soft target. Thus, the simpler model is trained to generalise the same way as the cumbersome model. If the soft targets have high entropy, they provide much more information than the hard targets and the gradient (between training examples) would vary lesser. One approach is to minimise the L2 difference between logits produced by the cumbersome model and the simpler model. This approach was pursued by Buciluǎ et al. The paper proposes a more general solution which they name “distillation”. The temperature of the final softmax is increased till the cumbersome model produces a set of soft targets (from the final softmax layer). These soft targets are then used to train the simpler model. It also shows that the proposed approach is, in fact, a more general case of the first approach. Approach  In the simplest setting, the cumbersome model is first trained with a high value of temperature and then the same temperature value is used to train the simpler model. The temperature is set to 1 when making predictions using the simpler model. It helps to add an auxiliary objective function which corresponds to the cross-entropy loss with the correct labels. The second objective function should be given a much lower weight though. Further, the magnitude of the soft targets needs to be scaled by multiplying with the square of temperature. Experiment  The paper reports favourable results for distillation task for the following domains:  Image Classification (on MNIST dataset)  An extra experiment is performed where the simpler model is not shown any images of “3” but the model fails for only 133 cases out of 1010 cases involving “3”. Automatic Speech Recognition (ASR)  An extra experiment is performed where the baseline model is trained using both hard targets and soft targets alternatively. Further, only 3% of the total dataset is used. The model using hard targets overfits and has poor test accuracy while the model using soft targets does not overfit and gets much better test accuracy. This shows the regularizing effect of soft targets. Training ensemble specialists for very large datasets (JFT dataset - an internal dataset at Google)  The experiment shows that while training a single large model would take a lot of time, the performance of the model can be improved by learning a small number of specialised networks (which are faster to train). Though it is yet to be shown that the knowledge of such specialist models can be distilled back into a single model.", "pdf_url": "https://arxiv.org/pdf/1503.02531", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/distilling-the-knowledge-in-a-neural-network.json"}
{"id": "21542239", "bin": "500_600", "summary_sentences": ["Chandra, Kokkinos, 2016  Semantic Image Segmentation  Image Example  Put simply, the task is to cluster pixels in an image together and to assign all pixels in a cluster a meaningful label.", "Labels are generally “high-level” concepts, such as “horse”, “dog”, “boat”, etc.", "Here is the VOC PASCAL 2012 dataset used in this paper.", "Discussion Q’s  Why G-CRFs?", "Joint distributions are hard, so exploit factorizable (exponential-family) probability distributions and conditional independence.", "This results in a graph-like structure.", "If the label assigned to each pixel is a random variable, then an conditional random field suggests that there’s a dependence between a pixel’s label and the label’s of other nearby pixels.", "This seems intuitive.", "Author’s argue that continuous Gaussian outputs are unimodal conditioned on the data, so given an image, one solution dominates the posterior.", "The log-likelihood is a quadratic, which allows the approximate inference task to be cast a quadratic optimization in an energy-minimization setting.", "Deep G-CRF architecture  The basic idea is to set the outputs of a convolutional neural network to be the energy of a segmentation hypothesis, in the form of  .", "The network predicts what the $A$ (pairwise) and $B$ (unary) terms in the energy function are for an image.", "$\\lambda$ is set manually to enforce positive-definiteness.", "Then a Quadratic Optimization + softmax module gives the final per-class scores for each pixel in the image (See Fig.", "1) by solving for $x$.", "Originally, $A$ is a $(P \\times L) \\times (P \\times L)$ matrix, where $L$ is the number of class labels and $P$ is number of pixels.", "Shared pair-wise terms  $A$ is no longer dependent on the number of class labels; only care about pixel interactions independent on what the label is.", "Now, the inference equation $(A + \\lambda I) x = B$ is reduced to a system of $L + 1$ equations for $A$ of dim $P \\times P$.", "Conjugate Gradient method  Need to solve $x = A^{-1}b$.", "The $A$ matrix is very sparse, since it only deals with 4, 8, or 12-connected neighborhoods.", "CG is the current recommended approach for solving $Ax = b$ when $A$ is sparse why?", "When $A$ is dense, it is recommended to factorize A and then use backsubstitution.", "see here  Experiments  Try out the above + fusing information across 3 different resolutions.", "metric - Intersection over Union  baselines - Multiresolution DeepLab-LargeFOV, CRF-RNN  QO network - Baseline extended to have a binary and unary stream.", "QO module can be shared by all resolutions, or replicated three times for each scale  75.5% mean IOU on VOC PASCAL 2012 for this approach.", "Without more details and tests of significance, hard to say whether this method is really more effective than prev SOTA.", "It seems to do about ~1-2% IOU better than the baselines.", "Also seems to be much faster."], "summary_text": "Chandra, Kokkinos, 2016  Semantic Image Segmentation  Image Example  Put simply, the task is to cluster pixels in an image together and to assign all pixels in a cluster a meaningful label. Labels are generally “high-level” concepts, such as “horse”, “dog”, “boat”, etc. Here is the VOC PASCAL 2012 dataset used in this paper. Discussion Q’s  Why G-CRFs? Joint distributions are hard, so exploit factorizable (exponential-family) probability distributions and conditional independence. This results in a graph-like structure. If the label assigned to each pixel is a random variable, then an conditional random field suggests that there’s a dependence between a pixel’s label and the label’s of other nearby pixels. This seems intuitive. Author’s argue that continuous Gaussian outputs are unimodal conditioned on the data, so given an image, one solution dominates the posterior. The log-likelihood is a quadratic, which allows the approximate inference task to be cast a quadratic optimization in an energy-minimization setting. Deep G-CRF architecture  The basic idea is to set the outputs of a convolutional neural network to be the energy of a segmentation hypothesis, in the form of  . The network predicts what the $A$ (pairwise) and $B$ (unary) terms in the energy function are for an image. $\\lambda$ is set manually to enforce positive-definiteness. Then a Quadratic Optimization + softmax module gives the final per-class scores for each pixel in the image (See Fig. 1) by solving for $x$. Originally, $A$ is a $(P \\times L) \\times (P \\times L)$ matrix, where $L$ is the number of class labels and $P$ is number of pixels. Shared pair-wise terms  $A$ is no longer dependent on the number of class labels; only care about pixel interactions independent on what the label is. Now, the inference equation $(A + \\lambda I) x = B$ is reduced to a system of $L + 1$ equations for $A$ of dim $P \\times P$. Conjugate Gradient method  Need to solve $x = A^{-1}b$. The $A$ matrix is very sparse, since it only deals with 4, 8, or 12-connected neighborhoods. CG is the current recommended approach for solving $Ax = b$ when $A$ is sparse why? When $A$ is dense, it is recommended to factorize A and then use backsubstitution. see here  Experiments  Try out the above + fusing information across 3 different resolutions. metric - Intersection over Union  baselines - Multiresolution DeepLab-LargeFOV, CRF-RNN  QO network - Baseline extended to have a binary and unary stream. QO module can be shared by all resolutions, or replicated three times for each scale  75.5% mean IOU on VOC PASCAL 2012 for this approach. Without more details and tests of significance, hard to say whether this method is really more effective than prev SOTA. It seems to do about ~1-2% IOU better than the baselines. Also seems to be much faster.", "pdf_url": "https://arxiv.org/pdf/1603.08358", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/semantic-image-seg-with-deep-g-crfs.json"}
{"id": "88049354", "bin": "500_600", "summary_sentences": ["What  They suggest a new model architecture for human pose estimation (i.e. \"lay a skeleton over a person\").", "Their architecture is based progressive pooling followed by progressive upsampling, creating an hourglass form.", "Input are images showing a person's body.", "Outputs are K heatmaps (for K body joints), with each heatmap showing the likely position of a single joint on the person (e.g. \"akle\", \"wrist\", \"left hand\", ...).", "How  Basic building block  They use residuals as their basic building block.", "Each residual has three layers: One 1x1 convolution for dimensionality reduction (from 256 to 128 channels), a 3x3 convolution, a 1x1 convolution for dimensionality increase (back to 256).", "Visualized:  Architecture  Their architecture starts with one standard 7x7 convolutions that has strides of (2, 2).", "They use MaxPooling (2x2, strides of (2, 2)) to downsample the images/feature maps.", "They use Nearest Neighbour upsampling (factor 2) to upsample the images/feature maps.", "After every pooling step they add three of their basic building blocks.", "Before each pooling step they branch off the current feature map as a minor branch and apply three basic building blocks to it.", "Then they add it back to the main branch after that one has been upsampeled again to the original size.", "The feature maps between each basic building block have (usually) 256 channels.", "Their HourGlass ends in two 1x1 convolutions that create the heatmaps.", "They stack two of their HourGlass networks after each other.", "Between them they place an intermediate loss.", "That way, the second network can learn to improve the predictions of the first network.", "Architecture visualized:  Heatmaps  The output generated by the network are heatmaps, one per joint.", "Each ground truth heatmap has a small gaussian peak at the correct position of a joint, everything else has value 0.", "If a joint isn't visible, the ground truth heatmap for that joint is all zeros.", "Other stuff  They use batch normalization.", "Activation functions are ReLUs.", "They use RMSprob as their optimizer.", "Implemented in Torch.", "Results  They train and test on FLIC (only one HourGlass) and MPII (two stacked HourGlass networks).", "Training is done with augmentations (horizontal flip, up to 30 degress rotation, scaling, no translation to keep the body of interest in the center of the image).", "Evaluation is done via PCK@0.2 (i.e. percentage of predicted keypoints that are within 0.2 head sizes of their ground truth annotation (head size of the specific body)).", "Results on FLIC are at >95%.", "Results on MPII are between 80.6% (ankle) and 97.6% (head).", "Average is 89.4%.", "Using two stacked HourGlass networks performs around 3% better than one HourGlass network (even when adjusting for parameters).", "Training time was 5 days on a Titan X (9xx generation)."], "summary_text": "What  They suggest a new model architecture for human pose estimation (i.e. \"lay a skeleton over a person\"). Their architecture is based progressive pooling followed by progressive upsampling, creating an hourglass form. Input are images showing a person's body. Outputs are K heatmaps (for K body joints), with each heatmap showing the likely position of a single joint on the person (e.g. \"akle\", \"wrist\", \"left hand\", ...). How  Basic building block  They use residuals as their basic building block. Each residual has three layers: One 1x1 convolution for dimensionality reduction (from 256 to 128 channels), a 3x3 convolution, a 1x1 convolution for dimensionality increase (back to 256). Visualized:  Architecture  Their architecture starts with one standard 7x7 convolutions that has strides of (2, 2). They use MaxPooling (2x2, strides of (2, 2)) to downsample the images/feature maps. They use Nearest Neighbour upsampling (factor 2) to upsample the images/feature maps. After every pooling step they add three of their basic building blocks. Before each pooling step they branch off the current feature map as a minor branch and apply three basic building blocks to it. Then they add it back to the main branch after that one has been upsampeled again to the original size. The feature maps between each basic building block have (usually) 256 channels. Their HourGlass ends in two 1x1 convolutions that create the heatmaps. They stack two of their HourGlass networks after each other. Between them they place an intermediate loss. That way, the second network can learn to improve the predictions of the first network. Architecture visualized:  Heatmaps  The output generated by the network are heatmaps, one per joint. Each ground truth heatmap has a small gaussian peak at the correct position of a joint, everything else has value 0. If a joint isn't visible, the ground truth heatmap for that joint is all zeros. Other stuff  They use batch normalization. Activation functions are ReLUs. They use RMSprob as their optimizer. Implemented in Torch. Results  They train and test on FLIC (only one HourGlass) and MPII (two stacked HourGlass networks). Training is done with augmentations (horizontal flip, up to 30 degress rotation, scaling, no translation to keep the body of interest in the center of the image). Evaluation is done via PCK@0.2 (i.e. percentage of predicted keypoints that are within 0.2 head sizes of their ground truth annotation (head size of the specific body)). Results on FLIC are at >95%. Results on MPII are between 80.6% (ankle) and 97.6% (head). Average is 89.4%. Using two stacked HourGlass networks performs around 3% better than one HourGlass network (even when adjusting for parameters). Training time was 5 days on a Titan X (9xx generation).", "pdf_url": "https://arxiv.org/pdf/1603.06937", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/stacked_hourglass_networks_for_human_pose_estimation.json"}
{"id": "45043151", "bin": "500_600", "summary_sentences": ["What  They present a method to learn mapping functions that transform images from one style to another style.", "(E.g. photos from daylight to nighttime.)", "Their method only requires example images for both styles (i.e. class labels per image).", "How  Architecture  Their method is based on VAEs (i.e. autoencoders) and GANs.", "Their architecture is kinda similar to an autoencoder.", "For an image style A, an encoder E first transform an image to a vector representation z.", "Then a generator G transforms z into an image.", "There are two encoders (E1, E2), one per image style (A, B).", "There are two generators (G1, G2), one per image style (A, B).", "There are two discriminators (D1, D2), one per generator (and therefore style).", "An image can be changed in style from A to B using e.g. G2(E1(I_A)).", "The weights of the encoders are mostly tied/shared.", "Only the last layers are not-shared.", "The weights of the generators are mostly tied/shared.", "Only the last layers are not-shared.", "They use 3 convs + 4 residual blocks for the encoders and 4 residual blocks + 3 transposes convs for the generators.", "They use normal convs for the discriminators.", "Nonlinearities are LeakyReLUs.", "The encoders are VAEs and trained with common VAE-losses (i.e. lower bound optimization).", "However, they only predict mean values per component in z, not variances.", "The variances are all 1.", "Visualization of the architecture:  Loss  Their loss consists of three components:  VAE-loss: Reconstruction loss (absolute distance) and KL term on z (to keep it close to the standard normal distribution).", "Most weight is put on the reconstruction loss.", "GAN-loss: Standard as in other GANs, i.e. cross-entropy.", "Cycle-Consistency-loss: For an image I_A, it is expected to look the same after switching back and forth between image styles, i.e. I_A = G1(E2( G2(E1(I_A)) )) (switch from style A to B, then from B to A).", "The cycle consistency loss uses a reconstruction loss and two KL-terms (one for the first E(.)", "and one for the second).", "Results  When testing on the (satellite) map dataset:  Weight sharing between encoders and between generators improved accuracy.", "The cycle consistency loss improved accuracy.", "Using 4-6 layers (as opposed to just 3) in the discriminator improved accuracy.", "Translations that added details (e.g. night to day) were harder for the model.", "After training, the features from each discriminator seem to be quite good for the respective dataset (i.e. unsupervised learned features).", "Example translations:"], "summary_text": "What  They present a method to learn mapping functions that transform images from one style to another style. (E.g. photos from daylight to nighttime.) Their method only requires example images for both styles (i.e. class labels per image). How  Architecture  Their method is based on VAEs (i.e. autoencoders) and GANs. Their architecture is kinda similar to an autoencoder. For an image style A, an encoder E first transform an image to a vector representation z. Then a generator G transforms z into an image. There are two encoders (E1, E2), one per image style (A, B). There are two generators (G1, G2), one per image style (A, B). There are two discriminators (D1, D2), one per generator (and therefore style). An image can be changed in style from A to B using e.g. G2(E1(I_A)). The weights of the encoders are mostly tied/shared. Only the last layers are not-shared. The weights of the generators are mostly tied/shared. Only the last layers are not-shared. They use 3 convs + 4 residual blocks for the encoders and 4 residual blocks + 3 transposes convs for the generators. They use normal convs for the discriminators. Nonlinearities are LeakyReLUs. The encoders are VAEs and trained with common VAE-losses (i.e. lower bound optimization). However, they only predict mean values per component in z, not variances. The variances are all 1. Visualization of the architecture:  Loss  Their loss consists of three components:  VAE-loss: Reconstruction loss (absolute distance) and KL term on z (to keep it close to the standard normal distribution). Most weight is put on the reconstruction loss. GAN-loss: Standard as in other GANs, i.e. cross-entropy. Cycle-Consistency-loss: For an image I_A, it is expected to look the same after switching back and forth between image styles, i.e. I_A = G1(E2( G2(E1(I_A)) )) (switch from style A to B, then from B to A). The cycle consistency loss uses a reconstruction loss and two KL-terms (one for the first E(.) and one for the second). Results  When testing on the (satellite) map dataset:  Weight sharing between encoders and between generators improved accuracy. The cycle consistency loss improved accuracy. Using 4-6 layers (as opposed to just 3) in the discriminator improved accuracy. Translations that added details (e.g. night to day) were harder for the model. After training, the features from each discriminator seem to be quite good for the respective dataset (i.e. unsupervised learned features). Example translations:", "pdf_url": "https://arxiv.org/pdf/1703.00848", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/unsupervised_image-to-image_translation_networks.json"}
{"id": "42045072", "bin": "500_600", "summary_sentences": ["What  They describe an architecture for deep CNNs that contains short and long paths.", "(Short = few convolutions between input and output, long = many convolutions between input and output)  They achieve comparable accuracy to residual networks, without using residuals.", "How  Basic principle:  They start with two branches.", "The left branch contains one convolutional layer, the right branch contains a subnetwork.", "That subnetwork again contains a left branch (one convolutional layer) and a right branch (a subnetwork).", "This creates a recursion.", "At the last step of the recursion they simply insert two convolutional layers as the subnetwork.", "Each pair of branches (left and right) is merged using a pair-wise mean.", "(Result: One of the branches can be skipped or removed and the result after the merge will still be sound.)", "Their recursive expansion rule (left) and architecture (middle and right) visualized:  Blocks:  Each of the recursively generated networks is one block.", "They chain five blocks in total to create the network that they use for their experiments.", "After each block they add a max pooling layer.", "Their first block uses 64 filters per convolutional layer, the second one 128, followed by 256, 512 and again 512.", "Drop-path:  They randomly dropout whole convolutional layers between merge-layers.", "They define two methods for that:  Local drop-path: Drops each input to each merge layer with a fixed probability, but at least one always survives.", "(See image, first three examples.)", "Global drop-path: Drops convolutional layers so that only a single columns (and thereby path) in the whole network survives.", "(See image, right.)", "Visualization:  Results  They test on CIFAR-10, CIFAR-100 and SVHN with no or mild (crops, flips) augmentation.", "They add dropout at the start of each block (probabilities: 0%, 10%, 20%, 30%, 40%).", "They use for 50% of the batches local drop-path at 15% and for the other 50% global drop-path.", "They achieve comparable accuracy to ResNets (a bit behind them actually).", "Note: The best ResNet that they compare to is \"ResNet with Identity Mappings\".", "They don't compare to Wide ResNets, even though they perform best.", "If they use image augmentations, dropout and drop-path don't seem to provide much benefit (only small improvement).", "If they extract the deepest column and test on that one alone, they achieve nearly the same performance as with the whole network.", "They derive from that, that their fractal architecture is actually only really used to help that deepest column to learn anything.", "(Without shorter paths it would just learn nothing due to vanishing gradients.)"], "summary_text": "What  They describe an architecture for deep CNNs that contains short and long paths. (Short = few convolutions between input and output, long = many convolutions between input and output)  They achieve comparable accuracy to residual networks, without using residuals. How  Basic principle:  They start with two branches. The left branch contains one convolutional layer, the right branch contains a subnetwork. That subnetwork again contains a left branch (one convolutional layer) and a right branch (a subnetwork). This creates a recursion. At the last step of the recursion they simply insert two convolutional layers as the subnetwork. Each pair of branches (left and right) is merged using a pair-wise mean. (Result: One of the branches can be skipped or removed and the result after the merge will still be sound.) Their recursive expansion rule (left) and architecture (middle and right) visualized:  Blocks:  Each of the recursively generated networks is one block. They chain five blocks in total to create the network that they use for their experiments. After each block they add a max pooling layer. Their first block uses 64 filters per convolutional layer, the second one 128, followed by 256, 512 and again 512. Drop-path:  They randomly dropout whole convolutional layers between merge-layers. They define two methods for that:  Local drop-path: Drops each input to each merge layer with a fixed probability, but at least one always survives. (See image, first three examples.) Global drop-path: Drops convolutional layers so that only a single columns (and thereby path) in the whole network survives. (See image, right.) Visualization:  Results  They test on CIFAR-10, CIFAR-100 and SVHN with no or mild (crops, flips) augmentation. They add dropout at the start of each block (probabilities: 0%, 10%, 20%, 30%, 40%). They use for 50% of the batches local drop-path at 15% and for the other 50% global drop-path. They achieve comparable accuracy to ResNets (a bit behind them actually). Note: The best ResNet that they compare to is \"ResNet with Identity Mappings\". They don't compare to Wide ResNets, even though they perform best. If they use image augmentations, dropout and drop-path don't seem to provide much benefit (only small improvement). If they extract the deepest column and test on that one alone, they achieve nearly the same performance as with the whole network. They derive from that, that their fractal architecture is actually only really used to help that deepest column to learn anything. (Without shorter paths it would just learn nothing due to vanishing gradients.)", "pdf_url": "http://arxiv.org/pdf/1605.07648v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/fractalnet_ultra-deep_networks_without_residuals.json"}
{"id": "69694249", "bin": "500_600", "summary_sentences": ["The paper presents Neural Relational Inference (NRI) model which can infer underlying interactions in a dynamical system in an unsupervised manner, using just the observational data in terms of the trajectories.", "For instance, consider a simulated system where the particles are connected to each other by springs.", "The observational data does not explicitly specify which particles are connected to each other and only contains information like position and velocity of each particle at different timesteps.", "The task is to explicitly infer the interaction structure (in this example, which pair of particles are connected to each other) while learning the dynamical model of the system itself.", "Link to the implementation  Model  The model consists of an encoder that encodes the given trajectories into an interaction graph and a decoder that decodes the dynamical model given the interaction graph.", "The model starts by assuming that a full connected interaction graph exists between the objects in the system.", "For this latent graph z, zi, j denotes the (discrete) edge type between object vi and vj with the assumption that there are K edge types.", "The object vi has a feature vector xit associated with it at time t. This feature vector captures information like location and velocity.", "Encoder  A Graph Neural Network (GNN) acts on the fully connected latent graph z, performs message passing from node to node via edges and predicts the discrete label for each edge.", "The GNN architecture may itself use MLPs or ConvNets and returns a factorised distribution over the edge types qφ(z|x).", "Decoder  The decoder is another GNN (with separate params for each edge type) that predicts the future dynamics of the system and returns pθ(x|z).", "The overall model is a VAE that optimizes the ELBO given as:  Eqφ(z|x)[log pθ(x|z)] − KL[qφ(z|x)||pθ(z)]  pθ(x) is the prior which is assumed to be uniform distribution over the edge types.", "Instead of predicting the dynamics of the system for just the next timestep, the paper chooses to use the prediction multiple steps (10) in the future.", "This ensures that the interactions can have a significant effect on the dynamics of the system.", "In some cases, like real humans playing a physical sport, the dynamics of the system need not be Markovian and a recurrent decoder is used to model the time dependence.", "Pipeline  Given the dynamical system, run the encoder to obtain qφ(z|x).", "Sample zi, j from qφ(z|x).", "Run the decoder to predict the future dynamics for the next T timesteps.", "Optimise the ELBO loss.", "Note that since the latent variables (edge labels) are discrete in this case, the sampling is done from a continuous approximation of the discrete distribution and reparameterization trick is applied over this discrete approximation to get the (biased) gradients.", "Observations  Experiments are performed using simulated systems like particles connected to springs, phase coupled oscillators and charged particles and using real-world data like CMU Motion Capture database and NBA tracking data.", "The NRI system effectively predicts the dynamics of the systems and is able to reconstruct the ground truth interaction graph (for simulated systems)."], "summary_text": "The paper presents Neural Relational Inference (NRI) model which can infer underlying interactions in a dynamical system in an unsupervised manner, using just the observational data in terms of the trajectories. For instance, consider a simulated system where the particles are connected to each other by springs. The observational data does not explicitly specify which particles are connected to each other and only contains information like position and velocity of each particle at different timesteps. The task is to explicitly infer the interaction structure (in this example, which pair of particles are connected to each other) while learning the dynamical model of the system itself. Link to the implementation  Model  The model consists of an encoder that encodes the given trajectories into an interaction graph and a decoder that decodes the dynamical model given the interaction graph. The model starts by assuming that a full connected interaction graph exists between the objects in the system. For this latent graph z, zi, j denotes the (discrete) edge type between object vi and vj with the assumption that there are K edge types. The object vi has a feature vector xit associated with it at time t. This feature vector captures information like location and velocity. Encoder  A Graph Neural Network (GNN) acts on the fully connected latent graph z, performs message passing from node to node via edges and predicts the discrete label for each edge. The GNN architecture may itself use MLPs or ConvNets and returns a factorised distribution over the edge types qφ(z|x). Decoder  The decoder is another GNN (with separate params for each edge type) that predicts the future dynamics of the system and returns pθ(x|z). The overall model is a VAE that optimizes the ELBO given as:  Eqφ(z|x)[log pθ(x|z)] − KL[qφ(z|x)||pθ(z)]  pθ(x) is the prior which is assumed to be uniform distribution over the edge types. Instead of predicting the dynamics of the system for just the next timestep, the paper chooses to use the prediction multiple steps (10) in the future. This ensures that the interactions can have a significant effect on the dynamics of the system. In some cases, like real humans playing a physical sport, the dynamics of the system need not be Markovian and a recurrent decoder is used to model the time dependence. Pipeline  Given the dynamical system, run the encoder to obtain qφ(z|x). Sample zi, j from qφ(z|x). Run the decoder to predict the future dynamics for the next T timesteps. Optimise the ELBO loss. Note that since the latent variables (edge labels) are discrete in this case, the sampling is done from a continuous approximation of the discrete distribution and reparameterization trick is applied over this discrete approximation to get the (biased) gradients. Observations  Experiments are performed using simulated systems like particles connected to springs, phase coupled oscillators and charged particles and using real-world data like CMU Motion Capture database and NBA tracking data. The NRI system effectively predicts the dynamics of the systems and is able to reconstruct the ground truth interaction graph (for simulated systems).", "pdf_url": "https://arxiv.org/pdf/1802.04687", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/neural-relational-inference-for-interacting-systems.json"}
{"id": "75389943", "bin": "500_600", "summary_sentences": ["The paper demonstrates that Memory Augmented Neural Networks (MANN) are suitable for one-shot learning by introducing a new method for accessing an external memory.", "This method focuses on memory content while earlier methods additionally used memory location based focusing mechanisms.", "Here, MANN refers to neural networks that have an external memory.", "This includes Neural Turning Machines (NTMs) and excludes LSTMs.", "Meta-Learning  In meta-learning, a learner is learning at two levels.", "The learner is shown a sequence of tasks D1, D2, …, DT.", "When it is training on one of the datasets (say DT), it learns to solve the current dataset.", "At the same time, the learner tries to incorporate knowledge about how task structure changes across different datasets (second level of learning).", "MANN + Meta Learning  Following are the desirable characteristics for a scalable, combined architecture:  Memory representation should be both stable and element-wise accessible.", "Number of model parameters should not be tied to the size of the memory.", "Task Setup  In standard learning, the goal is to reduce error on some dataset D. In meta-learning, the goal is to reduce the error across a distribution of datasets p(D).", "Each dataset is presented to the model in the form (x1, null), (x1, y0), …, (xt+1, yt) where yt is the correct label (or value) corresponding to the inpuit xt.", "Further, the data labels are shuffled from dataset to dataset.", "The model must learn to hold the data samples in memory till the appropriate candidate labels are presented in the next step.", "The idea is that a model that meta learns would learn to map data representation to correct labels regardless of the actual context of data representation or the label.", "The paper uses NTM as the MANN with one modification.", "In the original formulation, the memories were addressed by both context and location.", "Location-based addressing is not optimal for the current setup where information encoding is not independent of the sequence.", "A new access module - LRUA - Least Recent Used Access - is used to write to memory.", "LRUA is purely content-based and writes to either least used memory location (to preserve recent information) or most recently used memory location (to overwrite recent information with more relevant information).", "This is decided on the basis of interpolation between previous read weights and weights scaled according to the usage weight.", "Datasets  Omniglot (classification)  Sampled functions from Gaussian Processes  Results  For the omniglot dataset, the model was trained with various combinations of randomly chosen classes with randomly chosen labels.", "As baselines, following models were considered:  Regular NTM  LSTM  Feedforward RNN  Nearest Neighbour Classifier  Since each episode (dataset created by the combination of classes) contains unique classes (with their own unique labels) it is important to clear the memory across different episodes.", "For the regression task, the data was generated from a GP prior with a fixed set of hyper-parameters which resulted in different functions.", "For both the tasks, the MANN architecture outperforms the LSTM architecture baseline NTMs."], "summary_text": "The paper demonstrates that Memory Augmented Neural Networks (MANN) are suitable for one-shot learning by introducing a new method for accessing an external memory. This method focuses on memory content while earlier methods additionally used memory location based focusing mechanisms. Here, MANN refers to neural networks that have an external memory. This includes Neural Turning Machines (NTMs) and excludes LSTMs. Meta-Learning  In meta-learning, a learner is learning at two levels. The learner is shown a sequence of tasks D1, D2, …, DT. When it is training on one of the datasets (say DT), it learns to solve the current dataset. At the same time, the learner tries to incorporate knowledge about how task structure changes across different datasets (second level of learning). MANN + Meta Learning  Following are the desirable characteristics for a scalable, combined architecture:  Memory representation should be both stable and element-wise accessible. Number of model parameters should not be tied to the size of the memory. Task Setup  In standard learning, the goal is to reduce error on some dataset D. In meta-learning, the goal is to reduce the error across a distribution of datasets p(D). Each dataset is presented to the model in the form (x1, null), (x1, y0), …, (xt+1, yt) where yt is the correct label (or value) corresponding to the inpuit xt. Further, the data labels are shuffled from dataset to dataset. The model must learn to hold the data samples in memory till the appropriate candidate labels are presented in the next step. The idea is that a model that meta learns would learn to map data representation to correct labels regardless of the actual context of data representation or the label. The paper uses NTM as the MANN with one modification. In the original formulation, the memories were addressed by both context and location. Location-based addressing is not optimal for the current setup where information encoding is not independent of the sequence. A new access module - LRUA - Least Recent Used Access - is used to write to memory. LRUA is purely content-based and writes to either least used memory location (to preserve recent information) or most recently used memory location (to overwrite recent information with more relevant information). This is decided on the basis of interpolation between previous read weights and weights scaled according to the usage weight. Datasets  Omniglot (classification)  Sampled functions from Gaussian Processes  Results  For the omniglot dataset, the model was trained with various combinations of randomly chosen classes with randomly chosen labels. As baselines, following models were considered:  Regular NTM  LSTM  Feedforward RNN  Nearest Neighbour Classifier  Since each episode (dataset created by the combination of classes) contains unique classes (with their own unique labels) it is important to clear the memory across different episodes. For the regression task, the data was generated from a GP prior with a fixed set of hyper-parameters which resulted in different functions. For both the tasks, the MANN architecture outperforms the LSTM architecture baseline NTMs.", "pdf_url": "https://arxiv.org/pdf/1605.06065", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/one-shot-learning-with-memory-augmented-neural-networks.json"}
{"id": "15643187", "bin": "500_600", "summary_sentences": ["What  They add a dilation factor to recurrent neural networks (e.g. LSTMs), similar to dilation in convolutions.", "This enables learning of long-term dependencies, prevents vanishing/exploding gradients and makes the networks sometimes more parallelizable.", "How  Dilation  Dilation d here simply means, that each timestep gets input from the d-th previous timestep.", "With d=1, this is identical to a normal reccurent network.", "With d=2 there is a gap of 1 between each timestep.", "They suggest to let the dilation exponentially increase per layer, e.g. 1, 2, 4, ...  Visualization:  Using dilation can make it possible to execute some steps of the RNN in parallel.", "The following visualization shows that:  The dilation may start at a value higher than 1.", "This however should be compensated before generating the output vector.", "To do that, output of the last layer at timestep t and t-1 has to be used.", "Visualization:  Memory capacity  They show that the memory capacity of dilated RNNs is better than in skip RNNs, i.e. the average path length in the network is shorter.", "Results  Copy-Task  This task involves copying of inputs.", "I.e. the network gets some integers at the start, then T={500, 1000} timesteps pass, then it has to output the input values.", "They use 9 layers with a dilation of up to 256.", "(This means that it is really almost raw copying of data for the network.)", "Dilated RNNs perform best here, followed by dilated GRUs and dilated LSTMs.", "All non-dilated networks resort to random guessing (i.e. fail to learn anything).", "MNIST  They predict classes on MNIST, where each image is turned into a 784-element vector.", "All networks, including competitors, can handle that task.", "They make the task harder by padding the vectors to 1000 and 2000 elements length.", "Then only their dilated networks, dilated (1d-)CNNs and RNNs with skip connections can handle the task.", "Their dilated RNNs learn faster the more layers they have.", "With 2 layers they learn nothing.", "With few layers they can sometimes also have major swings in accuracy during training.", "When increasing the minimum dilation, they find that they can drop layers and still achieve almost the same accuracy, leading to much faster training (wall-clock time).", "Training with minimum dilation 2 leads to same accuracy at half the training time.", "Language modelling  They test their models on Penn Treebank character predictions.", "Their models get beaten by LayerNorm HM-LSTM, HyperNetworks, Zoneout.", "They argue though that their models achieve the highest scores among models that do not use any normalization.", "Speaker identification from raw waveform  They train on VCTK.", "Their dilated models achieve much higher accuracy than non-dilated ones and can compete with models using MFCC features."], "summary_text": "What  They add a dilation factor to recurrent neural networks (e.g. LSTMs), similar to dilation in convolutions. This enables learning of long-term dependencies, prevents vanishing/exploding gradients and makes the networks sometimes more parallelizable. How  Dilation  Dilation d here simply means, that each timestep gets input from the d-th previous timestep. With d=1, this is identical to a normal reccurent network. With d=2 there is a gap of 1 between each timestep. They suggest to let the dilation exponentially increase per layer, e.g. 1, 2, 4, ...  Visualization:  Using dilation can make it possible to execute some steps of the RNN in parallel. The following visualization shows that:  The dilation may start at a value higher than 1. This however should be compensated before generating the output vector. To do that, output of the last layer at timestep t and t-1 has to be used. Visualization:  Memory capacity  They show that the memory capacity of dilated RNNs is better than in skip RNNs, i.e. the average path length in the network is shorter. Results  Copy-Task  This task involves copying of inputs. I.e. the network gets some integers at the start, then T={500, 1000} timesteps pass, then it has to output the input values. They use 9 layers with a dilation of up to 256. (This means that it is really almost raw copying of data for the network.) Dilated RNNs perform best here, followed by dilated GRUs and dilated LSTMs. All non-dilated networks resort to random guessing (i.e. fail to learn anything). MNIST  They predict classes on MNIST, where each image is turned into a 784-element vector. All networks, including competitors, can handle that task. They make the task harder by padding the vectors to 1000 and 2000 elements length. Then only their dilated networks, dilated (1d-)CNNs and RNNs with skip connections can handle the task. Their dilated RNNs learn faster the more layers they have. With 2 layers they learn nothing. With few layers they can sometimes also have major swings in accuracy during training. When increasing the minimum dilation, they find that they can drop layers and still achieve almost the same accuracy, leading to much faster training (wall-clock time). Training with minimum dilation 2 leads to same accuracy at half the training time. Language modelling  They test their models on Penn Treebank character predictions. Their models get beaten by LayerNorm HM-LSTM, HyperNetworks, Zoneout. They argue though that their models achieve the highest scores among models that do not use any normalization. Speaker identification from raw waveform  They train on VCTK. Their dilated models achieve much higher accuracy than non-dilated ones and can compete with models using MFCC features.", "pdf_url": "https://arxiv.org/pdf/1710.02224", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/dilated_recurrent_neural_networks.json"}
{"id": "97980035", "bin": "500_600", "summary_sentences": ["What  They propose a CNN-based approach to detect faces in a wide range of orientations using a single model.", "However, since the training set is skewed, the network is more confident about up-right faces.", "The model does not require additional components such as segmentation, bounding-box regression, segmentation, or SVM classifiers  How  Data augmentation: to increase the number of positive samples (24K face annotations), the authors used randomly sampled sub-windows of the images with IOU > 50% and also randomly flipped these images.", "In total, there were 20K positive and 20M negative training samples.", "CNN Architecture: 5 convolutional layers followed by 3 fully-connected.", "The fully-connected layers were converted to convolutional layers.", "Non-Maximal Suppression is applied to merge predicted bounding boxes.", "Training: the CNN was trained using Caffe Library in the AFLW dataset with the following parameters:  Fine-tuning with AlexNet model  Input image size = 227x227  Batch size = 128 (32+, 96-)  Stride = 32  Test: the model was evaluated on PASCAL FACE, AFW, and FDDB dataset.", "Running time: since the fully-connected layers were converted to convolutional layers, the input image in running time may be of any size, obtaining a heat map as output.", "To detect faces of different sizes though, the image is scaled up/down and new heatmaps are obtained.", "The authors found that rescaling image 3 times per octave gives reasonable good performance.", "The authors realized that the model is more confident about up-right faces than rotated/occluded ones.", "This trend is because the lack of good training examples to represent such faces in the training process.", "Better results can be achieved by using better sampling strategies and more sophisticated data augmentation techniques.", "The authors tested different strategies for NMS and the effect of bounding-box regression for improving face detection.", "They NMS-avg had better performance compared to NMS-max in terms of average precision.", "On the other hand, adding a bounding-box regressor degraded the performance for both NMS strategies due to the mismatch between annotations of the training set and the test set.", "This mismatch is mostly for side-view faces.", "Results:  In comparison to R-CNN, the proposed face detector had significantly better performance independent of the NMS strategy.", "The authors believe the inferior performance of R-CNN due to the loss of recall since selective search may miss some of the face regions; and loss in localization since bounding-box regression is not perfect and may not be able to fully align the segmentation bounding-boxes, provided by selective search, with the ground truth.", "In comparison to other state-of-art methods like structural model, TSM and cascade-based methods the DDFD achieve similar or better results.", "However, this comparison is not completely fair since the most of methods use extra information of pose annotation or information about facial landmarks during the training."], "summary_text": "What  They propose a CNN-based approach to detect faces in a wide range of orientations using a single model. However, since the training set is skewed, the network is more confident about up-right faces. The model does not require additional components such as segmentation, bounding-box regression, segmentation, or SVM classifiers  How  Data augmentation: to increase the number of positive samples (24K face annotations), the authors used randomly sampled sub-windows of the images with IOU > 50% and also randomly flipped these images. In total, there were 20K positive and 20M negative training samples. CNN Architecture: 5 convolutional layers followed by 3 fully-connected. The fully-connected layers were converted to convolutional layers. Non-Maximal Suppression is applied to merge predicted bounding boxes. Training: the CNN was trained using Caffe Library in the AFLW dataset with the following parameters:  Fine-tuning with AlexNet model  Input image size = 227x227  Batch size = 128 (32+, 96-)  Stride = 32  Test: the model was evaluated on PASCAL FACE, AFW, and FDDB dataset. Running time: since the fully-connected layers were converted to convolutional layers, the input image in running time may be of any size, obtaining a heat map as output. To detect faces of different sizes though, the image is scaled up/down and new heatmaps are obtained. The authors found that rescaling image 3 times per octave gives reasonable good performance. The authors realized that the model is more confident about up-right faces than rotated/occluded ones. This trend is because the lack of good training examples to represent such faces in the training process. Better results can be achieved by using better sampling strategies and more sophisticated data augmentation techniques. The authors tested different strategies for NMS and the effect of bounding-box regression for improving face detection. They NMS-avg had better performance compared to NMS-max in terms of average precision. On the other hand, adding a bounding-box regressor degraded the performance for both NMS strategies due to the mismatch between annotations of the training set and the test set. This mismatch is mostly for side-view faces. Results:  In comparison to R-CNN, the proposed face detector had significantly better performance independent of the NMS strategy. The authors believe the inferior performance of R-CNN due to the loss of recall since selective search may miss some of the face regions; and loss in localization since bounding-box regression is not perfect and may not be able to fully align the segmentation bounding-boxes, provided by selective search, with the ground truth. In comparison to other state-of-art methods like structural model, TSM and cascade-based methods the DDFD achieve similar or better results. However, this comparison is not completely fair since the most of methods use extra information of pose annotation or information about facial landmarks during the training.", "pdf_url": "https://arxiv.org/pdf/1502.02766", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/multi-view_face_detection_using_deep_convolutional_neural_networks.json"}
{"id": "88548359", "bin": "500_600", "summary_sentences": ["Veritas: shared verifiable databases and tables in the cloud Allen et al., CIDR’19  Two (or more) parties want to transact based on the sharing of information (e.g. current offers).", "In order to have trust in the system and provide a foundation for resolving disputes, we’d like a tamperproof and immutable audit log of all shared data and actions, such that an independent auditor can reconstruct the state of the system at any point in time.", "Enter the blockchain?!", "Not so fast say Allen et al., blockchain technology as we know it today is ‘ one step forward, two steps back ’ ;).", "Today, for gaining immutability and auditability with new blockchain platforms, we give up decades of research in data management— and hardened, enterprise-ready code that implements these ideas.", "We’d still like to be able to use SQL for example.", "We want transaction throughput much closer to a traditional database, and we want to take advantage of query optimisation and sophisticated query processing engines.", "We could try adding database like features to blockchain systems, but that looks to be a long road:  There are now a gazillion start-ups that are adding these basic database features to blockchains, but it will take years if not decades to catch up.", "How about trying it the other way round then?", "Start with a mature database system, and add a sprinkling of blockchain?", "Instead of adding database capabilities to blockchains, we propose to address the problem from the opposite approach: we add trust and auditability to existing database management systems.", "The key notions in the paper are verifiable databases and verifiable tables.", "A verifiable database has all the features of a regular database, but in addition it supports tamper-evident collaboration across mutually untrusted entities.", "The idea of a shared verifiable table goes one step further: integrating a special table directly into the existing databases of the transacting parties.", "The same instance of the table is visible to all parties, and all activities are written to a tamper-proof log.", "There is an N:1 relationship between shared tables and tamper-proof logs.", "Verifiable databases (and tables) provide a set of cryptographic guarantees:  each party can verify the actions (updates) of all other parties and provide proof of its own actions  all parties can verify that the state of the shared database (or table) and its responses to queries is consistent with the prior actions of legitimate actors  unauthorized parties (hackers or operators with administrative privileges) cannot tamper with the state of the verifiable database (table) without being detected by the verification mechanism  So we’re looking at a permissioned system supporting a set of verifiers.", "The assumption in this work is that verifiers have access to the full log.", "Confidentiality is an orthogonal concern that could be addressed by frameworks such as Coco , Quorum , Spice , or [Corda](  [url]"], "summary_text": "Veritas: shared verifiable databases and tables in the cloud Allen et al., CIDR’19  Two (or more) parties want to transact based on the sharing of information (e.g. current offers). In order to have trust in the system and provide a foundation for resolving disputes, we’d like a tamperproof and immutable audit log of all shared data and actions, such that an independent auditor can reconstruct the state of the system at any point in time. Enter the blockchain?! Not so fast say Allen et al., blockchain technology as we know it today is ‘ one step forward, two steps back ’ ;). Today, for gaining immutability and auditability with new blockchain platforms, we give up decades of research in data management— and hardened, enterprise-ready code that implements these ideas. We’d still like to be able to use SQL for example. We want transaction throughput much closer to a traditional database, and we want to take advantage of query optimisation and sophisticated query processing engines. We could try adding database like features to blockchain systems, but that looks to be a long road:  There are now a gazillion start-ups that are adding these basic database features to blockchains, but it will take years if not decades to catch up. How about trying it the other way round then? Start with a mature database system, and add a sprinkling of blockchain? Instead of adding database capabilities to blockchains, we propose to address the problem from the opposite approach: we add trust and auditability to existing database management systems. The key notions in the paper are verifiable databases and verifiable tables. A verifiable database has all the features of a regular database, but in addition it supports tamper-evident collaboration across mutually untrusted entities. The idea of a shared verifiable table goes one step further: integrating a special table directly into the existing databases of the transacting parties. The same instance of the table is visible to all parties, and all activities are written to a tamper-proof log. There is an N:1 relationship between shared tables and tamper-proof logs. Verifiable databases (and tables) provide a set of cryptographic guarantees:  each party can verify the actions (updates) of all other parties and provide proof of its own actions  all parties can verify that the state of the shared database (or table) and its responses to queries is consistent with the prior actions of legitimate actors  unauthorized parties (hackers or operators with administrative privileges) cannot tamper with the state of the verifiable database (table) without being detected by the verification mechanism  So we’re looking at a permissioned system supporting a set of verifiers. The assumption in this work is that verifiers have access to the full log. Confidentiality is an orthogonal concern that could be addressed by frameworks such as Coco , Quorum , Spice , or [Corda](  [url]", "pdf_url": "http://cidrdb.org/cidr2019/papers/p111-gehrke-cidr19.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/veritas-shared-verifiable-databases-and-tables-in-the-cloud.json"}
{"id": "31183232", "bin": "500_600", "summary_sentences": ["The paper describes a \"compositional\" training approach for vector space models, corresponding to Knowledge Bases (KBs).", "The new approach improves the system's ability to answer path queries and impute missing information for the KBs.", "Task  Given a KB, knowledge graph, G, is defined as the set of triplets (s, r, t) where s, t ∈ Entities and r ∈ Relations.", "A path query q consists of an initial entity, s, followed by a sequence of relations, p, to be traversed.", "The answer to the query is the set of all the entities that can be reached from s by traversing p.  Knowledge base completion (KBC) is the task of predicting if an edge (s, r, t) belongs in the graph.", "Compositionalization  Given a triplet (s, r, t), define score(s/r, t) as the liklihood of s being connected to t via r.  In general, score(s/r, t) = M(Tr(xs), xt) for some membership operator M and some traversal operator T.  Given a dataset of form (q, t) where q is the path query and t is the answer to the path query,  Minimize the max-margin objective 1 - margin(q, t, t')  margin(q, t, t') = score(q, t) - score(q, t')  This objective function is better that the existing objectives which only train on queries of length 1 (single-edge training).", "Candidates Models  TransE  score(s/r, t) = -|| x<sub>s</sub> + w><sub>r</sub> - x<sub>t</sub>||<sub>2</sub><sup>2</sup>  Bilinear-Diag  Similar to TransE, but with multiplicative interactions between entity and relation vectors.", "Datasets  Single-Edge Query datasets:  Freebase  WordNet  Path Query Dataset  Given a base knowledge graph, generate path queries of different lengths by performing random walks on the graph.", "Results  Evalution Metric  Mean Quantile - For a query q, the quantile of a correct answer t is the fraction of incorrect answers ranked after t.  hit at 10 - Percentage of correct answers ranked among top 10 results.", "Compositional training improves path querying performance across all models and metrics on both the datasets.", "TransE(COMP) is the best model in terms of mean quantile.", "Performance improves for both induction and deduction based queries.", "Analysis  Why does compositional training improve path query answering?", "Cascading nature of errors along the path - For a given edge (s, r, t) on the path, the single-edge training encourages xt to be closer to xs, only to the extent that margin is 1 and does not push them any closer.", "The remaining discrepancy gets added as noise at each step of the traversal.", "Why does compositional training improve knowledge base completion?", "Paths in a knowledge graph are an important feature for predicting the existence of single edges and training on paths should provide some form of structural regularisation which should reduce cascading errors."], "summary_text": "The paper describes a \"compositional\" training approach for vector space models, corresponding to Knowledge Bases (KBs). The new approach improves the system's ability to answer path queries and impute missing information for the KBs. Task  Given a KB, knowledge graph, G, is defined as the set of triplets (s, r, t) where s, t ∈ Entities and r ∈ Relations. A path query q consists of an initial entity, s, followed by a sequence of relations, p, to be traversed. The answer to the query is the set of all the entities that can be reached from s by traversing p.  Knowledge base completion (KBC) is the task of predicting if an edge (s, r, t) belongs in the graph. Compositionalization  Given a triplet (s, r, t), define score(s/r, t) as the liklihood of s being connected to t via r.  In general, score(s/r, t) = M(Tr(xs), xt) for some membership operator M and some traversal operator T.  Given a dataset of form (q, t) where q is the path query and t is the answer to the path query,  Minimize the max-margin objective 1 - margin(q, t, t')  margin(q, t, t') = score(q, t) - score(q, t')  This objective function is better that the existing objectives which only train on queries of length 1 (single-edge training). Candidates Models  TransE  score(s/r, t) = -|| x<sub>s</sub> + w><sub>r</sub> - x<sub>t</sub>||<sub>2</sub><sup>2</sup>  Bilinear-Diag  Similar to TransE, but with multiplicative interactions between entity and relation vectors. Datasets  Single-Edge Query datasets:  Freebase  WordNet  Path Query Dataset  Given a base knowledge graph, generate path queries of different lengths by performing random walks on the graph. Results  Evalution Metric  Mean Quantile - For a query q, the quantile of a correct answer t is the fraction of incorrect answers ranked after t.  hit at 10 - Percentage of correct answers ranked among top 10 results. Compositional training improves path querying performance across all models and metrics on both the datasets. TransE(COMP) is the best model in terms of mean quantile. Performance improves for both induction and deduction based queries. Analysis  Why does compositional training improve path query answering? Cascading nature of errors along the path - For a given edge (s, r, t) on the path, the single-edge training encourages xt to be closer to xs, only to the extent that margin is 1 and does not push them any closer. The remaining discrepancy gets added as noise at each step of the traversal. Why does compositional training improve knowledge base completion? Paths in a knowledge graph are an important feature for predicting the existence of single edges and training on paths should provide some form of structural regularisation which should reduce cascading errors.", "pdf_url": "https://arxiv.org/pdf/1506.01094", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/e6213906ec2642f27b1aca3a6201c6.json"}
{"id": "26473847", "bin": "500_600", "summary_sentences": ["What  They suggest a method to train a model for self-driving cars in a (virtual) game, while letting each frame look like a real one.", "This allows to learn driving policies via reinforcement learning in a virtual environment, yet (later on) use them in real cars.", "How  Basics  The method is based on GANs, similar to something like CycleGAN.", "The basic idea is to transform each game image/frame into a (semantic) segmentation map.", "Then the segmentation map is transformed into a realistic looking image.", "Both steps use GANs.", "The model is then trained on the realistic looking images, instead of the game frames.", "(Why not just train it on the segmentation maps...?)", "They argue that the segmentation maps can be viewed as the semantic representation between both (fake & real) images.", "(Similar to how machine translation models often convert each sentence to a vector representing the semantics before generating the translated sentence.)", "Visualization of the architecture:  Loss  They use conditional GANs.", "The generator gets the frame image x and a noise vector z and has to generate a segmentation map s.  The discriminator gets the frame image x and a segmentation map s and has to tell whether s is real or fake.", "A second pair of generator and discriminator is then used to turn s into real images.", "They use the standard GAN loss.", "They add an L1 loss to \"suppress blurring\".", "(???", "GANs shouldn't generate blurry images.", "This sounds more like they train G to predict s using the L1 loss.", "They then end up with blurry images, so they add the GAN loss to make them sharp.)", "Full loss:  Agent  They use the A3C algorithm for training.", "(12 threads)  Their reward function incentivizes fast speeds with the car being close to the road's center.", "They punish collisions.", "Reward function:  v_t is the speed in m/s  alpha is the angle in rad  beta = 0.006  gamma = -0.025  They predict 9 actions: left/straight/right, each with accelerate/brake/nothing.", "Game  They train on the game \"TORCS\".", "(I guess that game provides segmentation maps for each game frame?)", "Results  They train their model in TORCS on track X and evaluate on Y.", "They achieve slightly better scores than a competing model trained on several tracks (A, B, C, D, ..., but not on X).", "A model trained directly on X peforms significantly better.", "They test their model on a dataset associated with the NVIDIA self-driving paper.", "They reach 43% correct, while the supervised method reaches 53%.", "A competing model \"B-RL\" reaches 28% (reinforcement learned, but only on game images).", "Example translations from game to real:"], "summary_text": "What  They suggest a method to train a model for self-driving cars in a (virtual) game, while letting each frame look like a real one. This allows to learn driving policies via reinforcement learning in a virtual environment, yet (later on) use them in real cars. How  Basics  The method is based on GANs, similar to something like CycleGAN. The basic idea is to transform each game image/frame into a (semantic) segmentation map. Then the segmentation map is transformed into a realistic looking image. Both steps use GANs. The model is then trained on the realistic looking images, instead of the game frames. (Why not just train it on the segmentation maps...?) They argue that the segmentation maps can be viewed as the semantic representation between both (fake & real) images. (Similar to how machine translation models often convert each sentence to a vector representing the semantics before generating the translated sentence.) Visualization of the architecture:  Loss  They use conditional GANs. The generator gets the frame image x and a noise vector z and has to generate a segmentation map s.  The discriminator gets the frame image x and a segmentation map s and has to tell whether s is real or fake. A second pair of generator and discriminator is then used to turn s into real images. They use the standard GAN loss. They add an L1 loss to \"suppress blurring\". (??? GANs shouldn't generate blurry images. This sounds more like they train G to predict s using the L1 loss. They then end up with blurry images, so they add the GAN loss to make them sharp.) Full loss:  Agent  They use the A3C algorithm for training. (12 threads)  Their reward function incentivizes fast speeds with the car being close to the road's center. They punish collisions. Reward function:  v_t is the speed in m/s  alpha is the angle in rad  beta = 0.006  gamma = -0.025  They predict 9 actions: left/straight/right, each with accelerate/brake/nothing. Game  They train on the game \"TORCS\". (I guess that game provides segmentation maps for each game frame?) Results  They train their model in TORCS on track X and evaluate on Y. They achieve slightly better scores than a competing model trained on several tracks (A, B, C, D, ..., but not on X). A model trained directly on X peforms significantly better. They test their model on a dataset associated with the NVIDIA self-driving paper. They reach 43% correct, while the supervised method reaches 53%. A competing model \"B-RL\" reaches 28% (reinforcement learned, but only on game images). Example translations from game to real:", "pdf_url": "https://arxiv.org/pdf/1704.03952", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/virtual_to_real_rl_for_ad.json"}
{"id": "93460473", "bin": "500_600", "summary_sentences": ["Automated Theorem Proving (ATP) - Attempting to prove mathematical theorems automatically.", "Bottlenecks in ATP:  Autoformalization - Semantic or formal parsing of informal proofs.", "Automated Reasoning - Reasoning about already formalised proofs.", "Paper evaluates the effectiveness of neural sequence models for premise selection (related to automated reasoning) without using hand engineered features.", "Premise Selection  Given a large set of premises P, an ATP system A with given resource limits, and a new conjecture C, predict those premises from P that will most likely lead to an automatically constructed proof of C by A  Dataset  Mizar Mathematical Library (MML) used as the formal corpus.", "The premise length varies from 5 to 84299 characters and over 60% if the words occur fewer than 10 times (rare word problem).", "Approach  The model predicts the probability that a given axiom is useful for proving a given conjecture.", "Conjecture and axiom sequences are separately embedded into fixed length real vectors, then concatenated and passed to a third network with few fully connected layers and logistic loss.", "The two embedding networks and the joint predictor path are trained jointly.", "Stage 1: Character-level Models  Treat premises on character level using an 80-dimensional one hot encoding vector.", "Architectures for embedding:  pure recurrent LSTM and GRU Network  CNN (with max pooling)  Recurrent-convolutional network that shortens the sequence using convolutional layer before feeding it to LSTM.", "Stage 2: Word-level Models  MML dataset contains both implicit and explicit definitions.", "To avoid manually encoding the implicit definitions, the entire statement defining an identifier is embedded and the definition embeddings are used as word level embeddings.", "This is better than recursively expanding and embedding the word definition as the definition chains can be very deep.", "Once word level embeddings are obtained, the architecture from Character-level models can be reused.", "Experiments  Metrics  For each conjecture, the model ranks the possible premises.", "Primary metric is the number of conjectures proved from top-k premises.", "Average Max Relative Rank (AMMR) is  more sophisticated measure based on the motivation that conjectures are easier to prove if all their dependencies occur earlier in ranking.", "Since it is very costly to rank all axioms for a conjecture, an approximation is made and a fixed number of random false dependencies are used for evaluating AMMR.", "Network Training  Asynchronous distributed stochastic gradient descent using Adam optimizer.", "Clipped vs Non-clipped Gradients.", "Max Sequence length of 2048 for character-level sequences and 500 for word-level sequences.", "Results  Best Selection Pipeline - Stage 1 character-level CNN which produces word-level embeddings for the next stage.", "Best models use simple CNNs followed by max pooling and two-stage definition-based def-CNN outperforms naive word-CNN (where word embeddings are learnt in a single pass).", "This comment has been minimized.", "Sign in to view  Copy link  Quote reply  Autoformalization commented  Mar 18, 2017  Thank you for your  interest to the concept \"autoformalization\" (some times spelled also as \"Autoformalisation\".", "Just thought that you might  be interested how it was described when I coined the term about 30 years ago ...   [url]"], "summary_text": "Automated Theorem Proving (ATP) - Attempting to prove mathematical theorems automatically. Bottlenecks in ATP:  Autoformalization - Semantic or formal parsing of informal proofs. Automated Reasoning - Reasoning about already formalised proofs. Paper evaluates the effectiveness of neural sequence models for premise selection (related to automated reasoning) without using hand engineered features. Premise Selection  Given a large set of premises P, an ATP system A with given resource limits, and a new conjecture C, predict those premises from P that will most likely lead to an automatically constructed proof of C by A  Dataset  Mizar Mathematical Library (MML) used as the formal corpus. The premise length varies from 5 to 84299 characters and over 60% if the words occur fewer than 10 times (rare word problem). Approach  The model predicts the probability that a given axiom is useful for proving a given conjecture. Conjecture and axiom sequences are separately embedded into fixed length real vectors, then concatenated and passed to a third network with few fully connected layers and logistic loss. The two embedding networks and the joint predictor path are trained jointly. Stage 1: Character-level Models  Treat premises on character level using an 80-dimensional one hot encoding vector. Architectures for embedding:  pure recurrent LSTM and GRU Network  CNN (with max pooling)  Recurrent-convolutional network that shortens the sequence using convolutional layer before feeding it to LSTM. Stage 2: Word-level Models  MML dataset contains both implicit and explicit definitions. To avoid manually encoding the implicit definitions, the entire statement defining an identifier is embedded and the definition embeddings are used as word level embeddings. This is better than recursively expanding and embedding the word definition as the definition chains can be very deep. Once word level embeddings are obtained, the architecture from Character-level models can be reused. Experiments  Metrics  For each conjecture, the model ranks the possible premises. Primary metric is the number of conjectures proved from top-k premises. Average Max Relative Rank (AMMR) is  more sophisticated measure based on the motivation that conjectures are easier to prove if all their dependencies occur earlier in ranking. Since it is very costly to rank all axioms for a conjecture, an approximation is made and a fixed number of random false dependencies are used for evaluating AMMR. Network Training  Asynchronous distributed stochastic gradient descent using Adam optimizer. Clipped vs Non-clipped Gradients. Max Sequence length of 2048 for character-level sequences and 500 for word-level sequences. Results  Best Selection Pipeline - Stage 1 character-level CNN which produces word-level embeddings for the next stage. Best models use simple CNNs followed by max pooling and two-stage definition-based def-CNN outperforms naive word-CNN (where word embeddings are learnt in a single pass). This comment has been minimized. Sign in to view  Copy link  Quote reply  Autoformalization commented  Mar 18, 2017  Thank you for your  interest to the concept \"autoformalization\" (some times spelled also as \"Autoformalisation\". Just thought that you might  be interested how it was described when I coined the term about 30 years ago ...   [url]", "pdf_url": "https://arxiv.org/pdf/1606.04442", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/387256f2bb08f39509600f9d7db498.json"}
{"id": "94646863", "bin": "500_600", "summary_sentences": ["Semantic parsing is the problem of mapping natural language utterances into logical forms that can be executed on a Knowledge Base (KB).", "The paper presents a new approach to semantic parsing that uses paraphrasing to leverage the large amount of text which is not covered by the KB.", "Approach  Given an input utterance x, construct a set of logical forms Zx using a different kind of logical form templates.", "For each logical form z in Zx, generate a small set Cz, of canonical utterances using Freebase description of the type, entity and property involved in the logical form.", "Note: Both the steps above are performed with a small, simple set of deterministic rules which the authors found sufficient for their datasets.", "For each z in Z and each c in Cz, use a paraphrase model to score pairs (c, z) given x.", "The paraphrase model has two parts:  Association Model  For each pair of (x, c), the model goes through all spans of x and c and identifies pairs of potential paraphrases (associations).", "To determine the associations, the model uses  Phrase pairs from a phrase table, constructed using Paralex corpus.", "Linguistic features like lemma, POS tag and Wordnet derivations.", "During training, the model learns to weight the associations appropriately.", "Vector Space Model  Assign vector representations to x and c by averaging over the word2vec representations corresponding to the different words in these utterances.", "Estimate paraphrase score for (x, c) via weighted combination of their vector representations.", "The two paraphrase models are complementary to each other in terms of the information they capture.", "Training  Dataset  WEBQUESTIONS dataset - 5810 question answer pairs.", "FREE917 dataset - 917 questions (annoted with logical form).", "Learning  Given the question-answer pair (xi, yi), the objective function minimizes the log-likelihood of the correct answer along with the restriction of L1 regularization.", "Results  The proposed model improves the accuracy on WEBQUESTIONS by 12% and matches the best results on FREE917.", "Removing the association model results in a much larger degradation of performance as compared to removing the VS model.", "Error analysis suggests that the model:  can not handle temporal relations.", "suffers from ambiguity in entity recognition.", "counts multiple associations multiple times and assigns inflated scores to such associations.", "Comments  The core idea of using paraphrasing for semantic parsing seems promising and would further benefit from advanced models like skip-thought vectors which provide a more natural vector representation for the sentences, thereby helping to reduce the dependence on handcrafted features.", "This comment has been minimized.", "Sign in to view  Copy link  Quote reply  BrijeshKaria commented  Jan 20, 2018  Is there any good implementation of this available as open source?", "I am trying sempre which is based on this same concept.", "I am however stuck there with lack of good documentation about how to create paraphrase models for custom domain."], "summary_text": "Semantic parsing is the problem of mapping natural language utterances into logical forms that can be executed on a Knowledge Base (KB). The paper presents a new approach to semantic parsing that uses paraphrasing to leverage the large amount of text which is not covered by the KB. Approach  Given an input utterance x, construct a set of logical forms Zx using a different kind of logical form templates. For each logical form z in Zx, generate a small set Cz, of canonical utterances using Freebase description of the type, entity and property involved in the logical form. Note: Both the steps above are performed with a small, simple set of deterministic rules which the authors found sufficient for their datasets. For each z in Z and each c in Cz, use a paraphrase model to score pairs (c, z) given x. The paraphrase model has two parts:  Association Model  For each pair of (x, c), the model goes through all spans of x and c and identifies pairs of potential paraphrases (associations). To determine the associations, the model uses  Phrase pairs from a phrase table, constructed using Paralex corpus. Linguistic features like lemma, POS tag and Wordnet derivations. During training, the model learns to weight the associations appropriately. Vector Space Model  Assign vector representations to x and c by averaging over the word2vec representations corresponding to the different words in these utterances. Estimate paraphrase score for (x, c) via weighted combination of their vector representations. The two paraphrase models are complementary to each other in terms of the information they capture. Training  Dataset  WEBQUESTIONS dataset - 5810 question answer pairs. FREE917 dataset - 917 questions (annoted with logical form). Learning  Given the question-answer pair (xi, yi), the objective function minimizes the log-likelihood of the correct answer along with the restriction of L1 regularization. Results  The proposed model improves the accuracy on WEBQUESTIONS by 12% and matches the best results on FREE917. Removing the association model results in a much larger degradation of performance as compared to removing the VS model. Error analysis suggests that the model:  can not handle temporal relations. suffers from ambiguity in entity recognition. counts multiple associations multiple times and assigns inflated scores to such associations. Comments  The core idea of using paraphrasing for semantic parsing seems promising and would further benefit from advanced models like skip-thought vectors which provide a more natural vector representation for the sentences, thereby helping to reduce the dependence on handcrafted features. This comment has been minimized. Sign in to view  Copy link  Quote reply  BrijeshKaria commented  Jan 20, 2018  Is there any good implementation of this available as open source? I am trying sempre which is based on this same concept. I am however stuck there with lack of good documentation about how to create paraphrase models for custom domain.", "pdf_url": "http://nlp.stanford.edu/pubs/berant14paraphrasing.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/c96d7dd0488d0d00bd7078889dd6f6.json"}
{"id": "65102009", "bin": "600_700", "summary_sentences": ["Machine Comprehension (MC) - given a natural language sentence, answer a natural language question.", "End-To-End MC - can not use language resources like dependency parsers.", "The only supervision during training is the correct answer.", "Query Regression Network (QRN) - Variant of Recurrent Neural Network (RNN).", "Related Work  Long Short-Term Memory (LSTM) and Gated Recurrence Unit (GRU) are popular choices to model sequential data but perform poorly on end-to-end MC due to long-term dependencies.", "Attention Models with shared external memory focus on single sentences in each layer but the models tend to be insensitive to the time step of the sentence being accessed.", "Memory Networks (and MemN2N)  Add time-dependent variable to the sentence representation.", "Summarize the memory in each layer to control attention in the next layer.", "Dynamic Memory Networks (and DMN+)  Combine RNN and attention mechanism to incorporate time dependency.", "Uses 2 GRU  time-axis GRU - Summarize the memory in each layer.", "layer-axis GRU - Control the attention in each layer.", "QRN is a much simpler model without any memory summarized node.", "QRN  Single recurrent unit that updates its internal state through time and layers.", "Inputs  qt - local query vector  xt - sentence vector  Outputs  ht - reduced query vector  xt - sentence vector without any modifications  Equations  zt = α(xt, qt)  &alpha is the update gate function to measure the relevance between input sentence and local query.", "h`t = γ(xt, qt)  &gamma is the regression function to transform the local query into regressed query.", "ht = zt*h`t + (1 - zt)*ht-1  To create a multi layer model, output of current layer becomes input to the next layer.", "Variants  Reset gate function (rt) to reset or nullify the regressed query h`t (inspired from GRU).", "The new equation becomes ht = zt*rt*h`t + (1 - zt)*ht-1  Vector gates - update and reset gate functions can produce vectors instead of scalar values (for finer control).", "Bidirectional - QRN can look at both past and future sentences while regressing the queries.", "qtk+1 = htk, forward + htk, backward.", "The variables of update and regress functions are shared between the two directions.", "Parallelization  Unlike most RNN based models, recurrent updates in QRN can be computed in parallel across time.", "For details and equations, refer the paper .", "Module Details  Input Modules  A trainable embedding matrix A is used to encode the one-hot vector of each word in the input sentence into a d-dimensional vector.", "Position Encoder is used to obtain the sentence representation from the d-dimensional vectors.", "Question vectors are also obtained in a similar manner.", "Output Module  A V-way single-layer softmax classifier is used to map predicted answer vector y to a V-dimensional sparse vector v.  The natural language answer y is the arg max word in v.  Results  bAbI QA dataset used.", "QRN on 1K dataset with '2rb' (2 layers + reset gate + bidirectional) model and on 10K dataset with '2rvb' (2 layers + reset gate + vector gate + bidirectional) outperforms MemN2N 1K and 10K models respectively.", "Though DMN+ outperforms QRN with a small margin, QRN are simpler and faster to train (the paper made the comment on the speed of training without reporting the training time of the two models).", "With very few layers, the model lacks reasoning ability while with too many layers, the model becomes difficult to train.", "Using vector gates works for large datasets while hurts for small datasets.", "Unidirectional models perform poorly.", "The intermediate query updates can be interpreted in natural language to understand the flow of information in the network."], "summary_text": "Machine Comprehension (MC) - given a natural language sentence, answer a natural language question. End-To-End MC - can not use language resources like dependency parsers. The only supervision during training is the correct answer. Query Regression Network (QRN) - Variant of Recurrent Neural Network (RNN). Related Work  Long Short-Term Memory (LSTM) and Gated Recurrence Unit (GRU) are popular choices to model sequential data but perform poorly on end-to-end MC due to long-term dependencies. Attention Models with shared external memory focus on single sentences in each layer but the models tend to be insensitive to the time step of the sentence being accessed. Memory Networks (and MemN2N)  Add time-dependent variable to the sentence representation. Summarize the memory in each layer to control attention in the next layer. Dynamic Memory Networks (and DMN+)  Combine RNN and attention mechanism to incorporate time dependency. Uses 2 GRU  time-axis GRU - Summarize the memory in each layer. layer-axis GRU - Control the attention in each layer. QRN is a much simpler model without any memory summarized node. QRN  Single recurrent unit that updates its internal state through time and layers. Inputs  qt - local query vector  xt - sentence vector  Outputs  ht - reduced query vector  xt - sentence vector without any modifications  Equations  zt = α(xt, qt)  &alpha is the update gate function to measure the relevance between input sentence and local query. h`t = γ(xt, qt)  &gamma is the regression function to transform the local query into regressed query. ht = zt*h`t + (1 - zt)*ht-1  To create a multi layer model, output of current layer becomes input to the next layer. Variants  Reset gate function (rt) to reset or nullify the regressed query h`t (inspired from GRU). The new equation becomes ht = zt*rt*h`t + (1 - zt)*ht-1  Vector gates - update and reset gate functions can produce vectors instead of scalar values (for finer control). Bidirectional - QRN can look at both past and future sentences while regressing the queries. qtk+1 = htk, forward + htk, backward. The variables of update and regress functions are shared between the two directions. Parallelization  Unlike most RNN based models, recurrent updates in QRN can be computed in parallel across time. For details and equations, refer the paper . Module Details  Input Modules  A trainable embedding matrix A is used to encode the one-hot vector of each word in the input sentence into a d-dimensional vector. Position Encoder is used to obtain the sentence representation from the d-dimensional vectors. Question vectors are also obtained in a similar manner. Output Module  A V-way single-layer softmax classifier is used to map predicted answer vector y to a V-dimensional sparse vector v.  The natural language answer y is the arg max word in v.  Results  bAbI QA dataset used. QRN on 1K dataset with '2rb' (2 layers + reset gate + bidirectional) model and on 10K dataset with '2rvb' (2 layers + reset gate + vector gate + bidirectional) outperforms MemN2N 1K and 10K models respectively. Though DMN+ outperforms QRN with a small margin, QRN are simpler and faster to train (the paper made the comment on the speed of training without reporting the training time of the two models). With very few layers, the model lacks reasoning ability while with too many layers, the model becomes difficult to train. Using vector gates works for large datasets while hurts for small datasets. Unidirectional models perform poorly. The intermediate query updates can be interpreted in natural language to understand the flow of information in the network.", "pdf_url": "http://arxiv.org/pdf/1606.04582", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/caa283af3c151372f4be86ed4c4b99.json"}
{"id": "86524123", "bin": "600_700", "summary_sentences": ["What  They introduce a dynamic gating mechanism for ResNet that decides on-the-fly which layers to execute for a given input image.", "This improves performance, classification accuracy and robustness to adversarial attacks.", "Note that the decision of whether to execute a layer during training is stochastic, making it closely related to stochastic depth.", "How  Their technique is based on residual connections, i.e. on ResNet.", "They add a gating function, so that x_l = x_{l-1} + F(x_{l-1}) becomes x_l = x_{l-1} + gate(x_{l-1}) * F(x_{l-1}).", "Gating  The gating function produces a discrete output (0 or 1), enabling to completely skip a layer.", "They implement the gating function by applying the following steps to an input feature map:  Global average pooling, leads to Cx1x1 output.", "Flatten to vector  Apply fully connected layer with d outputs and ReLU  Apply fully connected layer with 2 outputs  Apply straight-through gumbel-softmax to output  Straight-through Gumbel-softmax is a standard technique that is comparable to softmax, but produces discrete 0/1 outputs during the forward pass and uses a softmax during the backward pass to approximate gradients.", "For small d, the additional execution time of the gating function is negligible.", "Visualization:  They also introduce a target rate loss, which pushes the network to execute t percent of all layers (i.e. let t percent of all gates produce 1 as the output):  where z_l is the fraction of gates that produced 1 for layer l within the minibatch  and t is the target rate.", "Note that this will probably cause problems for small minibatches.", "Results  CIFAR-10  They choose d=16, adds 0.01% FLOPS and 4.8% parameters to ResNet-110.", "At t=0.7 they observe that on average 82% of all layers are executed.", "The model chooses to always execute downsampling layers.", "They seem to be important.", "When always executing all layers and scaling their outputs with their likelihood of being executed (similar to Dropout, Stochastic Depth), they achieve higher score than a network trained with stochastic depth.", "This indicates that their results do not just come from a regularizing effect.", "ImageNet  They compare gated ResNet-50 and ResNet-110.", "In ResNet-110 they always execute the first couple of layers (ungated), in ResNet-50 they gate all layers.", "Trade-off between FLOPs and accuracy (ResNet-50/101 vs their model \"ConvNet-AIG\" at different t):  Their model becomes more accurate as t is lowered down from 1.0 to 0.7.", "After that the accuracy starts to drop.", "Visualization of layerwise execution rate:  The model learns to always execute downsampling layers.", "The model seems to treat all man-made objects and all animals as two different groups with similar layers.", "Execution distribution and execution frequency during training:  The model quickly learns to always execute the last layer.", "The model requires especially many layers to classify non-iconic views of objects (e.g. dog face from unusual perspective instead of whole dog from the side).", "When running adversarial attacks on ResNet-50 and their ConvNet-AIG-50 (via Fast Gradient Sign Attack) they observe that:  Their model is overall more robust to adversarial attacks, i.e. keeps higher accuracy (both with/without protection via JPEG compression).", "The executed layers remain mostly the same in their model.", "They seem to not get attacked."], "summary_text": "What  They introduce a dynamic gating mechanism for ResNet that decides on-the-fly which layers to execute for a given input image. This improves performance, classification accuracy and robustness to adversarial attacks. Note that the decision of whether to execute a layer during training is stochastic, making it closely related to stochastic depth. How  Their technique is based on residual connections, i.e. on ResNet. They add a gating function, so that x_l = x_{l-1} + F(x_{l-1}) becomes x_l = x_{l-1} + gate(x_{l-1}) * F(x_{l-1}). Gating  The gating function produces a discrete output (0 or 1), enabling to completely skip a layer. They implement the gating function by applying the following steps to an input feature map:  Global average pooling, leads to Cx1x1 output. Flatten to vector  Apply fully connected layer with d outputs and ReLU  Apply fully connected layer with 2 outputs  Apply straight-through gumbel-softmax to output  Straight-through Gumbel-softmax is a standard technique that is comparable to softmax, but produces discrete 0/1 outputs during the forward pass and uses a softmax during the backward pass to approximate gradients. For small d, the additional execution time of the gating function is negligible. Visualization:  They also introduce a target rate loss, which pushes the network to execute t percent of all layers (i.e. let t percent of all gates produce 1 as the output):  where z_l is the fraction of gates that produced 1 for layer l within the minibatch  and t is the target rate. Note that this will probably cause problems for small minibatches. Results  CIFAR-10  They choose d=16, adds 0.01% FLOPS and 4.8% parameters to ResNet-110. At t=0.7 they observe that on average 82% of all layers are executed. The model chooses to always execute downsampling layers. They seem to be important. When always executing all layers and scaling their outputs with their likelihood of being executed (similar to Dropout, Stochastic Depth), they achieve higher score than a network trained with stochastic depth. This indicates that their results do not just come from a regularizing effect. ImageNet  They compare gated ResNet-50 and ResNet-110. In ResNet-110 they always execute the first couple of layers (ungated), in ResNet-50 they gate all layers. Trade-off between FLOPs and accuracy (ResNet-50/101 vs their model \"ConvNet-AIG\" at different t):  Their model becomes more accurate as t is lowered down from 1.0 to 0.7. After that the accuracy starts to drop. Visualization of layerwise execution rate:  The model learns to always execute downsampling layers. The model seems to treat all man-made objects and all animals as two different groups with similar layers. Execution distribution and execution frequency during training:  The model quickly learns to always execute the last layer. The model requires especially many layers to classify non-iconic views of objects (e.g. dog face from unusual perspective instead of whole dog from the side). When running adversarial attacks on ResNet-50 and their ConvNet-AIG-50 (via Fast Gradient Sign Attack) they observe that:  Their model is overall more robust to adversarial attacks, i.e. keeps higher accuracy (both with/without protection via JPEG compression). The executed layers remain mostly the same in their model. They seem to not get attacked.", "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Andreas_Veit_Convolutional_Networks_with_ECCV_2018_paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/convolutional_networks_with_adaptive_inference_graphs.json"}
{"id": "86675541", "bin": "600_700", "summary_sentences": ["What  They describe a convolutional network that takes in photos and returns where (on the planet) these photos were likely made.", "The output is a distribution over locations around the world (so not just one single location).", "This can be useful in the case of ambiguous images.", "How  Basic architecture  They simply use the Inception architecture for their model.", "They have 97M parameters.", "Grid  The network uses a grid of cells over the planet.", "For each photo and every grid cell it returns the likelihood that the photo was made within the region covered by the cell (simple softmax layer).", "The naive way would be to use a regular grid around the planet (i.e. a grid in which all cells have the same size).", "Possible disadvantages:  In places where lots of photos are taken you still have the same grid cell size as in places where barely any photos are taken.", "Maps are often distorted towards the poles (countries are represented much larger than they really are).", "This will likely affect the grid cells too.", "They instead use an adaptive grid pattern based on S2 cells.", "S2 cells interpret the planet as a sphere and project a cube onto it.", "The 6 sides of the cube are then partitioned using quad trees, creating the grid cells.", "They don't use the same depth for all quad trees.", "Instead they subdivide them only if their leafs contain enough photos (based on their dataset of geolocated images).", "They remove some cells for which their dataset does not contain enough images, e.g. cells on oceans.", "(They also remove these images from the dataset.", "They don't say how many images are affected by this.)", "They end up with roughly 26k cells, some of them reaching the street level of major cities.", "Visualization of their cells:  Training  For each example photo that they feed into the network, they set the correct grid cell to 1.0 and all other grid cells to 0.0.", "They train on a dataset of 126M images with Exif geolocation information.", "The images were collected from all over the web.", "They used Adagrad.", "They trained on 200 CPUs for 2.5 months.", "Album network  For photo albums they develop variations of their network.", "They do that because albums often contain images that are very hard to geolocate on their own, but much easier if the other images of the album are seen.", "They use LSTMs for their album network.", "The simplest one just iterates over every photo, applies their previously described model to it and extracts the last layer (before output) from that model.", "These vectors (one per image) are then fed into an LSTM, which is trained to predict (again) the grid cell location per image.", "More complicated versions use multiple passes or are bidirectional LSTMs (to use the information from the last images to classify the first ones in the album).", "Results  They beat previous models (based on hand-engineered features or nearest neighbour methods) by a significant margin.", "In a small experiment they can beat experienced humans in geoguessr.com.", "Based on a dataset of 2.3M photos from Flickr, their method correctly predicts the country where the photo was made in 30% of all cases (top-1; top-5: about 50%).", "City-level accuracy is about 10% (top-1; top-5: about 18%).", "Example predictions (using in coarser grid with 354 cells):  Using the LSTM-technique for albums significantly improves prediction accuracy for these images."], "summary_text": "What  They describe a convolutional network that takes in photos and returns where (on the planet) these photos were likely made. The output is a distribution over locations around the world (so not just one single location). This can be useful in the case of ambiguous images. How  Basic architecture  They simply use the Inception architecture for their model. They have 97M parameters. Grid  The network uses a grid of cells over the planet. For each photo and every grid cell it returns the likelihood that the photo was made within the region covered by the cell (simple softmax layer). The naive way would be to use a regular grid around the planet (i.e. a grid in which all cells have the same size). Possible disadvantages:  In places where lots of photos are taken you still have the same grid cell size as in places where barely any photos are taken. Maps are often distorted towards the poles (countries are represented much larger than they really are). This will likely affect the grid cells too. They instead use an adaptive grid pattern based on S2 cells. S2 cells interpret the planet as a sphere and project a cube onto it. The 6 sides of the cube are then partitioned using quad trees, creating the grid cells. They don't use the same depth for all quad trees. Instead they subdivide them only if their leafs contain enough photos (based on their dataset of geolocated images). They remove some cells for which their dataset does not contain enough images, e.g. cells on oceans. (They also remove these images from the dataset. They don't say how many images are affected by this.) They end up with roughly 26k cells, some of them reaching the street level of major cities. Visualization of their cells:  Training  For each example photo that they feed into the network, they set the correct grid cell to 1.0 and all other grid cells to 0.0. They train on a dataset of 126M images with Exif geolocation information. The images were collected from all over the web. They used Adagrad. They trained on 200 CPUs for 2.5 months. Album network  For photo albums they develop variations of their network. They do that because albums often contain images that are very hard to geolocate on their own, but much easier if the other images of the album are seen. They use LSTMs for their album network. The simplest one just iterates over every photo, applies their previously described model to it and extracts the last layer (before output) from that model. These vectors (one per image) are then fed into an LSTM, which is trained to predict (again) the grid cell location per image. More complicated versions use multiple passes or are bidirectional LSTMs (to use the information from the last images to classify the first ones in the album). Results  They beat previous models (based on hand-engineered features or nearest neighbour methods) by a significant margin. In a small experiment they can beat experienced humans in geoguessr.com. Based on a dataset of 2.3M photos from Flickr, their method correctly predicts the country where the photo was made in 30% of all cases (top-1; top-5: about 50%). City-level accuracy is about 10% (top-1; top-5: about 18%). Example predictions (using in coarser grid with 354 cells):  Using the LSTM-technique for albums significantly improves prediction accuracy for these images.", "pdf_url": "http://arxiv.org/pdf/1602.05314", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/planet.json"}
{"id": "91325283", "bin": "600_700", "summary_sentences": ["The paper describes an unsupervised approach to train a generic, distributed sentence encoder.", "It also describes a vocabulary expansion method to encode words not seen at training time.", "Skip-Thoughts  Train an encoder-decoder model where the encoder maps the input sentence to a sentence vector and the decoder generates the sentences surrounding the original sentence.", "The model is called skip-thoughts and the encoded vectors are called skip-thought vectors.", "Similar to the skip-gram model in the sense that surrounding sentences are used to learn sentence vectors.", "Architecture  Training data is in form of sentence tuples (previous sentence, current sentence, next sentence).", "Encoder  RNN Encoder with GRU.", "Decoder  RNN Decoder with conditional GRU.", "Conditioned on encoder output.", "Extra matrices introduced to bias the update gate, reset gate and hidden state, given the encoder output.", "Vocabulary matrix (V) - Weight matrix having one row (vector) for each word in the vocabulary.", "Separate decoders for the previous and next sentence which share only V.  Given the decoder context h (at any time), encoder output, and list of words already generated for the output sentence, the probability of choosing w as the next word is proportional to exp(V(word)h)  Objective  Sum of the log-probabilities for the forward and backwards sentences conditioned on the encoder output.", "Vocabulary Expansion  Use a model like Word2Vec which can be trained to induce word representations and train it to obtain embeddings for all the words that are likely to be seen by the encoder.", "Learn a matrix W such that encoder(word) = cross-product(W, Word2Vec(word)) for all words that are common to both Word2Vec model and encoder model.", "Use W to generate embeddings for words are not seen during encoder training.", "Dataset  BookCorpus dataset having books across 16 genres.", "Training  uni-skip  Unidirectional auto-encoder with 2400 dimensions.", "bi-skip  Bidirectional model with forward (sentence given in correct order) and backward (sentence given in reverse order) encoders of 1200 dimensions each.", "combine-skip  concatenation of uni-skip and bi-skip vectors.", "Initialization  Recurrent matricies - orthogonal initialization.", "Non-recurrent matricies - uniform distribution in [-0.1,0.1].", "Mini-batches of size 128.", "Gradient Clipping at norm = 10.", "Adam optimizer.", "Experiments  After learning skip-thoughts, freeze the model and use the encoder as feature extractor only.", "Evaluated the vectors with linear models on following tasks:  Semantic Relatedness  Given a sentence pair, predict how closely related the two sentences are.", "skip-thoughts method outperforms all systems from SemEval 2014 competition and is outperformed only by dependency tree-LSTMs.", "Using features learned from image-sentence embedding model on COCO boosts performance and brings it at par with dependency tree-LSTMs.", "Paraphrase detection  skip-thoughts outperforms recursive nets with dynamic pooling if no hand-crafted features are used.", "skip-thoughts with basic pairwise statistics produce results comparable with the state-of-the-art systems that house complicated features and hand engineering.", "Image-sentence Ranking  MS COCO dataset  Task  Image annotation  Given an image, rank the sentences on basis of how well they describe the image.", "Image search - Given a caption, find the image that is being described.", "Though the system does not outperform baseline system in all cases, the results does indicate that skip-thought vectors can capture image descriptions without having to learn their representations from scratch.", "Classification  skip-thoughts perform about as good as bag-of-words baselines but are outperformed by methods where sentence representation has been learnt for the task at hand.", "Combining skip-thoughts with bi-gram Naive Bayes (NB) features improves the performance.", "Future Work  Variants to be explored include:  Fine tuning the encoder-decoder model during the downstream task instead of freezing the weights.", "Deep encoders and decoders.", "Larger context windows.", "Encoding and decoding paragraphs.", "Encoders, such as convnets."], "summary_text": "The paper describes an unsupervised approach to train a generic, distributed sentence encoder. It also describes a vocabulary expansion method to encode words not seen at training time. Skip-Thoughts  Train an encoder-decoder model where the encoder maps the input sentence to a sentence vector and the decoder generates the sentences surrounding the original sentence. The model is called skip-thoughts and the encoded vectors are called skip-thought vectors. Similar to the skip-gram model in the sense that surrounding sentences are used to learn sentence vectors. Architecture  Training data is in form of sentence tuples (previous sentence, current sentence, next sentence). Encoder  RNN Encoder with GRU. Decoder  RNN Decoder with conditional GRU. Conditioned on encoder output. Extra matrices introduced to bias the update gate, reset gate and hidden state, given the encoder output. Vocabulary matrix (V) - Weight matrix having one row (vector) for each word in the vocabulary. Separate decoders for the previous and next sentence which share only V.  Given the decoder context h (at any time), encoder output, and list of words already generated for the output sentence, the probability of choosing w as the next word is proportional to exp(V(word)h)  Objective  Sum of the log-probabilities for the forward and backwards sentences conditioned on the encoder output. Vocabulary Expansion  Use a model like Word2Vec which can be trained to induce word representations and train it to obtain embeddings for all the words that are likely to be seen by the encoder. Learn a matrix W such that encoder(word) = cross-product(W, Word2Vec(word)) for all words that are common to both Word2Vec model and encoder model. Use W to generate embeddings for words are not seen during encoder training. Dataset  BookCorpus dataset having books across 16 genres. Training  uni-skip  Unidirectional auto-encoder with 2400 dimensions. bi-skip  Bidirectional model with forward (sentence given in correct order) and backward (sentence given in reverse order) encoders of 1200 dimensions each. combine-skip  concatenation of uni-skip and bi-skip vectors. Initialization  Recurrent matricies - orthogonal initialization. Non-recurrent matricies - uniform distribution in [-0.1,0.1]. Mini-batches of size 128. Gradient Clipping at norm = 10. Adam optimizer. Experiments  After learning skip-thoughts, freeze the model and use the encoder as feature extractor only. Evaluated the vectors with linear models on following tasks:  Semantic Relatedness  Given a sentence pair, predict how closely related the two sentences are. skip-thoughts method outperforms all systems from SemEval 2014 competition and is outperformed only by dependency tree-LSTMs. Using features learned from image-sentence embedding model on COCO boosts performance and brings it at par with dependency tree-LSTMs. Paraphrase detection  skip-thoughts outperforms recursive nets with dynamic pooling if no hand-crafted features are used. skip-thoughts with basic pairwise statistics produce results comparable with the state-of-the-art systems that house complicated features and hand engineering. Image-sentence Ranking  MS COCO dataset  Task  Image annotation  Given an image, rank the sentences on basis of how well they describe the image. Image search - Given a caption, find the image that is being described. Though the system does not outperform baseline system in all cases, the results does indicate that skip-thought vectors can capture image descriptions without having to learn their representations from scratch. Classification  skip-thoughts perform about as good as bag-of-words baselines but are outperformed by methods where sentence representation has been learnt for the task at hand. Combining skip-thoughts with bi-gram Naive Bayes (NB) features improves the performance. Future Work  Variants to be explored include:  Fine tuning the encoder-decoder model during the downstream task instead of freezing the weights. Deep encoders and decoders. Larger context windows. Encoding and decoding paragraphs. Encoders, such as convnets.", "pdf_url": "https://arxiv.org/pdf/1506.06726", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/4eb32de8cabf21bda9a4ada15c46e8.json"}
{"id": "78861774", "bin": "600_700", "summary_sentences": ["Large scale natural language understanding task - predict text values given a knowledge base.", "Accompanied by a large dataset generated using Wikipedia     Dataset  WikiReading dataset built using Wikidata and Wikipedia.", "Wikidata consists of statements of the form (property, value) about different items  80M statements, 16M items and 884 properties.", "These statements are grouped by items to get (item, property, answer) tuples where the answer is a set of values.", "Items are further replaced by their Wikipedia documents to generate 18.58M statements of the form (document, property, answer).", "Task is to predict answer given document and property.", "Properties are divided into 2 classes:  Categorical properties - properties with a small number of possible answers.", "Eg gender.", "Relational properties - properties with unique answers.", "Eg date of birth.", "This classification is done on the basis of the entropy of answer distribution.", "Properties with entropy less than 0.7 are classified as categorical properties.", "Answer distribution has a small number of very high-frequency answers (head) and a large number of answers with very small frequency (tail).", "30% of the answers do not appear in the training set and must be inferred from the document.", "Models  Answer Classification  Consider WikiReading as classification task and treat each answer as a class label.", "Baseline  Linear model over Bag of Words (BoW) features.", "Two BoW vectors computed - one for the document and other for the property.", "These are concatenated into a single feature vector.", "Neural Networks Method  Encode property and document into a joint representation which is fed into a softmax layer.", "Average Embeddings BoW  Average the BoW embeddings for documents and property and concatenate to get joint representation.", "Paragraph Vectors  As a variant of the previous method, encode document as a paragraph vector.", "LSTM Reader  LSTM reads the property and document sequence, word-by-word, and uses the final state as joint representation.", "Attentive Reader  Use attention mechanism to focus on relevant parts of the document for a given property.", "Memory Networks  Maps a property p and list of sentences x1, x2, ...xn in a joint representation by attention over the sentences in the document.", "Answer Extraction  For relational properties, it makes more sense to model the problem as information extraction than classification.", "RNNLabeler  Use an RNN to read the sequence of words and estimate if a given word is part of the answer.", "Basic SeqToSeq (Sequence to Sequence)  Similar to LSTM Reader but augmented with a second RNN to decode answer as a sequence of words.", "Placeholder SeqToSeq  Extends Basic SeqToSeq to handle OOV (Out of Vocabulary) words by adding placeholders to the vocabulary.", "OOV words in the document and answer are replaced by placeholders so that input and output sentences are a mixture of words and placeholders only.", "Basic Character SeqToSeq  Property encoder RNN reads the property, character-by-character and transforms it into a fixed length vector.", "This becomes the initial hidden state for the second layer of a 2-layer document encoder RNN.", "Final state of this RNN is used by answer decoder RNN to generate answer as a character sequence.", "Character SeqToSeq with pretraining  Train a character-level language model on input character sequence from the training set and use the weights to initiate the first layer of encoder and decoder.", "Experiments  Evaluation metric is F1 score (harmonic mean of precision and accuracy).", "All models perform well on categorical properties with neural models outperforming others.", "In the case of relational properties, SeqToSeq models have a clear edge.", "SeqToSeq models also show a great deal of balance between relational and categorical properties.", "Language model pretraining enhances the performance of character SeqToSeq approach.", "Results demonstrate that end-to-end SeqToSeq models are most promising for WikiReading like tasks."], "summary_text": "Large scale natural language understanding task - predict text values given a knowledge base. Accompanied by a large dataset generated using Wikipedia     Dataset  WikiReading dataset built using Wikidata and Wikipedia. Wikidata consists of statements of the form (property, value) about different items  80M statements, 16M items and 884 properties. These statements are grouped by items to get (item, property, answer) tuples where the answer is a set of values. Items are further replaced by their Wikipedia documents to generate 18.58M statements of the form (document, property, answer). Task is to predict answer given document and property. Properties are divided into 2 classes:  Categorical properties - properties with a small number of possible answers. Eg gender. Relational properties - properties with unique answers. Eg date of birth. This classification is done on the basis of the entropy of answer distribution. Properties with entropy less than 0.7 are classified as categorical properties. Answer distribution has a small number of very high-frequency answers (head) and a large number of answers with very small frequency (tail). 30% of the answers do not appear in the training set and must be inferred from the document. Models  Answer Classification  Consider WikiReading as classification task and treat each answer as a class label. Baseline  Linear model over Bag of Words (BoW) features. Two BoW vectors computed - one for the document and other for the property. These are concatenated into a single feature vector. Neural Networks Method  Encode property and document into a joint representation which is fed into a softmax layer. Average Embeddings BoW  Average the BoW embeddings for documents and property and concatenate to get joint representation. Paragraph Vectors  As a variant of the previous method, encode document as a paragraph vector. LSTM Reader  LSTM reads the property and document sequence, word-by-word, and uses the final state as joint representation. Attentive Reader  Use attention mechanism to focus on relevant parts of the document for a given property. Memory Networks  Maps a property p and list of sentences x1, x2, ...xn in a joint representation by attention over the sentences in the document. Answer Extraction  For relational properties, it makes more sense to model the problem as information extraction than classification. RNNLabeler  Use an RNN to read the sequence of words and estimate if a given word is part of the answer. Basic SeqToSeq (Sequence to Sequence)  Similar to LSTM Reader but augmented with a second RNN to decode answer as a sequence of words. Placeholder SeqToSeq  Extends Basic SeqToSeq to handle OOV (Out of Vocabulary) words by adding placeholders to the vocabulary. OOV words in the document and answer are replaced by placeholders so that input and output sentences are a mixture of words and placeholders only. Basic Character SeqToSeq  Property encoder RNN reads the property, character-by-character and transforms it into a fixed length vector. This becomes the initial hidden state for the second layer of a 2-layer document encoder RNN. Final state of this RNN is used by answer decoder RNN to generate answer as a character sequence. Character SeqToSeq with pretraining  Train a character-level language model on input character sequence from the training set and use the weights to initiate the first layer of encoder and decoder. Experiments  Evaluation metric is F1 score (harmonic mean of precision and accuracy). All models perform well on categorical properties with neural models outperforming others. In the case of relational properties, SeqToSeq models have a clear edge. SeqToSeq models also show a great deal of balance between relational and categorical properties. Language model pretraining enhances the performance of character SeqToSeq approach. Results demonstrate that end-to-end SeqToSeq models are most promising for WikiReading like tasks.", "pdf_url": "http://www.aclweb.org/anthology/P/P16/P16-1145.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/88ac9dbcac5523cb8b2d0a3d70f2d2.json"}
{"id": "78499306", "bin": "600_700", "summary_sentences": ["Continuum: a platform for cost-aware low-latency continual learning Tian et al., SoCC’18  Let’s start with some broad approximations.", "Batching leads to higher throughput at the cost of higher latency.", "Processing items one at a time leads to lower latency and often reduced throughput.", "We can recover throughput to a degree by throwing horizontally scalable resources at the problem, but it’s hard to recover latency.", "In many business scenarios latency matters, so we’ve been seeing a movement overtime from batching through micro-batching to online streaming.", "Continuum looks at the same issues from the perspective of machine learning models.", "Offline (batch) trained models can suffer from concept drift (loss of accuracy over time) as a result of not incorporating the latest data.", "I.e., there’s a business cost incurred for higher latency of update incorporation.", "Online models support incremental updates.", "Continuum determines the optimum time to retrain models in the presence of incoming data, based on user policy (best effort, cost-aware, or user-defined).", "There’s some great data here about the need for and benefit of continual learning, and a surprising twist in the tale where it turns out that even if you can afford it, updating the model on every incoming data point is not the best strategy even when optimising for lowest latency of update incorporation.", "When good models go stale  There are a number of purpose-designed online learning algorithms (e.g. Latent Dirichlet Allocation for topic models, matrix factorization for recommender systems, and Bayesian inference for stream analytics).", "However, many mainstream ML frameworks including TensorFlow, MLib, XGBoost, scikit-learn and MALLET do not explicitly support continual model updating.", "It’s up to the user to code custom training loops to manually trigger retraining.", "Often such models are updated on much slower timescales (e.g. daily, or maybe hourly) than the generation of the data they operate over.", "This causes a loss in model accuracy when concept drift occurs.", "Consider a Personalized PageRank (PPR) algorithm being fed data from Twitter.", "The following chart shows how L1 error and MAP metrics degrade over time as compared to a model trained on the very most recent data.", "The base model decays by about 10% in one hour.", "Using offline (batch) training to retrain the model from scratch every 10 minutes also takes orders of magnitude more compute than online learning, making short retraining windows using the full training data set impractical.", "When a topic modelling model (also trained using tweets) is updated once every ten minutes, we can see that it’s perplexity (lower is better) decreases with every retraining.", "The performance of an ad recommendation system classifier similarly improves over time with model updating every five minutes.", "Data recency clearly matters in a number of applications.", "But writing your own continual learning training loops can be as much if not more work than implementing the key logic of online learning in the first place.", "An overview of Continuum  Continuum is designed to support continual learning a cross a broad set of ML frameworks.", "Based on an update policy, Continuum decides when to update the model.", "At runtime it looks like this:  Clients send data updates (or pointers to updated data) to Continuum  Continuum stores the updated data  Continuum evaluates the update policy and triggers retraining if needed  The backend fetches the updated data from data storage and trains the model  The backend notifies Continuum that an updated model is available.", "The updated model can then be shipped to a model serving system, e.g. Clipper .", "Continuum is about 4,000 lines of C++ and Python, and is available in open-source here:  [url]"], "summary_text": "Continuum: a platform for cost-aware low-latency continual learning Tian et al., SoCC’18  Let’s start with some broad approximations. Batching leads to higher throughput at the cost of higher latency. Processing items one at a time leads to lower latency and often reduced throughput. We can recover throughput to a degree by throwing horizontally scalable resources at the problem, but it’s hard to recover latency. In many business scenarios latency matters, so we’ve been seeing a movement overtime from batching through micro-batching to online streaming. Continuum looks at the same issues from the perspective of machine learning models. Offline (batch) trained models can suffer from concept drift (loss of accuracy over time) as a result of not incorporating the latest data. I.e., there’s a business cost incurred for higher latency of update incorporation. Online models support incremental updates. Continuum determines the optimum time to retrain models in the presence of incoming data, based on user policy (best effort, cost-aware, or user-defined). There’s some great data here about the need for and benefit of continual learning, and a surprising twist in the tale where it turns out that even if you can afford it, updating the model on every incoming data point is not the best strategy even when optimising for lowest latency of update incorporation. When good models go stale  There are a number of purpose-designed online learning algorithms (e.g. Latent Dirichlet Allocation for topic models, matrix factorization for recommender systems, and Bayesian inference for stream analytics). However, many mainstream ML frameworks including TensorFlow, MLib, XGBoost, scikit-learn and MALLET do not explicitly support continual model updating. It’s up to the user to code custom training loops to manually trigger retraining. Often such models are updated on much slower timescales (e.g. daily, or maybe hourly) than the generation of the data they operate over. This causes a loss in model accuracy when concept drift occurs. Consider a Personalized PageRank (PPR) algorithm being fed data from Twitter. The following chart shows how L1 error and MAP metrics degrade over time as compared to a model trained on the very most recent data. The base model decays by about 10% in one hour. Using offline (batch) training to retrain the model from scratch every 10 minutes also takes orders of magnitude more compute than online learning, making short retraining windows using the full training data set impractical. When a topic modelling model (also trained using tweets) is updated once every ten minutes, we can see that it’s perplexity (lower is better) decreases with every retraining. The performance of an ad recommendation system classifier similarly improves over time with model updating every five minutes. Data recency clearly matters in a number of applications. But writing your own continual learning training loops can be as much if not more work than implementing the key logic of online learning in the first place. An overview of Continuum  Continuum is designed to support continual learning a cross a broad set of ML frameworks. Based on an update policy, Continuum decides when to update the model. At runtime it looks like this:  Clients send data updates (or pointers to updated data) to Continuum  Continuum stores the updated data  Continuum evaluates the update policy and triggers retraining if needed  The backend fetches the updated data from data storage and trains the model  The backend notifies Continuum that an updated model is available. The updated model can then be shipped to a model serving system, e.g. Clipper . Continuum is about 4,000 lines of C++ and Python, and is available in open-source here:  [url]", "pdf_url": "https://www.cse.ust.hk/~weiwa/papers/huangshi-socc18.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/continuum-a-platform-for-cost-aware-low-latency-continual-learning.json"}
{"id": "36272485", "bin": "600_700", "summary_sentences": ["What  They find that artificial neural networks show critical periods, similar to biological neural networks.", "Critical periods in biological neural networks are phases in the network's development during which a malformed sensory input can irreversibly harm the capabilities of the network.", "E. g. a disease causing blurry vision for some time during early development can permanently harm the ability to interpret visual stimuli.", "The same disease very early or later in life will cause no permanent damage (after being healing).", "How  Blur  They train networks in CIFAR10 and blur all input images by down- and then upscaling.", "They vary when the blurring is removed.", "They always train for 300 epochs more after the blurring was removed.", "So the network always sees the unaltered input images for at least the same amount of epochs.", "Vertical flipping  Same as blurring, but they vertically flip the image.", "This is expected to keep low and mid level statistics the same.", "Only the final layers have to change.", "This is expected to be easy to adapt to, even if the flipping is removed fairly late into the training.", "Label permutation  They randomly permute the class labels and remove the effect after some epochs.", "This is expected to only affect the last layer and hence should have similar effects to vertical flipping.", "Sensory deprivation  They test the effect of sensory deprivation on neural nets.", "They make the input of the networks uninformative by replacing it with gaussian noise.", "This is assumed to have less effect than adding blur, because the network does not learn significantly wrong statistics (due to the input being uninformative).", "Mutual Information Noise  They add random gaussian noise to each layer's output (or to the weights - not really clear).", "They allow the network to learn the variance of that noise.", "They add a regularization based on mutual information.", "This adds a \"cost proportional to the quantity of mutual information I(w;D) that the weights retain about the training data D after the learning process\" (?).", "So the network can retain more information, but has to pay for that.", "It is expected to set the variance to low values for layers which are critical for the predictions.", "Results  Blur  Removing the blur early leads to only a small loss in final accuracy.", "Removing the blur too late leads to a large and permanent loss in final accuracy.", "The decline in accuracy is not linear with respect to when the blur is removed.", "The effect is similar to biological neural nets.", "Making the network deeper does not help, but instead worsens the effect.", "Fixing the learning rate doesn't help either.", "This is basically like starting to let the network learn once the blur is removed, but using a weirdly bad initialization.", "(Weird in the sense that it starts with great accuracy, but is barely able to improve.)", "Vertical flipping  As expected, adding vertical flips does not significantly affect long term accuracy.", "Label permutation  Same as for vertical flipping, only minor effect.", "Sensory deprivation  This has worse effects than vertical flipping / label permutation.", "Overall less decrease in accuracy than with blur.", "The effect is more linear with respect to the epoch (remove early: hardly any decline in accuracy, remove after half of training: medium decline, remove late: strong decline).", "Mutual Information Noise  Without deficit, the network will put most weight (least amount of noise) on the middle layers (3-5 of 7).", "With deficit, it will put more weight on the last layers and is only able to partially reconfigure if the deficit is removed early enough."], "summary_text": "What  They find that artificial neural networks show critical periods, similar to biological neural networks. Critical periods in biological neural networks are phases in the network's development during which a malformed sensory input can irreversibly harm the capabilities of the network. E. g. a disease causing blurry vision for some time during early development can permanently harm the ability to interpret visual stimuli. The same disease very early or later in life will cause no permanent damage (after being healing). How  Blur  They train networks in CIFAR10 and blur all input images by down- and then upscaling. They vary when the blurring is removed. They always train for 300 epochs more after the blurring was removed. So the network always sees the unaltered input images for at least the same amount of epochs. Vertical flipping  Same as blurring, but they vertically flip the image. This is expected to keep low and mid level statistics the same. Only the final layers have to change. This is expected to be easy to adapt to, even if the flipping is removed fairly late into the training. Label permutation  They randomly permute the class labels and remove the effect after some epochs. This is expected to only affect the last layer and hence should have similar effects to vertical flipping. Sensory deprivation  They test the effect of sensory deprivation on neural nets. They make the input of the networks uninformative by replacing it with gaussian noise. This is assumed to have less effect than adding blur, because the network does not learn significantly wrong statistics (due to the input being uninformative). Mutual Information Noise  They add random gaussian noise to each layer's output (or to the weights - not really clear). They allow the network to learn the variance of that noise. They add a regularization based on mutual information. This adds a \"cost proportional to the quantity of mutual information I(w;D) that the weights retain about the training data D after the learning process\" (?). So the network can retain more information, but has to pay for that. It is expected to set the variance to low values for layers which are critical for the predictions. Results  Blur  Removing the blur early leads to only a small loss in final accuracy. Removing the blur too late leads to a large and permanent loss in final accuracy. The decline in accuracy is not linear with respect to when the blur is removed. The effect is similar to biological neural nets. Making the network deeper does not help, but instead worsens the effect. Fixing the learning rate doesn't help either. This is basically like starting to let the network learn once the blur is removed, but using a weirdly bad initialization. (Weird in the sense that it starts with great accuracy, but is barely able to improve.) Vertical flipping  As expected, adding vertical flips does not significantly affect long term accuracy. Label permutation  Same as for vertical flipping, only minor effect. Sensory deprivation  This has worse effects than vertical flipping / label permutation. Overall less decrease in accuracy than with blur. The effect is more linear with respect to the epoch (remove early: hardly any decline in accuracy, remove after half of training: medium decline, remove late: strong decline). Mutual Information Noise  Without deficit, the network will put most weight (least amount of noise) on the middle layers (3-5 of 7). With deficit, it will put more weight on the last layers and is only able to partially reconfigure if the deficit is removed early enough.", "pdf_url": "https://arxiv.org/pdf/1711.08856", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/critical_learning_periods_in_deep_neural_networks.json"}
{"id": "78841597", "bin": "600_700", "summary_sentences": ["A first look at the usability of Bitcoin key management Eskandari et al., USEC 2015  This is the third of five papers from the ACM Queue Research for Practice selections on ‘ Cryptocurrencies, Blockchains, and Smart Contracts .’ And thankfully it’s much easier to read and understand than yesterdays!", "The authors point out that a cryptocurrency intended for use by the general public also needs a means by which the general public can come to understand and manage collections of public and private keys.", "To date, this has proved elusive.", "As the authors of “ Why Johnny Can’t Encrypt ” point out, “Security mechanisms are effective only when used correctly.” One of the interesting takeaways for me is that the familiar metaphors of ‘coin’ and ‘password’ can harm as much as they help.", "In all of the excitement surrounding Bitcoin, it is easy to forget that the decentralized currency assumes a solution to the longstanding problem of usable public key cryptography for user authentication.", "If you lose a Bitcoin private key, you lose the monetary value of the coins it protects.", "Moreover, Bitcoin users typically have no legal protection against loss or theft.", "So can we find a safe, usable key management approach for novice users?", "The findings from previous studies on the usability of public key cryptography include:  The metaphor and terminology behind public and private keys is confusing  It is difficult to correctly obtain other user’s public keys  Key migration between devices is difficult  Many of the confusions seem to stem from the fact that users are used to passwords for protecting information, and public/private key pairs don’t fit neatly into that mental model.", "The main bulk of the paper compares and contrasts six different approaches to Bitcoin key management, followed by a more detailed usability study of six actual Bitcoin clients, one from each category.", "Six approaches to Bitcoin key management  Here are six different approaches that you might use for managing private keys:  Keep them on device local storage.", "Bitcoin Core uses this approach.", "It’s easy, but vulnerable to malware since the file storing private keys can be read by any process with applications to the user’s folder.", "Private-key stealing malware has been known since at least 2011.", "Users need to be very careful not to inadvertently share this folder outside of their computer, but at the same time they need to periodically create new backups of the key storage file to ensure that new keypool keys are stored.", "Use a password-protected encrypted wallet.", "MultiBit uses this approach.", "These solutions encrypt the key storage file that is held on device local storage.", "The user is therefore protected against theft of the file (to the degree the password cannot be cracked), but malware on the machine will still be able to use e.g. a keystroke logger to capture the password.", "“Password-protected wallets may mislead the user to believe that the password itself provides access to their funds regardless of the location of the device storing the wallet.", "Offline storage of keys (e.g., Bitaddress) To protect against malware-based threats, wallets can be stored offline on portable media (e.g., a USB drive).", "Of course, this makes the wallet unavailable for immediate use, and the wallet is still exposed and potentially vulnerable to malware when mounted on a computational device.", "An interesting example here is paper wallets (e.g. [url]"], "summary_text": "A first look at the usability of Bitcoin key management Eskandari et al., USEC 2015  This is the third of five papers from the ACM Queue Research for Practice selections on ‘ Cryptocurrencies, Blockchains, and Smart Contracts .’ And thankfully it’s much easier to read and understand than yesterdays! The authors point out that a cryptocurrency intended for use by the general public also needs a means by which the general public can come to understand and manage collections of public and private keys. To date, this has proved elusive. As the authors of “ Why Johnny Can’t Encrypt ” point out, “Security mechanisms are effective only when used correctly.” One of the interesting takeaways for me is that the familiar metaphors of ‘coin’ and ‘password’ can harm as much as they help. In all of the excitement surrounding Bitcoin, it is easy to forget that the decentralized currency assumes a solution to the longstanding problem of usable public key cryptography for user authentication. If you lose a Bitcoin private key, you lose the monetary value of the coins it protects. Moreover, Bitcoin users typically have no legal protection against loss or theft. So can we find a safe, usable key management approach for novice users? The findings from previous studies on the usability of public key cryptography include:  The metaphor and terminology behind public and private keys is confusing  It is difficult to correctly obtain other user’s public keys  Key migration between devices is difficult  Many of the confusions seem to stem from the fact that users are used to passwords for protecting information, and public/private key pairs don’t fit neatly into that mental model. The main bulk of the paper compares and contrasts six different approaches to Bitcoin key management, followed by a more detailed usability study of six actual Bitcoin clients, one from each category. Six approaches to Bitcoin key management  Here are six different approaches that you might use for managing private keys:  Keep them on device local storage. Bitcoin Core uses this approach. It’s easy, but vulnerable to malware since the file storing private keys can be read by any process with applications to the user’s folder. Private-key stealing malware has been known since at least 2011. Users need to be very careful not to inadvertently share this folder outside of their computer, but at the same time they need to periodically create new backups of the key storage file to ensure that new keypool keys are stored. Use a password-protected encrypted wallet. MultiBit uses this approach. These solutions encrypt the key storage file that is held on device local storage. The user is therefore protected against theft of the file (to the degree the password cannot be cracked), but malware on the machine will still be able to use e.g. a keystroke logger to capture the password. “Password-protected wallets may mislead the user to believe that the password itself provides access to their funds regardless of the location of the device storing the wallet. Offline storage of keys (e.g., Bitaddress) To protect against malware-based threats, wallets can be stored offline on portable media (e.g., a USB drive). Of course, this makes the wallet unavailable for immediate use, and the wallet is still exposed and potentially vulnerable to malware when mounted on a computational device. An interesting example here is paper wallets (e.g. [url]", "pdf_url": "http://users.encs.concordia.ca/~clark/papers/2015_usec.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/a-first-look-at-the-usabilty-of-bitcoin-key-management.json"}
{"id": "60516406", "bin": "600_700", "summary_sentences": ["What  Autoencoders typically have some additional criterion that pushes them towards learning meaningful representations.", "E.g. L1-Penalty on the code layer (z), Dropout on z, Noise on z.", "Often, representations with sparse activations are considered meaningful (so that each activation reflects are clear concept).", "This paper introduces another technique that leads to sparsity.", "They use a rank ordering on z.", "The first (according to the ranking) activations have to do most of the reconstruction work of the data (i.e. image).", "How  Basic architecture:  They use an Autoencoder architecture: Input -> Encoder -> z -> Decoder -> Output.", "Their encoder and decoder seem to be empty, i.e. z is the only hidden layer in the network.", "Their output is not just one image (or whatever is encoded), instead they generate one for every unit in layer z.", "Then they order these outputs based on the activation of the units in z (rank ordering), i.e. the output of the unit with the highest activation is placed in the first position, the output of the unit with the 2nd highest activation gets the 2nd position and so on.", "They then generate the final output image based on a cumulative sum.", "So for three reconstructed output images I1, I2, I3 (rank ordered that way) they would compute final image = I1 + (I1+I2) + (I1+I2+I3).", "They then compute the error based on that reconstruction (reconstruction - input image) and backpropagate it.", "Cumulative sum:  Using the cumulative sum puts most optimization pressure on units with high activation, as they have the largest influence on the reconstruction error.", "The cumulative sum is best optimized by letting few units have high activations and generate most of the output (correctly).", "All the other units have ideally low to zero activations and low or no influence on the output.", "(Though if the output generated by the first units is wrong, you should then end up with an extremely high cumulative error sum...)  So their z coding should end up with few but high activations, i.e. it should become very sparse.", "The cumulative generates an individual error per output, while an ordinary sum generates the same error for every output.", "They argue that this \"blurs\" the error less.", "To avoid blow ups in their network they use TReLUs, which saturate below 0 and above 1, i.e. min(1, max(0, input)).", "They use a custom derivative function for the TReLUs, which is dependent on both the input value of the unit and its gradient.", "Basically, if the input is >1 (saturated) and the error is high, then the derivative pushes the weight down, so that the input gets into the unsaturated regime.", "Similarly for input values <0 (pushed up).", "If the input value is between 0 and 1 and/or the error is low, then nothing is changed.", "They argue that the algorithmic complexity of the rank ordering should be low, due to sorts being O(n log(n)), where n is the number of hidden units in z.", "Results  They autoencode 7x7 patches from CIFAR-10.", "They get very sparse activations.", "Training and test loss develop identically, i.e. no overfitting."], "summary_text": "What  Autoencoders typically have some additional criterion that pushes them towards learning meaningful representations. E.g. L1-Penalty on the code layer (z), Dropout on z, Noise on z. Often, representations with sparse activations are considered meaningful (so that each activation reflects are clear concept). This paper introduces another technique that leads to sparsity. They use a rank ordering on z. The first (according to the ranking) activations have to do most of the reconstruction work of the data (i.e. image). How  Basic architecture:  They use an Autoencoder architecture: Input -> Encoder -> z -> Decoder -> Output. Their encoder and decoder seem to be empty, i.e. z is the only hidden layer in the network. Their output is not just one image (or whatever is encoded), instead they generate one for every unit in layer z. Then they order these outputs based on the activation of the units in z (rank ordering), i.e. the output of the unit with the highest activation is placed in the first position, the output of the unit with the 2nd highest activation gets the 2nd position and so on. They then generate the final output image based on a cumulative sum. So for three reconstructed output images I1, I2, I3 (rank ordered that way) they would compute final image = I1 + (I1+I2) + (I1+I2+I3). They then compute the error based on that reconstruction (reconstruction - input image) and backpropagate it. Cumulative sum:  Using the cumulative sum puts most optimization pressure on units with high activation, as they have the largest influence on the reconstruction error. The cumulative sum is best optimized by letting few units have high activations and generate most of the output (correctly). All the other units have ideally low to zero activations and low or no influence on the output. (Though if the output generated by the first units is wrong, you should then end up with an extremely high cumulative error sum...)  So their z coding should end up with few but high activations, i.e. it should become very sparse. The cumulative generates an individual error per output, while an ordinary sum generates the same error for every output. They argue that this \"blurs\" the error less. To avoid blow ups in their network they use TReLUs, which saturate below 0 and above 1, i.e. min(1, max(0, input)). They use a custom derivative function for the TReLUs, which is dependent on both the input value of the unit and its gradient. Basically, if the input is >1 (saturated) and the error is high, then the derivative pushes the weight down, so that the input gets into the unsaturated regime. Similarly for input values <0 (pushed up). If the input value is between 0 and 1 and/or the error is low, then nothing is changed. They argue that the algorithmic complexity of the rank ordering should be low, due to sorts being O(n log(n)), where n is the number of hidden units in z. Results  They autoencode 7x7 patches from CIFAR-10. They get very sparse activations. Training and test loss develop identically, i.e. no overfitting.", "pdf_url": "https://arxiv.org/pdf/1605.01749", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/rank_ordered_autoencoders.json"}
{"id": "62302416", "bin": "600_700", "summary_sentences": ["Explanation for all the math  More explanation for all the math  What  They suggest a slightly altered algorithm for GANs.", "The new algorithm is more stable than previous ones.", "How  Each GAN contains a Generator that generates (fake-)examples and a Discriminator that discriminates between fake and real examples.", "Both fake and real examples can be interpreted as coming from a probability distribution.", "The basis of each GAN algorithm is to somehow measure the difference between these probability distributions and change the network parameters of G so that the fake-distribution becomes more and more similar to the real distribution.", "There are multiple distance measures to do that:  Total Variation (TV)  KL-Divergence (KL)  Jensen-Shannon divergence (JS)  This one is based on the KL-Divergence and is the basis of the original GAN, as well as LAPGAN and DCGAN.", "Earth-Mover distance (EM), aka Wasserstein-1  Intuitively, one can imagine both probability distributions as hilly surfaces.", "EM then reflects, how much mass has to be moved to convert the fake distribution to the real one.", "Ideally, a distance measure has everywhere nice values and gradients (e.g. no +/- infinity values; no binary 0 or 1 gradients; gradients that get continously smaller when the generator produces good outputs).", "In that regard, EM beats JS and JS beats TV and KL (roughly speaking).", "So they use EM.", "EM  EM is defined as  (inf = infinum, more or less a minimum)  which is intractable, but following the Kantorovich-Rubinstein duality it can also be calculated via  (sup = supremum, more or less a maximum)  However, the second formula is here only valid if the network is a K-Lipschitz function (under every set of parameters).", "This can be guaranteed by simply clipping the discriminator's weights to the range [-0.01, 0.01].", "Then in practice the following version of the tractable EM is used, where w are the parameters of the discriminator:  The full algorithm is mostly the same as for DCGAN:  Line 2 leads to training the discriminator multiple times per batch (i.e. more often than the generator).", "This is similar to the max w in W in the third formula (above).", "This was already part of the original GAN algorithm, but is here more actively used.", "Because of the EM distance, even a \"perfect\" discriminator still gives good gradient (in contrast to e.g. JS, where the discriminator should not be too far ahead).", "So the discriminator can be safely trained more often than the generator.", "Line 5 and 10 are derived from EM.", "Note that there is no more Sigmoid at the end of the discriminator!", "Line 7 is derived from the K-Lipschitz requirement (clipping of weights).", "High learning rates or using momentum-based optimizers (e.g. Adam) made the training unstable, which is why they use a small learning rate with RMSprop.", "Results  Improved stability.", "The method converges to decent images with models which failed completely when using JS-divergence (like in DCGAN).", "For example, WGAN worked with generators that did not have batch normalization or only consisted of fully connected layers.", "Apparently no more mode collapse.", "(Mode collapse in GANs = the generator starts to generate often/always the practically same image, independent of the noise input.)", "There is a relationship between loss and image quality.", "Lower loss (at the generator) indicates higher image quality.", "Such a relationship did not exist for JS divergence.", "Example images:"], "summary_text": "Explanation for all the math  More explanation for all the math  What  They suggest a slightly altered algorithm for GANs. The new algorithm is more stable than previous ones. How  Each GAN contains a Generator that generates (fake-)examples and a Discriminator that discriminates between fake and real examples. Both fake and real examples can be interpreted as coming from a probability distribution. The basis of each GAN algorithm is to somehow measure the difference between these probability distributions and change the network parameters of G so that the fake-distribution becomes more and more similar to the real distribution. There are multiple distance measures to do that:  Total Variation (TV)  KL-Divergence (KL)  Jensen-Shannon divergence (JS)  This one is based on the KL-Divergence and is the basis of the original GAN, as well as LAPGAN and DCGAN. Earth-Mover distance (EM), aka Wasserstein-1  Intuitively, one can imagine both probability distributions as hilly surfaces. EM then reflects, how much mass has to be moved to convert the fake distribution to the real one. Ideally, a distance measure has everywhere nice values and gradients (e.g. no +/- infinity values; no binary 0 or 1 gradients; gradients that get continously smaller when the generator produces good outputs). In that regard, EM beats JS and JS beats TV and KL (roughly speaking). So they use EM. EM  EM is defined as  (inf = infinum, more or less a minimum)  which is intractable, but following the Kantorovich-Rubinstein duality it can also be calculated via  (sup = supremum, more or less a maximum)  However, the second formula is here only valid if the network is a K-Lipschitz function (under every set of parameters). This can be guaranteed by simply clipping the discriminator's weights to the range [-0.01, 0.01]. Then in practice the following version of the tractable EM is used, where w are the parameters of the discriminator:  The full algorithm is mostly the same as for DCGAN:  Line 2 leads to training the discriminator multiple times per batch (i.e. more often than the generator). This is similar to the max w in W in the third formula (above). This was already part of the original GAN algorithm, but is here more actively used. Because of the EM distance, even a \"perfect\" discriminator still gives good gradient (in contrast to e.g. JS, where the discriminator should not be too far ahead). So the discriminator can be safely trained more often than the generator. Line 5 and 10 are derived from EM. Note that there is no more Sigmoid at the end of the discriminator! Line 7 is derived from the K-Lipschitz requirement (clipping of weights). High learning rates or using momentum-based optimizers (e.g. Adam) made the training unstable, which is why they use a small learning rate with RMSprop. Results  Improved stability. The method converges to decent images with models which failed completely when using JS-divergence (like in DCGAN). For example, WGAN worked with generators that did not have batch normalization or only consisted of fully connected layers. Apparently no more mode collapse. (Mode collapse in GANs = the generator starts to generate often/always the practically same image, independent of the noise input.) There is a relationship between loss and image quality. Lower loss (at the generator) indicates higher image quality. Such a relationship did not exist for JS divergence. Example images:", "pdf_url": "https://arxiv.org/pdf/1701.07875", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/wgan.json"}
{"id": "14310989", "bin": "600_700", "summary_sentences": ["Notes  This paper surveys progress on adapting deep learning techniques to non-Euclidean data and suggests future directions.", "One of the strengths (and weaknesses) of deep learning–specifically exploited by convolutional neural networks–is that the data is assumed to exhibit translation invariance/equivariance and invariance to local deformations.", "Hence, long-range dependencies can be learned with multi-scale, hierarchical techniques where spatial resolution is reduced.", "However, this means that any information about the data that can’t be learned when spatial resolution is reduced can get lost (I believe that residual networks aim to address this by the skip connections that are able to learn an identity operation; also, in computer vision, multi-scale versions of the data are often fed to CNNs).", "Key areas where this assumption about the data appears to be true is computer vision and speech recognition.", "Some quick background  The Laplacian, a self-adjoint (symmetric) positive semi-definite operator, which is defined for smooth manifolds and graphs in this paper, can be thought of as the difference between the local average of a function around a point and the value of the function at the point itself.", "It’s generally defined as $\\triangle = -\\text{div} \\nabla$.", "When discretizing a continuous, smooth manifold with a mesh, note that the graph Laplacian might not converge to the continuous Laplacian operator with increasing sampling density.", "To be consistent, need to create a triangular mesh, i.e., represent the manifold as a polyhedral surface.", "Spectral methods  Fourier analysis on non-Euclidean domains is possible by considering the eigendecomposition of the Laplacian operator.", "A possible transformation of the Convolution Theorem to functions on manifolds and graphs is discussed, but is noted as not being shift-invariant.", "The Spectral CNN can be defined by introducing a spectral convolutional layer acting on the vertices of the graph and using filters in the frequency domain and the eigenvectors of the Laplacian.", "However, the spectral filter coefficients will be dependent on the particular eigenvectors (basis) - domain dependency == bad for generalization!", "The non-Euclidean analogy of pooling is graph coarsening- only a fraction of the vertices of the graph are retained.", "Strided convolutions can be generalized to the spectral construction by only keeping the low-frequency components - must recompute the graph Laplacian after applying the nonlinearity in the spatial domain, however.", "Performing matrix multiplications on the eigendecomposition of the Laplacian is expensive!", "Spectrum-free Methods  A polynomial of the Laplacian acts as a polynomial on the eigenvalues.", "ChebNet (Defferrard et al.)", "and Graph Convolutional Networks (Kipf et al.)", "boil down to applying simple filters acting on the r- or 1-hop neighborhood of the graph in the spatial domain.", "Some examples of generalizations of CNNs that define weighting functions for a locally Euclidean coordinate system around a point on a manifold are the  Geodesic CNN  Anisotropic CNN  Mixture Model network (MoNet)  What problems are being solved with these methods?", "Ranking and community detection on social networks  Recommender systems  3D geometric data in Computer Vision/Graphics  Shape classification  Feature correspondence for 3D shapes  Behavior of N-particle systems (particle physics, LHC)  Molecule design  Medical imaging  Open Problems  Generalization spectral analogues of convolution learned on one graph cannot be readily applied to other ones (domain dependency).", "Spatial methods generalize across different domains, but come with their own subtleties  Time-varying domains  Directed graphs non-symmetric Laplacian that do not have orthogonal eigendecompositions for interpretable spectral-domain constructions  Synthesis problems generative models  Computation extending deep learning frameworks for non-Euclidean data"], "summary_text": "Notes  This paper surveys progress on adapting deep learning techniques to non-Euclidean data and suggests future directions. One of the strengths (and weaknesses) of deep learning–specifically exploited by convolutional neural networks–is that the data is assumed to exhibit translation invariance/equivariance and invariance to local deformations. Hence, long-range dependencies can be learned with multi-scale, hierarchical techniques where spatial resolution is reduced. However, this means that any information about the data that can’t be learned when spatial resolution is reduced can get lost (I believe that residual networks aim to address this by the skip connections that are able to learn an identity operation; also, in computer vision, multi-scale versions of the data are often fed to CNNs). Key areas where this assumption about the data appears to be true is computer vision and speech recognition. Some quick background  The Laplacian, a self-adjoint (symmetric) positive semi-definite operator, which is defined for smooth manifolds and graphs in this paper, can be thought of as the difference between the local average of a function around a point and the value of the function at the point itself. It’s generally defined as $\\triangle = -\\text{div} \\nabla$. When discretizing a continuous, smooth manifold with a mesh, note that the graph Laplacian might not converge to the continuous Laplacian operator with increasing sampling density. To be consistent, need to create a triangular mesh, i.e., represent the manifold as a polyhedral surface. Spectral methods  Fourier analysis on non-Euclidean domains is possible by considering the eigendecomposition of the Laplacian operator. A possible transformation of the Convolution Theorem to functions on manifolds and graphs is discussed, but is noted as not being shift-invariant. The Spectral CNN can be defined by introducing a spectral convolutional layer acting on the vertices of the graph and using filters in the frequency domain and the eigenvectors of the Laplacian. However, the spectral filter coefficients will be dependent on the particular eigenvectors (basis) - domain dependency == bad for generalization! The non-Euclidean analogy of pooling is graph coarsening- only a fraction of the vertices of the graph are retained. Strided convolutions can be generalized to the spectral construction by only keeping the low-frequency components - must recompute the graph Laplacian after applying the nonlinearity in the spatial domain, however. Performing matrix multiplications on the eigendecomposition of the Laplacian is expensive! Spectrum-free Methods  A polynomial of the Laplacian acts as a polynomial on the eigenvalues. ChebNet (Defferrard et al.) and Graph Convolutional Networks (Kipf et al.) boil down to applying simple filters acting on the r- or 1-hop neighborhood of the graph in the spatial domain. Some examples of generalizations of CNNs that define weighting functions for a locally Euclidean coordinate system around a point on a manifold are the  Geodesic CNN  Anisotropic CNN  Mixture Model network (MoNet)  What problems are being solved with these methods? Ranking and community detection on social networks  Recommender systems  3D geometric data in Computer Vision/Graphics  Shape classification  Feature correspondence for 3D shapes  Behavior of N-particle systems (particle physics, LHC)  Molecule design  Medical imaging  Open Problems  Generalization spectral analogues of convolution learned on one graph cannot be readily applied to other ones (domain dependency). Spatial methods generalize across different domains, but come with their own subtleties  Time-varying domains  Directed graphs non-symmetric Laplacian that do not have orthogonal eigendecompositions for interpretable spectral-domain constructions  Synthesis problems generative models  Computation extending deep learning frameworks for non-Euclidean data", "pdf_url": "https://arxiv.org/pdf/1611.08097", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/geometric-deep-learning.json"}
{"id": "69481324", "bin": "600_700", "summary_sentences": ["Most existing GNN (Graph Neural Network) methods are inherently flat and are unable to process the information in a hierarchical manner.", "The paper proposes a differentiable graph pooling operation, DIFFPOOL, that can generate hierarchical graph representations and can be easily plugged into many GNN architectures.", "Key Idea  CNNs have spatial pooling operation that allows for deep CNN architectures to operate on coarse graph representations of input images.", "This notion cannot be applied as-is to graphs as they do not have a natural notion of spatial locality like images do.", "DIFFPOOL attempts to resolve this problem by learning a differentiable soft-assignment at each layer which is equivalent to pooling the cluster of nodes to obtain a sparse representation.", "Approach  Given a graph G(A, F), where A is the adjacency matrix and F is the feature matrix.", "Given a permutation invariant GNN that follows the message passing architecture.", "The output of this GNN can be expressed as Z = GNN(A, X) where X is the current feature matrix.", "Goal is to stack L GNN layers on top of each other such that the lth layer uses coarsened output from the (l-1)th layer.", "This coarsening operation uses a cluster assignment matrix S.  The learned cluster assignment matrix at layer l is denoted at Sl  Given Sl, the embedding matrix for the (l+1)th layer is given as transpose(Sl)Zl and adjancecy matrix is given by transpose(Sl)AlSl  A new GNN, called as GNNpool is used to produce the assignment matrix S by taking a softmax over GNNpool(Al, Xl)  As long as the GNN model is permutation invariant, the resulting DIFFPOOL model is also permutation invariant.", "Auxiliary Losses  The paper uses 2 auxiliary losses to push the model away from spurious local minima early in the training.", "Link prediction objective - at each layer, link prediction loss ( = A - S(transpose(S))) is minimized with the intuition that the nearby nodes should be pooled together.", "Ideally, the cluster assignment for each node should be a one-hot vector so the entropy for cluster assignment per node is regularized.", "Baselines  GNN based models  GraphSage  Mean pooling  Set2Set pooling  Sort pooling  Structure2vec  Edge conditioned filters in CNN  PatchySan  Kernel based models  Graphlet, shortest path etc  Model Variants  GraphSage  Mean pool + Diff pool (3 or 2 layers)  Structure2Vec + Diffpool  Diffpool-Det  The assignment matrix S are generated using graph clustering algorithms.", "Diffpool-NoLP  The link prediction objective function is turned off.", "At each DiffPool layer, the number of classes is set to 25% of the number of nodes before the DiffPool layer.", "Results  DiffPool obtains the highest average performance across all the pooling approaches and improves upon the base GraphSage architecture by an average of around 7%.", "In terms of runtime complexity, the paper reports that DiffPool does not incur any significant additional running time.", "But given that now there are 2 GNN models per layer, the size of the model should increase.", "DiffPool can capture hierarchical community structure even when trained on just the graph classification loss.", "One advantage of DiffPool is that the nodes are pooled in a non-uniform way so densely connected group of nodes would collapse into one cluster while sparsely connected nodes can retain their identity."], "summary_text": "Most existing GNN (Graph Neural Network) methods are inherently flat and are unable to process the information in a hierarchical manner. The paper proposes a differentiable graph pooling operation, DIFFPOOL, that can generate hierarchical graph representations and can be easily plugged into many GNN architectures. Key Idea  CNNs have spatial pooling operation that allows for deep CNN architectures to operate on coarse graph representations of input images. This notion cannot be applied as-is to graphs as they do not have a natural notion of spatial locality like images do. DIFFPOOL attempts to resolve this problem by learning a differentiable soft-assignment at each layer which is equivalent to pooling the cluster of nodes to obtain a sparse representation. Approach  Given a graph G(A, F), where A is the adjacency matrix and F is the feature matrix. Given a permutation invariant GNN that follows the message passing architecture. The output of this GNN can be expressed as Z = GNN(A, X) where X is the current feature matrix. Goal is to stack L GNN layers on top of each other such that the lth layer uses coarsened output from the (l-1)th layer. This coarsening operation uses a cluster assignment matrix S.  The learned cluster assignment matrix at layer l is denoted at Sl  Given Sl, the embedding matrix for the (l+1)th layer is given as transpose(Sl)Zl and adjancecy matrix is given by transpose(Sl)AlSl  A new GNN, called as GNNpool is used to produce the assignment matrix S by taking a softmax over GNNpool(Al, Xl)  As long as the GNN model is permutation invariant, the resulting DIFFPOOL model is also permutation invariant. Auxiliary Losses  The paper uses 2 auxiliary losses to push the model away from spurious local minima early in the training. Link prediction objective - at each layer, link prediction loss ( = A - S(transpose(S))) is minimized with the intuition that the nearby nodes should be pooled together. Ideally, the cluster assignment for each node should be a one-hot vector so the entropy for cluster assignment per node is regularized. Baselines  GNN based models  GraphSage  Mean pooling  Set2Set pooling  Sort pooling  Structure2vec  Edge conditioned filters in CNN  PatchySan  Kernel based models  Graphlet, shortest path etc  Model Variants  GraphSage  Mean pool + Diff pool (3 or 2 layers)  Structure2Vec + Diffpool  Diffpool-Det  The assignment matrix S are generated using graph clustering algorithms. Diffpool-NoLP  The link prediction objective function is turned off. At each DiffPool layer, the number of classes is set to 25% of the number of nodes before the DiffPool layer. Results  DiffPool obtains the highest average performance across all the pooling approaches and improves upon the base GraphSage architecture by an average of around 7%. In terms of runtime complexity, the paper reports that DiffPool does not incur any significant additional running time. But given that now there are 2 GNN models per layer, the size of the model should increase. DiffPool can capture hierarchical community structure even when trained on just the graph classification loss. One advantage of DiffPool is that the nodes are pooled in a non-uniform way so densely connected group of nodes would collapse into one cluster while sparsely connected nodes can retain their identity.", "pdf_url": "https://arxiv.org/pdf/1806.08804", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/hierarchical-graph-representation-learning-with-differentiable-pooling.json"}
{"id": "71370947", "bin": "600_700", "summary_sentences": ["What  They change standard ResNets to contain higher resolution feature maps (i.e. more height/width) and to use dilated convolutions .", "This makes them more useful for e.g. segmentation.", "They identify a gridding related problem and how to fix it.", "How  Changes to ResNet  ResNets are organized in five blocks of (each multiple) residual convolutions.", "They increase the feature map resolutions of the fourth and fifth block (to 2x and 4x).", "They increase simultanously the dilation of each convolution in the fourth and fifth block (to dilation 2 and 4).", "This is a common practice and known as the \"á trous trick\".", "Degridding  When using dilated convolutions one can end up with grid-like patterns in the generated feature maps.", "This happens when the image has higher-frequency content than the sampling of the dilated convolution.", "The below image shows an example for a convolution with dilation 2 and equal weights for all 3x3 parameters:  These grid-like patterns make the network less useful for segmentation tasks.", "They fix the gridding problems with two steps:  They remove the max pooling at the start of the network and replace it with convolutions.", "This is done, because max pooling led to high-frequency high-amplitude components in their tests.", "They add convolutions with less or no dilation after the dilated convolutions (i.e. at the end of the network).", "These \"smoothen\" the feature maps, thereby fixing the grid-like patterns.", "They get three networks:  DRN-A-18: ResNet with 18 layers and dilation (2 in block 4 and 4 in block 5).", "DRN-B-26: Like DRN-A-18, but max pooling is replaced by four residual convolutions (in two blocks, each two convs).", "They also add four residual convolutions at the end of the network (in two blocks, each two convs).", "DRN-C-26: Like DRN-B-26, but the four convolutions at the end of the network are not residual.", "The motivation to use non-residual convolutions in DRN-C-26 is that residual ones can propagate the problems (grid-like patterns) more easily to the output.", "Visualization of the architectures:  Visualization of the effect of using max pooling (DRN-A-XX) and replacing it with convolutions (DRN-B-XX):  Results  Example visualizations of the generated feature maps by each network:  ImageNet classification  Their networks perform quite a bit better than ResNets of comparable size.", "The ranking of their networks seems to be: 1.", "DRN-C-XX, 2.", "DRN-B-XX, 3.", "DRN-C-XX.", "The difference between B and C isn't that big, but between A and B it is.", "To a degree this is expected, as B and C have four more convolutions than A (due to max pooling being replaced by convs).", "The effect though seems to be a bit stronger than just that.", "Their DRN-C-42 network is only a bit less accurate than ResNet-101.", "Object Localization  They suggest a method for bounding box detection without explicit training for that (i.e. when only training on ImageNet for classification).", "Sounds like they just \"try\" every possible bounding box within a range of heights/widths at every location in the final feature map.", "Then they pick the one with highest activation, if it is above a threshold.", "When applying that method to their networks and ResNets they get significantly better results with their networks.", "Not that surprising, considering their final feature map resolutions are four times higher than in ResNet.", "Semantic Segmentation  They apply their models to the Cityscapes dataset.", "They get a 70.9 mean IoU for DRN-C-42, while the reported best value for ResNet-101 is 66.6.", "No information regarding runtimes in the paper.", "Dilated ResNets are most likely going to run slower as they work with higher resolution feature maps.", "They will also require more RAM."], "summary_text": "What  They change standard ResNets to contain higher resolution feature maps (i.e. more height/width) and to use dilated convolutions . This makes them more useful for e.g. segmentation. They identify a gridding related problem and how to fix it. How  Changes to ResNet  ResNets are organized in five blocks of (each multiple) residual convolutions. They increase the feature map resolutions of the fourth and fifth block (to 2x and 4x). They increase simultanously the dilation of each convolution in the fourth and fifth block (to dilation 2 and 4). This is a common practice and known as the \"á trous trick\". Degridding  When using dilated convolutions one can end up with grid-like patterns in the generated feature maps. This happens when the image has higher-frequency content than the sampling of the dilated convolution. The below image shows an example for a convolution with dilation 2 and equal weights for all 3x3 parameters:  These grid-like patterns make the network less useful for segmentation tasks. They fix the gridding problems with two steps:  They remove the max pooling at the start of the network and replace it with convolutions. This is done, because max pooling led to high-frequency high-amplitude components in their tests. They add convolutions with less or no dilation after the dilated convolutions (i.e. at the end of the network). These \"smoothen\" the feature maps, thereby fixing the grid-like patterns. They get three networks:  DRN-A-18: ResNet with 18 layers and dilation (2 in block 4 and 4 in block 5). DRN-B-26: Like DRN-A-18, but max pooling is replaced by four residual convolutions (in two blocks, each two convs). They also add four residual convolutions at the end of the network (in two blocks, each two convs). DRN-C-26: Like DRN-B-26, but the four convolutions at the end of the network are not residual. The motivation to use non-residual convolutions in DRN-C-26 is that residual ones can propagate the problems (grid-like patterns) more easily to the output. Visualization of the architectures:  Visualization of the effect of using max pooling (DRN-A-XX) and replacing it with convolutions (DRN-B-XX):  Results  Example visualizations of the generated feature maps by each network:  ImageNet classification  Their networks perform quite a bit better than ResNets of comparable size. The ranking of their networks seems to be: 1. DRN-C-XX, 2. DRN-B-XX, 3. DRN-C-XX. The difference between B and C isn't that big, but between A and B it is. To a degree this is expected, as B and C have four more convolutions than A (due to max pooling being replaced by convs). The effect though seems to be a bit stronger than just that. Their DRN-C-42 network is only a bit less accurate than ResNet-101. Object Localization  They suggest a method for bounding box detection without explicit training for that (i.e. when only training on ImageNet for classification). Sounds like they just \"try\" every possible bounding box within a range of heights/widths at every location in the final feature map. Then they pick the one with highest activation, if it is above a threshold. When applying that method to their networks and ResNets they get significantly better results with their networks. Not that surprising, considering their final feature map resolutions are four times higher than in ResNet. Semantic Segmentation  They apply their models to the Cityscapes dataset. They get a 70.9 mean IoU for DRN-C-42, while the reported best value for ResNet-101 is 66.6. No information regarding runtimes in the paper. Dilated ResNets are most likely going to run slower as they work with higher resolution feature maps. They will also require more RAM.", "pdf_url": "https://arxiv.org/pdf/1705.09914", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/dilated_residual_networks.json"}
{"id": "82314288", "bin": "600_700", "summary_sentences": ["What  They describe a model to locate faces in images.", "Their model uses information from suspected face regions and from the corresponding suspected body regions to classify whether a region contains a face.", "The intuition is, that seeing the region around the face (specifically where the body should be) can help in estimating whether a suspected face is really a face (e.g. it might also be part of a painting, statue or doll).", "How  Their whole model is called \"CMS-RCNN\" (Contextual Multi-Scale Region-CNN).", "It is based on the \"Faster R-CNN\" architecture.", "It uses the VGG network.", "Subparts of their model are: MS-RPN, CMS-CNN.", "MS-RPN finds candidate face regions.", "CMS-CNN refines their bounding boxes and classifies them (face / not face).", "MS-RPN (Multi-Scale Region Proposal Network)  \"Looks\" at the feature maps of the network (VGG) at multiple scales (i.e. before/after pooling layers) and suggests regions for possible faces.", "Steps:  Feed an image through the VGG network.", "Extract the feature maps of the three last convolutions that are before a pooling layer.", "Pool these feature maps so that they have the same heights and widths.", "Apply L2 normalization to each feature map so that they all have the same scale.", "Apply a 1x1 convolution to merge them to one feature map.", "Regress face bounding boxes from that feature map according to the Faster R-CNN technique.", "CMS-CNN (Contextual Multi-Scale CNN):  \"Looks\" at feature maps of face candidates found by MS-RPN and classifies whether these regions contains faces.", "It also uses the same multi-scale technique (i.e. take feature maps from convs before pooling layers).", "It uses some area around these face regions as additional information (suspected regions of bodies).", "Steps:  Receive face candidate regions from MS-RPN.", "Do per candidate region:  Calculate the suspected coordinates of the body (only based on the x/y-position and size of the face region, i.e. not learned).", "Extract the feature maps of the face region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).", "Extract the feature maps of the body region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).", "L2-normalize each feature map.", "Concatenate the (RoI-pooled and normalized) feature maps of the face (at multiple scales) with each other (creates one tensor).", "Concatenate the (RoI-pooled and normalized) feature maps of the body (at multiple scales) with each other (creates another tensor).", "Apply a 1x1 convolution to the face tensor.", "Apply a 1x1 convolution to the body tensor.", "Apply two fully connected layers to the face tensor, creating a vector.", "Apply two fully connected layers to the body tensor, creating a vector.", "Concatenate both vectors.", "Based on that vector, make a classification of whether it is really a face.", "Based on that vector, make a regression of the face's final bounding box coordinates and dimensions.", "Note: They use in both networks the multi-scale approach in order to be able to find small or tiny faces.", "Otherwise, after pooling these small faces would be hard or impossible to detect.", "Results  Adding context to the classification (i.e. the body regions) empirically improves the results.", "Their model achieves the highest recall rate on FDDB compared to other models.", "However, it has lower recall if only very few false positives are accepted.", "FDDB ROC curves (theirs is bold red):  Example results on FDDB:"], "summary_text": "What  They describe a model to locate faces in images. Their model uses information from suspected face regions and from the corresponding suspected body regions to classify whether a region contains a face. The intuition is, that seeing the region around the face (specifically where the body should be) can help in estimating whether a suspected face is really a face (e.g. it might also be part of a painting, statue or doll). How  Their whole model is called \"CMS-RCNN\" (Contextual Multi-Scale Region-CNN). It is based on the \"Faster R-CNN\" architecture. It uses the VGG network. Subparts of their model are: MS-RPN, CMS-CNN. MS-RPN finds candidate face regions. CMS-CNN refines their bounding boxes and classifies them (face / not face). MS-RPN (Multi-Scale Region Proposal Network)  \"Looks\" at the feature maps of the network (VGG) at multiple scales (i.e. before/after pooling layers) and suggests regions for possible faces. Steps:  Feed an image through the VGG network. Extract the feature maps of the three last convolutions that are before a pooling layer. Pool these feature maps so that they have the same heights and widths. Apply L2 normalization to each feature map so that they all have the same scale. Apply a 1x1 convolution to merge them to one feature map. Regress face bounding boxes from that feature map according to the Faster R-CNN technique. CMS-CNN (Contextual Multi-Scale CNN):  \"Looks\" at feature maps of face candidates found by MS-RPN and classifies whether these regions contains faces. It also uses the same multi-scale technique (i.e. take feature maps from convs before pooling layers). It uses some area around these face regions as additional information (suspected regions of bodies). Steps:  Receive face candidate regions from MS-RPN. Do per candidate region:  Calculate the suspected coordinates of the body (only based on the x/y-position and size of the face region, i.e. not learned). Extract the feature maps of the face region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width). Extract the feature maps of the body region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width). L2-normalize each feature map. Concatenate the (RoI-pooled and normalized) feature maps of the face (at multiple scales) with each other (creates one tensor). Concatenate the (RoI-pooled and normalized) feature maps of the body (at multiple scales) with each other (creates another tensor). Apply a 1x1 convolution to the face tensor. Apply a 1x1 convolution to the body tensor. Apply two fully connected layers to the face tensor, creating a vector. Apply two fully connected layers to the body tensor, creating a vector. Concatenate both vectors. Based on that vector, make a classification of whether it is really a face. Based on that vector, make a regression of the face's final bounding box coordinates and dimensions. Note: They use in both networks the multi-scale approach in order to be able to find small or tiny faces. Otherwise, after pooling these small faces would be hard or impossible to detect. Results  Adding context to the classification (i.e. the body regions) empirically improves the results. Their model achieves the highest recall rate on FDDB compared to other models. However, it has lower recall if only very few false positives are accepted. FDDB ROC curves (theirs is bold red):  Example results on FDDB:", "pdf_url": "http://arxiv.org/pdf/1606.05413v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/cms-rcnn.json"}
{"id": "31277852", "bin": "600_700", "summary_sentences": ["Algorithm to derive similarity between 2 nodes of a graph (or graphical model derived from any other kind of dataset).", "SimRank  Input: A directed graph G = (V, E) where V represents vertices and E represents edges.", "SimRank defines similarity between 2 vertices (or nodes) i and j as the average of the similarity between their in-neighbours decayed by a constant factor C.  Any node is maximally similar to itself (with similarity = 1).", "PageRank analyses the individual vertices of the graph with respect to the global structure, while SimRank analyses the relationship between a pair of vertices (edges).", "SimRank scores are symmetric and can be defined between all pair of vertices.", "G2 is defined as the node pair graph such that each node in G2 corresponds to an ordered pair of nodes of G and there exists an edge between node pair (a, b) and (c, d) if there exists an edge between (a, c) and (b, d).", "In G2, similarity flows from node to node with singleton nodes (nodes of the form (a, a)) as the source of similarity.", "Variants  Minimax Variant  Defines similarity of nodes i and j as the minimum of maximum similarity between i and any in-neighbour of j and between j and any in-neighbour of i.  Computing SimRank  A naive solution can be obtained by iteration to a fixed point.", "Space complexity is O(n2) and time complexity is *O(kn2d) where k is the number of iterations, n is the number of vertices and d is the average of product of indegrees of pair of vertices.", "Optimisations can be made by setting the similarity between far off nodes as 0 and considering only nearby nodes for an update.", "Different Interpretations  Co-citation Score  The first iteration of SimRank produces results same as co-citation score between a pair of vertices.", "Successive iterations improve these initial scores.", "Random Surfer-Pairs Model  SimRank s(a, b) can be interpreted as the measure of how soon two random surfers are expected to meet at the same node if they start at nodes a and b and walk the graph backwards.", "Expected Meeting Distance (EMD) between 2 nodes a and b is the expected number of steps required before 2 surfers (starting at a and b) would meet if they walked randomly in locked step.", "Surfers are allowed to teleport with a small probability - to circumvent the infinite EMD problem.", "Expected-f Meeting Distance (EMD) - Given length l of a tour, compute f(l) (where f is a non-negative monotonic function) to bound the expected distance to a finite interval.", "Common choice for f is f(z) = Cz where C &epsilon; (0, 1)  The SimRank score for two nodes, with parameter C, is the expected-f meeting distance travelling back-edges with f(z) = Cz  Evaluation  Experiments on 2 datasets:  Corpus of scientific research papers from ResearchIndex.", "Transcripts of undergrad students at Stanford.", "Domain specific properties used to measure similarity and compared with SimRank scores.", "Results show improvement over co-citation scores.", "This comment has been minimized.", "Sign in to view  Copy link  Quote reply  tiankonghenlan20113046 commented  Oct 25, 2018  I read this paper before,but i cannot understand some aspects about the definition.", "can I  have a discussion wtih you ?"], "summary_text": "Algorithm to derive similarity between 2 nodes of a graph (or graphical model derived from any other kind of dataset). SimRank  Input: A directed graph G = (V, E) where V represents vertices and E represents edges. SimRank defines similarity between 2 vertices (or nodes) i and j as the average of the similarity between their in-neighbours decayed by a constant factor C.  Any node is maximally similar to itself (with similarity = 1). PageRank analyses the individual vertices of the graph with respect to the global structure, while SimRank analyses the relationship between a pair of vertices (edges). SimRank scores are symmetric and can be defined between all pair of vertices. G2 is defined as the node pair graph such that each node in G2 corresponds to an ordered pair of nodes of G and there exists an edge between node pair (a, b) and (c, d) if there exists an edge between (a, c) and (b, d). In G2, similarity flows from node to node with singleton nodes (nodes of the form (a, a)) as the source of similarity. Variants  Minimax Variant  Defines similarity of nodes i and j as the minimum of maximum similarity between i and any in-neighbour of j and between j and any in-neighbour of i.  Computing SimRank  A naive solution can be obtained by iteration to a fixed point. Space complexity is O(n2) and time complexity is *O(kn2d) where k is the number of iterations, n is the number of vertices and d is the average of product of indegrees of pair of vertices. Optimisations can be made by setting the similarity between far off nodes as 0 and considering only nearby nodes for an update. Different Interpretations  Co-citation Score  The first iteration of SimRank produces results same as co-citation score between a pair of vertices. Successive iterations improve these initial scores. Random Surfer-Pairs Model  SimRank s(a, b) can be interpreted as the measure of how soon two random surfers are expected to meet at the same node if they start at nodes a and b and walk the graph backwards. Expected Meeting Distance (EMD) between 2 nodes a and b is the expected number of steps required before 2 surfers (starting at a and b) would meet if they walked randomly in locked step. Surfers are allowed to teleport with a small probability - to circumvent the infinite EMD problem. Expected-f Meeting Distance (EMD) - Given length l of a tour, compute f(l) (where f is a non-negative monotonic function) to bound the expected distance to a finite interval. Common choice for f is f(z) = Cz where C &epsilon; (0, 1)  The SimRank score for two nodes, with parameter C, is the expected-f meeting distance travelling back-edges with f(z) = Cz  Evaluation  Experiments on 2 datasets:  Corpus of scientific research papers from ResearchIndex. Transcripts of undergrad students at Stanford. Domain specific properties used to measure similarity and compared with SimRank scores. Results show improvement over co-citation scores. This comment has been minimized. Sign in to view  Copy link  Quote reply  tiankonghenlan20113046 commented  Oct 25, 2018  I read this paper before,but i cannot understand some aspects about the definition. can I  have a discussion wtih you ?", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/775047.775126?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/29486212643fd61f58a5a3eb5abb3c.json"}
{"id": "97932426", "bin": "600_700", "summary_sentences": ["The current trend in deep learning is to design, train and fine tune a separate model for each problem.", "Though multi-task models have been explored, they have been trained for problems from the same domain only and no competitive multi-task, multi-modal models have been proposed.", "The paper explores the possibility of such a unified deep learning model that can solve different tasks across multiple domains by training concurrently on them.", "Design Philosophy  Small, modality-specific subnetworks (called modality nets) should be used to map input data to a joint representation space and back.", "The joint representation is to be of variable size.", "Different tasks from the same domain share the modality net.", "MultiModel networks should use computational blocks from different domains even if they are not specifically designed for the task at hand.", "Eg the paper reports that attention and mixture-of-experts (MOE) layers slightly improve the performance on ImageNet even though they are not explicitly needed.", "Architecture  MulitModel Network consists of few, small modality nets, an encoder, I/O mixer and an autoregressive decoder.", "Encoder and decoder use the following computational blocks:  Convolutional Block  ReLU activations on inputs followed by depthwise separable convolutions and layer normalization.", "Attention Block  Multihead, dot product based attention mechanism.", "Mixture-of-Experts (MoE) Block  Consists of simple feed-forward networks (called experts) and a trainable gating network which selects a sparse combination of experts to process the inputs.", "For further details, refer the original paper .", "Encoder consists of 6 conv blocks with a MoE block in the middle.", "I/O mixer consists of an attention block and 2 conv blocks.", "Decoder consists of 4 blocks of convolution and attention with a MoE block in the middle.", "Modality Nets  Language Data  Input is the sequence of tokens ending in a termination token.", "This sequence is mapped to correct dimensionality using a learned embedding.", "For output, the network takes the decoded output and performs a learned linear mapping followed by Softmax.", "Image and Categorical Data  Uses residual convolution blocks.", "Similar to the exit flow for Xception Network  Audio Data  1-d waveform over time or 2-d spectrogram operated upon by stack of 8 residual convolution blocks.", "Tasks  WSJ speech corpus  ImageNet dataset  COCO image captioning dataset  WSJ parsing dataset  WMT English-German translation corpus  German-English translation  WMT English-French translation corpus  German-French translation  Experiments  The experimental section is not very rigorous with many details skipped (would probably be added later).", "While MultiModel does not beat the state of the art models, it does outperform some recent models.", "Jointly trained model performs similar to single trained models on tasks with a lot of data and sometimes outperformed single trained models on tasks with less data (like parsing).", "Interestingly, jointly training the model for parsing task and Imagenet tasks improves the performance of parsing task even though the two tasks are seemingly unrelated.", "Another experiment was done to evaluate the effect of components (like MoE) on tasks (like Imagenet) which do not explicitly need them.", "It was observed that either the performance either went down or remained the same when MoE component was removed.", "This indicates that mixing different components does help to improve performance over multiple tasks.", "But this observation is not conclusive as a different combination of say the encoder (that does not use MoE) could achieve better performance than one that does.", "The paper does not explore possibilities like these."], "summary_text": "The current trend in deep learning is to design, train and fine tune a separate model for each problem. Though multi-task models have been explored, they have been trained for problems from the same domain only and no competitive multi-task, multi-modal models have been proposed. The paper explores the possibility of such a unified deep learning model that can solve different tasks across multiple domains by training concurrently on them. Design Philosophy  Small, modality-specific subnetworks (called modality nets) should be used to map input data to a joint representation space and back. The joint representation is to be of variable size. Different tasks from the same domain share the modality net. MultiModel networks should use computational blocks from different domains even if they are not specifically designed for the task at hand. Eg the paper reports that attention and mixture-of-experts (MOE) layers slightly improve the performance on ImageNet even though they are not explicitly needed. Architecture  MulitModel Network consists of few, small modality nets, an encoder, I/O mixer and an autoregressive decoder. Encoder and decoder use the following computational blocks:  Convolutional Block  ReLU activations on inputs followed by depthwise separable convolutions and layer normalization. Attention Block  Multihead, dot product based attention mechanism. Mixture-of-Experts (MoE) Block  Consists of simple feed-forward networks (called experts) and a trainable gating network which selects a sparse combination of experts to process the inputs. For further details, refer the original paper . Encoder consists of 6 conv blocks with a MoE block in the middle. I/O mixer consists of an attention block and 2 conv blocks. Decoder consists of 4 blocks of convolution and attention with a MoE block in the middle. Modality Nets  Language Data  Input is the sequence of tokens ending in a termination token. This sequence is mapped to correct dimensionality using a learned embedding. For output, the network takes the decoded output and performs a learned linear mapping followed by Softmax. Image and Categorical Data  Uses residual convolution blocks. Similar to the exit flow for Xception Network  Audio Data  1-d waveform over time or 2-d spectrogram operated upon by stack of 8 residual convolution blocks. Tasks  WSJ speech corpus  ImageNet dataset  COCO image captioning dataset  WSJ parsing dataset  WMT English-German translation corpus  German-English translation  WMT English-French translation corpus  German-French translation  Experiments  The experimental section is not very rigorous with many details skipped (would probably be added later). While MultiModel does not beat the state of the art models, it does outperform some recent models. Jointly trained model performs similar to single trained models on tasks with a lot of data and sometimes outperformed single trained models on tasks with less data (like parsing). Interestingly, jointly training the model for parsing task and Imagenet tasks improves the performance of parsing task even though the two tasks are seemingly unrelated. Another experiment was done to evaluate the effect of components (like MoE) on tasks (like Imagenet) which do not explicitly need them. It was observed that either the performance either went down or remained the same when MoE component was removed. This indicates that mixing different components does help to improve performance over multiple tasks. But this observation is not conclusive as a different combination of say the encoder (that does not use MoE) could achieve better performance than one that does. The paper does not explore possibilities like these.", "pdf_url": "https://arxiv.org/pdf/1706.05137", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/one-model-to-learn-them-all.json"}
{"id": "27300559", "bin": "600_700", "summary_sentences": ["Why do Record/Replay Tests of Web Applications Break?", "– Hammoudi et al. ICST ’16  Your web application regression tests created using record/replay tools are fragile and keep breaking.", "Hammoudi et al. set out to find out why.", "If we knew that, perhaps we could design mechanisms to automatically repair broken tests, or to build more robust tests.", "The authors look at 300 different versions of five open source web applications, creating test suites for their initial versions using Selenium IDE and then following the evolution of the projects.", "When a test broke in a given version, it was repaired so that the process could continue.", "At the end of this process, data had been gathered on 722 individual test breakages.", "Using the data we gathered, we developed a taxonomy of the causes of test breakages that categorizes all of the breakages observed on the applications we studied.", "We then gathered 153 versions of three additional web applications and applied the foregoing process to them as well; this yielded data on 343 additional test breakages.", "We analyzed these in light of our taxonomy and were able to accommodate all of them without further changing the taxonomy; this provides evidence that our taxonomy may be more generally applicable.", "Hammoudi et al. had to create their own tests since, “In searching for web applications, we discovered that few open-source applications are provided with capture-replay test suites; in fact, few are provided with any test suites at all….”  This will be a shorter paper write-up than usual.", "What you really need to know is your tests are breaking because the information used to locate page elements keeps breaking.", "After collating and clustering all of the breakages across the web tests, the authors create a taxonomy with 5 high level causes of test breakages:  Causes related to locators used in tests  Causes related to values and actions used in tests  Causes related to page reloading  Causes related to changes in user sessions times  Causes related to popup boxes.", "Locators are used by JavaScript and other languages, and by record/replay tools, to identify and manipulate elements.", "We identify two classes of locators, the second of which is composed of two sub-classes…  Attribute-based locators use element attributes such as element ids and names.", "Structure-based locators rely on the structure of a web page and may locate elements via a hierarchy (e.g. xpaths or CSS selectors) or via an index in the case of multiple otherwise identical elements.", "Over 70% of all breakages are due to locator fragility in the face of change, and over 50% of all breakages are further due to attribute-based locators.", "Note that we cannot conclude from this that element attribute based location is inferior to other forms of location – even though it is responsible for most breakages – since we don’t know the base rate of usage of the different location strategies.", "My personal suspicion is that attribute-based location is one of the more robust strategies.", "In terms of giving guidance to practitioners seeking to write more robust tests, it would be really nice to see this additional level of analysis.", "Our data suggests which categories of test breakages merit the greatest attention.", "Locators caused over 73% of the test breakages we observed, and attribute-based locators caused the majority of these.", "Clearly, addressing just this class of errors by finding ways to repair them if they break would have the largest overall impact on the reusability of tests across releases.", "The data also suggest where subsequent priorities should be placed in terms of finding methods for repairing test breakages."], "summary_text": "Why do Record/Replay Tests of Web Applications Break? – Hammoudi et al. ICST ’16  Your web application regression tests created using record/replay tools are fragile and keep breaking. Hammoudi et al. set out to find out why. If we knew that, perhaps we could design mechanisms to automatically repair broken tests, or to build more robust tests. The authors look at 300 different versions of five open source web applications, creating test suites for their initial versions using Selenium IDE and then following the evolution of the projects. When a test broke in a given version, it was repaired so that the process could continue. At the end of this process, data had been gathered on 722 individual test breakages. Using the data we gathered, we developed a taxonomy of the causes of test breakages that categorizes all of the breakages observed on the applications we studied. We then gathered 153 versions of three additional web applications and applied the foregoing process to them as well; this yielded data on 343 additional test breakages. We analyzed these in light of our taxonomy and were able to accommodate all of them without further changing the taxonomy; this provides evidence that our taxonomy may be more generally applicable. Hammoudi et al. had to create their own tests since, “In searching for web applications, we discovered that few open-source applications are provided with capture-replay test suites; in fact, few are provided with any test suites at all….”  This will be a shorter paper write-up than usual. What you really need to know is your tests are breaking because the information used to locate page elements keeps breaking. After collating and clustering all of the breakages across the web tests, the authors create a taxonomy with 5 high level causes of test breakages:  Causes related to locators used in tests  Causes related to values and actions used in tests  Causes related to page reloading  Causes related to changes in user sessions times  Causes related to popup boxes. Locators are used by JavaScript and other languages, and by record/replay tools, to identify and manipulate elements. We identify two classes of locators, the second of which is composed of two sub-classes…  Attribute-based locators use element attributes such as element ids and names. Structure-based locators rely on the structure of a web page and may locate elements via a hierarchy (e.g. xpaths or CSS selectors) or via an index in the case of multiple otherwise identical elements. Over 70% of all breakages are due to locator fragility in the face of change, and over 50% of all breakages are further due to attribute-based locators. Note that we cannot conclude from this that element attribute based location is inferior to other forms of location – even though it is responsible for most breakages – since we don’t know the base rate of usage of the different location strategies. My personal suspicion is that attribute-based location is one of the more robust strategies. In terms of giving guidance to practitioners seeking to write more robust tests, it would be really nice to see this additional level of analysis. Our data suggests which categories of test breakages merit the greatest attention. Locators caused over 73% of the test breakages we observed, and attribute-based locators caused the majority of these. Clearly, addressing just this class of errors by finding ways to repair them if they break would have the largest overall impact on the reusability of tests across releases. The data also suggest where subsequent priorities should be placed in terms of finding methods for repairing test breakages.", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7515470", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/why-do-recordreplay-tests-of-web-applications-break.json"}
{"id": "70889889", "bin": "600_700", "summary_sentences": ["What it is  Weight Normalization (WN) is a normalization technique, similar to Batch Normalization (BN).", "It normalizes each layer's weights.", "Differences to BN  WN normalizes based on each weight vector's orientation and magnitude.", "BN normalizes based on each weight's mean and variance in a batch.", "WN works on each example on its own.", "BN works on whole batches.", "WN is more deterministic than BN (due to not working an batches).", "WN is better suited for noisy environment (RNNs, LSTMs, reinforcement learning, generative models).", "(Due to being more deterministic.)", "WN is computationally simpler than BN.", "How its done  WN is a module added on top of a linear or convolutional layer.", "If that layer's weights are w then WN learns two parameters g (scalar) and v (vector, identical dimension to w) so that w = gv / ||v|| is fullfilled (||v|| = euclidean norm of v).", "g is the magnitude of the weights, v are their orientation.", "v is initialized to zero mean and a standard deviation of 0.05.", "For networks without recursions (i.e. not RNN/LSTM/GRU):  Right after initialization, they feed a single batch through the network.", "For each neuron/weight, they calculate the mean and standard deviation after the WN layer.", "They then adjust the bias to -mean/stdDev and g to 1/stdDev.", "That makes the network start with each feature being roughly zero-mean and unit-variance.", "The same method can also be applied to networks without WN.", "Results:  They define BN-MEAN as a variant of BN which only normalizes to zero-mean (not unit-variance).", "CIFAR-10 image classification (no data augmentation, some dropout, some white noise):  WN, BN, BN-MEAN all learn similarly fast.", "Network without normalization learns slower, but catches up towards the end.", "BN learns \"more\" per example, but is about 16% slower (time-wise) than WN.", "WN reaches about same test error as no normalization (both ~8.4%), BN achieves better results (~8.0%).", "WN + BN-MEAN achieves best results with 7.31%.", "Optimizer: Adam  Convolutional VAE on MNIST and CIFAR-10:  WN learns more per example und plateaus at better values than network without normalization.", "(BN was not tested.)", "Optimizer: Adamax  DRAW on MNIST (heavy on LSTMs):  WN learns significantly more example than network without normalization.", "Also ends up with better results.", "(Normal network might catch up though if run longer.)", "Deep Reinforcement Learning (Space Invaders):  WN seemed to overall acquire a bit more reward per epoch than network without normalization.", "Variance (in acquired reward) however also grew.", "Results not as clear as in DRAW.", "Optimizer: Adamax  Extensions  They argue that initializing g to exp(cs) (c constant, s learned) might be better, but they didn't get better test results with that.", "Due to some gradient effects, ||v|| currently grows monotonically with every weight update.", "(Not necessarily when using optimizers that use separate learning rates per parameters.)", "That grow effect leads the network to be more robust to different learning rates.", "Setting a small hard limit/constraint for ||v|| can lead to better test set performance (parameter updates are larger, introducing more noise).", "Performance of WN on CIFAR-10 compared to BN, BN-MEAN and no normalization.", "Performance of WN for DRAW (left) and deep reinforcement learning (right)."], "summary_text": "What it is  Weight Normalization (WN) is a normalization technique, similar to Batch Normalization (BN). It normalizes each layer's weights. Differences to BN  WN normalizes based on each weight vector's orientation and magnitude. BN normalizes based on each weight's mean and variance in a batch. WN works on each example on its own. BN works on whole batches. WN is more deterministic than BN (due to not working an batches). WN is better suited for noisy environment (RNNs, LSTMs, reinforcement learning, generative models). (Due to being more deterministic.) WN is computationally simpler than BN. How its done  WN is a module added on top of a linear or convolutional layer. If that layer's weights are w then WN learns two parameters g (scalar) and v (vector, identical dimension to w) so that w = gv / ||v|| is fullfilled (||v|| = euclidean norm of v). g is the magnitude of the weights, v are their orientation. v is initialized to zero mean and a standard deviation of 0.05. For networks without recursions (i.e. not RNN/LSTM/GRU):  Right after initialization, they feed a single batch through the network. For each neuron/weight, they calculate the mean and standard deviation after the WN layer. They then adjust the bias to -mean/stdDev and g to 1/stdDev. That makes the network start with each feature being roughly zero-mean and unit-variance. The same method can also be applied to networks without WN. Results:  They define BN-MEAN as a variant of BN which only normalizes to zero-mean (not unit-variance). CIFAR-10 image classification (no data augmentation, some dropout, some white noise):  WN, BN, BN-MEAN all learn similarly fast. Network without normalization learns slower, but catches up towards the end. BN learns \"more\" per example, but is about 16% slower (time-wise) than WN. WN reaches about same test error as no normalization (both ~8.4%), BN achieves better results (~8.0%). WN + BN-MEAN achieves best results with 7.31%. Optimizer: Adam  Convolutional VAE on MNIST and CIFAR-10:  WN learns more per example und plateaus at better values than network without normalization. (BN was not tested.) Optimizer: Adamax  DRAW on MNIST (heavy on LSTMs):  WN learns significantly more example than network without normalization. Also ends up with better results. (Normal network might catch up though if run longer.) Deep Reinforcement Learning (Space Invaders):  WN seemed to overall acquire a bit more reward per epoch than network without normalization. Variance (in acquired reward) however also grew. Results not as clear as in DRAW. Optimizer: Adamax  Extensions  They argue that initializing g to exp(cs) (c constant, s learned) might be better, but they didn't get better test results with that. Due to some gradient effects, ||v|| currently grows monotonically with every weight update. (Not necessarily when using optimizers that use separate learning rates per parameters.) That grow effect leads the network to be more robust to different learning rates. Setting a small hard limit/constraint for ||v|| can lead to better test set performance (parameter updates are larger, introducing more noise). Performance of WN on CIFAR-10 compared to BN, BN-MEAN and no normalization. Performance of WN for DRAW (left) and deep reinforcement learning (right).", "pdf_url": "http://arxiv.org/pdf/1602.07868", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/weight_normalization.json"}
{"id": "16280529", "bin": "600_700", "summary_sentences": ["This paper derives an algorithm for passing gradients through a sample from a mixture of Gaussians.", "While the reparameterization trick allows to get the gradients with respect to the Gaussian means and covariances, the same trick cannot be invoked for the mixing proportions parameters (essentially because they are the parameters of a multinomial discrete distribution over the Gaussian components, and the reparameterization trick doesn't extend to discrete distributions).", "One can think of the derivation as proceeding in 3 steps:  1.", "Deriving an estimator for gradients a sample from a 1-dimensional density $f(x)$ that is such that $f(x)$ is differentiable and its cumulative distribution function (CDF) $F(x)$ is tractable:    $\\frac{\\partial \\hat{x}}{\\partial \\theta} = - \\frac{1}{f(\\hat{x})}\\int_{t=-\\infty}^{\\hat{x}} \\frac{\\partial f(t)}{\\partial \\theta} dt$    where $\\hat{x}$ is a sample from density $f(x)$ and $\\theta$ is any parameter of $f(x)$ (the above is a simplified version of Equation 6).", "This is probably the most important result of the paper, and is based on a really clever use of the general form of the Leibniz integral rule.", "2.", "Noticing that one can sample from a $D$-dimensional Gaussian mixture by decomposing it with the product rule $f({\\bf x}) = \\prod_{d=1}^D f(x_d|{\\bf x}_{<d})$ and using ancestral sampling, where each $f(x_d|{\\bf x}_{<d})$ are themselves 1-dimensional mixtures (i.e. with differentiable densities and tractable CDFs)  3.", "Using the 1-dimensional gradient estimator (of Equation 6) and the chain rule to backpropagate through the ancestral sampling procedure.", "This requires computing the integral in the expression for $\\frac{\\partial \\hat{x}}{\\partial \\theta}$ above, where $f(x)$ is one of the 1D conditional Gaussian mixtures and $\\theta$ is a mixing proportion parameter $\\pi_j$.", "As it turns out, this integral has an analytical form (see Equation 22).", "**My two cents**  This is a really surprising and neat result.", "The author mentions it could be applicable to variational autoencoders (to support posteriors that are mixtures of Gaussians), and I'm really looking forward to read about whether that can be successfully done in practice.", "The paper provides the derivation only for mixtures of Gaussians with diagonal covariance matrices.", "It is mentioned that extending to non-diagonal covariances is doable.", "That said, ancestral sampling with non-diagonal covariances would become more computationally expensive, since the conditionals under each Gaussian involves a matrix inverse.", "Beyond the case of Gaussian mixtures, Equation 6 is super interesting in itself as its application could go beyond that case.", "This is probably why the paper also derived a sampling-based estimator for Equation 6, in Equation 9.", "However, that estimator might be inefficient, since it involves sampling from Equation 10 with rejection, and it might take a lot of time to get an accepted sample if $\\hat{x}$ is very small.", "Also, a good estimate of Equation 6 might require *multiple* samples from Equation 10.", "Finally, while I couldn't find any obvious problem with the mathematical derivation, I'd be curious to see whether using the same approach to derive a gradient on one of the Gaussian mean or standard deviation parameters gave a gradient that is consistent with what the reparameterization trick provides."], "summary_text": "This paper derives an algorithm for passing gradients through a sample from a mixture of Gaussians. While the reparameterization trick allows to get the gradients with respect to the Gaussian means and covariances, the same trick cannot be invoked for the mixing proportions parameters (essentially because they are the parameters of a multinomial discrete distribution over the Gaussian components, and the reparameterization trick doesn't extend to discrete distributions). One can think of the derivation as proceeding in 3 steps:  1. Deriving an estimator for gradients a sample from a 1-dimensional density $f(x)$ that is such that $f(x)$ is differentiable and its cumulative distribution function (CDF) $F(x)$ is tractable:    $\\frac{\\partial \\hat{x}}{\\partial \\theta} = - \\frac{1}{f(\\hat{x})}\\int_{t=-\\infty}^{\\hat{x}} \\frac{\\partial f(t)}{\\partial \\theta} dt$    where $\\hat{x}$ is a sample from density $f(x)$ and $\\theta$ is any parameter of $f(x)$ (the above is a simplified version of Equation 6). This is probably the most important result of the paper, and is based on a really clever use of the general form of the Leibniz integral rule. 2. Noticing that one can sample from a $D$-dimensional Gaussian mixture by decomposing it with the product rule $f({\\bf x}) = \\prod_{d=1}^D f(x_d|{\\bf x}_{<d})$ and using ancestral sampling, where each $f(x_d|{\\bf x}_{<d})$ are themselves 1-dimensional mixtures (i.e. with differentiable densities and tractable CDFs)  3. Using the 1-dimensional gradient estimator (of Equation 6) and the chain rule to backpropagate through the ancestral sampling procedure. This requires computing the integral in the expression for $\\frac{\\partial \\hat{x}}{\\partial \\theta}$ above, where $f(x)$ is one of the 1D conditional Gaussian mixtures and $\\theta$ is a mixing proportion parameter $\\pi_j$. As it turns out, this integral has an analytical form (see Equation 22). **My two cents**  This is a really surprising and neat result. The author mentions it could be applicable to variational autoencoders (to support posteriors that are mixtures of Gaussians), and I'm really looking forward to read about whether that can be successfully done in practice. The paper provides the derivation only for mixtures of Gaussians with diagonal covariance matrices. It is mentioned that extending to non-diagonal covariances is doable. That said, ancestral sampling with non-diagonal covariances would become more computationally expensive, since the conditionals under each Gaussian involves a matrix inverse. Beyond the case of Gaussian mixtures, Equation 6 is super interesting in itself as its application could go beyond that case. This is probably why the paper also derived a sampling-based estimator for Equation 6, in Equation 9. However, that estimator might be inefficient, since it involves sampling from Equation 10 with rejection, and it might take a lot of time to get an accepted sample if $\\hat{x}$ is very small. Also, a good estimate of Equation 6 might require *multiple* samples from Equation 10. Finally, while I couldn't find any obvious problem with the mathematical derivation, I'd be curious to see whether using the same approach to derive a gradient on one of the Gaussian mean or standard deviation parameters gave a gradient that is consistent with what the reparameterization trick provides.", "pdf_url": "http://arxiv.org/pdf/1607.05690v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/1607.05690.json"}
{"id": "27051635", "bin": "600_700", "summary_sentences": ["Distributed consensus and the implications of NVM on database management systems Fournier, Arulraj, & Pavlo ACM Queue Vol 14, issue 3  As you may recall, Peter Bailis and ACM Queue have started a “Research for Practice” series introducing “expert curated guides to the best of CS research.” Aka, reading lists for The Morning Paper!", "I previously covered the papers from the first edition (blog entries dated June 14th-21st, 2016).", "Today we’re turning our attention to the the second edition:  I am thrilled to introduce our second instalment of Research for Practice, which provides highlights from two critical areas in storage and large-scale services: distributed consensus and non-volatile memory.", "Distributed consensus  The first topic area is Distributed Consensus, with papers selected by Camille Fournier.", "“The three papers included in this selection address the real world of consensus system: Why are they needed?", "Why are they difficult to understand?", "What happens when you try to implement.", "Them?", "Is there an easier way, something that more developers can understand and therefore implement?”  Fournier’s three choices are:  Paxos made live – an engineering perspective  The Chubby lock service for loosely coupled distributed systems  In search of an understandable consensus algorithm  All of which will be familiar to regular readers of The Morning Paper ;) (Links above are to my write-ups).", "If you want more of this kind of thing, I did a two-week mini-series on consensus back in March of last year.", "Here are three additional picks of my own:  Viewstamped Replication Revisited  Raft Refloated  And yesterday’s paper, Flexible Paxos  Implications of NVM on database management systems  Joy Arulrja and Andrew Pavlo introduce a selection of three papers looking at the implications of NVM for database management systems:  The advent of non-volatile memory (NVM) will fundamentally change the dichotomy between memory and durable storage in a database management systems (DBMS).", "This is a topic area that really caught my attention earlier this year, and I wrote a short post entitled “ All change please ” summarizing some of the hardware advances hitting our data centers, including NVM.", "On the subject of NVM itself and its implications, the papers I’ve covered so far can be found by searching on the blog for the keyword ‘NVM’ .", "The first of Arulja and Pavlo’s picks is  From ARIES to MARS  Which looks at the classic ARIES recovery protocol and how it can be optimized for NVM.", "Their second and third paper choices are ones that I haven’t covered before.", "So we’ll be looking at those papers in the next two days.", "The links below will go live as each days’ post goes up.", "Let’s talk about storage and recovery methods for non-volatile memory database systems – a wonderful tour of common DBMS storage engine designs, and how they can be adapted to NVM.", "How did I miss this paper first time around???", "It’s a real gem.", "Write-limited sorts and joins for persistent memory , which looks at the implications of read/write cost imbalance and limited write endurance in NVM.", "As Arulja and Pavlo say,  The common theme for these papers is that you cannot just run an existing DBMS on NVM and expect it to leverage its unique set of properties.", "The only way to achieve that is to come up with novel architectures, protocols, and algorithms that are tailor-made for NVM.", "The third edition of Reseach for Practice must be due out soon – I’m very much looking forward to seeing where it goes next!"], "summary_text": "Distributed consensus and the implications of NVM on database management systems Fournier, Arulraj, & Pavlo ACM Queue Vol 14, issue 3  As you may recall, Peter Bailis and ACM Queue have started a “Research for Practice” series introducing “expert curated guides to the best of CS research.” Aka, reading lists for The Morning Paper! I previously covered the papers from the first edition (blog entries dated June 14th-21st, 2016). Today we’re turning our attention to the the second edition:  I am thrilled to introduce our second instalment of Research for Practice, which provides highlights from two critical areas in storage and large-scale services: distributed consensus and non-volatile memory. Distributed consensus  The first topic area is Distributed Consensus, with papers selected by Camille Fournier. “The three papers included in this selection address the real world of consensus system: Why are they needed? Why are they difficult to understand? What happens when you try to implement. Them? Is there an easier way, something that more developers can understand and therefore implement?”  Fournier’s three choices are:  Paxos made live – an engineering perspective  The Chubby lock service for loosely coupled distributed systems  In search of an understandable consensus algorithm  All of which will be familiar to regular readers of The Morning Paper ;) (Links above are to my write-ups). If you want more of this kind of thing, I did a two-week mini-series on consensus back in March of last year. Here are three additional picks of my own:  Viewstamped Replication Revisited  Raft Refloated  And yesterday’s paper, Flexible Paxos  Implications of NVM on database management systems  Joy Arulrja and Andrew Pavlo introduce a selection of three papers looking at the implications of NVM for database management systems:  The advent of non-volatile memory (NVM) will fundamentally change the dichotomy between memory and durable storage in a database management systems (DBMS). This is a topic area that really caught my attention earlier this year, and I wrote a short post entitled “ All change please ” summarizing some of the hardware advances hitting our data centers, including NVM. On the subject of NVM itself and its implications, the papers I’ve covered so far can be found by searching on the blog for the keyword ‘NVM’ . The first of Arulja and Pavlo’s picks is  From ARIES to MARS  Which looks at the classic ARIES recovery protocol and how it can be optimized for NVM. Their second and third paper choices are ones that I haven’t covered before. So we’ll be looking at those papers in the next two days. The links below will go live as each days’ post goes up. Let’s talk about storage and recovery methods for non-volatile memory database systems – a wonderful tour of common DBMS storage engine designs, and how they can be adapted to NVM. How did I miss this paper first time around??? It’s a real gem. Write-limited sorts and joins for persistent memory , which looks at the implications of read/write cost imbalance and limited write endurance in NVM. As Arulja and Pavlo say,  The common theme for these papers is that you cannot just run an existing DBMS on NVM and expect it to leverage its unique set of properties. The only way to achieve that is to come up with novel architectures, protocols, and algorithms that are tailor-made for NVM. The third edition of Reseach for Practice must be due out soon – I’m very much looking forward to seeing where it goes next!", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2956641.2967618?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/distributed-consensus-and-the-implications-of-nvm-on-database-management-systems.json"}
{"id": "57228237", "bin": "600_700", "summary_sentences": ["Towards usable checksums: automating the integrity verification of web downloads for the masses Cherubini et al., CCS’18  If you tackled Monday’s paper on BEAT you deserve something a little easier to digest today, and ‘Towards usable checksums’ fits the bill nicely!", "There’s some great data-driven product management going on here as the authors set out to quantify current attitudes and behaviours regarding downloading files from the Internet, design a solution to improve security and ease-of-use, and then test their solution to gather feedback and prepare for a more widely deployed beta version.", "When I was growing up we were all taught “Don’t talk to strangers”, and “Never get in a stranger’s car”.", "As has been well noted by others, so much for that advice!", "Perhaps the modern equivalent is “Don’t download unknown files from the Internet!” This paper specifically looks at applications made directly available from developer websites (vs downloads made through app stores).", "A popular and convenient way to download programs is to use official app stores such as Apple’s Mac App Store and Microsoft’s Windows Store.", "Such platforms, however, have several drawbacks for developers, including long review and validation times, technical restrictions (e.g., sandboxing), incompatibility with software licenses, and substantial commissions.", "Therefore, it is quite common that developers make their programs available directly from their websites.", "This is the case of popular programs such as VLC media player, OpenOffice, and GIMP.", "If you’re reading The Morning Paper, you probably know what a checksum is for and how to use it to verify the integrity of a download.", "You’re probably also well aware of the importance of doing so.", "Even so, I wouldn’t be surprised if on at least one occasion it’s been too awkward / you’ve been in too much of a hurry to get some other task done you’re focused on / you rated the risk as low enough, and so on, that you failed to do so.", "I’ve seen up close the apparent struggles of very bright professional people without IT backgrounds to manage basics such as passwords.", "I hold out little hope of them navigating checksums— from “What’s a command-line?” on up— even though I know they’re more than capable of understanding if only it seemed sufficiently important to them.", "Yet checksums are an important line of defence to protect against adversaries tampering files to inject malware etc..  A popular way for developers to enable users to detect accidental or intentional modifications of their program files hosted on external platforms, such as mirrors and CDNs, is to provide so-called checksums on their websites.", "This practice is quite common in the open-source community but also for companies such as Google…  For a restricted subset of downloadable assets -chiefly JavaScript and style sheets, included via script and link tags, integrated checksum support is available via the Subresource Integrity (SRI) specification introduced by the W3C in 2016.", "It’s supported by all major browsers (including Edge, but not IE).", "If you’re not already using it for externally hosted assets you include in your site then you really should look into it.", "Here’s an example of the integrity attribute in action, taken from the MDN site:  <script src=\" [url]"], "summary_text": "Towards usable checksums: automating the integrity verification of web downloads for the masses Cherubini et al., CCS’18  If you tackled Monday’s paper on BEAT you deserve something a little easier to digest today, and ‘Towards usable checksums’ fits the bill nicely! There’s some great data-driven product management going on here as the authors set out to quantify current attitudes and behaviours regarding downloading files from the Internet, design a solution to improve security and ease-of-use, and then test their solution to gather feedback and prepare for a more widely deployed beta version. When I was growing up we were all taught “Don’t talk to strangers”, and “Never get in a stranger’s car”. As has been well noted by others, so much for that advice! Perhaps the modern equivalent is “Don’t download unknown files from the Internet!” This paper specifically looks at applications made directly available from developer websites (vs downloads made through app stores). A popular and convenient way to download programs is to use official app stores such as Apple’s Mac App Store and Microsoft’s Windows Store. Such platforms, however, have several drawbacks for developers, including long review and validation times, technical restrictions (e.g., sandboxing), incompatibility with software licenses, and substantial commissions. Therefore, it is quite common that developers make their programs available directly from their websites. This is the case of popular programs such as VLC media player, OpenOffice, and GIMP. If you’re reading The Morning Paper, you probably know what a checksum is for and how to use it to verify the integrity of a download. You’re probably also well aware of the importance of doing so. Even so, I wouldn’t be surprised if on at least one occasion it’s been too awkward / you’ve been in too much of a hurry to get some other task done you’re focused on / you rated the risk as low enough, and so on, that you failed to do so. I’ve seen up close the apparent struggles of very bright professional people without IT backgrounds to manage basics such as passwords. I hold out little hope of them navigating checksums— from “What’s a command-line?” on up— even though I know they’re more than capable of understanding if only it seemed sufficiently important to them. Yet checksums are an important line of defence to protect against adversaries tampering files to inject malware etc..  A popular way for developers to enable users to detect accidental or intentional modifications of their program files hosted on external platforms, such as mirrors and CDNs, is to provide so-called checksums on their websites. This practice is quite common in the open-source community but also for companies such as Google…  For a restricted subset of downloadable assets -chiefly JavaScript and style sheets, included via script and link tags, integrated checksum support is available via the Subresource Integrity (SRI) specification introduced by the W3C in 2016. It’s supported by all major browsers (including Edge, but not IE). If you’re not already using it for externally hosted assets you include in your site then you really should look into it. Here’s an example of the integrity attribute in action, taken from the MDN site:  <script src=\" [url]", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3243734.3243746?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/towards-usable-checksums-automating-the-integrity-verification-of-web-downloads-for-the-masses.json"}
{"id": "89581601", "bin": "600_700", "summary_sentences": ["BatchNorm  Batch Renormalization is a follow-up to the 2015 paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "The original motivation for BatchNorm came from the fact that the distribution of the inputs to each layer in a deep network changes throughout training as the parameters change.", "Since this slows down training, the authors reasoned that normalization of these distributions should allow for the use of higher learning rates and increased insensitivity to initializations.", "BatchNorm achieves the same accuracy as previous networks but with signifcantly fewer training steps.", "BatchNorm is an added “layer” to deep networks placed after the output of the layer transformation (e.g., the convolution operation) but before the nonlinearity (e.g., a ReLu layer).", "During training, the sample estimates for the mean and variance of the layer’s outputs are generated from mini-batch statistics.", "A moving average estimate is maintained as well, and is used during inference.", "Some key points about BatchNorm:  Normalization cannot be interleaved with gradient descent optimization.", "This is because the gradient descent optimization wouldn’t be taking into account the fact that normalization takes place.", "BatchNorm requires backpropagation to compute derivatives for the normalization w.r.t.", "minibatch statistics; otherwise, model parameters explode without the loss decreasing.", "Full whitening of each layer’s inputs is costly and not everywhere differentiable, so each scalar feature is whitened independently  To prevent BatchNorm from changing what the network can represent, parameters are introduced that allow the BatchNorm layer to represent the identity transform.", "These parameters are also optimized with SGD  BatchRenorm  BatchNorm came with pros and cons.", "It is less effective when the training minibatches are small or do not consist of independent samples.", "Small minibatches mean that the sample estimates of the mean and variance during training are less accurate.", "These inaccuracies are compounded with depth.", "For non-iid minibatches at train-time, BatchNorm will tend to overfit to the specific distribution of the examples; BatchRenorm aims to break up the dependence between similar samples.", "Therefore, the goal of BatchRenorm is to provide the model with the capacity to overcome differences in activitations between training and inference.", "Essentially  parameters $r$ and $d$ are introduced that relate the output of the normalizations between train time (computed with minibatch statistics) and inference (computed with population statistics for entire training set).", "If $\\mu$ is an estimate of the mean of some particular node $x$, and $\\sigma$ is an estimate of its standard deviation perhaps computed as a moving average over the last several minibatches, we have  $r$ and $d$ are held constant during backprop  This transform is identity in expectation, and BatchNorm is $r = 1$, $d = 0$.", "This allows the layers to observe the “correct” activiations that would be seen during inference.", "In practice, you start with BatchNorm for a few thousand training steps, then switch to BatchRenorm.", "Questions for discussion  What is the cost of BatchRenorm?", "Slightly more model complexity because of the added $r$ and $d$, and slightly more complex backprop equations?", "Hyper-parameter search needed for $r_max$ and $d_max$?", "Doesn’t seem to always be necessary to use BatchRenorm - in real world problems, however, training data probably won’t be “nice” and iid  BatchNorm for Recurrent Networks ?", "This was used in Deep Reinforcement Learning to stabilize policy gradient methods!"], "summary_text": "BatchNorm  Batch Renormalization is a follow-up to the 2015 paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift . The original motivation for BatchNorm came from the fact that the distribution of the inputs to each layer in a deep network changes throughout training as the parameters change. Since this slows down training, the authors reasoned that normalization of these distributions should allow for the use of higher learning rates and increased insensitivity to initializations. BatchNorm achieves the same accuracy as previous networks but with signifcantly fewer training steps. BatchNorm is an added “layer” to deep networks placed after the output of the layer transformation (e.g., the convolution operation) but before the nonlinearity (e.g., a ReLu layer). During training, the sample estimates for the mean and variance of the layer’s outputs are generated from mini-batch statistics. A moving average estimate is maintained as well, and is used during inference. Some key points about BatchNorm:  Normalization cannot be interleaved with gradient descent optimization. This is because the gradient descent optimization wouldn’t be taking into account the fact that normalization takes place. BatchNorm requires backpropagation to compute derivatives for the normalization w.r.t. minibatch statistics; otherwise, model parameters explode without the loss decreasing. Full whitening of each layer’s inputs is costly and not everywhere differentiable, so each scalar feature is whitened independently  To prevent BatchNorm from changing what the network can represent, parameters are introduced that allow the BatchNorm layer to represent the identity transform. These parameters are also optimized with SGD  BatchRenorm  BatchNorm came with pros and cons. It is less effective when the training minibatches are small or do not consist of independent samples. Small minibatches mean that the sample estimates of the mean and variance during training are less accurate. These inaccuracies are compounded with depth. For non-iid minibatches at train-time, BatchNorm will tend to overfit to the specific distribution of the examples; BatchRenorm aims to break up the dependence between similar samples. Therefore, the goal of BatchRenorm is to provide the model with the capacity to overcome differences in activitations between training and inference. Essentially  parameters $r$ and $d$ are introduced that relate the output of the normalizations between train time (computed with minibatch statistics) and inference (computed with population statistics for entire training set). If $\\mu$ is an estimate of the mean of some particular node $x$, and $\\sigma$ is an estimate of its standard deviation perhaps computed as a moving average over the last several minibatches, we have  $r$ and $d$ are held constant during backprop  This transform is identity in expectation, and BatchNorm is $r = 1$, $d = 0$. This allows the layers to observe the “correct” activiations that would be seen during inference. In practice, you start with BatchNorm for a few thousand training steps, then switch to BatchRenorm. Questions for discussion  What is the cost of BatchRenorm? Slightly more model complexity because of the added $r$ and $d$, and slightly more complex backprop equations? Hyper-parameter search needed for $r_max$ and $d_max$? Doesn’t seem to always be necessary to use BatchRenorm - in real world problems, however, training data probably won’t be “nice” and iid  BatchNorm for Recurrent Networks ? This was used in Deep Reinforcement Learning to stabilize policy gradient methods!", "pdf_url": "https://arxiv.org/pdf/1702.03275", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/batch-renorm.json"}
{"id": "51458146", "bin": "600_700", "summary_sentences": ["Improving Cloud Service Resilience using Brownout-aware Load Balancing – Klein et al 2014  This is what the previous two #themorningpaper selections have been building to.", "What happens when you apply brownout techniques to a set of load-balanced servers?", "We study how to extend the classical cloud service architecture composed of a load-balancer and replicas with a recently proposed self-adaptive paradigm called brownout.", "The following diagram (click to see larger view) tells a compelling story.", "Taking a load-balanced system with 5 servers (replicas) numbered 0-4, one replica is killed every 100 seconds until only 1 remains, then the replicas are brought back online one-at-a-time, also at 100 second intervals.", "The request rate is held constant throughout.", "The top plot shows what happens without brownout, and the bottom chart shows what happens with it.", "Note that without brownout controls in place, a large number of requests start timing out after the third replica has failed.", "These timeouts continue even when replicas are restored.", "This is attributed to ‘rotten’ requests that have timed out from the user perspective but are still active on the server-side – thus e.g. the database is wasting all its time on transactions that either will time out, or have already timed-out on the user side.", "The bottom plot shows the dramatic difference made to this particular system by applying brownout controls.", "The red line shows the automatically controlled setting of the dimmer switch (altering the ratio of responses that include optional content to those that don’t).", "The blue line again shows the number of requests that time out: a dramatic improvement in user experience and system resiliency.", "It would be great to see a further study that also includes circuit-breakers.", "The basic brownout technique as applied to an individual server/replica we covered in the previous edition of #themorningpaper.", "If you are using a load-balancer that depends in some way on response times, the adaptive behaviour arising from the brownout dimmer switch may confuse it.", "The issue is to ensure that replica self-adaptivity would not confuse the load-balancing algorithm, overloading replicas that are already struggling with capacity shortage.", "Recall that for a brownout-compliant service, the mandatory part of the response is always computed, but the optional part is computed only with a certain probability governed by a control variable called the dimmer.", "Klein et.", "al.", "piggy-back the current dimmer-switch value from a replica onto the normal response flow so that it can be used in load-balancing decisions.", "Compared to the near-optimal (for non-adaptive workloads) Join the Shortest Queue (JSQ) algorithm,  results show that the resulting service can tolerate more replica failures and that the novel load-balancing algorithms improve the number of requests served with optional content by up to 5%.", "From the charts earlier therefore, a lot of the gains come from simply having brownout controlled replicas.", "The load-balancing enhancements are then the icing on the cake.", "Two algorithms are explored: the first is a variant of a PI controller which adjusts queue-offsets (for a base JSQ algorithm) based on dimmer switch settings; the second strives to keep the dimmer value the same across all replicas.", "In summary, adding brownout to a replicated service improves its resilience, even when using a brownout-unaware load-balancing algorithm… However, we observed that in scenarios featuring capacity heterogeneity, our algorithms performed better than shortest queue first (JSQ) with respect to the optional content ratio."], "summary_text": "Improving Cloud Service Resilience using Brownout-aware Load Balancing – Klein et al 2014  This is what the previous two #themorningpaper selections have been building to. What happens when you apply brownout techniques to a set of load-balanced servers? We study how to extend the classical cloud service architecture composed of a load-balancer and replicas with a recently proposed self-adaptive paradigm called brownout. The following diagram (click to see larger view) tells a compelling story. Taking a load-balanced system with 5 servers (replicas) numbered 0-4, one replica is killed every 100 seconds until only 1 remains, then the replicas are brought back online one-at-a-time, also at 100 second intervals. The request rate is held constant throughout. The top plot shows what happens without brownout, and the bottom chart shows what happens with it. Note that without brownout controls in place, a large number of requests start timing out after the third replica has failed. These timeouts continue even when replicas are restored. This is attributed to ‘rotten’ requests that have timed out from the user perspective but are still active on the server-side – thus e.g. the database is wasting all its time on transactions that either will time out, or have already timed-out on the user side. The bottom plot shows the dramatic difference made to this particular system by applying brownout controls. The red line shows the automatically controlled setting of the dimmer switch (altering the ratio of responses that include optional content to those that don’t). The blue line again shows the number of requests that time out: a dramatic improvement in user experience and system resiliency. It would be great to see a further study that also includes circuit-breakers. The basic brownout technique as applied to an individual server/replica we covered in the previous edition of #themorningpaper. If you are using a load-balancer that depends in some way on response times, the adaptive behaviour arising from the brownout dimmer switch may confuse it. The issue is to ensure that replica self-adaptivity would not confuse the load-balancing algorithm, overloading replicas that are already struggling with capacity shortage. Recall that for a brownout-compliant service, the mandatory part of the response is always computed, but the optional part is computed only with a certain probability governed by a control variable called the dimmer. Klein et. al. piggy-back the current dimmer-switch value from a replica onto the normal response flow so that it can be used in load-balancing decisions. Compared to the near-optimal (for non-adaptive workloads) Join the Shortest Queue (JSQ) algorithm,  results show that the resulting service can tolerate more replica failures and that the novel load-balancing algorithms improve the number of requests served with optional content by up to 5%. From the charts earlier therefore, a lot of the gains come from simply having brownout controlled replicas. The load-balancing enhancements are then the icing on the cake. Two algorithms are explored: the first is a variant of a PI controller which adjusts queue-offsets (for a base JSQ algorithm) based on dimmer switch settings; the second strives to keep the dimmer value the same across all replicas. In summary, adding brownout to a replicated service improves its resilience, even when using a brownout-unaware load-balancing algorithm… However, we observed that in scenarios featuring capacity heterogeneity, our algorithms performed better than shortest queue first (JSQ) with respect to the optional content ratio.", "pdf_url": "https://people.cs.umu.se/hernandf/pubs/srds2014-preprint.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/improving-cloud-service-resilience-using-brownout-aware-load-balancing.json"}
{"id": "91929013", "bin": "600_700", "summary_sentences": ["The paper presents NeuroSAT, a message passing neural network that is trained to predict if a given SAT can be solved.", "As a side effect of training, the model also learns how to solve the SAT problem itself without any extra supervision.", "Background  Given an expression in the propositional logic, the task is to predict if there exists a substitution of variables that make the expression true.", "The expression itself can be written as a conjunction of disjunctions (“and” over “or”) where each conjunct is called a clause and each variable within a clause is called a literal.", "Invariants  The variables or clauses or literals (within the clauses) can be permuted.", "Every occurrence of a variable can be negated.", "Model  Given the SAT problem,  create an undirected graph of literals, their negations and the clauses they belong to.", "Put an edge between every literal and the clause to which it belongs and another kind of edge between every literal and its negation.", "Perform message passing between nodes to obtain vector representations corresponding to each node.", "Specifically, first, each clause received a message from its neighbours (literals) and updates its embeddings.", "Then every literal receives a message from its neighbours (both literals and clauses) and updates its embeddings.", "After T iterations, the nodes vote to decide the prediction of the model as a whole.", "The model is trained end-to-end using the cross-entropy loss between logit and the true label.", "Permutation invariance is ensured by operating on the nodes and the edges in the topological order and negation invariance is ensured by treating all literals as the same.", "Decoding Satisfying Assignment  The most interesting aspect of this work is that even though the model was trained to predict if the SAT problem can be satisfied, it is actually possible to extract the correct assignment from the classifier.", "In the early iterations, all the nodes vote “unsolvable” with low confidence.", "Then a few nodes start voting “solvable” and then a phase transition happens where most of the nodes start voting “solvable” with high confidence.", "The model never becomes highly confident that problem is “unsolvable” and almost never guesses “solvable” on an “unsolvable” problem.", "So in some sense, the model is looking for the combination of literals that actually solves the problem.", "The authors found that the 2 dimensional PCA projections of the literal embeddings are initially mixed up but become more and more linearly separable as the phase transition happens.", "Based on this insight, the authors propose to obtain cluster centres C1 and C2, partition the variables according to the cluster centres and then try assignments from both the partitions.", "This alone provides a satisfying solution in over 70% of the cases when though there is no explicit supervising signal about how to solve the problem.", "The other strengths of the paper includes  Generalizing to longer and more difficult SAT problems (than those seen during training).", "Generalizing to another kind of search problems like graph colouring, clique detection etc (over small random graphs).", "The paper also reports that by adding supervising signal about which clauses in the given expression are unsatisfiable, it is possible to decode the literals which prove the “unsatisfiability” of an expression at test time.", "Though not a lot of details have been provided about this part and would probably be covered in the next iteration of the paper."], "summary_text": "The paper presents NeuroSAT, a message passing neural network that is trained to predict if a given SAT can be solved. As a side effect of training, the model also learns how to solve the SAT problem itself without any extra supervision. Background  Given an expression in the propositional logic, the task is to predict if there exists a substitution of variables that make the expression true. The expression itself can be written as a conjunction of disjunctions (“and” over “or”) where each conjunct is called a clause and each variable within a clause is called a literal. Invariants  The variables or clauses or literals (within the clauses) can be permuted. Every occurrence of a variable can be negated. Model  Given the SAT problem,  create an undirected graph of literals, their negations and the clauses they belong to. Put an edge between every literal and the clause to which it belongs and another kind of edge between every literal and its negation. Perform message passing between nodes to obtain vector representations corresponding to each node. Specifically, first, each clause received a message from its neighbours (literals) and updates its embeddings. Then every literal receives a message from its neighbours (both literals and clauses) and updates its embeddings. After T iterations, the nodes vote to decide the prediction of the model as a whole. The model is trained end-to-end using the cross-entropy loss between logit and the true label. Permutation invariance is ensured by operating on the nodes and the edges in the topological order and negation invariance is ensured by treating all literals as the same. Decoding Satisfying Assignment  The most interesting aspect of this work is that even though the model was trained to predict if the SAT problem can be satisfied, it is actually possible to extract the correct assignment from the classifier. In the early iterations, all the nodes vote “unsolvable” with low confidence. Then a few nodes start voting “solvable” and then a phase transition happens where most of the nodes start voting “solvable” with high confidence. The model never becomes highly confident that problem is “unsolvable” and almost never guesses “solvable” on an “unsolvable” problem. So in some sense, the model is looking for the combination of literals that actually solves the problem. The authors found that the 2 dimensional PCA projections of the literal embeddings are initially mixed up but become more and more linearly separable as the phase transition happens. Based on this insight, the authors propose to obtain cluster centres C1 and C2, partition the variables according to the cluster centres and then try assignments from both the partitions. This alone provides a satisfying solution in over 70% of the cases when though there is no explicit supervising signal about how to solve the problem. The other strengths of the paper includes  Generalizing to longer and more difficult SAT problems (than those seen during training). Generalizing to another kind of search problems like graph colouring, clique detection etc (over small random graphs). The paper also reports that by adding supervising signal about which clauses in the given expression are unsatisfiable, it is possible to decode the literals which prove the “unsatisfiability” of an expression at test time. Though not a lot of details have been provided about this part and would probably be covered in the next iteration of the paper.", "pdf_url": "https://arxiv.org/pdf/1802.03685", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/learning-a-sat-solver-from-single-bit-supervision.json"}
{"id": "42443464", "bin": "600_700", "summary_sentences": ["The paper proposes a two-stage synthesis network that can perform transfer learning for the task of machine comprehension.", "The problem is the following:  We have a domain DS for which we have labelled dataset of question-answer pairs and another domain DT for which we do not have any labelled dataset.", "We use the data for domain DS to train SynNet and use that to generate synthetic question-answer pairs for domain DT.", "Now we can train a machine comprehension model M on DS and finetune using the synthetic data for DT.", "SynNet  Works in two stages:  Answer Synthesis - Given a text paragraph, generate an answer.", "Question Synthesis - Given a text paragraph and an answer, generate a question.", "Answer Synthesis Network  Given the labelled dataset for DS, generate a labelled dataset of <word, tag> pair such that each word in the given paragraph is assigned one of the 4 tags:  IOBstart - if it is the starting word of an answer  IOBmid - if it is the intermediate word of an answer  IOBend - if it is the ending word of an answer  IOBnone - if it is not part of any answer  For training, map the words to their GloVe embeddings and pass through a Bi-LSTM.", "Next, pass them through two-FC layers followed by a softmax layer.", "For the target domain DT, all the consecutive word spans where no label is IOBnone are returned as candidate answers.", "Question Synthesis Network  Given an input paragraph and a candidate answer, Question Synthesis network generates question one word at a time.", "Map each word in the paragraph to their GloVe embedding.", "After the word vector, append a ‘1’ if the word was part of the candidate answer else append a ‘0’.", "Feed to a Bi-LSTM network (encoder-decoder) where the decoder conditions on the representation generated by the encoder as well as the question tokens generated so far.", "Decoding is stopped when “END” token is produced.", "The paragraph may contain some named entities or rare words which do not appear in the softmax vocabulary.", "To account for such words, a copying mechanism is also incorporated.", "At each time step, a Pointer Network (CP) and a Vocabulary Predictor (VP) are used to generate probability distribution for the next word and a Latent Predictor Network is used to decide which of the two networks would be used for the prediction.", "At inference time, a greedy decoding is used where the most likely predictor is chosen and then the most likely word from that predictor is chosen.", "Machine Comprehension Model  Given any MC model, first train it over domain DS and then fine-tune using the artificial questions generated using DT.", "Implementation Details  Data Regularization - There is a need to alternate between mini batches from source and target domain while fine-tuning the MC model.", "At inference time, the fine-tuned MC model is used to get the distribution P(i=start) and P(i=end) (corresponding to the likelihood of choosing word I as the starting or ending word for the answer) for all the words and DP is used to find the optimal answer span.", "Checkpoint Averaging - Use the different checkpointed models to average the answer likelihood before running DP.", "Using the synthetically generated dataset helps to gain a 2% improvement in terms of F-score (from SQuAD -> NewsQA).", "Using checkpointed models further improves the performance to overall 46.6% F score which closes the gap with respect to the performance of model trained on NewsQA itself (~52.3% F score)"], "summary_text": "The paper proposes a two-stage synthesis network that can perform transfer learning for the task of machine comprehension. The problem is the following:  We have a domain DS for which we have labelled dataset of question-answer pairs and another domain DT for which we do not have any labelled dataset. We use the data for domain DS to train SynNet and use that to generate synthetic question-answer pairs for domain DT. Now we can train a machine comprehension model M on DS and finetune using the synthetic data for DT. SynNet  Works in two stages:  Answer Synthesis - Given a text paragraph, generate an answer. Question Synthesis - Given a text paragraph and an answer, generate a question. Answer Synthesis Network  Given the labelled dataset for DS, generate a labelled dataset of <word, tag> pair such that each word in the given paragraph is assigned one of the 4 tags:  IOBstart - if it is the starting word of an answer  IOBmid - if it is the intermediate word of an answer  IOBend - if it is the ending word of an answer  IOBnone - if it is not part of any answer  For training, map the words to their GloVe embeddings and pass through a Bi-LSTM. Next, pass them through two-FC layers followed by a softmax layer. For the target domain DT, all the consecutive word spans where no label is IOBnone are returned as candidate answers. Question Synthesis Network  Given an input paragraph and a candidate answer, Question Synthesis network generates question one word at a time. Map each word in the paragraph to their GloVe embedding. After the word vector, append a ‘1’ if the word was part of the candidate answer else append a ‘0’. Feed to a Bi-LSTM network (encoder-decoder) where the decoder conditions on the representation generated by the encoder as well as the question tokens generated so far. Decoding is stopped when “END” token is produced. The paragraph may contain some named entities or rare words which do not appear in the softmax vocabulary. To account for such words, a copying mechanism is also incorporated. At each time step, a Pointer Network (CP) and a Vocabulary Predictor (VP) are used to generate probability distribution for the next word and a Latent Predictor Network is used to decide which of the two networks would be used for the prediction. At inference time, a greedy decoding is used where the most likely predictor is chosen and then the most likely word from that predictor is chosen. Machine Comprehension Model  Given any MC model, first train it over domain DS and then fine-tune using the artificial questions generated using DT. Implementation Details  Data Regularization - There is a need to alternate between mini batches from source and target domain while fine-tuning the MC model. At inference time, the fine-tuned MC model is used to get the distribution P(i=start) and P(i=end) (corresponding to the likelihood of choosing word I as the starting or ending word for the answer) for all the words and DP is used to find the optimal answer span. Checkpoint Averaging - Use the different checkpointed models to average the answer likelihood before running DP. Using the synthetically generated dataset helps to gain a 2% improvement in terms of F-score (from SQuAD -> NewsQA). Using checkpointed models further improves the performance to overall 46.6% F score which closes the gap with respect to the performance of model trained on NewsQA itself (~52.3% F score)", "pdf_url": "https://arxiv.org/pdf/1706.09789.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/two-stage-synthesis-networks-for-transfer-learning-in-machine-comprehension.json"}
{"id": "7157784", "bin": "600_700", "summary_sentences": ["This paper describes a learning algorithm for deep neural networks that can be understood as an extension of stacked denoising autoencoders.", "In short, instead of reconstructing one layer at a time and greedily stacking, a unique unsupervised objective involving the reconstruction of all layers is optimized jointly by all parameters (with the relative importance of each layer cost controlled by hyper-parameters).", "In more details:  * The encoding (forward propagation) adds noise (Gaussian) at all layers, while decoding is noise-free.", "* The target at each layer is the result of noise-less forward propagation.", "* Direct connections (also known as skip-connections) between a layer and its decoded reconstruction are used.", "The resulting encoder/decoder architecture thus ressembles a ladder (hence the name Ladder Networks).", "* Miniature neural networks with a single hidden unit and skip-connections are used to decode the left and top layers into a reconstruction.", "Each network is applied element-wise (without parameter sharing across reconstructed units).", "* The unsupervised objective is combined with a supervised objective, corresponding to the regular negative class log-likelihood objective (using an output softmax layer).", "Two losses are used for each input/target pair: one based on the noise-free forward propagation (which also provides the target of the denoising objective) and one with the noise added (which also corresponds to the encoding stage of the unsupervised autoencoder objective).", "Batch normalization is used to train the network.", "Since the model combines unsupervised and supervised learning, it can be used for semi-supervised learning, where unlabeled examples can be used to update the network using the unsupervised objective only.", "State of the art results in the semi-supervised setting are presented, for both the MNIST and CIFAR-10 datasets.", "#### My two cents  What I find most exciting about this paper is its performance.", "On MNIST, with only 100 labeled examples, it achieves 1.13% error!", "That is essentially the performance of stacked denoising autoencoders, trained on the entire training set (though that was before ReLUs and batch normalization, which this paper uses)!", "This confirms a current line of thought in Deep Learning (DL) that, while recent progress in DL applied on large labeled datasets does not rely on any unsupervised learning (unlike at the \"beginning\" of DL in the mid 2000s), unsupervised learning might instead be crucial for success in low-labeled data regime, in the semi-supervised setting.", "Unfortunately, there is one little issue in the experiments, disclosed by the authors: while they used few labeled examples for training, model selection did use all 10k labels in the validation set.", "This is of course unrealistic.", "But model selection in the low data regime is arguably, in itself, an open problem.", "So I like to think that this doesn't invalidate the progress made in this paper, and only suggests that some research needs to be done on doing effective hyper-parameter search with a small validation set.", "Generally, I really hope this paper will stimulate more research on DL methods to the specific case of small labeled dataset / large unlabeled dataset.", "While this isn't a problem that is as \"flashy\" as tasks such as the ImageNet Challenge which comes with lots of labeled data, I think this is a crucial research direction for AI in general.", "Indeed, it seems naive to me to expect that we will be able to collect large labeled dataset for each and every task, on our way to real AI."], "summary_text": "This paper describes a learning algorithm for deep neural networks that can be understood as an extension of stacked denoising autoencoders. In short, instead of reconstructing one layer at a time and greedily stacking, a unique unsupervised objective involving the reconstruction of all layers is optimized jointly by all parameters (with the relative importance of each layer cost controlled by hyper-parameters). In more details:  * The encoding (forward propagation) adds noise (Gaussian) at all layers, while decoding is noise-free. * The target at each layer is the result of noise-less forward propagation. * Direct connections (also known as skip-connections) between a layer and its decoded reconstruction are used. The resulting encoder/decoder architecture thus ressembles a ladder (hence the name Ladder Networks). * Miniature neural networks with a single hidden unit and skip-connections are used to decode the left and top layers into a reconstruction. Each network is applied element-wise (without parameter sharing across reconstructed units). * The unsupervised objective is combined with a supervised objective, corresponding to the regular negative class log-likelihood objective (using an output softmax layer). Two losses are used for each input/target pair: one based on the noise-free forward propagation (which also provides the target of the denoising objective) and one with the noise added (which also corresponds to the encoding stage of the unsupervised autoencoder objective). Batch normalization is used to train the network. Since the model combines unsupervised and supervised learning, it can be used for semi-supervised learning, where unlabeled examples can be used to update the network using the unsupervised objective only. State of the art results in the semi-supervised setting are presented, for both the MNIST and CIFAR-10 datasets. #### My two cents  What I find most exciting about this paper is its performance. On MNIST, with only 100 labeled examples, it achieves 1.13% error! That is essentially the performance of stacked denoising autoencoders, trained on the entire training set (though that was before ReLUs and batch normalization, which this paper uses)! This confirms a current line of thought in Deep Learning (DL) that, while recent progress in DL applied on large labeled datasets does not rely on any unsupervised learning (unlike at the \"beginning\" of DL in the mid 2000s), unsupervised learning might instead be crucial for success in low-labeled data regime, in the semi-supervised setting. Unfortunately, there is one little issue in the experiments, disclosed by the authors: while they used few labeled examples for training, model selection did use all 10k labels in the validation set. This is of course unrealistic. But model selection in the low data regime is arguably, in itself, an open problem. So I like to think that this doesn't invalidate the progress made in this paper, and only suggests that some research needs to be done on doing effective hyper-parameter search with a small validation set. Generally, I really hope this paper will stimulate more research on DL methods to the specific case of small labeled dataset / large unlabeled dataset. While this isn't a problem that is as \"flashy\" as tasks such as the ImageNet Challenge which comes with lots of labeled data, I think this is a crucial research direction for AI in general. Indeed, it seems naive to me to expect that we will be able to collect large labeled dataset for each and every task, on our way to real AI.", "pdf_url": "http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/rasmusbhvr15.json"}
{"id": "55410278", "bin": "600_700", "summary_sentences": ["What  They describe a model that automatically steers cars on roads.", "The model runs in realtime.", "The model is trained from images and corresponding (correct) steering wheel angles.", "How  Architecture  Their model is just a standard CNN with a few convolutional and fully connected layers.", "They have a single output: the correct steering wheel angle predicted by the model.", "(The model does not predict whether to accelerate or brake.)", "To be more precise: The steering wheel angle is predicted as 1/r, where r is the turning radius of the car.", "This way, the prediction is more independent of the used car.", "They use 1/r instead of r in order to avoid getting infinity when driving straight.", "Their input images are in YUV color space.", "They use a hard coded (not learned) normalization layer (no further explanations in paper).", "Visualization:  Data collection  They drive on roads with cars, collecting photos and corresponding steering wheel angles.", "They drive on different road settings (e.g. highway, tunnels, residential roads) and weather (sunny, cloudy, foggy, ...).", "They collected about 72 hours of driving.", "They annotate each example with the road center location, road type and lane switching information (\"stays in lane\"/\"is switching lanes\").", "Data augmentation  Training on just the collected images is not going to work, because they only show correct driving.", "Sooner or later the model would make a mistake and end up going slightly off road.", "This would then be a situation that it has never seen before and the predicted output becomes more or less random, leading to crashes.", "They attached two more cameras to the cars during data collection, one pointing to the left and one to the right.", "They use these images as examples of bad situations and change the ground truth steering wheel angle so that it would steer the car back onto the road within 2 seconds.", "They also generate additional images between the center and left/right cameras.", "They do this by viewpoint transformation and assume that all point above the horizon line are infinitely far away, while all below the horizon line are on flat grounds.", "(No further explanation here on how they transform images and annotations exactly.", "And where do they get the horizon line from?)", "They also seem to apply random rotations and translations to the images (or maybe that is meant by viewpoint transformations?).", "Training  They only train on examples where the driver stays in a lane (as opposed to switching lanes).", "They subsample the input at 10fps (collection was at 30fps) in order to not get too similar examples.", "Training and application happens in Torch7.", "The system can predict at 30fps.", "Results  Simulation  They simulate on the collected data how the model would steer.", "I.e. they predict steering wheel angles and then transform future images as if that angle had been chosen.", "They measure how far the simulated car would be away from the road center.", "They measure an virtual, human intervention (of 6 seconds) whenever the car gets too far away from the center.", "They can then estimate the simulated time that the car would have been autonomous.", "The actual number though is nowhere in the paper.", "x(  They drive in Monmouth County, being autonomous roughly 98% of the time.", "They drive 16.09km on Garden State Parkway, being autonomous 100% of the time."], "summary_text": "What  They describe a model that automatically steers cars on roads. The model runs in realtime. The model is trained from images and corresponding (correct) steering wheel angles. How  Architecture  Their model is just a standard CNN with a few convolutional and fully connected layers. They have a single output: the correct steering wheel angle predicted by the model. (The model does not predict whether to accelerate or brake.) To be more precise: The steering wheel angle is predicted as 1/r, where r is the turning radius of the car. This way, the prediction is more independent of the used car. They use 1/r instead of r in order to avoid getting infinity when driving straight. Their input images are in YUV color space. They use a hard coded (not learned) normalization layer (no further explanations in paper). Visualization:  Data collection  They drive on roads with cars, collecting photos and corresponding steering wheel angles. They drive on different road settings (e.g. highway, tunnels, residential roads) and weather (sunny, cloudy, foggy, ...). They collected about 72 hours of driving. They annotate each example with the road center location, road type and lane switching information (\"stays in lane\"/\"is switching lanes\"). Data augmentation  Training on just the collected images is not going to work, because they only show correct driving. Sooner or later the model would make a mistake and end up going slightly off road. This would then be a situation that it has never seen before and the predicted output becomes more or less random, leading to crashes. They attached two more cameras to the cars during data collection, one pointing to the left and one to the right. They use these images as examples of bad situations and change the ground truth steering wheel angle so that it would steer the car back onto the road within 2 seconds. They also generate additional images between the center and left/right cameras. They do this by viewpoint transformation and assume that all point above the horizon line are infinitely far away, while all below the horizon line are on flat grounds. (No further explanation here on how they transform images and annotations exactly. And where do they get the horizon line from?) They also seem to apply random rotations and translations to the images (or maybe that is meant by viewpoint transformations?). Training  They only train on examples where the driver stays in a lane (as opposed to switching lanes). They subsample the input at 10fps (collection was at 30fps) in order to not get too similar examples. Training and application happens in Torch7. The system can predict at 30fps. Results  Simulation  They simulate on the collected data how the model would steer. I.e. they predict steering wheel angles and then transform future images as if that angle had been chosen. They measure how far the simulated car would be away from the road center. They measure an virtual, human intervention (of 6 seconds) whenever the car gets too far away from the center. They can then estimate the simulated time that the car would have been autonomous. The actual number though is nowhere in the paper. x(  They drive in Monmouth County, being autonomous roughly 98% of the time. They drive 16.09km on Garden State Parkway, being autonomous 100% of the time.", "pdf_url": "https://arxiv.org/pdf/1604.07316", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/end_to_end_learning_for_self-driving_cars.json"}
{"id": "19050347", "bin": "600_700", "summary_sentences": ["What  They suggest a method to predict for images the semantic segmentation maps, instance segmentation maps and depth maps.", "Semantic segmentation = \"assign color X to all pixels showing a car, color Y to all pixels showing people, ...\"  Instance segmentation = \"assign color X to all pixels showing car number 1, color Y to all pixels showing car number 2, ...\"  Their method is optimized to run fast and with low memory demands.", "The method is aimed at self-driving cars.", "How  Architecture  They base their model on ENet.", "ENet is a network for fast segmentation.", "It uses three blocks of (several) residual convolutions for downscaling, followed by two blocks of upscaling.", "Some convolutions are dilated.", "Number of filters does not exceed 128.", "They share two blocks of downscaling between all three branches (semantic segmentation / instance segmentation / depth maps).", "Each branch gets one unique downscaling module and two upscaling modules.", "Losses  Semantic Segmentation: Pixelwise cross-entropy.", "Instance Segmentation:  They do not directly predict some kind of instance flag per pixel.", "Instead they generate per pixel an embedding vector.", "These are supposed to be similar for pixels belonging to the same instance (and dissimilar for different instances).", "They can then cluster these vectors to find all pixels belonging to an instance (they use GPU based mean shift clustering for that).", "In order to train the network to generate such embeddings, they view each instance as a class and design the losses so that the intra-class variance (distances) is minimized, while the intra-class distances are maximized.", "So they get a variance term/loss and a distance term/loss.", "Both of these are hinged (so e.g. once the variance goes below a certain threshold, the corresponding loss just becomes zero).", "L_var = intraclass variance loss, L_dist = interclass variance loss, L_reg = regularization loss  ||.|| = L2 distance, [.", "]_+ = max(0, x) = hinge  C = class, N_c = pixels in class, mu_c = mean embedding, x_i = pixel embedding, delta_v = variance hinge, delta_d = interclass distance hinge  Depth Estimation:  Common loss here (according to the authors) would be the L2 distance with a scale invariance term and local structure similarity term.", "But they don't use these as it performs worse than their loss.", "Instead they use a reverse Huber loss, which seems to have some similarity with a combination of L1 and L2 loss:  Other  Input image size is 1024x512.", "They use Adam with learning rate 5e-4 and batch size 10.", "They keep the batch norm parameters fixed (no explanation why).", "Results  For semantic segmentation on Cityscapes they reach the same score as ENet (59.3 class IoU, 80.4 category IoU).", "For instance segmentation on Cityscapes they reach 21.0 AP as opposed to 46.9 AP of Mask R-CNN.", "For depth map prediction on Cityscapes their result differ by distance of objects.", "The difference is lower for close objects (MAE of 1.5m for objects <25m away) and higher for the ones further apart (MAE of 7.5m for objects <100m away).", "Visualization of predicted depth vs real depth:  Example results:  They reach 21fps (that should be around 3x or so faster than Mask R-CNN) and 1.2GB memory footprint.", "Training all branches jointly in one network (as described above) vs. training them completely separately (fully disjoint networks) improves accuracy by a bit."], "summary_text": "What  They suggest a method to predict for images the semantic segmentation maps, instance segmentation maps and depth maps. Semantic segmentation = \"assign color X to all pixels showing a car, color Y to all pixels showing people, ...\"  Instance segmentation = \"assign color X to all pixels showing car number 1, color Y to all pixels showing car number 2, ...\"  Their method is optimized to run fast and with low memory demands. The method is aimed at self-driving cars. How  Architecture  They base their model on ENet. ENet is a network for fast segmentation. It uses three blocks of (several) residual convolutions for downscaling, followed by two blocks of upscaling. Some convolutions are dilated. Number of filters does not exceed 128. They share two blocks of downscaling between all three branches (semantic segmentation / instance segmentation / depth maps). Each branch gets one unique downscaling module and two upscaling modules. Losses  Semantic Segmentation: Pixelwise cross-entropy. Instance Segmentation:  They do not directly predict some kind of instance flag per pixel. Instead they generate per pixel an embedding vector. These are supposed to be similar for pixels belonging to the same instance (and dissimilar for different instances). They can then cluster these vectors to find all pixels belonging to an instance (they use GPU based mean shift clustering for that). In order to train the network to generate such embeddings, they view each instance as a class and design the losses so that the intra-class variance (distances) is minimized, while the intra-class distances are maximized. So they get a variance term/loss and a distance term/loss. Both of these are hinged (so e.g. once the variance goes below a certain threshold, the corresponding loss just becomes zero). L_var = intraclass variance loss, L_dist = interclass variance loss, L_reg = regularization loss  ||.|| = L2 distance, [. ]_+ = max(0, x) = hinge  C = class, N_c = pixels in class, mu_c = mean embedding, x_i = pixel embedding, delta_v = variance hinge, delta_d = interclass distance hinge  Depth Estimation:  Common loss here (according to the authors) would be the L2 distance with a scale invariance term and local structure similarity term. But they don't use these as it performs worse than their loss. Instead they use a reverse Huber loss, which seems to have some similarity with a combination of L1 and L2 loss:  Other  Input image size is 1024x512. They use Adam with learning rate 5e-4 and batch size 10. They keep the batch norm parameters fixed (no explanation why). Results  For semantic segmentation on Cityscapes they reach the same score as ENet (59.3 class IoU, 80.4 category IoU). For instance segmentation on Cityscapes they reach 21.0 AP as opposed to 46.9 AP of Mask R-CNN. For depth map prediction on Cityscapes their result differ by distance of objects. The difference is lower for close objects (MAE of 1.5m for objects <25m away) and higher for the ones further apart (MAE of 7.5m for objects <100m away). Visualization of predicted depth vs real depth:  Example results:  They reach 21fps (that should be around 3x or so faster than Mask R-CNN) and 1.2GB memory footprint. Training all branches jointly in one network (as described above) vs. training them completely separately (fully disjoint networks) improves accuracy by a bit.", "pdf_url": "https://arxiv.org/pdf/1708.02550", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/fast_scene_understanding_for_autonomous_driving.json"}
{"id": "45435119", "bin": "600_700", "summary_sentences": ["Overview  Inception v4 is like Inception v3, but  Slimmed down, i.e. some parts were simplified  One new version with residual connections (Inception-ResNet-v2), one without (Inception-v4)  They didn't observe an improved error rate when using residual connections.", "They did however oberserve that using residual connections decreased their training times.", "They had to scale down the results of their residual modules (multiply them by a constant ~0.1).", "Otherwise their networks would die (only produce 0s).", "Results on ILSVRC 2012 (val set, 144 crops/image):  Top-1 Error:  Inception-v4: 17.7%  Inception-ResNet-v2: 17.8%  Top-5 Error (ILSVRC 2012 val set, 144 crops/image):  Inception-v4: 3.8%  Inception-ResNet-v2: 3.7%  Architecture  Basic structure of Inception-ResNet-v2 (layers, dimensions):  Image -> Stem -> 5x Module A -> Reduction-A -> 10x Module B -> Reduction B -> 5x Module C -> AveragePooling -> Droput 20% -> Linear, Softmax  299x299x3 -> 35x35x256 -> 35x35x256 -> 17x17x896 -> 17x17x896 -> 8x8x1792 -> 8x8x1792 -> 1792 -> 1792 -> 1000  Modules A, B, C are very similar.", "They contain 2 (B, C) or 3 (A) branches.", "Each branch starts with a 1x1 convolution on the input.", "All branches merge into one 1x1 convolution (which is then added to the original input, as usually in residual architectures).", "Module A uses 3x3 convolutions, B 7x1 and 1x7, C 3x1 and 1x3.", "The reduction modules also contain multiple branches.", "One has max pooling (3x3 stride 2), the other branches end in convolutions with stride 2.", "From top to bottom: Module A, Module B, Module C, Reduction Module A.", "Top 5 eror by epoch, models with (red, solid, bottom) and without (green, dashed) residual connections.", "Rough chapter-wise notes  Introduction, Related Work  Inception v3 was adapted to run on DistBelief.", "Inception v4 is designed for TensorFlow, which gets rid of some constraints and allows a simplified architecture.", "Authors don't think that residual connections are inherently needed to train deep nets, but they do speed up the training.", "History:  Inception v1 - Introduced inception blocks  Inception v2 - Added Batch Normalization  Inception v3 - Factorized the inception blocks further (more submodules)  Inception v4 - Adds residual connections  Architectural Choices  Previous architectures were constrained due to memory problems.", "TensorFlow got rid of that problem.", "Previous architectures were carefully/conservatively extended.", "Architectures ended up being quite complicated.", "This version slims down everything.", "They had problems with residual networks dieing when they contained more than 1000 filters (per inception module apparently?).", "They could fix that by multiplying the results of the residual subnetwork (before the element-wise addition) with a constant factor of ~0.1.", "Training methodology  Kepler GPUs, TensorFlow, RMSProb (SGD+Momentum apprently performed worse)  Experimental Results  Their residual version of Inception v4 (\"Inception-ResNet-v2\") seemed to learn faster than the non-residual version.", "They both peaked out at almost the same value.", "Top-1 Error (ILSVRC 2012 val set, 144 crops/image):  Inception-v4: 17.7%  Inception-ResNet-v2: 17.8%  Top-5 Error (ILSVRC 2012 val set, 144 crops/image):  Inception-v4: 3.8%  Inception-ResNet-v2: 3.7%"], "summary_text": "Overview  Inception v4 is like Inception v3, but  Slimmed down, i.e. some parts were simplified  One new version with residual connections (Inception-ResNet-v2), one without (Inception-v4)  They didn't observe an improved error rate when using residual connections. They did however oberserve that using residual connections decreased their training times. They had to scale down the results of their residual modules (multiply them by a constant ~0.1). Otherwise their networks would die (only produce 0s). Results on ILSVRC 2012 (val set, 144 crops/image):  Top-1 Error:  Inception-v4: 17.7%  Inception-ResNet-v2: 17.8%  Top-5 Error (ILSVRC 2012 val set, 144 crops/image):  Inception-v4: 3.8%  Inception-ResNet-v2: 3.7%  Architecture  Basic structure of Inception-ResNet-v2 (layers, dimensions):  Image -> Stem -> 5x Module A -> Reduction-A -> 10x Module B -> Reduction B -> 5x Module C -> AveragePooling -> Droput 20% -> Linear, Softmax  299x299x3 -> 35x35x256 -> 35x35x256 -> 17x17x896 -> 17x17x896 -> 8x8x1792 -> 8x8x1792 -> 1792 -> 1792 -> 1000  Modules A, B, C are very similar. They contain 2 (B, C) or 3 (A) branches. Each branch starts with a 1x1 convolution on the input. All branches merge into one 1x1 convolution (which is then added to the original input, as usually in residual architectures). Module A uses 3x3 convolutions, B 7x1 and 1x7, C 3x1 and 1x3. The reduction modules also contain multiple branches. One has max pooling (3x3 stride 2), the other branches end in convolutions with stride 2. From top to bottom: Module A, Module B, Module C, Reduction Module A. Top 5 eror by epoch, models with (red, solid, bottom) and without (green, dashed) residual connections. Rough chapter-wise notes  Introduction, Related Work  Inception v3 was adapted to run on DistBelief. Inception v4 is designed for TensorFlow, which gets rid of some constraints and allows a simplified architecture. Authors don't think that residual connections are inherently needed to train deep nets, but they do speed up the training. History:  Inception v1 - Introduced inception blocks  Inception v2 - Added Batch Normalization  Inception v3 - Factorized the inception blocks further (more submodules)  Inception v4 - Adds residual connections  Architectural Choices  Previous architectures were constrained due to memory problems. TensorFlow got rid of that problem. Previous architectures were carefully/conservatively extended. Architectures ended up being quite complicated. This version slims down everything. They had problems with residual networks dieing when they contained more than 1000 filters (per inception module apparently?). They could fix that by multiplying the results of the residual subnetwork (before the element-wise addition) with a constant factor of ~0.1. Training methodology  Kepler GPUs, TensorFlow, RMSProb (SGD+Momentum apprently performed worse)  Experimental Results  Their residual version of Inception v4 (\"Inception-ResNet-v2\") seemed to learn faster than the non-residual version. They both peaked out at almost the same value. Top-1 Error (ILSVRC 2012 val set, 144 crops/image):  Inception-v4: 17.7%  Inception-ResNet-v2: 17.8%  Top-5 Error (ILSVRC 2012 val set, 144 crops/image):  Inception-v4: 3.8%  Inception-ResNet-v2: 3.7%", "pdf_url": "http://arxiv.org/pdf/1602.07261v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/inception_v4.json"}
{"id": "75952682", "bin": "600_700", "summary_sentences": ["What  They suggest a variation of ELUs, which leads to networks being automatically normalized.", "The effects are comparable to Batch Normalization, while requiring significantly less computation (barely more than a normal ReLU).", "How  They define Self-Normalizing Neural Networks (SNNs) as neural networks, which automatically keep their activations at zero-mean and unit-variance (per neuron).", "SELUs  They use SELUs to turn their networks into SNNs.", "Formula:  with alpha = 1.6733 and lambda = 1.0507.", "They proof that with properly normalized weights the activations approach a fixed point of zero-mean and unit-variance.", "(Different settings for alpha and lambda can lead to other fixed points.)", "They proof that this is still the case when previous layer activations and weights do not have optimal values.", "They proof that this is still the case when the variance of previous layer activations is very high or very low and argue that the mean of those activations is not so important.", "Hence, SELUs with these hyperparameters should have self-normalizing properties.", "SELUs are here used as a basis because:  They can have negative and positive values, which allows to control the mean.", "They have saturating regions, which allows to dampen high variances from previous layers.", "They have a slope larger than one, which allows to increase low variances from previous layers.", "They generate a continuous curve, which ensures that there is a fixed point between variance damping and increasing.", "ReLUs, Leaky ReLUs, Sigmoids and Tanhs do not offer the above properties.", "Initialization  SELUs for SNNs work best with normalized weights.", "They suggest to make sure per layer that:  The first moment (sum of weights) is zero.", "The second moment (sum of squared weights) is one.", "This can be done by drawing weights from a normal distribution N(0, 1/n), where n is the number of neurons in the layer.", "Alpha-dropout  SELUs don't perform as well with normal Dropout, because their point of low variance is not 0.", "They suggest a modification of Dropout called Alpha-dropout.", "In this technique, values are not dropped to 0 but to alpha' = -lambda * alpha = -1.0507 * 1.6733 = -1.7581.", "Similar to dropout, activations are changed during training to compensate for the dropped units.", "Each activation x is changed to a(xd+alpha'(1-d))+b.", "d = B(1, q) is the dropout variable consisting of 1s and 0s.", "a = (q + alpha'^2 q(1-q))^(-1/2)  b = -(q + alpha'^2 q(1-q))^(-1/2) ((1-q)alpha')  They made good experiences with dropout rates around 0.05 to 0.1.", "Results  Note: All of their tests are with fully connected networks.", "No convolutions.", "Example training results:  Left: MNIST, Right: CIFAR10  Networks have N layers each, see legend.", "No convolutions.", "121 UCI Tasks  They manage to beat SVMs and RandomForests, while other networks (Layer Normalization, BN, Weight Normalization, Highway Networks, ResNet) perform significantly worse than their network (and usually don't beat SVMs/RFs).", "Tox21  They achieve better results than other networks (again, Layer Normalization, BN, etc.).", "They achive almost the same result as the so far best model on the dataset, which consists of a mixture of neural networks, SVMs and Random Forests.", "HTRU2  They achieve better results than other networks.", "They beat the best non-neural method (Naive Bayes).", "Among all tested other networks, MSRAinit performs best, which references a network withput any normalization, only ReLUs and Microsoft Weight Initialization (see paper: Delving deep into rectifiers: Surpassing human-level performance on imagenet classification)."], "summary_text": "What  They suggest a variation of ELUs, which leads to networks being automatically normalized. The effects are comparable to Batch Normalization, while requiring significantly less computation (barely more than a normal ReLU). How  They define Self-Normalizing Neural Networks (SNNs) as neural networks, which automatically keep their activations at zero-mean and unit-variance (per neuron). SELUs  They use SELUs to turn their networks into SNNs. Formula:  with alpha = 1.6733 and lambda = 1.0507. They proof that with properly normalized weights the activations approach a fixed point of zero-mean and unit-variance. (Different settings for alpha and lambda can lead to other fixed points.) They proof that this is still the case when previous layer activations and weights do not have optimal values. They proof that this is still the case when the variance of previous layer activations is very high or very low and argue that the mean of those activations is not so important. Hence, SELUs with these hyperparameters should have self-normalizing properties. SELUs are here used as a basis because:  They can have negative and positive values, which allows to control the mean. They have saturating regions, which allows to dampen high variances from previous layers. They have a slope larger than one, which allows to increase low variances from previous layers. They generate a continuous curve, which ensures that there is a fixed point between variance damping and increasing. ReLUs, Leaky ReLUs, Sigmoids and Tanhs do not offer the above properties. Initialization  SELUs for SNNs work best with normalized weights. They suggest to make sure per layer that:  The first moment (sum of weights) is zero. The second moment (sum of squared weights) is one. This can be done by drawing weights from a normal distribution N(0, 1/n), where n is the number of neurons in the layer. Alpha-dropout  SELUs don't perform as well with normal Dropout, because their point of low variance is not 0. They suggest a modification of Dropout called Alpha-dropout. In this technique, values are not dropped to 0 but to alpha' = -lambda * alpha = -1.0507 * 1.6733 = -1.7581. Similar to dropout, activations are changed during training to compensate for the dropped units. Each activation x is changed to a(xd+alpha'(1-d))+b. d = B(1, q) is the dropout variable consisting of 1s and 0s. a = (q + alpha'^2 q(1-q))^(-1/2)  b = -(q + alpha'^2 q(1-q))^(-1/2) ((1-q)alpha')  They made good experiences with dropout rates around 0.05 to 0.1. Results  Note: All of their tests are with fully connected networks. No convolutions. Example training results:  Left: MNIST, Right: CIFAR10  Networks have N layers each, see legend. No convolutions. 121 UCI Tasks  They manage to beat SVMs and RandomForests, while other networks (Layer Normalization, BN, Weight Normalization, Highway Networks, ResNet) perform significantly worse than their network (and usually don't beat SVMs/RFs). Tox21  They achieve better results than other networks (again, Layer Normalization, BN, etc.). They achive almost the same result as the so far best model on the dataset, which consists of a mixture of neural networks, SVMs and Random Forests. HTRU2  They achieve better results than other networks. They beat the best non-neural method (Naive Bayes). Among all tested other networks, MSRAinit performs best, which references a network withput any normalization, only ReLUs and Microsoft Weight Initialization (see paper: Delving deep into rectifiers: Surpassing human-level performance on imagenet classification).", "pdf_url": "https://arxiv.org/pdf/1706.02515", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/self-normalizing_neural_networks.json"}
{"id": "50538257", "bin": "600_700", "summary_sentences": ["What  They suggest a new architecture for GANs.", "Their architecture adds another Generator for a reverse branch (from images to noise vector z).", "Their architecture takes some ideas from VAEs/variational neural nets.", "Overall they can improve on the previous state of the art (DCGAN).", "How  Architecture  Usually, in GANs one feeds a noise vector z into a Generator (G), which then generates an image (x) from that noise.", "They add a reverse branch (G2), in which another Generator takes a real image (x) and generates a noise vector z from that.", "The noise vector can now be viewed as a latent space vector.", "Instead of letting G2 generate discrete values for z (as it is usually done), they instead take the approach commonly used VAEs and use continuous variables instead.", "That is, if z represents N latent variables, they let G2 generate N means and N variances of gaussian distributions, with each distribution representing one value of z.", "So the model could e.g. represent something along the lines of \"this face looks a lot like a female, but with very low probability could also be male\".", "Training  The Discriminator (D) is now trained on pairs of either (real image, generated latent space vector) or (generated image, randomly sampled latent space vector) and has to tell them apart from each other.", "Both Generators are trained to maximally confuse D.  G1 (from z to x) confuses D maximally, if it generates new images that (a) look real and (b) fit well to the latent variables in z (e.g. if z says \"image contains a cat\", then the image should contain a cat).", "G2 (from x to z) confuses D maximally, if it generates good latent variables z that fit to the image x.", "Continuous variables  The variables in z follow gaussian distributions, which makes the training more complicated, as you can't trivially backpropagate through gaussians.", "When training G1 (from z to x) the situation is easy: You draw a random z-vector following a gaussian distribution (N(0, I)).", "(This is basically the same as in \"normal\" GANs.", "They just often use uniform distributions instead.)", "When training G2 (from x to z) the situation is a bit harder.", "Here we need to use the reparameterization trick here.", "That roughly means, that G2 predicts the means and variances of the gaussian variables in z and then we draw a sample of z according to exactly these means and variances.", "That sample gives us discrete values for our backpropagation.", "If we do that sampling often enough, we get a good approximation of the true gradient (of the continuous variables).", "(Monte Carlo approximation.)", "Results  Images generated based on Celeb-A dataset:  Left column per pair: Real image, right column per pair: reconstruction (x -> z via G2, then z -> x via G1)  Reconstructions of SVHN, notice how the digits often stay the same, while the font changes:  CIFAR-10 samples, still lots of errors, but some quite correct:"], "summary_text": "What  They suggest a new architecture for GANs. Their architecture adds another Generator for a reverse branch (from images to noise vector z). Their architecture takes some ideas from VAEs/variational neural nets. Overall they can improve on the previous state of the art (DCGAN). How  Architecture  Usually, in GANs one feeds a noise vector z into a Generator (G), which then generates an image (x) from that noise. They add a reverse branch (G2), in which another Generator takes a real image (x) and generates a noise vector z from that. The noise vector can now be viewed as a latent space vector. Instead of letting G2 generate discrete values for z (as it is usually done), they instead take the approach commonly used VAEs and use continuous variables instead. That is, if z represents N latent variables, they let G2 generate N means and N variances of gaussian distributions, with each distribution representing one value of z. So the model could e.g. represent something along the lines of \"this face looks a lot like a female, but with very low probability could also be male\". Training  The Discriminator (D) is now trained on pairs of either (real image, generated latent space vector) or (generated image, randomly sampled latent space vector) and has to tell them apart from each other. Both Generators are trained to maximally confuse D.  G1 (from z to x) confuses D maximally, if it generates new images that (a) look real and (b) fit well to the latent variables in z (e.g. if z says \"image contains a cat\", then the image should contain a cat). G2 (from x to z) confuses D maximally, if it generates good latent variables z that fit to the image x. Continuous variables  The variables in z follow gaussian distributions, which makes the training more complicated, as you can't trivially backpropagate through gaussians. When training G1 (from z to x) the situation is easy: You draw a random z-vector following a gaussian distribution (N(0, I)). (This is basically the same as in \"normal\" GANs. They just often use uniform distributions instead.) When training G2 (from x to z) the situation is a bit harder. Here we need to use the reparameterization trick here. That roughly means, that G2 predicts the means and variances of the gaussian variables in z and then we draw a sample of z according to exactly these means and variances. That sample gives us discrete values for our backpropagation. If we do that sampling often enough, we get a good approximation of the true gradient (of the continuous variables). (Monte Carlo approximation.) Results  Images generated based on Celeb-A dataset:  Left column per pair: Real image, right column per pair: reconstruction (x -> z via G2, then z -> x via G1)  Reconstructions of SVHN, notice how the digits often stay the same, while the font changes:  CIFAR-10 samples, still lots of errors, but some quite correct:", "pdf_url": "http://arxiv.org/pdf/1606.00704", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/adversarially_learned_inference.json"}
{"id": "59320034", "bin": "600_700", "summary_sentences": ["Contributions  A new (and more realistic) evaluation protocol for lifelong learning where each data point is observed just once and a disjoint set of tasks are used for training and validation.", "A new metric that focuses on the efficiency of the models - in terms of sample complexity and computational (and memory) costs.", "Modification of Gradient Episodic Memory ie GEM which reduces the computational overhead of GEM without compromising on the results.", "Empirical validation that using task descriptors help lifelong learning models and improve their few-shot learning capabilities.", "Link to the code  Learning Protocol  Two group of datasets - one for training and evaluation (DEV) and other for cross validation (DCV).", "Data can be sampled multiple times for cross-validation dataset but only once from the training dataset.", "Each group of dataset (say DEV or DCV) is a list of task-specific datasets Dk (k is the task index).", "Each sample in Dk is of the form (x, t, y) where x is the data, t is the task descriptor and y is the output.", "Dk contains Bk minibatches of data.", "Metrics  Accuracy  ak,i,j = accuracy on test task j after training on ith minibatch of training task k.  Ak = mean over all j = 1 to k (ak, Bk, j) ie train the model on data for task k and then test it on all the tasks.", "Forgetting Measure  fjk = forgetting on task j after training on all minibatches upto task k.  fjk = max over all l = 1 to k-1 (al, Blj - ak, Bkj)  Forgetting = Fk = mean over all j = 1 to k-1 (fjk)  LCA - Learning Curve Area  Zb = average b shot performance where b is the minibatch number.", "Zb = mean over all k = 0 to T (ak, b, k)  LCAβ = mean over all b = 0 to β (Zb)  One special case is LCA0 which is the forward transfer performance or performance on the unseen task.", "In experiments, β is kept small as we want the model to learn from few examples.", "Model  GEM has been shown to be very effective in single epoch setting but introduces a very high computational overhead.", "Average GEM (AGEM) reduces this overhead by sampling (and using) only some examples from the episodic memory instead of using all the examples.", "While GEM provides better guarantees in terms of worst-case forgetting, AGEM provides better guarantees in terms of average accuracy.", "Joint Embedding Model Using Compositional Task Descriptors  Compositional Task Descriptors are used to speed training on the subsequent tasks.", "A matrix specifying the attribute value of objects (to be recognized in the task) are used.", "A joint-embedding space between image features and attribute embeddings is learned.", "Experiments  Datasets  Permuted MNIST  Split CIFAR  Split CUB  Split AWA  Setup  Integer task descriptors for MNIST and CIFAR and class attributes as descriptors for CUB and AWA  Baselines include GEM , iCaRL , Elastic Weight Consolidation , Progressive Neural Networks etc.", "Results  AGEM outperforms other models on all the datasets expect MNIST where the Progressive Neural Networks lead.", "One reason could be that MNIST has a large number of training examples per task.", "But Progressive Neural Networks lead to bad utilization of capacity.", "While AGEM and GEM have similar performance, GEM has a much higher computational and memory overhead.", "Use of task descriptors improves the accuracy for all the models.", "It seems that AGEM offers a good tradeoff between average accuracy performance and efficiency - in terms of sample efficiency, memory requirements and computational costs."], "summary_text": "Contributions  A new (and more realistic) evaluation protocol for lifelong learning where each data point is observed just once and a disjoint set of tasks are used for training and validation. A new metric that focuses on the efficiency of the models - in terms of sample complexity and computational (and memory) costs. Modification of Gradient Episodic Memory ie GEM which reduces the computational overhead of GEM without compromising on the results. Empirical validation that using task descriptors help lifelong learning models and improve their few-shot learning capabilities. Link to the code  Learning Protocol  Two group of datasets - one for training and evaluation (DEV) and other for cross validation (DCV). Data can be sampled multiple times for cross-validation dataset but only once from the training dataset. Each group of dataset (say DEV or DCV) is a list of task-specific datasets Dk (k is the task index). Each sample in Dk is of the form (x, t, y) where x is the data, t is the task descriptor and y is the output. Dk contains Bk minibatches of data. Metrics  Accuracy  ak,i,j = accuracy on test task j after training on ith minibatch of training task k.  Ak = mean over all j = 1 to k (ak, Bk, j) ie train the model on data for task k and then test it on all the tasks. Forgetting Measure  fjk = forgetting on task j after training on all minibatches upto task k.  fjk = max over all l = 1 to k-1 (al, Blj - ak, Bkj)  Forgetting = Fk = mean over all j = 1 to k-1 (fjk)  LCA - Learning Curve Area  Zb = average b shot performance where b is the minibatch number. Zb = mean over all k = 0 to T (ak, b, k)  LCAβ = mean over all b = 0 to β (Zb)  One special case is LCA0 which is the forward transfer performance or performance on the unseen task. In experiments, β is kept small as we want the model to learn from few examples. Model  GEM has been shown to be very effective in single epoch setting but introduces a very high computational overhead. Average GEM (AGEM) reduces this overhead by sampling (and using) only some examples from the episodic memory instead of using all the examples. While GEM provides better guarantees in terms of worst-case forgetting, AGEM provides better guarantees in terms of average accuracy. Joint Embedding Model Using Compositional Task Descriptors  Compositional Task Descriptors are used to speed training on the subsequent tasks. A matrix specifying the attribute value of objects (to be recognized in the task) are used. A joint-embedding space between image features and attribute embeddings is learned. Experiments  Datasets  Permuted MNIST  Split CIFAR  Split CUB  Split AWA  Setup  Integer task descriptors for MNIST and CIFAR and class attributes as descriptors for CUB and AWA  Baselines include GEM , iCaRL , Elastic Weight Consolidation , Progressive Neural Networks etc. Results  AGEM outperforms other models on all the datasets expect MNIST where the Progressive Neural Networks lead. One reason could be that MNIST has a large number of training examples per task. But Progressive Neural Networks lead to bad utilization of capacity. While AGEM and GEM have similar performance, GEM has a much higher computational and memory overhead. Use of task descriptors improves the accuracy for all the models. It seems that AGEM offers a good tradeoff between average accuracy performance and efficiency - in terms of sample efficiency, memory requirements and computational costs.", "pdf_url": "https://arxiv.org/pdf/1812.00420", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/efficient-lifelong-learning-with-a-gem.json"}
{"id": "48074885", "bin": "700_800", "summary_sentences": ["Introduces a new global log-bilinear regression model which combines the benefits of both global matrix factorization and local context window methods.", "Global Matrix Factorization Methods  Decompose large matrices into low-rank approximations.", "eg - Latent Semantic Analysis (LSA)  Limitations  Poor performance on word analogy task  Frequent words contribute disproportionately high to the similarity measure.", "Shallow, Local Context-Based Window Methods  Learn word representations using adjacent words.", "eg - Continous bag-of-words (CBOW) model and skip-gram model.", "Limitations  Since they do not operate directly on the global co-occurrence counts, they can not utilise the statistics of the corpus effectively.", "GloVe Model  To capture the relationship between words i and j, word vector models should use ratios of co-occurene probabilites (with other words k) instead of using raw probabilites themselves.", "In most general form:  F(wi, wj, wk~ ) = Pik/Pjk  We want F to encode information in the vector space (which have a linear structure), so we can restrict to the difference of wi and wj  F(wi - wj, wk~ ) = Pik/Pjk  Since right hand side is a scalar and left hand side is a vector, we take dot product of the arguments.", "F( (wi - wj)T, wk~ ) = Pik/Pjk  F should be invariant to order of the word pair i and j.  F(wiTwk~) = Pik  Doing further simplifications and optimisations (refer paper), we get cost function,  J = Sum (over all i, j pairs in the vocabulary)[wiTwk  + bi + bk - log(Xik)]2  f is a weighing function.", "f(x) = min((x/xmax)α, 1)  Typical values, xmax = 100 and α = 3/4  b are the bias terms.", "Complexity  Depends on a number of non-zero elements in the input matrix.", "Upper bound by the square of vocabulary size  Since for shallow window-based approaches, complexity depends on |C| (size of the corpus), tighter bounds are needed.", "By modelling number of co-occurrences of words as power law function of frequency rank, the complexity can be shown to be proportional to |C|0.8  Evaluation  Tasks  Word Analogies  a is to b as c is to ___?", "Both semantic and syntactic pairs  Find closest d to wb - wc + wa (using cosine similarity)  Word Similarity  Named Entity Recognition  Datasets  Wikipedia Dumps - 2010 and 2014  Gigaword5  Combination of Gigaword5 and Wikipedia2014  CommonCrawl  400,000 most frequent words considered from the corpus.", "Hyperparameters  Size of context window.", "Whether to distinguish left context from right context.", "f - Word pairs that are d words apart contribute 1/d to the total count.", "xmax = 100  α = 3/4  AdaGrad update  Models Compared With  Singular Value Decomposition  Continous Bag-Of-Words  Skip-Gram  Results  Glove outperforms all other models significantly.", "Diminishing returns for vectors larger than 200 dimensions.", "Small and asymmetric context windows (context window only to the left) works better for syntactic tasks.", "Long and symmetric context windows (context window to both the sides) works better for semantic tasks.", "Syntactic task benefited from larger corpus though semantic task performed better with Wikipedia instead of Gigaword5 probably due to the comprehensiveness of Wikipedia and slightly outdated nature of Gigaword5.", "Word2vec’s performance decreases if the number of negative samples increases beyond about 10.", "For the same corpus, vocabulary, and window size GloVe consistently achieves better results, faster.", "This comment has been minimized.", "Sign in to view  Copy link  Quote reply  Liskutin commented  Mar 12, 2019  I know its old post, but I have a question.", "If the model is basically trying to reconstruct the big co-occurrence matrix [well in the paper they said they transform that into probability ratios matrix, i guess], is the model reconstructing the matrix line by line or word by word?"], "summary_text": "Introduces a new global log-bilinear regression model which combines the benefits of both global matrix factorization and local context window methods. Global Matrix Factorization Methods  Decompose large matrices into low-rank approximations. eg - Latent Semantic Analysis (LSA)  Limitations  Poor performance on word analogy task  Frequent words contribute disproportionately high to the similarity measure. Shallow, Local Context-Based Window Methods  Learn word representations using adjacent words. eg - Continous bag-of-words (CBOW) model and skip-gram model. Limitations  Since they do not operate directly on the global co-occurrence counts, they can not utilise the statistics of the corpus effectively. GloVe Model  To capture the relationship between words i and j, word vector models should use ratios of co-occurene probabilites (with other words k) instead of using raw probabilites themselves. In most general form:  F(wi, wj, wk~ ) = Pik/Pjk  We want F to encode information in the vector space (which have a linear structure), so we can restrict to the difference of wi and wj  F(wi - wj, wk~ ) = Pik/Pjk  Since right hand side is a scalar and left hand side is a vector, we take dot product of the arguments. F( (wi - wj)T, wk~ ) = Pik/Pjk  F should be invariant to order of the word pair i and j.  F(wiTwk~) = Pik  Doing further simplifications and optimisations (refer paper), we get cost function,  J = Sum (over all i, j pairs in the vocabulary)[wiTwk  + bi + bk - log(Xik)]2  f is a weighing function. f(x) = min((x/xmax)α, 1)  Typical values, xmax = 100 and α = 3/4  b are the bias terms. Complexity  Depends on a number of non-zero elements in the input matrix. Upper bound by the square of vocabulary size  Since for shallow window-based approaches, complexity depends on |C| (size of the corpus), tighter bounds are needed. By modelling number of co-occurrences of words as power law function of frequency rank, the complexity can be shown to be proportional to |C|0.8  Evaluation  Tasks  Word Analogies  a is to b as c is to ___? Both semantic and syntactic pairs  Find closest d to wb - wc + wa (using cosine similarity)  Word Similarity  Named Entity Recognition  Datasets  Wikipedia Dumps - 2010 and 2014  Gigaword5  Combination of Gigaword5 and Wikipedia2014  CommonCrawl  400,000 most frequent words considered from the corpus. Hyperparameters  Size of context window. Whether to distinguish left context from right context. f - Word pairs that are d words apart contribute 1/d to the total count. xmax = 100  α = 3/4  AdaGrad update  Models Compared With  Singular Value Decomposition  Continous Bag-Of-Words  Skip-Gram  Results  Glove outperforms all other models significantly. Diminishing returns for vectors larger than 200 dimensions. Small and asymmetric context windows (context window only to the left) works better for syntactic tasks. Long and symmetric context windows (context window to both the sides) works better for semantic tasks. Syntactic task benefited from larger corpus though semantic task performed better with Wikipedia instead of Gigaword5 probably due to the comprehensiveness of Wikipedia and slightly outdated nature of Gigaword5. Word2vec’s performance decreases if the number of negative samples increases beyond about 10. For the same corpus, vocabulary, and window size GloVe consistently achieves better results, faster. This comment has been minimized. Sign in to view  Copy link  Quote reply  Liskutin commented  Mar 12, 2019  I know its old post, but I have a question. If the model is basically trying to reconstruct the big co-occurrence matrix [well in the paper they said they transform that into probability ratios matrix, i guess], is the model reconstructing the matrix line by line or word by word?", "pdf_url": "http://arxiv.org/pdf/1607.01759v3", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/ea5a42d17e0fcf18374df8e3e4b3e8.json"}
{"id": "27321339", "bin": "700_800", "summary_sentences": ["The paper presents a general message passing architecture called as Message Passing Neural Networks (MPNNs) that unify various existing models for performing supervised learning on molecules.", "Variants of the MPNN model achieve very good performance on the task of predicting the property of the molecules.", "MPNN  Setting  The input to the model is an undirected graph G where node features are represented as xv (corresponding to node v) and edge features are ev, w (corresponding to edge between nodes v, w).", "The idea is to learn a representation (or feature vector) for all the nodes (and possibly edges) in the graph and use that for the downstream supervised learning task.", "The model can be easily extended to the setting of directed graphs.", "The model works in 2 phases:  Message Passing Phase  All nodes send a message to their neighbouring nodes.", "The message is a function of the feature vectors corresponding to the sender node (or vertex), the receiver node and the edge connecting the two nodes.", "The feature vectors can be combined to form the message using the message function which can be implemented as a neural network.", "Once a node has received messages from all its neighbours, it updated its feature vector by aggregating all the message.", "The function used to aggregate and update the feature vector is called as the update function and can be implemented as a neural network.", "After updating the feature vectors, the graph could initiate another round of message passing.", "After a sufficient number of message passing rounds, the Readout phase is invoked.", "Readout Phase  The feature vectors corresponding to different nodes in the graph are aggregated into a single feature vector (corresponding to the feature vector of the graph) using the readout function.", "The readout function can also be implemented using a neural network with the condition that it is invariant to the permutation of the nodes within the graph (to ensure that the MPNN is independent of the graph isomorphism).", "Existing Variants in literature  The paper provides various examples where the existing architectures could be explained in terms of the message passing framework.", "This includes examples like Convolutional Networks on Graphs for Learning Molecular Fingerprints , Gated Graph Sequence Neural Networks , Graph Convolutional Networks etc.", "Experiments  Setup  Broadly speaking, the task is to predict the properties of given molecules (regression problem).", "The QM9 dataset consists of 130K molecules whose properties have been measured using Quantum Mechanical Simulations (DFT).", "Properties to be predicted include atomization energy, enthalpy, highest fundamental vibrational frequency etc.", "There are two benchmarks for error:  DFT Error - Estimated average error of DFT approximation  Chemical Accuracy - As established by the chemistry community  Model  Following variants of message function are explored:  Matrix multiplication between Aevw and hv where A is the adjacency matrix hv is the feature corresponding to node v.  Edge Network which is same as matrix multiplication case with the difference that A is a learned matrix for each edge type.", "Pair Network where the feature vector corresponding to the source node, target node and edge is fed to a neural network.", "Virtual Elements  Since all messages are shared via edges, it could take a long time for the message to move between two ends of the graph.", "To fasten this process, virtual elements are provided.", "In the first setting, “virtual edges” are inserted between nodes.", "In the second setting, a “master” node connects to all the other nodes.", "Message Passing Complexity  In a graph with n nodes and d dimensional feature vectors, a single step of message passing would have the worst case time complexity of O(n2d2.", "This complexity can be reduced by breaking the d dimensional embedding into k different groups of d/k embeddings which can be updated in parallel.", "The complexity of the modified approach is O(n2d2/k.", "Results  Best performing MPNN model uses edge network as the message function and set2set as the readout function.", "Using group of embeddings helps to improve generalization.", "This effect could also be because of ensemble-like nature of the modified architecture.", "The model performs worse without the virtual elements.", "Takeaways  Long range interaction between vertices is necessary.", "Scaling to larger molecule sizes is challenging because the model creates a fully connected graph by incorporating virtual elements."], "summary_text": "The paper presents a general message passing architecture called as Message Passing Neural Networks (MPNNs) that unify various existing models for performing supervised learning on molecules. Variants of the MPNN model achieve very good performance on the task of predicting the property of the molecules. MPNN  Setting  The input to the model is an undirected graph G where node features are represented as xv (corresponding to node v) and edge features are ev, w (corresponding to edge between nodes v, w). The idea is to learn a representation (or feature vector) for all the nodes (and possibly edges) in the graph and use that for the downstream supervised learning task. The model can be easily extended to the setting of directed graphs. The model works in 2 phases:  Message Passing Phase  All nodes send a message to their neighbouring nodes. The message is a function of the feature vectors corresponding to the sender node (or vertex), the receiver node and the edge connecting the two nodes. The feature vectors can be combined to form the message using the message function which can be implemented as a neural network. Once a node has received messages from all its neighbours, it updated its feature vector by aggregating all the message. The function used to aggregate and update the feature vector is called as the update function and can be implemented as a neural network. After updating the feature vectors, the graph could initiate another round of message passing. After a sufficient number of message passing rounds, the Readout phase is invoked. Readout Phase  The feature vectors corresponding to different nodes in the graph are aggregated into a single feature vector (corresponding to the feature vector of the graph) using the readout function. The readout function can also be implemented using a neural network with the condition that it is invariant to the permutation of the nodes within the graph (to ensure that the MPNN is independent of the graph isomorphism). Existing Variants in literature  The paper provides various examples where the existing architectures could be explained in terms of the message passing framework. This includes examples like Convolutional Networks on Graphs for Learning Molecular Fingerprints , Gated Graph Sequence Neural Networks , Graph Convolutional Networks etc. Experiments  Setup  Broadly speaking, the task is to predict the properties of given molecules (regression problem). The QM9 dataset consists of 130K molecules whose properties have been measured using Quantum Mechanical Simulations (DFT). Properties to be predicted include atomization energy, enthalpy, highest fundamental vibrational frequency etc. There are two benchmarks for error:  DFT Error - Estimated average error of DFT approximation  Chemical Accuracy - As established by the chemistry community  Model  Following variants of message function are explored:  Matrix multiplication between Aevw and hv where A is the adjacency matrix hv is the feature corresponding to node v.  Edge Network which is same as matrix multiplication case with the difference that A is a learned matrix for each edge type. Pair Network where the feature vector corresponding to the source node, target node and edge is fed to a neural network. Virtual Elements  Since all messages are shared via edges, it could take a long time for the message to move between two ends of the graph. To fasten this process, virtual elements are provided. In the first setting, “virtual edges” are inserted between nodes. In the second setting, a “master” node connects to all the other nodes. Message Passing Complexity  In a graph with n nodes and d dimensional feature vectors, a single step of message passing would have the worst case time complexity of O(n2d2. This complexity can be reduced by breaking the d dimensional embedding into k different groups of d/k embeddings which can be updated in parallel. The complexity of the modified approach is O(n2d2/k. Results  Best performing MPNN model uses edge network as the message function and set2set as the readout function. Using group of embeddings helps to improve generalization. This effect could also be because of ensemble-like nature of the modified architecture. The model performs worse without the virtual elements. Takeaways  Long range interaction between vertices is necessary. Scaling to larger molecule sizes is challenging because the model creates a fully connected graph by incorporating virtual elements.", "pdf_url": "https://arxiv.org/pdf/1704.01212", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/neural-message-passing-for-quantum-chemistry.json"}
{"id": "75442340", "bin": "700_800", "summary_sentences": ["This paper considers the problem of structured output prediction, in the specific case where the output is a sequence and we represent the sequence as a (conditional) directed graphical model that generates from the first token to the last.", "The paper starts from the observation that training such models by maximum likelihood (ML) does not reflect well how the model is actually used at test time.", "Indeed, ML training implies that the model is effectively trained to predict each token conditioned on the previous tokens *from the ground truth* sequence (this is known as \"teacher forcing\").", "Yet, when making a prediction for a new input, the model will actually generate a sequence by generating tokens one after another and conditioning on *its own predicted tokens* instead.", "So the authors propose a different training procedure, where at training time each *conditioning* ground truth token is sometimes replaced by the model's previous prediction.", "The choice of replacing the ground truth by the model's prediction is made by \"flipping a coin\" with some probability, independently for each token.", "Importantly, the authors propose to start with a high probability of using the ground truth (i.e. start close to ML) and anneal that probability closer to 0, according to some schedule (thus the name Schedule Sampling).", "Experiments on 3 tasks (image caption generation, constituency parsing and speech recognition) based on neural networks with LSTM units, demonstrate that this approach indeed improves over ML training in terms of the various performance metrics appropriate for each problem, and yields better sequence prediction models.", "#### My two cents  Big fan of this paper.", "It both identifies an important flaw in how sequential prediction models are currently trained and, most importantly, suggests a solution that is simple yet effective.", "I also believe that this approach played a non-negligible role in Google's winner system for image caption generation, in the Microsoft COCO competition.", "My alternative interpretation of why Scheduled Sampling helps is that ML training does not inform the model about the relative quality of the errors it can make.", "In terms of ML, it is as bad to put high probability on an output sequence that has just 1 token that's wrong, than it is to put the same amount of probability on a sequence that has all tokens wrong.", "Yet, say for image caption generation, outputting a sentence that is one word away from the ground truth is clearly preferable from making a mistake on a words (something that is also reflected in the performance metrics, such as BLEU).", "By training the model to be robust to its own mistakes, Scheduled Sampling ensures that errors won't accumulate and makes predictions that are entirely off much less likely.", "An alternative to Scheduled Sampling is DAgger (Dataset Aggregation:  [ref] ), which briefly put alternates between training the model and adding to the training set examples that mix model predictions and the ground truth.", "However, Scheduled Sampling has the advantage that there is no need to explicitly create and store that increasingly large dataset of sampled examples, something that isn't appealing for online learning or learning on large datasets.", "I'm also very curious and interested by one of the direction of future work mentioned in the conclusion: figuring out a way to backprop through the stochastic predictions made by the model.", "Indeed, as the authors point out, the current algorithm ignores the fact that, by sometimes taking as input its previous prediction, this induces an additional relationship between the model's parameters and its ultimate prediction, a relationship that isn't taken into account during training.", "To take it into account, you'd need to somehow backpropagate through the stochastic process that generated the previous token prediction.", "While the work on variational autoencoders has shown that we can backprop through gaussian samples, backpropagating through the sampling of a discrete multinomial distribution is essentially an open problem.", "I do believe that there is work that tried to tackle propagating through stochastic binary units however, so perhaps that's a start.", "Anyways, if the authors could make progress on that specific issue, it could be quite useful not just in the context of Schedule Sampling, but possibly in the context of training networks with discrete stochastic units in general!"], "summary_text": "This paper considers the problem of structured output prediction, in the specific case where the output is a sequence and we represent the sequence as a (conditional) directed graphical model that generates from the first token to the last. The paper starts from the observation that training such models by maximum likelihood (ML) does not reflect well how the model is actually used at test time. Indeed, ML training implies that the model is effectively trained to predict each token conditioned on the previous tokens *from the ground truth* sequence (this is known as \"teacher forcing\"). Yet, when making a prediction for a new input, the model will actually generate a sequence by generating tokens one after another and conditioning on *its own predicted tokens* instead. So the authors propose a different training procedure, where at training time each *conditioning* ground truth token is sometimes replaced by the model's previous prediction. The choice of replacing the ground truth by the model's prediction is made by \"flipping a coin\" with some probability, independently for each token. Importantly, the authors propose to start with a high probability of using the ground truth (i.e. start close to ML) and anneal that probability closer to 0, according to some schedule (thus the name Schedule Sampling). Experiments on 3 tasks (image caption generation, constituency parsing and speech recognition) based on neural networks with LSTM units, demonstrate that this approach indeed improves over ML training in terms of the various performance metrics appropriate for each problem, and yields better sequence prediction models. #### My two cents  Big fan of this paper. It both identifies an important flaw in how sequential prediction models are currently trained and, most importantly, suggests a solution that is simple yet effective. I also believe that this approach played a non-negligible role in Google's winner system for image caption generation, in the Microsoft COCO competition. My alternative interpretation of why Scheduled Sampling helps is that ML training does not inform the model about the relative quality of the errors it can make. In terms of ML, it is as bad to put high probability on an output sequence that has just 1 token that's wrong, than it is to put the same amount of probability on a sequence that has all tokens wrong. Yet, say for image caption generation, outputting a sentence that is one word away from the ground truth is clearly preferable from making a mistake on a words (something that is also reflected in the performance metrics, such as BLEU). By training the model to be robust to its own mistakes, Scheduled Sampling ensures that errors won't accumulate and makes predictions that are entirely off much less likely. An alternative to Scheduled Sampling is DAgger (Dataset Aggregation:  [ref] ), which briefly put alternates between training the model and adding to the training set examples that mix model predictions and the ground truth. However, Scheduled Sampling has the advantage that there is no need to explicitly create and store that increasingly large dataset of sampled examples, something that isn't appealing for online learning or learning on large datasets. I'm also very curious and interested by one of the direction of future work mentioned in the conclusion: figuring out a way to backprop through the stochastic predictions made by the model. Indeed, as the authors point out, the current algorithm ignores the fact that, by sometimes taking as input its previous prediction, this induces an additional relationship between the model's parameters and its ultimate prediction, a relationship that isn't taken into account during training. To take it into account, you'd need to somehow backpropagate through the stochastic process that generated the previous token prediction. While the work on variational autoencoders has shown that we can backprop through gaussian samples, backpropagating through the sampling of a discrete multinomial distribution is essentially an open problem. I do believe that there is work that tried to tackle propagating through stochastic binary units however, so perhaps that's a start. Anyways, if the authors could make progress on that specific issue, it could be quite useful not just in the context of Schedule Sampling, but possibly in the context of training networks with discrete stochastic units in general!", "pdf_url": "http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/bengiovjs15.json"}
{"id": "7255717", "bin": "700_800", "summary_sentences": ["What  They propose a two-stage GAN architecture that generates 256x256 images of (relatively) high quality.", "The model gets text as an additional input and the images match the text.", "How  Most of the architecture is the same as in any GAN:  Generator G generates images.", "Discriminator D discriminates betweens fake and real images.", "G gets a noise variable z, so that it doesn't always do the same thing.", "Two-staged image generation:  Instead of one step, as in most GANs, they use two steps, each consisting of a G and D.  The first generator creates 64x64 images via upsampling.", "The first discriminator judges these images via downsampling convolutions.", "The second generator takes the image from the first generator, downsamples it via convolutions, then applies some residual convolutions and then re-upsamples it to 256x256.", "The second discriminator is comparable to the first one (downsampling convolutions).", "Note that the second generator does not get an additional noise term z, only the first one gets it.", "For upsampling, they use 3x3 convolutions with ReLUs, BN and nearest neighbour upsampling.", "For downsampling, they use 4x4 convolutions with stride 2, Leaky ReLUs and BN (the first convolution doesn't seem to use BN).", "Text embedding:  The generated images are supposed to match input texts.", "These input texts are embedded to vectors.", "These vectors are added as:  An additional input to the first generator.", "An additional input to the second generator (concatenated after the downsampling and before the residual convolutions).", "An additional input to the first discriminator (concatenated after the downsampling).", "An additional input to the second discriminator (concatenated after the downsampling).", "In case the text embeddings need to be matrices, the values are simply reshaped to (N, 1, 1) and then repeated to (N, H, W).", "The texts are converted to embeddings via a network at the start of the model.", "Input to that vector: Unclear.", "(Concatenated word vectors?", "Seems to not be described in the text.)", "The input is transformed to a vector via a fully connected layer (the text model is apparently not recurrent).", "The vector is transformed via fully connected layers to a mean vector and a sigma vector.", "These are then interpreted as normal distributions, from which the final output vector is sampled.", "This uses the reparameterization trick, similar to the method in VAEs.", "Just like in VAEs, a KL-divergence term is added to the loss, which prevents each single normal distribution from deviating too far from the unit normal distribution N(0,1).", "The authors argue, that using the VAE-like formulation -- instead of directly predicting an output vector (via FC layers) -- compensated for the lack of labels (smoother manifold).", "Note: This way of generating text embeddings seems very simple.", "(No recurrence, only about two layers.)", "It probably won't do much more than just roughly checking for the existence of specific words and word combinations (e.g. \"red head\").", "Visualization of the architecture:  Results  Note: No example images of the two-stage architecture for LSUN bedrooms.", "Using only the first stage of the architecture (first G and D) reduces the Inception score significantly.", "Adding the text to both the first and second generator improves the Inception score slightly.", "Adding the VAE-like text embedding generation (as opposed to only FC layers) improves the Inception score slightly.", "Generating images at higher resolution (256x256 instead of 128x128) improves the Inception score significantly  Note: The 256x256 architecture has more residual convolutions than the 128x128 one.", "Note: The 128x128 and the 256x256 are both upscaled to 299x299 images before computing the Inception score.", "That should make the 128x128 images quite blurry and hence of low quality.", "Example images, with text and stage 1/2 results:  More examples of birds:  Examples of failures:  The authors argue, that most failure cases happen when stage 1 messes up."], "summary_text": "What  They propose a two-stage GAN architecture that generates 256x256 images of (relatively) high quality. The model gets text as an additional input and the images match the text. How  Most of the architecture is the same as in any GAN:  Generator G generates images. Discriminator D discriminates betweens fake and real images. G gets a noise variable z, so that it doesn't always do the same thing. Two-staged image generation:  Instead of one step, as in most GANs, they use two steps, each consisting of a G and D.  The first generator creates 64x64 images via upsampling. The first discriminator judges these images via downsampling convolutions. The second generator takes the image from the first generator, downsamples it via convolutions, then applies some residual convolutions and then re-upsamples it to 256x256. The second discriminator is comparable to the first one (downsampling convolutions). Note that the second generator does not get an additional noise term z, only the first one gets it. For upsampling, they use 3x3 convolutions with ReLUs, BN and nearest neighbour upsampling. For downsampling, they use 4x4 convolutions with stride 2, Leaky ReLUs and BN (the first convolution doesn't seem to use BN). Text embedding:  The generated images are supposed to match input texts. These input texts are embedded to vectors. These vectors are added as:  An additional input to the first generator. An additional input to the second generator (concatenated after the downsampling and before the residual convolutions). An additional input to the first discriminator (concatenated after the downsampling). An additional input to the second discriminator (concatenated after the downsampling). In case the text embeddings need to be matrices, the values are simply reshaped to (N, 1, 1) and then repeated to (N, H, W). The texts are converted to embeddings via a network at the start of the model. Input to that vector: Unclear. (Concatenated word vectors? Seems to not be described in the text.) The input is transformed to a vector via a fully connected layer (the text model is apparently not recurrent). The vector is transformed via fully connected layers to a mean vector and a sigma vector. These are then interpreted as normal distributions, from which the final output vector is sampled. This uses the reparameterization trick, similar to the method in VAEs. Just like in VAEs, a KL-divergence term is added to the loss, which prevents each single normal distribution from deviating too far from the unit normal distribution N(0,1). The authors argue, that using the VAE-like formulation -- instead of directly predicting an output vector (via FC layers) -- compensated for the lack of labels (smoother manifold). Note: This way of generating text embeddings seems very simple. (No recurrence, only about two layers.) It probably won't do much more than just roughly checking for the existence of specific words and word combinations (e.g. \"red head\"). Visualization of the architecture:  Results  Note: No example images of the two-stage architecture for LSUN bedrooms. Using only the first stage of the architecture (first G and D) reduces the Inception score significantly. Adding the text to both the first and second generator improves the Inception score slightly. Adding the VAE-like text embedding generation (as opposed to only FC layers) improves the Inception score slightly. Generating images at higher resolution (256x256 instead of 128x128) improves the Inception score significantly  Note: The 256x256 architecture has more residual convolutions than the 128x128 one. Note: The 128x128 and the 256x256 are both upscaled to 299x299 images before computing the Inception score. That should make the 128x128 images quite blurry and hence of low quality. Example images, with text and stage 1/2 results:  More examples of birds:  Examples of failures:  The authors argue, that most failure cases happen when stage 1 messes up.", "pdf_url": "https://arxiv.org/pdf/1612.03242", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/stackgan.json"}
{"id": "68642594", "bin": "700_800", "summary_sentences": ["Proposes a novel, end-to-end architecture for generating short email responses.", "Single most important benchmark of its success is that it is deployed in Inbox by Gmail and assists with around 10% of all mobile responses.", ".", "Challenges in deploying Smart Reply in a user-facing product  Responses must always be of high quality.", "Ensured by constructing a target response set to select responses from.", "The likelihood of choosing the responses must be maximised.", "Ensured by normalising the responses and enforcing diversity.", "The system should not add latency to emails.", "Ensured by using a triggering model to decide if the email is suitable to undergo the response generation pipeline.", "Computation time is further reduced by finding approximate best result instead of the best result.", "Ensure privacy by encrypting all the data which adds challenge in verifying the model's quality and debugging the system.", "Architecture  Preprocess Email  Perform actions like language detection, tokenization, sentence segmentation etc on the input email.", "Triggering Model  A feed-forward neural network (with embedding layer and 3 fully connected hidden layers) to decide if the input email is suitable for suggesting responses.", "Data  Training set of pairs (o, y) where o is the incoming message and y is a boolean variable to indicate if the message had a response.", "Features  Unigrams, bigrams from the messages.", "Signals like - is the recipient in the contact list of the sender.", "Response Selection  LSTM network to predict the approximate best response for an incoming message o  Network  Sequence to Sequence Learning.", "Reads the input message (token by token) and encode a vector representation.", "Compute softmax to get the probability of first output token given the input token sequence.", "Keep feeding in the previous response tokens and the input token sequence to compute the probability of next output token.", "During inference, approximate the most likely response greedily by taking the most likely response at each timestamp and feeding it back or by using the beam search approach.", "Response Set Generation  Generate a set of high-quality responses that also capture the variability in the intent of the response.", "Canonicalize the email response by extracting the semantic structure using a dependency parser.", "Partition all response messages into \"semantic\" clusters.", "These semantic clusters define the response space for scoring and selecting possible responses and for promoting diversity among the responses.", "Semantic Intent Clustering  Since a large, labelled dataset is not available, a graph based, semi-supervised approach is used.", "Graph Construction  Manually define a few clusters with a small number of example responses for each cluster.", "Construct a graph with frequent response messages (including the labelled nodes) as response nodes (VR).", "For each response node, extract a set of feature nodes (VF) corresponding to features like skip-gram and n-grams and add an edge between the response node and the feature node.", "Learn a semantic labelling for all response nodes by propagating semantic intent information (available because of labelled nodes) throughout the graph.", "After some iterations, sample some of the unlabeled nodes from the graph, manually label these sample nodes and repeat this algorithm until convergence.", "For validation, extract the top k members of each cluster and validate the quality with help of human evaluators.", "Suggestion Diversity  Provide users with a varied set of response by omitting redundant response (by not selecting more than one response from any semantic cluster) and by enforcing negative (or positive) responses.", "If the top two responses contain at least one positive (negative) response and none of the top three responses is negative (positive), the third response is replaced with a negative (positive) one.", "This is done by performing a second LSTM pass where the search is restricted to only positive (or negative) responses in the target set.", "Strengths  The system is already in production and assists with around 10% of all mobile responses.", "This comment has been minimized.", "Sign in to view  Copy link  Quote reply  vegetakarthhik commented  Nov 26, 2018  hey do you have python implementation?"], "summary_text": "Proposes a novel, end-to-end architecture for generating short email responses. Single most important benchmark of its success is that it is deployed in Inbox by Gmail and assists with around 10% of all mobile responses. . Challenges in deploying Smart Reply in a user-facing product  Responses must always be of high quality. Ensured by constructing a target response set to select responses from. The likelihood of choosing the responses must be maximised. Ensured by normalising the responses and enforcing diversity. The system should not add latency to emails. Ensured by using a triggering model to decide if the email is suitable to undergo the response generation pipeline. Computation time is further reduced by finding approximate best result instead of the best result. Ensure privacy by encrypting all the data which adds challenge in verifying the model's quality and debugging the system. Architecture  Preprocess Email  Perform actions like language detection, tokenization, sentence segmentation etc on the input email. Triggering Model  A feed-forward neural network (with embedding layer and 3 fully connected hidden layers) to decide if the input email is suitable for suggesting responses. Data  Training set of pairs (o, y) where o is the incoming message and y is a boolean variable to indicate if the message had a response. Features  Unigrams, bigrams from the messages. Signals like - is the recipient in the contact list of the sender. Response Selection  LSTM network to predict the approximate best response for an incoming message o  Network  Sequence to Sequence Learning. Reads the input message (token by token) and encode a vector representation. Compute softmax to get the probability of first output token given the input token sequence. Keep feeding in the previous response tokens and the input token sequence to compute the probability of next output token. During inference, approximate the most likely response greedily by taking the most likely response at each timestamp and feeding it back or by using the beam search approach. Response Set Generation  Generate a set of high-quality responses that also capture the variability in the intent of the response. Canonicalize the email response by extracting the semantic structure using a dependency parser. Partition all response messages into \"semantic\" clusters. These semantic clusters define the response space for scoring and selecting possible responses and for promoting diversity among the responses. Semantic Intent Clustering  Since a large, labelled dataset is not available, a graph based, semi-supervised approach is used. Graph Construction  Manually define a few clusters with a small number of example responses for each cluster. Construct a graph with frequent response messages (including the labelled nodes) as response nodes (VR). For each response node, extract a set of feature nodes (VF) corresponding to features like skip-gram and n-grams and add an edge between the response node and the feature node. Learn a semantic labelling for all response nodes by propagating semantic intent information (available because of labelled nodes) throughout the graph. After some iterations, sample some of the unlabeled nodes from the graph, manually label these sample nodes and repeat this algorithm until convergence. For validation, extract the top k members of each cluster and validate the quality with help of human evaluators. Suggestion Diversity  Provide users with a varied set of response by omitting redundant response (by not selecting more than one response from any semantic cluster) and by enforcing negative (or positive) responses. If the top two responses contain at least one positive (negative) response and none of the top three responses is negative (positive), the third response is replaced with a negative (positive) one. This is done by performing a second LSTM pass where the search is restricted to only positive (or negative) responses in the target set. Strengths  The system is already in production and assists with around 10% of all mobile responses. This comment has been minimized. Sign in to view  Copy link  Quote reply  vegetakarthhik commented  Nov 26, 2018  hey do you have python implementation?", "pdf_url": "https://arxiv.org/pdf/1511.08130", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/411f15b71ed6a664f9d5ac46409b42.json"}
{"id": "26262723", "bin": "700_800", "summary_sentences": ["This paper aims to detect linguistic characteristics and grammatical patterns from speech transcriptions generated by Alzheimer’s disease (AD) patients.", "The authors propose several neural models such as CNNs and LSTM-RNNs — and combinations of them — to enhance an AD classification task.", "The trained neural models are used to interpret linguistic characteristics of AD patients (including gender variation) via activation clustering and first-derivative saliency techniques.", "Motivation  Language variation can serve as a proxy that monitors how patients’ cognitive functions have been affected (e.g., issues with word finding and impaired reasoning).", "This can equip machines with diagnostic capabilities, which are particularly effective for dealing with AD since it is neither curable or reversible.", "Challenges and Limitations  The challenge with detecting AD-positive patients is it requires diverse linguistic and world knowledge.", "Consider the following example:  “Well…there’s a mother standing there uh uh washing the dishes and the sink is overspilling…overflowing.” There are several linguistic cues, such as “overspilling…overflowing”, indicating signs of confusion and memory loss, which is very common in AD-positive patients.", "Therefore, instead of relying on hand-crafted features, the authors propose a neural model for automatically learning these linguistic cues from the data.", "Other important issues observed from the previous literature are as follows:  Hand-crafted features may not be suitable to analyze AD patients data, which convey progressive changes in linguistic patterns.", "Hand-crafted features quickly become outdated as language and culture evolve.", "Relying on neural networks alone doesn’t offer much interpretability.", "Data  This work uses the Dementia Bank dataset , which consists of transcripts and audio recordings of AD (and control) patients.", "These records were collected via interviews on several tasks such as “Recall Test” and “Boston Cookie Theft”.", "Transcripts were segmented into individual utterances with accompanying part-of-speech (POS) tags.", "Models  Three types of neural approaches are proposed: CNN (embedding + convolutional layer + max-pooling layer), LSTM-RNN (embedding + LSTM layer), and CNN-LSTM (basically laying an LSTM on top of CNN — architecture shown in the figure below).", "(See paper for more details.)", "Results  The best performing model (POS tags + CNN-LSTM) achieves 91.1% accuracy, which sets a new benchmark for the AD classification task.", "See other results below.", "The authors observed that almost all AD-positive results were classified correctly and that there were more errors in classifying non-AD samples.", "This could be because the dataset contained patients with various degree of symptoms related to AD.", "(See paper for more results.)", "Analysis  No significant differences in linguistic patterns were observed between male and female AD patients.", "Furthermore, interpretation of the linguistic cues captured by the neural models are conducted using two visualization techniques:  Activation Clustering — offers insights into sentence-level patterns  First Derivative Saliency — offers insights into word importance  Through the activation clustering, three common linguistic patterns found in AD patients emerged from the clusters: short answers and bursts of speech (e.g., “and” and “oh!”), repeated requests for clarification (e.g., “did I say fact?”), and starting with interjections (“so” and “well”).", "Moreover, for several tasks such as Cookie and Recall, the most commonly used POS tags for AD clusters were show to be distinct.", "Through the salience heat maps, the difference in word importance can be seen between control and AD patients.", "As shown in the figure below (left), the word “uh” and “um” are important and distinguishable speech traits for classifying AD patients.", "The figure (right) shows that the control group does not heavily use these type of filler words.", "Future Work and Conclusion  Neural models combined with visualization techniques can offer more insights into the linguistic cues and variations of AD patients.", "With that said, such models can be generalized to study other neurological diseases.", "Context-aware models and conversational context can help to improve the predictive performance of the models and also offer more interpretability.", "References  Detecting Linguistic Characteristics of Alzheimer’s Dementia by Interpreting Neural Models — (Sweta Karlekar, Tong Niu, and Mohit Bansal)"], "summary_text": "This paper aims to detect linguistic characteristics and grammatical patterns from speech transcriptions generated by Alzheimer’s disease (AD) patients. The authors propose several neural models such as CNNs and LSTM-RNNs — and combinations of them — to enhance an AD classification task. The trained neural models are used to interpret linguistic characteristics of AD patients (including gender variation) via activation clustering and first-derivative saliency techniques. Motivation  Language variation can serve as a proxy that monitors how patients’ cognitive functions have been affected (e.g., issues with word finding and impaired reasoning). This can equip machines with diagnostic capabilities, which are particularly effective for dealing with AD since it is neither curable or reversible. Challenges and Limitations  The challenge with detecting AD-positive patients is it requires diverse linguistic and world knowledge. Consider the following example:  “Well…there’s a mother standing there uh uh washing the dishes and the sink is overspilling…overflowing.” There are several linguistic cues, such as “overspilling…overflowing”, indicating signs of confusion and memory loss, which is very common in AD-positive patients. Therefore, instead of relying on hand-crafted features, the authors propose a neural model for automatically learning these linguistic cues from the data. Other important issues observed from the previous literature are as follows:  Hand-crafted features may not be suitable to analyze AD patients data, which convey progressive changes in linguistic patterns. Hand-crafted features quickly become outdated as language and culture evolve. Relying on neural networks alone doesn’t offer much interpretability. Data  This work uses the Dementia Bank dataset , which consists of transcripts and audio recordings of AD (and control) patients. These records were collected via interviews on several tasks such as “Recall Test” and “Boston Cookie Theft”. Transcripts were segmented into individual utterances with accompanying part-of-speech (POS) tags. Models  Three types of neural approaches are proposed: CNN (embedding + convolutional layer + max-pooling layer), LSTM-RNN (embedding + LSTM layer), and CNN-LSTM (basically laying an LSTM on top of CNN — architecture shown in the figure below). (See paper for more details.) Results  The best performing model (POS tags + CNN-LSTM) achieves 91.1% accuracy, which sets a new benchmark for the AD classification task. See other results below. The authors observed that almost all AD-positive results were classified correctly and that there were more errors in classifying non-AD samples. This could be because the dataset contained patients with various degree of symptoms related to AD. (See paper for more results.) Analysis  No significant differences in linguistic patterns were observed between male and female AD patients. Furthermore, interpretation of the linguistic cues captured by the neural models are conducted using two visualization techniques:  Activation Clustering — offers insights into sentence-level patterns  First Derivative Saliency — offers insights into word importance  Through the activation clustering, three common linguistic patterns found in AD patients emerged from the clusters: short answers and bursts of speech (e.g., “and” and “oh!”), repeated requests for clarification (e.g., “did I say fact?”), and starting with interjections (“so” and “well”). Moreover, for several tasks such as Cookie and Recall, the most commonly used POS tags for AD clusters were show to be distinct. Through the salience heat maps, the difference in word importance can be seen between control and AD patients. As shown in the figure below (left), the word “uh” and “um” are important and distinguishable speech traits for classifying AD patients. The figure (right) shows that the control group does not heavily use these type of filler words. Future Work and Conclusion  Neural models combined with visualization techniques can offer more insights into the linguistic cues and variations of AD patients. With that said, such models can be generalized to study other neurological diseases. Context-aware models and conversational context can help to improve the predictive performance of the models and also offer more interpretability. References  Detecting Linguistic Characteristics of Alzheimer’s Dementia by Interpreting Neural Models — (Sweta Karlekar, Tong Niu, and Mohit Bansal)", "pdf_url": "https://www.aclweb.org/anthology/N18-2110.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/using-deep-learning-to-detect-linguistic-cues-of-alzheimers-patients-a606693e54f9.json"}
{"id": "26668230", "bin": "700_800", "summary_sentences": ["The paper proposes an approach for using symbolic knowledge in deep learning systems.", "These constraints are often expressed as boolean constraints on the output of the deep learning system and directly incorporating these constraints break the differentiability of the system.", "Problem Setting  The model is given some input data to perform predictions and symbolic knowledge is provided in form of boolean constraints like exactly-one constraint for one-hot output encoding.", "Most approaches tend to encode the symbolic knowledge in the vector space embedding to keep the model pipeline differentiable.", "In this process, the precise meaning of symbolic knowledge is often lost.", "A differentiable “semantic loss” is derived which captures the meaning of the constraint while being independent of its syntax.", "Terminology  A state x (state refers to the instantiation of boolean variables) satisfies a sentence a if a evaluates to true when using the variables as specified by x.", "A sentence a entails another sentence b if all states that satisfy a also satisfy b.", "The row output vector of the neural network is denoted as p where each value in p denotes the probability of an output.", "Three different output constraints are studied:  Exactly-one constraint  Exactly one value in p should be true.", "Can be expressed in boolean logic as follows: Let (x1, x2, …, xn) be variables in p. Then (not xi or not xj) for all pair of variables and (x1 or x2 or … xn).", "Valid Simple Path Constraint  Set of edges must form a valid path.", "Ordering Constraint  Defining an ordering over the variables.", "Semantic Loss  The semantic loss Ls(a, p) is a function of a propositional logic sentence a (the symbolic knowldge constraint) and p (output of the neural network).", "a is defined over variables (x1, …, xn) and p is interpreted as a vector of probabilities corresponding to these variables xi’s.", "The semantic loss is directly proportional to the negative log likelihood of generating a state that satisfies the constraints when sampling values according to the distribution p.  Main Axioms and Insights  Monotonicity  If a sentence a entails another sentence b then for any given p, Ls(a, p) > Ls(b, p) ie adding more constraints cannot decrease the semantic loss.", "Semantic Equivalence  If two sentences are logically equivalent, their semantic loss is the same.", "Identity  For any given sentence a, its representation as a sentence is equivalent to its representation as a deterministic vector ie writing the “one-hot” constraint as a boolean expression is equivalent to a one-hot vector.", "Satisfaction  If p entails the sentence a then Ls(a, p) = 0.", "Label-literal correspondence  When the constraint is defined in terms of a single variable, it can be interpreted as the supervised label.", "Hence the semantic loss in case of a single variable should be equivalent to the cross-entropy loss.", "Truth  The semantic loss of a true sentence is 0  Non-negativity  Semantic loss should always be non-negative.", "Probabilities of variables that are not part of the constraint, do not affect the semantic loss.", "It can be shown that the semantic loss function satisfies all these axioms (and the other axioms specified in the paper) and is the only function to do so, up to a multiplicative constant.", "Experimental Evaluation  Semantic Loss is used in the semi-supervised setting for Permuted MNIST, Fashion MNIST and CIFAR-10.", "The key takeaway is that using semantic loss improves the performance of the state-of-the-art models for Fashion MNIST and CIFAR-10.", "One downside is that the effectiveness of the semantic loss in this type of constraint strongly depends on the performance of the underlying model.", "Further, the semantic loss does not improve the performance in case of fully supervised scenario.", "Further experiments are performed to evaluate the performance of the semantic loss on complex constraints.", "Since these tasks aim to highlight the effect of using semantic loss, only simple models (MLPs) are evaluated.", "Tractability of Semantic Loss  The semantic loss is similar to the automated reasoning task called as weight model counting (wmc).", "Circuit compiler techniques can be used to compute wmc while allowing backpropagation.", "Notes  The proposed idea is simple and intuitive and the results on semi-supervised classification task are quite good.", "It would be interesting to extend and scale this method for more complex constraints."], "summary_text": "The paper proposes an approach for using symbolic knowledge in deep learning systems. These constraints are often expressed as boolean constraints on the output of the deep learning system and directly incorporating these constraints break the differentiability of the system. Problem Setting  The model is given some input data to perform predictions and symbolic knowledge is provided in form of boolean constraints like exactly-one constraint for one-hot output encoding. Most approaches tend to encode the symbolic knowledge in the vector space embedding to keep the model pipeline differentiable. In this process, the precise meaning of symbolic knowledge is often lost. A differentiable “semantic loss” is derived which captures the meaning of the constraint while being independent of its syntax. Terminology  A state x (state refers to the instantiation of boolean variables) satisfies a sentence a if a evaluates to true when using the variables as specified by x. A sentence a entails another sentence b if all states that satisfy a also satisfy b. The row output vector of the neural network is denoted as p where each value in p denotes the probability of an output. Three different output constraints are studied:  Exactly-one constraint  Exactly one value in p should be true. Can be expressed in boolean logic as follows: Let (x1, x2, …, xn) be variables in p. Then (not xi or not xj) for all pair of variables and (x1 or x2 or … xn). Valid Simple Path Constraint  Set of edges must form a valid path. Ordering Constraint  Defining an ordering over the variables. Semantic Loss  The semantic loss Ls(a, p) is a function of a propositional logic sentence a (the symbolic knowldge constraint) and p (output of the neural network). a is defined over variables (x1, …, xn) and p is interpreted as a vector of probabilities corresponding to these variables xi’s. The semantic loss is directly proportional to the negative log likelihood of generating a state that satisfies the constraints when sampling values according to the distribution p.  Main Axioms and Insights  Monotonicity  If a sentence a entails another sentence b then for any given p, Ls(a, p) > Ls(b, p) ie adding more constraints cannot decrease the semantic loss. Semantic Equivalence  If two sentences are logically equivalent, their semantic loss is the same. Identity  For any given sentence a, its representation as a sentence is equivalent to its representation as a deterministic vector ie writing the “one-hot” constraint as a boolean expression is equivalent to a one-hot vector. Satisfaction  If p entails the sentence a then Ls(a, p) = 0. Label-literal correspondence  When the constraint is defined in terms of a single variable, it can be interpreted as the supervised label. Hence the semantic loss in case of a single variable should be equivalent to the cross-entropy loss. Truth  The semantic loss of a true sentence is 0  Non-negativity  Semantic loss should always be non-negative. Probabilities of variables that are not part of the constraint, do not affect the semantic loss. It can be shown that the semantic loss function satisfies all these axioms (and the other axioms specified in the paper) and is the only function to do so, up to a multiplicative constant. Experimental Evaluation  Semantic Loss is used in the semi-supervised setting for Permuted MNIST, Fashion MNIST and CIFAR-10. The key takeaway is that using semantic loss improves the performance of the state-of-the-art models for Fashion MNIST and CIFAR-10. One downside is that the effectiveness of the semantic loss in this type of constraint strongly depends on the performance of the underlying model. Further, the semantic loss does not improve the performance in case of fully supervised scenario. Further experiments are performed to evaluate the performance of the semantic loss on complex constraints. Since these tasks aim to highlight the effect of using semantic loss, only simple models (MLPs) are evaluated. Tractability of Semantic Loss  The semantic loss is similar to the automated reasoning task called as weight model counting (wmc). Circuit compiler techniques can be used to compute wmc while allowing backpropagation. Notes  The proposed idea is simple and intuitive and the results on semi-supervised classification task are quite good. It would be interesting to extend and scale this method for more complex constraints.", "pdf_url": "https://arxiv.org/pdf/1711.11157", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/a-semantic-loss-function-for-deep-learning-with-symbolic-knowledge.json"}
{"id": "35247822", "bin": "700_800", "summary_sentences": ["Problem: Building an expressive, tractable and scalable image model which can be used in downstream tasks like image generation, reconstruction, compression etc.", "Model  Scan the image, one row at a time and one pixel at a time (within each row).", "Given the scanned content, predict the distribution over the possible values for the next pixel.", "Joint distribution over the pixel values is factorised into a product of conditional distributions thus causing the problem as a sequence problem.", "Parameters used in prediction are shared across all the pixel positions.", "Since each pixel is jointly determined by 3 values (3 colour channels), each channel may be conditioned on other channels as well.", "Pixel as discrete value  The conditional distributions are multinomial (with channel variable taking 1 of 256 discrete values).", "This discrete representation is simpler and easier to learn.", "Pixel RNN  Row LSTM  Undirectional layer that processed image row by row.", "Uses one-dimensional convolution (kernel of size kx1, k>=3).", "Refer image 2 in the paper .", "Weight sharing in convolution ensures translation invariance of computed feature along each row.", "For LSTM, the input-to-state component is computed for the entire 2-d input map and then is masked to include only the valid context.", "For equations related to state-to-state component, refer to equation 3 in the paper  Diagonal BiLSTM  Bidirectional layer that processes the image in the diagonal fashion.", "Input map skewed by offsetting each row of the image by one position with respect to the previous row.", "Refer image 3 in the paper  For both directions, the input-to-state component is a 1 x 1 convolution while the state-to-state recurrent component is computed with column wise convolution using kernel size 2x1.", "Kernel size of 2x1 processes minimal information yielding a highly non-linear computation.", "Output map is skewed back by removing the offset positions.", "To prevent layers from seeing further pixels, the right output map is shifted down by one row and added to left output map.", "Residual Connections  Residual connections (or skip connections) are used to increase convergence speed and to propagate signals more explicitly.", "Refer image 4 in the paper  Masked Convolutions  Masks are used to enforce certain restrictions on the connections in the network (eg when predicting values for R channel, values of B channel can not be used).", "Mask A is applied to first convolution layer and restricts connections to only those neighbouring pixels and colour channels that have already been seen.", "Mask B is applied to all subsequent input-to-state convolution transactions and allows connections from a colour channel to itself.", "Refer image 4 in the paper  PixelCNN  Uses multiple convolution layers that preserve spatial resolution.", "Makes receptive field large but not unbounded.", "Mask used to avoid seeing the future context.", "Faster that PixelRNN at training or evaluation time (as convolutions can be parallelized easily).", "Multi-Scale PixelRNN  Composed of one unconditional PixelRNN and multiple conditional PixelRNNs.", "Unconditional network generates a smaller s x s image which is fed as input to the conditional PixelRNN.", "(n is a multiple of s)  Conditional PixelRNN is a standard PixelRNN with layers biased with an upsampled version of the s x s image.", "For upsampling, a convolution network with deconvolution layers constructs an enlarged feature map of size c x n x n.  For biasing, the c x n x n map is mapped to 4hxnxn map (using 1x1 unmasked convolution) and added to input-to-state map.", "Training and Evaluation  Pixel values are dequantized using real-valued noise and log likelihood of continuous and discrete models are compared.", "Update rule - RMSProp  Batch size - 16 for MNIST and CIFAR 10 and 32(or 64) for IMAGENET.", "Residual connections are as effective as Skip connections, in fact, the 2 can be used together as well.", "PixelRNN outperforms other models for Binary MNIST and CIFAR10.", "For CIFAR10, Diagonal BiLSTM > Row LSTM > PixelCNN.", "This is also the order of receptive field for the 3 architectures and the observation underlines the importance of having a large receptive field.", "The paper also provides new benchmarks for generative image modelling on IMAGENET dataset."], "summary_text": "Problem: Building an expressive, tractable and scalable image model which can be used in downstream tasks like image generation, reconstruction, compression etc. Model  Scan the image, one row at a time and one pixel at a time (within each row). Given the scanned content, predict the distribution over the possible values for the next pixel. Joint distribution over the pixel values is factorised into a product of conditional distributions thus causing the problem as a sequence problem. Parameters used in prediction are shared across all the pixel positions. Since each pixel is jointly determined by 3 values (3 colour channels), each channel may be conditioned on other channels as well. Pixel as discrete value  The conditional distributions are multinomial (with channel variable taking 1 of 256 discrete values). This discrete representation is simpler and easier to learn. Pixel RNN  Row LSTM  Undirectional layer that processed image row by row. Uses one-dimensional convolution (kernel of size kx1, k>=3). Refer image 2 in the paper . Weight sharing in convolution ensures translation invariance of computed feature along each row. For LSTM, the input-to-state component is computed for the entire 2-d input map and then is masked to include only the valid context. For equations related to state-to-state component, refer to equation 3 in the paper  Diagonal BiLSTM  Bidirectional layer that processes the image in the diagonal fashion. Input map skewed by offsetting each row of the image by one position with respect to the previous row. Refer image 3 in the paper  For both directions, the input-to-state component is a 1 x 1 convolution while the state-to-state recurrent component is computed with column wise convolution using kernel size 2x1. Kernel size of 2x1 processes minimal information yielding a highly non-linear computation. Output map is skewed back by removing the offset positions. To prevent layers from seeing further pixels, the right output map is shifted down by one row and added to left output map. Residual Connections  Residual connections (or skip connections) are used to increase convergence speed and to propagate signals more explicitly. Refer image 4 in the paper  Masked Convolutions  Masks are used to enforce certain restrictions on the connections in the network (eg when predicting values for R channel, values of B channel can not be used). Mask A is applied to first convolution layer and restricts connections to only those neighbouring pixels and colour channels that have already been seen. Mask B is applied to all subsequent input-to-state convolution transactions and allows connections from a colour channel to itself. Refer image 4 in the paper  PixelCNN  Uses multiple convolution layers that preserve spatial resolution. Makes receptive field large but not unbounded. Mask used to avoid seeing the future context. Faster that PixelRNN at training or evaluation time (as convolutions can be parallelized easily). Multi-Scale PixelRNN  Composed of one unconditional PixelRNN and multiple conditional PixelRNNs. Unconditional network generates a smaller s x s image which is fed as input to the conditional PixelRNN. (n is a multiple of s)  Conditional PixelRNN is a standard PixelRNN with layers biased with an upsampled version of the s x s image. For upsampling, a convolution network with deconvolution layers constructs an enlarged feature map of size c x n x n.  For biasing, the c x n x n map is mapped to 4hxnxn map (using 1x1 unmasked convolution) and added to input-to-state map. Training and Evaluation  Pixel values are dequantized using real-valued noise and log likelihood of continuous and discrete models are compared. Update rule - RMSProp  Batch size - 16 for MNIST and CIFAR 10 and 32(or 64) for IMAGENET. Residual connections are as effective as Skip connections, in fact, the 2 can be used together as well. PixelRNN outperforms other models for Binary MNIST and CIFAR10. For CIFAR10, Diagonal BiLSTM > Row LSTM > PixelCNN. This is also the order of receptive field for the 3 architectures and the observation underlines the importance of having a large receptive field. The paper also provides new benchmarks for generative image modelling on IMAGENET dataset.", "pdf_url": "https://arxiv.org/pdf/1601.06759", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/41ebd5ba0e0fc0f49d7836e30891a7.json"}
{"id": "72922440", "bin": "700_800", "summary_sentences": ["What  They suggest a model (\"YOLO\") to detect bounding boxes in images.", "In comparison to Faster R-CNN, this model is faster but less accurate.", "How  Architecture  Input are images with a resolution of 448x448.", "Output are S*S*(B*5 + C) values (per image).", "S is the grid size (default value: 7).", "Each image is split up into S*S cells.", "B is the number of \"tested\" bounding box shapes at each cell (default value: 2).", "So at each cell, the network might try one large and one small bounding box.", "The network predicts additionally for each such tested bounding box 5 values.", "These cover the exact position (x, y) and scale (height, width) of the bounding box as well as a confidence value.", "They allow the network to fine tune the bounding box shape and reject it, e.g. if there is no object in the grid cell.", "The confidence value is zero if there is no object in the grid cell and otherwise matches the IoU between predicted and true bounding box.", "C is the number of classes in the dataset (e.g. 20 in Pascal VOC).", "For each grid cell, the model decides once to which of the C objects the cell belongs.", "Rough overview of their outputs:  In contrast to Faster R-CNN, their model does not use a separate region proposal network (RPN).", "Per bounding box they actually predict the square root of height and width instead of the raw values.", "That is supposed to result in similar errors/losses for small and big bounding boxes.", "They use a total of 24 convolutional layers and 2 fully connected layers.", "Some of these convolutional layers are 1x1-convs that halve the number of channels (followed by 3x3s that double them again).", "Overview of the architecture:  They use Leaky ReLUs (alpha=0.1) throughout the network.", "The last layer uses linear activations (apparently even for the class prediction...!?).", "Similarly to Faster R-CNN, they use a non maximum suppression that drops predicted bounding boxes if they are too similar to other predictions.", "Training  They pretrain their network on ImageNet, then finetune on Pascal VOC.", "Loss  They use sum-squared losses (apparently even for the classification, i.e. the C values).", "They dont propagate classification loss (for C) for grid cells that don't contain an object.", "For each grid grid cell they \"test\" B example shapes of bounding boxes (see above).", "Among these B shapes, they only propagate the bounding box losses (regarding x, y, width, height, confidence) for the shape that has highest IoU with a ground truth bounding box.", "Most grid cells don't contain a bounding box.", "Their confidence values will all be zero, potentialle dominating the total loss.", "To prevent that, the weighting of the confidence values in the loss function is reduced relative to the regression components (x, y, height, width).", "Results  The coarse grid and B=2 setting lead to some problems.", "Namely, small objects are missed and bounding boxes can end up being dropped if they are too close to other bounding boxes.", "The model also has problems with unusual bounding box shapes.", "Overall their accuracy is about 10 percentage points lower than Faster R-CNN with VGG16 (63.4% vs 73.2%, measured in mAP on Pascal VOC 2007).", "They achieve 45fps (22ms/image), compared to 7fps (142ms/image) with Faster R-CNN + VGG16.", "Overview of results on Pascal VOC 2012:  They also suggest a faster variation of their model which reached 145fps (7ms/image) at a further drop of 10 percentage points mAP (to 52.7%).", "A significant part of their error seems to come from badly placed or sized bounding boxes (e.g. too wide or too much to the right).", "They mistake background less often for objects than Fast R-CNN.", "They test combining both models with each other and can improve Fast R-CNN's accuracy by about 2.5 percentage points mAP.", "They test their model on paintings/artwork (Picasso and People-Art datasets) and notice that it generalizes fairly well to that domain.", "Example results (notice the paintings at the top):"], "summary_text": "What  They suggest a model (\"YOLO\") to detect bounding boxes in images. In comparison to Faster R-CNN, this model is faster but less accurate. How  Architecture  Input are images with a resolution of 448x448. Output are S*S*(B*5 + C) values (per image). S is the grid size (default value: 7). Each image is split up into S*S cells. B is the number of \"tested\" bounding box shapes at each cell (default value: 2). So at each cell, the network might try one large and one small bounding box. The network predicts additionally for each such tested bounding box 5 values. These cover the exact position (x, y) and scale (height, width) of the bounding box as well as a confidence value. They allow the network to fine tune the bounding box shape and reject it, e.g. if there is no object in the grid cell. The confidence value is zero if there is no object in the grid cell and otherwise matches the IoU between predicted and true bounding box. C is the number of classes in the dataset (e.g. 20 in Pascal VOC). For each grid cell, the model decides once to which of the C objects the cell belongs. Rough overview of their outputs:  In contrast to Faster R-CNN, their model does not use a separate region proposal network (RPN). Per bounding box they actually predict the square root of height and width instead of the raw values. That is supposed to result in similar errors/losses for small and big bounding boxes. They use a total of 24 convolutional layers and 2 fully connected layers. Some of these convolutional layers are 1x1-convs that halve the number of channels (followed by 3x3s that double them again). Overview of the architecture:  They use Leaky ReLUs (alpha=0.1) throughout the network. The last layer uses linear activations (apparently even for the class prediction...!?). Similarly to Faster R-CNN, they use a non maximum suppression that drops predicted bounding boxes if they are too similar to other predictions. Training  They pretrain their network on ImageNet, then finetune on Pascal VOC. Loss  They use sum-squared losses (apparently even for the classification, i.e. the C values). They dont propagate classification loss (for C) for grid cells that don't contain an object. For each grid grid cell they \"test\" B example shapes of bounding boxes (see above). Among these B shapes, they only propagate the bounding box losses (regarding x, y, width, height, confidence) for the shape that has highest IoU with a ground truth bounding box. Most grid cells don't contain a bounding box. Their confidence values will all be zero, potentialle dominating the total loss. To prevent that, the weighting of the confidence values in the loss function is reduced relative to the regression components (x, y, height, width). Results  The coarse grid and B=2 setting lead to some problems. Namely, small objects are missed and bounding boxes can end up being dropped if they are too close to other bounding boxes. The model also has problems with unusual bounding box shapes. Overall their accuracy is about 10 percentage points lower than Faster R-CNN with VGG16 (63.4% vs 73.2%, measured in mAP on Pascal VOC 2007). They achieve 45fps (22ms/image), compared to 7fps (142ms/image) with Faster R-CNN + VGG16. Overview of results on Pascal VOC 2012:  They also suggest a faster variation of their model which reached 145fps (7ms/image) at a further drop of 10 percentage points mAP (to 52.7%). A significant part of their error seems to come from badly placed or sized bounding boxes (e.g. too wide or too much to the right). They mistake background less often for objects than Fast R-CNN. They test combining both models with each other and can improve Fast R-CNN's accuracy by about 2.5 percentage points mAP. They test their model on paintings/artwork (Picasso and People-Art datasets) and notice that it generalizes fairly well to that domain. Example results (notice the paintings at the top):", "pdf_url": "https://arxiv.org/pdf/1506.02640", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/yolo.json"}
{"id": "44210578", "bin": "700_800", "summary_sentences": ["Overview  Emoji usage has become a new form of social communication, which is important because it can help to improve communication systems, such as chat applications.", "This paper investigates the usage and semantics of emojis over time to analyze seasonal variation of emoji usage.", "In addition, they develop an emoji prediction model based on the time information.", "Contribution  Multiple emoji predictions studies have been carried out in the past (see notable work by Felbo et al., 2017 ), but non have considered temporal information.", "Temporal correlation between emojis and seasonal events are explored and used to disambiguate emoji meanings.", "Example  Consider the leaf clover emoji (), it is usually associated with good luck wishes all year round except in March, where it is mostly used to express event situations related to parties and drinking (due to St. Patrick day).", "Challenges  This study demonstrates that temporal information is useful for emoji prediction even for emojis that are not associated with time ( and ❤️).", "Emojis are innately subjective, which is why it is difficult to analyze their semantic meaning.", "Dataset  Twitter is used to collect a 100 million US tweets corpus and organized as follows:  Seasonal Emoji Dataset — data is divided into four subsets by seasons: Spring, Summer, Autumn, and Winter (see figure below)  Emoji Prediction Dataset — data is reduced to tweets that only contain one frequent emoji (emoji must belong to the 300 frequent emojis)  Seasonal Emoji Semantic and Usage  Skip-gram word embedding models are trained using the four subsets of the seasonal datasets.", "These models provide information that basically helps to describe emojis in terms of their semantic similarity to each other.", "(See paper for more details)  By comparing the top 10 emojis associated to each emoji in the embedding space, it was discovered that emojis related to music, animal, sweets, and emotions were not influenced by seasonality (e.g., , , , , , ).", "This means that these emojis preserved meaning across seasons.", "In contrast, sport-related emojis (e.g., , ) varied in meaning across seasons, probably due to the high-peak seasons when sports are played.", "Another interesting emoji related to school (), changed meaning across seasons; during Spring it was associated with party emojis, and during Autumn it was associated with school-related emojis.", "Check out the top 10 associated emojis per season for the pine emoji () in the figure below — very season-dependent don’t you think?", "Can you guess why?", "(hint: outdoors vs Christmas).", "(See paper for tons of interesting findings)  Emoji Prediction  The second dataset, which includes 300 emoji classes and 900,000 tweets total (3,000 tweets per class), is used for emoji prediction.", "The architecture of the emoji prediction model is as follows: character embeddings, word embeddings, and data embeddings are combined through both an early fusion approach and a late fusion approach.", "This produces two models (Early and Late).", "A third model is trained (W/O) which completely ignores the date embeddings.", "(See paper to find out how these embeddings are constructed)  Results  Precision, Recall, and F1 scores are reported for all models in the table below.", "We can observe that by combining time information using early fusion, the Early model outperforms the other models.", "The emojis that had higher gains in F1 score (without date vs. early date) are presented in the table below.", "You can definitely observe that many emojis are season-specific (e.g., , ) and thus benefit from the date embeddings.", "Even emojis that are not associated to time (e.g., , ❤️, ) benefit from the temporal information.", "Conclusion & Future Work  A multimodal architecture was proposed to conduct emoji prediction based on deep neural networks.", "More analysis on the emoji semantics and usage over specific time of the day or week may be able to help improve the date embeddings and the overall predictive models.", "This work has a lot of room for improvement and it could be a very interesting topic to combine with emotion recognition, event detection, and computational health studies.", "References  Ref:  [url]"], "summary_text": "Overview  Emoji usage has become a new form of social communication, which is important because it can help to improve communication systems, such as chat applications. This paper investigates the usage and semantics of emojis over time to analyze seasonal variation of emoji usage. In addition, they develop an emoji prediction model based on the time information. Contribution  Multiple emoji predictions studies have been carried out in the past (see notable work by Felbo et al., 2017 ), but non have considered temporal information. Temporal correlation between emojis and seasonal events are explored and used to disambiguate emoji meanings. Example  Consider the leaf clover emoji (), it is usually associated with good luck wishes all year round except in March, where it is mostly used to express event situations related to parties and drinking (due to St. Patrick day). Challenges  This study demonstrates that temporal information is useful for emoji prediction even for emojis that are not associated with time ( and ❤️). Emojis are innately subjective, which is why it is difficult to analyze their semantic meaning. Dataset  Twitter is used to collect a 100 million US tweets corpus and organized as follows:  Seasonal Emoji Dataset — data is divided into four subsets by seasons: Spring, Summer, Autumn, and Winter (see figure below)  Emoji Prediction Dataset — data is reduced to tweets that only contain one frequent emoji (emoji must belong to the 300 frequent emojis)  Seasonal Emoji Semantic and Usage  Skip-gram word embedding models are trained using the four subsets of the seasonal datasets. These models provide information that basically helps to describe emojis in terms of their semantic similarity to each other. (See paper for more details)  By comparing the top 10 emojis associated to each emoji in the embedding space, it was discovered that emojis related to music, animal, sweets, and emotions were not influenced by seasonality (e.g., , , , , , ). This means that these emojis preserved meaning across seasons. In contrast, sport-related emojis (e.g., , ) varied in meaning across seasons, probably due to the high-peak seasons when sports are played. Another interesting emoji related to school (), changed meaning across seasons; during Spring it was associated with party emojis, and during Autumn it was associated with school-related emojis. Check out the top 10 associated emojis per season for the pine emoji () in the figure below — very season-dependent don’t you think? Can you guess why? (hint: outdoors vs Christmas). (See paper for tons of interesting findings)  Emoji Prediction  The second dataset, which includes 300 emoji classes and 900,000 tweets total (3,000 tweets per class), is used for emoji prediction. The architecture of the emoji prediction model is as follows: character embeddings, word embeddings, and data embeddings are combined through both an early fusion approach and a late fusion approach. This produces two models (Early and Late). A third model is trained (W/O) which completely ignores the date embeddings. (See paper to find out how these embeddings are constructed)  Results  Precision, Recall, and F1 scores are reported for all models in the table below. We can observe that by combining time information using early fusion, the Early model outperforms the other models. The emojis that had higher gains in F1 score (without date vs. early date) are presented in the table below. You can definitely observe that many emojis are season-specific (e.g., , ) and thus benefit from the date embeddings. Even emojis that are not associated to time (e.g., , ❤️, ) benefit from the temporal information. Conclusion & Future Work  A multimodal architecture was proposed to conduct emoji prediction based on deep neural networks. More analysis on the emoji semantics and usage over specific time of the day or week may be able to help improve the date embeddings and the overall predictive models. This work has a lot of room for improvement and it could be a very interesting topic to combine with emotion recognition, event detection, and computational health studies. References  Ref:  [url]", "pdf_url": "https://arxiv.org/pdf/1805.00731", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/deep-learning-and-time-to-predict-emojis-4a6256c16475.json"}
{"id": "7912651", "bin": "700_800", "summary_sentences": ["Discretized Streams: Fault Tolerant Stream Computing at Scale – Zaharia et al. 2013  This is the Spark Streaming paper, and it sets out very clearly the problem that Discretized Streams were designed to solve: dealing effectively with faults and stragglers when processing streams in large clusters.", "This is hard to do in the traditional continuous operator model in which long-running operators receive and process each update in turn, maintaining their own internal state along the way.", "Specifically, given the continuous operator model, systems perform recovery through two approaches: replication, where there are two copies of each node, or upstream backup, where nodes buffer sent messages and replay them to a new copy of a failed node.", "Neither approach is attractive in large clusters: replication costs 2× the hardware, while upstream backup takes a long time to recover, as the whole system must wait for a new node to serially rebuild the failed node’s state by rerunning data through an operator.", "In addition, neither approach handles stragglers: in upstream backup, a straggler must be treated as a failure, incurring a costly recovery step, while replicated systems use synchronization protocols like Flux to coordinate replicas, so a straggler will slow down both replicas.", "Discretized Streams (D-Streams) work differently: instead of managing long-lived operators they structure a streaming computation as a series of stateless deterministic batch computations.", "Unlike traditional batch systems that write intermediate state to disk though, D-Streams keep data in memory using RDDs:  we use a data structure called Resilient Distributed Datasets (RDDs), which keeps data in memory and can recover it without replication by tracking the lineage graph of operations that were used to build it.", "With RDDs, we show that we can attain sub-second end-to-end latencies.", "We believe that this is sufficient for many real-world big data applications, where the timescale of the events tracked (e.g., trends in social media) is much higher.", "It’s worth stressing that D-Streams / Spark Streaming targets sub-second latency, but not latency in the few hundred milliseconds or below range.", "The key design is to provide both sub-second latency and sub-second recovery from faults and stragglers.", "Examples of use cases well matched to these requirements include site activity statistics, cluster monitoring, and tweet spam detection.", "In evaluation the authors showed per-node throughput comparable to commercial streaming databases, combined with linear scalability out to 100 nodes processing over 60M records/second.", "When a node fails, it recomputes the RDD partitions that were on it by re-running the tasks that built them from the original input data stored reliably in the cluster.", "The system also periodically checkpoints state RDDs (e.g., by asynchronously replicating every tenth RDD) to prevent infinite recomputation, but this does not need to happen for all data, because recovery is often fast: the lost partitions can be recomputed in parallel on separate nodes.", "In a similar way, if a node straggles, we can speculatively execute copies of its tasks on other nodes, because they will produce the same result.", "We note that the parallelism usable for recovery in D-Streams is higher than in upstream backup, even if one ran multiple operators per node.", "D-Streams expose parallelism across both partitions of an operator and time.", "The micro-batching in D-Streams partitions input records based on their arrival time.", "If instead you want to group records based on an external time, you can either add some delay (slack period) to wait for them al to come in, or handle late arrivals explicity in your application code.", "On top of the core model, the authors show it is possible to build computations that span several intervals including windowing, aggregation, and state tracking.", "The D-Streams model is implemented in Spark Streaming, which led to a number of improvements in the core of Spark itself as well.", "These optimizations also improved Spark’s performance in the batch case by 2x.", "Finally, because D-Streams use the same execution model as batch platforms, they compose seamlessly with batch and interactive queries.", "We used this capability in Spark Streaming to let users combine these models in powerful ways, and showed how it can add rich features to two real applications.", "(Those applications were video distribution monitoring, and crowdsourced traffic estimation)."], "summary_text": "Discretized Streams: Fault Tolerant Stream Computing at Scale – Zaharia et al. 2013  This is the Spark Streaming paper, and it sets out very clearly the problem that Discretized Streams were designed to solve: dealing effectively with faults and stragglers when processing streams in large clusters. This is hard to do in the traditional continuous operator model in which long-running operators receive and process each update in turn, maintaining their own internal state along the way. Specifically, given the continuous operator model, systems perform recovery through two approaches: replication, where there are two copies of each node, or upstream backup, where nodes buffer sent messages and replay them to a new copy of a failed node. Neither approach is attractive in large clusters: replication costs 2× the hardware, while upstream backup takes a long time to recover, as the whole system must wait for a new node to serially rebuild the failed node’s state by rerunning data through an operator. In addition, neither approach handles stragglers: in upstream backup, a straggler must be treated as a failure, incurring a costly recovery step, while replicated systems use synchronization protocols like Flux to coordinate replicas, so a straggler will slow down both replicas. Discretized Streams (D-Streams) work differently: instead of managing long-lived operators they structure a streaming computation as a series of stateless deterministic batch computations. Unlike traditional batch systems that write intermediate state to disk though, D-Streams keep data in memory using RDDs:  we use a data structure called Resilient Distributed Datasets (RDDs), which keeps data in memory and can recover it without replication by tracking the lineage graph of operations that were used to build it. With RDDs, we show that we can attain sub-second end-to-end latencies. We believe that this is sufficient for many real-world big data applications, where the timescale of the events tracked (e.g., trends in social media) is much higher. It’s worth stressing that D-Streams / Spark Streaming targets sub-second latency, but not latency in the few hundred milliseconds or below range. The key design is to provide both sub-second latency and sub-second recovery from faults and stragglers. Examples of use cases well matched to these requirements include site activity statistics, cluster monitoring, and tweet spam detection. In evaluation the authors showed per-node throughput comparable to commercial streaming databases, combined with linear scalability out to 100 nodes processing over 60M records/second. When a node fails, it recomputes the RDD partitions that were on it by re-running the tasks that built them from the original input data stored reliably in the cluster. The system also periodically checkpoints state RDDs (e.g., by asynchronously replicating every tenth RDD) to prevent infinite recomputation, but this does not need to happen for all data, because recovery is often fast: the lost partitions can be recomputed in parallel on separate nodes. In a similar way, if a node straggles, we can speculatively execute copies of its tasks on other nodes, because they will produce the same result. We note that the parallelism usable for recovery in D-Streams is higher than in upstream backup, even if one ran multiple operators per node. D-Streams expose parallelism across both partitions of an operator and time. The micro-batching in D-Streams partitions input records based on their arrival time. If instead you want to group records based on an external time, you can either add some delay (slack period) to wait for them al to come in, or handle late arrivals explicity in your application code. On top of the core model, the authors show it is possible to build computations that span several intervals including windowing, aggregation, and state tracking. The D-Streams model is implemented in Spark Streaming, which led to a number of improvements in the core of Spark itself as well. These optimizations also improved Spark’s performance in the batch case by 2x. Finally, because D-Streams use the same execution model as batch platforms, they compose seamlessly with batch and interactive queries. We used this capability in Spark Streaming to let users combine these models in powerful ways, and showed how it can add rich features to two real applications. (Those applications were video distribution monitoring, and crowdsourced traffic estimation).", "pdf_url": "https://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/discretized-streams-fault-tolerant-stream-computing-at-scale.json"}
{"id": "81057283", "bin": "700_800", "summary_sentences": ["Adversarial patch Brown, Mané et al., arXiv 2017  Today’s paper choice is short and sweet, but thought provoking nonetheless.", "To a man with a hammer (sticker), everything looks like a hammer.", "We’ve seen a number of examples of adversarial attacks on image recognition systems, where the perturbations are designed to be subtle and hard to detect.", "But what if you don’t mind the alteration being obvious to the human eye?", "Brown et al. show how to create stickers (image patches) that can be added to a scene, and force a classifier into reporting a class of the attacker’s choosing.", "We present a method to create universal, robust, targeted adversarial image patches in the real world.", "The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classifier to output any target class.", "Here’s an example of a printable sticker designed to cause classification as a toaster.", "And here’s the sticker placed on a table next to a banana.", "The sticker causes the previously reported class banana (97% confidence) to become toaster with 99% confidence.", "Because this patch is scene-independent, it allows attackers to create a physical-world attack without prior knowledge of the lighting conditions, camera angle, type of classifier being attacked, or even the other items within the scene… Additionally, because the attack uses a large perturbation, the existing defense techniques which focus on defending against small perturbations may not be robust to larger perturbations such as these.", "Why does it work?", "An image may contain several items, but a classifier outputting only one target output has to determine which is the most ‘salient’ item in the frame.", "The adversarial patch exploits this by producing inputs that scream out “I’m an X” for some value of X.", "Generating adversarial patches  The patch application operator , A, takes an input a patch p, image x, location l, and transformations t. It first applies the transformations to the patch, and then applies the transformed patch to the image x at location l.  If we’re targeting an output class  , then we can train to optimise the objective function  Where X is a training set of images, T is a distribution over transformations of the patch, and L is a distribution over locations in the image.", "This departs from most prior work on adversarial perturbations in the fact that this perturbation is universal in that it works for any background.", "Experimental results  The experiment compares four different variations of patches, targeting recognition of an image as a toaster.", "Five different ImageNet models are used: inceptionv3, resnet50, xception, VGG16, and VGG19.", "The control is a simple picture of a toaster!", "The white box single trains a patch and evaluates it on a single model.", "The white box ensemble jointly trains a single patch across all five models, and is evaluated by averaging the win rate across them.", "The blackbox attack jointly trains a single patch across four of the models, and then evaluates against the fifth (held out) model.", "The white box models are very successful even when only 10% of the overall image size.", "They clearly capture the essence of toaster much better than the picture of an actual toaster!!", "The adversarial patches above reveal their target class to a human observer.", "The authors also experimented with disguising the patches using a tie-dye pattern and also applying a peace sign mask.", "These look nothing like a toaster to me anymore, but still do pretty well at larger relative sizes:  Many ML models operate without human validation of every input and thus malicious attackers may not be concerned with the imperceptibility of their attacks.", "Even if humans are able to notice these patches, they may not understand the intent of the patch and instead view it as a form of art.", "This work shows that focusing only on defending against small perturbations is insufficient, as large, local perturbations can also break classifiers."], "summary_text": "Adversarial patch Brown, Mané et al., arXiv 2017  Today’s paper choice is short and sweet, but thought provoking nonetheless. To a man with a hammer (sticker), everything looks like a hammer. We’ve seen a number of examples of adversarial attacks on image recognition systems, where the perturbations are designed to be subtle and hard to detect. But what if you don’t mind the alteration being obvious to the human eye? Brown et al. show how to create stickers (image patches) that can be added to a scene, and force a classifier into reporting a class of the attacker’s choosing. We present a method to create universal, robust, targeted adversarial image patches in the real world. The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classifier to output any target class. Here’s an example of a printable sticker designed to cause classification as a toaster. And here’s the sticker placed on a table next to a banana. The sticker causes the previously reported class banana (97% confidence) to become toaster with 99% confidence. Because this patch is scene-independent, it allows attackers to create a physical-world attack without prior knowledge of the lighting conditions, camera angle, type of classifier being attacked, or even the other items within the scene… Additionally, because the attack uses a large perturbation, the existing defense techniques which focus on defending against small perturbations may not be robust to larger perturbations such as these. Why does it work? An image may contain several items, but a classifier outputting only one target output has to determine which is the most ‘salient’ item in the frame. The adversarial patch exploits this by producing inputs that scream out “I’m an X” for some value of X. Generating adversarial patches  The patch application operator , A, takes an input a patch p, image x, location l, and transformations t. It first applies the transformations to the patch, and then applies the transformed patch to the image x at location l.  If we’re targeting an output class  , then we can train to optimise the objective function  Where X is a training set of images, T is a distribution over transformations of the patch, and L is a distribution over locations in the image. This departs from most prior work on adversarial perturbations in the fact that this perturbation is universal in that it works for any background. Experimental results  The experiment compares four different variations of patches, targeting recognition of an image as a toaster. Five different ImageNet models are used: inceptionv3, resnet50, xception, VGG16, and VGG19. The control is a simple picture of a toaster! The white box single trains a patch and evaluates it on a single model. The white box ensemble jointly trains a single patch across all five models, and is evaluated by averaging the win rate across them. The blackbox attack jointly trains a single patch across four of the models, and then evaluates against the fifth (held out) model. The white box models are very successful even when only 10% of the overall image size. They clearly capture the essence of toaster much better than the picture of an actual toaster!! The adversarial patches above reveal their target class to a human observer. The authors also experimented with disguising the patches using a tie-dye pattern and also applying a peace sign mask. These look nothing like a toaster to me anymore, but still do pretty well at larger relative sizes:  Many ML models operate without human validation of every input and thus malicious attackers may not be concerned with the imperceptibility of their attacks. Even if humans are able to notice these patches, they may not understand the intent of the patch and instead view it as a form of art. This work shows that focusing only on defending against small perturbations is insufficient, as large, local perturbations can also break classifiers.", "pdf_url": "https://arxiv.org/pdf/1712.09665", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/adversarial-patch.json"}
{"id": "44148071", "bin": "700_800", "summary_sentences": ["This article presents a roadmap for future progress in developing machines that can learn and “think” like humans.", "The authors outline a number of ideas that they believe are crucial to making progress in artificial intelligence.", "A short-term goal of the authors is to improve today’s machine learning techniques so that complex concepts can be learned quickly with only small amounts of data.", "Currently, our algorithms require ungainly amounts of data and computation to perform at acceptable levels.", "The authors argue that a marriage of two distinct research areas in artificial intelligence, that of statistical pattern recognition and of model-building, will produce a promising avenue for new advances.", "Specifically, the authors suggest that learning as a form of explaining observed data by the construction of causal models of the world can be accomplished by deep neural networks.", "The first causal model that is discussed is dubbed intuitive physics.", "That is, a machine with an internal model of the physics of its environment is able to learn more quickly than without.", "This is because the machine will not need to re-learn basic physics principles when attempting to learn new tasks that require it.", "The second causal model is intuitive psychology.", "A learning agent that assumes that other agents in its environment are rational and have their own goals and beliefs can try to infer what these may be.", "Having knowledge of the intentions of other agents in the environment allow the learning agent to achieve its own goals in a much more efficient way.", "Compositionality  Compositionality is the classic idea that new representations can be constructed through the combination of primitive elements.", "See Lake et.", "all 2015 for a discussion on the use of compositionality to generate novel handwritten characters.", "This is central to training agents to learn models of complex symbolic concepts.", "Learning-to-Learn  Learning-to-learn is closely related to the concept of “transfer-learning” and “representation-learning” in machine learning.", "Machines that learn like humans will need to be able to learn rich, informative priors and models of the world that can be applied to a variety of tasks.", "Systems should be able to learn to do new tasks as flexibly and rapidly as humans do.", "The authors suggest compositionality and the use of causal models to augment the current deep-learning research in transfer-learning.", "Thinking fast  There are still a lot of challenges concerning intractability of certain methods and the amount of time necessary for doing optimization over high-dimensional potentially non-convex parameter spaces.", "Approximate inference is an area of research that aims to address these challenges.", "Monte Carlo methods are promising for many problems, but runs into problems when the hypothesis space is vast.", "Deep neural networks may potentially be used for performing probabilistic inference in a generative model or a probabilistic program.", "Dangers of model-free RL  There is evidence that the brain uses a form of model-free reinforcement learning to accomplish certain tasks.", "However, there is also evidence that the brain has a model-based learning system.", "Shifting between model-free and model-based learning methods can benefit from taking advantage of the causal-model/compositionality discussed above.", "Current model-free methods, as seen in DQN, Google DeepMind’s Atari agent, are severely limited when it comes to generalizing beyond a certain amount.", "For example, the DQN agent would need significant re-training to be able to play a game whose screen was locked to a different resolution, and it would fail to play a game that required a working memory of game states/frames from the past (i.e. the agent would be missing causal information that coudl explain what is happening at the current state of the game).", "Strengths  The authors address potential counter-arguments in Section 5.", "They are: “Comparing the learning speeds of humans and neural networks on specific tasks is not meaningful, because humans have extensive prior experience”, “Biological plausibility suggests theories of intelligence should start with networks”, “Language is essential for human intelligence.", "Why is it not more prominent here?”  Weaknesses  There isn’t a clear definition of “intelligence” stated in the article.", "The closest is the authors’ statement that the goal of those who wish to create machines that learn like humans (e.g. display human-level intelligence) should be to devise ways to carry out learning from far less data and to generalize in richer and more flexible ways."], "summary_text": "This article presents a roadmap for future progress in developing machines that can learn and “think” like humans. The authors outline a number of ideas that they believe are crucial to making progress in artificial intelligence. A short-term goal of the authors is to improve today’s machine learning techniques so that complex concepts can be learned quickly with only small amounts of data. Currently, our algorithms require ungainly amounts of data and computation to perform at acceptable levels. The authors argue that a marriage of two distinct research areas in artificial intelligence, that of statistical pattern recognition and of model-building, will produce a promising avenue for new advances. Specifically, the authors suggest that learning as a form of explaining observed data by the construction of causal models of the world can be accomplished by deep neural networks. The first causal model that is discussed is dubbed intuitive physics. That is, a machine with an internal model of the physics of its environment is able to learn more quickly than without. This is because the machine will not need to re-learn basic physics principles when attempting to learn new tasks that require it. The second causal model is intuitive psychology. A learning agent that assumes that other agents in its environment are rational and have their own goals and beliefs can try to infer what these may be. Having knowledge of the intentions of other agents in the environment allow the learning agent to achieve its own goals in a much more efficient way. Compositionality  Compositionality is the classic idea that new representations can be constructed through the combination of primitive elements. See Lake et. all 2015 for a discussion on the use of compositionality to generate novel handwritten characters. This is central to training agents to learn models of complex symbolic concepts. Learning-to-Learn  Learning-to-learn is closely related to the concept of “transfer-learning” and “representation-learning” in machine learning. Machines that learn like humans will need to be able to learn rich, informative priors and models of the world that can be applied to a variety of tasks. Systems should be able to learn to do new tasks as flexibly and rapidly as humans do. The authors suggest compositionality and the use of causal models to augment the current deep-learning research in transfer-learning. Thinking fast  There are still a lot of challenges concerning intractability of certain methods and the amount of time necessary for doing optimization over high-dimensional potentially non-convex parameter spaces. Approximate inference is an area of research that aims to address these challenges. Monte Carlo methods are promising for many problems, but runs into problems when the hypothesis space is vast. Deep neural networks may potentially be used for performing probabilistic inference in a generative model or a probabilistic program. Dangers of model-free RL  There is evidence that the brain uses a form of model-free reinforcement learning to accomplish certain tasks. However, there is also evidence that the brain has a model-based learning system. Shifting between model-free and model-based learning methods can benefit from taking advantage of the causal-model/compositionality discussed above. Current model-free methods, as seen in DQN, Google DeepMind’s Atari agent, are severely limited when it comes to generalizing beyond a certain amount. For example, the DQN agent would need significant re-training to be able to play a game whose screen was locked to a different resolution, and it would fail to play a game that required a working memory of game states/frames from the past (i.e. the agent would be missing causal information that coudl explain what is happening at the current state of the game). Strengths  The authors address potential counter-arguments in Section 5. They are: “Comparing the learning speeds of humans and neural networks on specific tasks is not meaningful, because humans have extensive prior experience”, “Biological plausibility suggests theories of intelligence should start with networks”, “Language is essential for human intelligence. Why is it not more prominent here?”  Weaknesses  There isn’t a clear definition of “intelligence” stated in the article. The closest is the authors’ statement that the goal of those who wish to create machines that learn like humans (e.g. display human-level intelligence) should be to devise ways to carry out learning from far less data and to generalize in richer and more flexible ways.", "pdf_url": "http://arxiv.org/pdf/1604.00289v2.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/learning-to-think.json"}
{"id": "91637218", "bin": "700_800", "summary_sentences": ["Dynamic Memory Networks (DMN) is a neural network based general framework that can be used for tasks like sequence tagging, classification, sequence to sequence and question answering requiring transitive reasoning.", "The basic idea is that all these tasks can be modelled as question answering task in general and a common architecture could be used for solving them.", "Architecture  DMN takes as input a document(sentence, story, article etc) and a question which is to be answered given the document.", "Input Module  Concatenate all the sentences (or facts) in the document and encode them by feeding the word embeddings of the text to a GRU.", "Each time a sentence ends, extract the hidden representation of the GRU till that point and use as the encoded representation of the sentence.", "Question Module  Similarly, feed the question to a GRU to obtain its representation.", "Episodic Memory Module  Episodic memory consists of an attention mechanism and a recurrent network with which it updates its memory.", "During each iteration, the network generates an episode e by attending over the representation of the sentences, question and the previous memory.", "The episodic memory is updated using the current episode and the previous memory.", "Depending on the amount of supervision available, the network may perform multiple passes.", "eg, in the bAbI dataset, some tasks specify how many passes would be needed and which sentence should be attended to in each pass.", "For others, a fixed number of passes are made.", "Multiple passes allow the network to perform transitive inference.", "Attention Mechanism  Given the input representation c, memory m and question q, produce a scalar score using a 2-layer feedforward network, to use as attention mechanism.", "A separate GRU encodes the input representation and weights it by the attention.", "Final state of the GRU is fed to the answer module.", "Answer Module  Use a GRU (initialized with the final state of the episodic module) and at each timestep, feed it the question vector, last hidden state of the same GRU and the previously predicted output.", "Training  There are two possible losses:  Cross-entropy loss of the predicted answer (all datasets)  Cross-entropy loss of the attention supervision (for datasets like bAbI)  Experiments  Question Answering  bAbI Dataset  For most tasks, DMN either outperforms or performs as good as Memory Networks.", "For tasks like answering with 2 or 3 supporting facts, DMN lags because of limitation of RNN in modelling long sentences.", "Text Classification  Stanford Sentiment Treebank Dataset  DMN outperforms all the baselines for both binary and fine-grained sentiment analysis.", "Sequence Tagging  Wall Street Journal Dataset  DMN archives state of the art accuracy of 97.56%  Observations  Multiple passes help in reasoning tasks but not so much for sentiment/POS tags.", "Attention in the case of 2-iteration DMN is more focused than attention in 1-iteration DMN.", "For 2-iteration DMN, attention in the second iteration focuses only on relevant words and less attention is paid to words that lose their relevance in the context of the entire document.", "Notes  It would be interesting to put some mechanism in place to determine the number of episodes that should be generated before an answer is predicted.", "A naive way would be to predict the answer after each episode and check if the softmax score of the predicted answer is more than a threshold.", "Alternatively, the softmax score and other information could be fed to a Reinforcement Learning (RL) agent which decided if the document should be read again.", "So every time an episode is generated, the state is passed to the RL agent which decides if another iteration should be performed.", "If it decides to predict the answer and correct answer is generated, the agent gets a large +ve reward else a large -ve reward.", "To discourage unnecessary iterations, a small -ve reward could be given everytime the agent decides to perform another iteration."], "summary_text": "Dynamic Memory Networks (DMN) is a neural network based general framework that can be used for tasks like sequence tagging, classification, sequence to sequence and question answering requiring transitive reasoning. The basic idea is that all these tasks can be modelled as question answering task in general and a common architecture could be used for solving them. Architecture  DMN takes as input a document(sentence, story, article etc) and a question which is to be answered given the document. Input Module  Concatenate all the sentences (or facts) in the document and encode them by feeding the word embeddings of the text to a GRU. Each time a sentence ends, extract the hidden representation of the GRU till that point and use as the encoded representation of the sentence. Question Module  Similarly, feed the question to a GRU to obtain its representation. Episodic Memory Module  Episodic memory consists of an attention mechanism and a recurrent network with which it updates its memory. During each iteration, the network generates an episode e by attending over the representation of the sentences, question and the previous memory. The episodic memory is updated using the current episode and the previous memory. Depending on the amount of supervision available, the network may perform multiple passes. eg, in the bAbI dataset, some tasks specify how many passes would be needed and which sentence should be attended to in each pass. For others, a fixed number of passes are made. Multiple passes allow the network to perform transitive inference. Attention Mechanism  Given the input representation c, memory m and question q, produce a scalar score using a 2-layer feedforward network, to use as attention mechanism. A separate GRU encodes the input representation and weights it by the attention. Final state of the GRU is fed to the answer module. Answer Module  Use a GRU (initialized with the final state of the episodic module) and at each timestep, feed it the question vector, last hidden state of the same GRU and the previously predicted output. Training  There are two possible losses:  Cross-entropy loss of the predicted answer (all datasets)  Cross-entropy loss of the attention supervision (for datasets like bAbI)  Experiments  Question Answering  bAbI Dataset  For most tasks, DMN either outperforms or performs as good as Memory Networks. For tasks like answering with 2 or 3 supporting facts, DMN lags because of limitation of RNN in modelling long sentences. Text Classification  Stanford Sentiment Treebank Dataset  DMN outperforms all the baselines for both binary and fine-grained sentiment analysis. Sequence Tagging  Wall Street Journal Dataset  DMN archives state of the art accuracy of 97.56%  Observations  Multiple passes help in reasoning tasks but not so much for sentiment/POS tags. Attention in the case of 2-iteration DMN is more focused than attention in 1-iteration DMN. For 2-iteration DMN, attention in the second iteration focuses only on relevant words and less attention is paid to words that lose their relevance in the context of the entire document. Notes  It would be interesting to put some mechanism in place to determine the number of episodes that should be generated before an answer is predicted. A naive way would be to predict the answer after each episode and check if the softmax score of the predicted answer is more than a threshold. Alternatively, the softmax score and other information could be fed to a Reinforcement Learning (RL) agent which decided if the document should be read again. So every time an episode is generated, the state is passed to the RL agent which decides if another iteration should be performed. If it decides to predict the answer and correct answer is generated, the agent gets a large +ve reward else a large -ve reward. To discourage unnecessary iterations, a small -ve reward could be given everytime the agent decides to perform another iteration.", "pdf_url": "https://arxiv.org/pdf/1506.07285", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/ask-me-anything-dynamic-memory-networks-for-natural-language-processing.json"}
{"id": "69502658", "bin": "700_800", "summary_sentences": ["Identifying impactful service system problems via log analysis He et al., ESEC/FSE’18  If something is going wrong in your system, chances are you’ve got two main sources to help you detect and resolve the issue: logs and metrics.", "You’re unlikely to be able to get to the bottom of a problem using metrics alone (though you might well detect one that way), so that leaves logs as the primary diagnosis tool.", "The online service at Microsoft used as the main case study in the paper produces dozens of Terabytes of logs every day.", "Logs play a crucial role in the diagnosis of modern cloud-based online service systems.", "Clearly, manual problem diagnosis is very time-consuming and error-prone due to the increasing scale and complexity of large-scale systems.", "Log3C analyses logs to look for indications of impactful problems, using correlated KPIs as a guide.", "It finds these needles in the haystack with an average precision of 0.877 and an average recall of 0.883.", "A distributed version of Log3C has been deployed and used in production at Microsoft for several years, both to support a massive online service (we are not told which one), and integrated into “Product B” where it is used as a log analysis engine to analyse tens of billions of log messages every day.", "Log3C greatly reduces engineer’s efforts on manually inspecting the logs and pinpointing root causes of failures.", "Furthermore, fault patterns are also extracted and maintained for analyzing similar problems in the future.", "From the experiences thus gained in log analysis the authors draw the following lessons:  Simple anomaly / outlier detection doesn’t cut it.", "It’s tempting to believe that systems are regular most of the time, and therefore any outliers are problems, but in practice there’s can be a long tail of infrequent (but genuine) user behaviours.", "“Our experiences with the production system reveal that there are indeed many rare user behaviors, which are not real problems.", "A lot of effort could be wasted by examining these false positives.“  It’s not just a single incidence of a problem that’s important, but also the underlying trend in the number of incidences of that problem.", "Are the number of incidences steadily increasing over a period of time?", "That’s of interest.", "Have totally new problems appeared?", "That could be an indication of a buggy deployment.", "Do the number of incidences of problem decrease after a fix, but then settle down to a steady but non-zero level?", "This can indicate an incomplete fix or partial solution.", "To work its magic, Log3C combines both metrics and logs.", "KPIs are used to guide log analysis to focus on periods in time when we have external evidence of a system problem (e.g., the error rate is up).", "This helps to separate the long tail of genuine user behaviours from true problems.", "Then Log3C uses a novel clustering algorithm called Cascading Clusters in order to be able to effectively cluster the massive amounts of log data generated by the system.", "Problems are identified by looking for rare clusters correlated with KPI degradation.", "Clustering is made more difficult because logs are highly imbalanced – the vast majority of log entries are for regular non-problem behaviours.", "High level approach  Log3C consists of four steps: log parsing, sequence vectorization, cascading clustering, and correlation analysis.", "( Enlarge )  …at each time interval, logs are parsed into log events and vectorized into sequence vectors, which are then grouped into multiple clusters through cascading clustering.", "However, we still cannot extrapolate whether a cluster is an impactful problem, which necessitates the use of KPIs.", "Consequently, in step four, we correlate clusters and KPIs over different time intervals to find impactful problems.", "From log messages to sequence vectors  The first step is to turn log messages into log events (on the assumption you’re not logging as events already).", "For each log message such as “HTTP Request URL:  [url]"], "summary_text": "Identifying impactful service system problems via log analysis He et al., ESEC/FSE’18  If something is going wrong in your system, chances are you’ve got two main sources to help you detect and resolve the issue: logs and metrics. You’re unlikely to be able to get to the bottom of a problem using metrics alone (though you might well detect one that way), so that leaves logs as the primary diagnosis tool. The online service at Microsoft used as the main case study in the paper produces dozens of Terabytes of logs every day. Logs play a crucial role in the diagnosis of modern cloud-based online service systems. Clearly, manual problem diagnosis is very time-consuming and error-prone due to the increasing scale and complexity of large-scale systems. Log3C analyses logs to look for indications of impactful problems, using correlated KPIs as a guide. It finds these needles in the haystack with an average precision of 0.877 and an average recall of 0.883. A distributed version of Log3C has been deployed and used in production at Microsoft for several years, both to support a massive online service (we are not told which one), and integrated into “Product B” where it is used as a log analysis engine to analyse tens of billions of log messages every day. Log3C greatly reduces engineer’s efforts on manually inspecting the logs and pinpointing root causes of failures. Furthermore, fault patterns are also extracted and maintained for analyzing similar problems in the future. From the experiences thus gained in log analysis the authors draw the following lessons:  Simple anomaly / outlier detection doesn’t cut it. It’s tempting to believe that systems are regular most of the time, and therefore any outliers are problems, but in practice there’s can be a long tail of infrequent (but genuine) user behaviours. “Our experiences with the production system reveal that there are indeed many rare user behaviors, which are not real problems. A lot of effort could be wasted by examining these false positives.“  It’s not just a single incidence of a problem that’s important, but also the underlying trend in the number of incidences of that problem. Are the number of incidences steadily increasing over a period of time? That’s of interest. Have totally new problems appeared? That could be an indication of a buggy deployment. Do the number of incidences of problem decrease after a fix, but then settle down to a steady but non-zero level? This can indicate an incomplete fix or partial solution. To work its magic, Log3C combines both metrics and logs. KPIs are used to guide log analysis to focus on periods in time when we have external evidence of a system problem (e.g., the error rate is up). This helps to separate the long tail of genuine user behaviours from true problems. Then Log3C uses a novel clustering algorithm called Cascading Clusters in order to be able to effectively cluster the massive amounts of log data generated by the system. Problems are identified by looking for rare clusters correlated with KPI degradation. Clustering is made more difficult because logs are highly imbalanced – the vast majority of log entries are for regular non-problem behaviours. High level approach  Log3C consists of four steps: log parsing, sequence vectorization, cascading clustering, and correlation analysis. ( Enlarge )  …at each time interval, logs are parsed into log events and vectorized into sequence vectors, which are then grouped into multiple clusters through cascading clustering. However, we still cannot extrapolate whether a cluster is an impactful problem, which necessitates the use of KPIs. Consequently, in step four, we correlate clusters and KPIs over different time intervals to find impactful problems. From log messages to sequence vectors  The first step is to turn log messages into log events (on the assumption you’re not logging as events already). For each log message such as “HTTP Request URL:  [url]", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3236024.3236083?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/identifying-impactful-service-system-problems-via-log-analysis.json"}
{"id": "35973587", "bin": "700_800", "summary_sentences": ["What  PixelRNN  PixelRNNs generate new images pixel by pixel (and row by row) via LSTMs (or other RNNs).", "Each pixel is therefore conditioned on the previously generated pixels.", "Training of PixelRNNs is slow due to the RNN-architecture (hard to parallelize).", "Previously PixelCNNs have been suggested, which use masked convolutions during training (instead of RNNs), but their image quality was worse.", "They suggest changes to PixelCNNs that improve the quality of the generated images (while still keeping them faster than RNNs).", "How  PixelRNNs split up the distribution p(image) into many conditional probabilities, one per pixel, each conditioned on all previous pixels: p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1).", "PixelCNNs implement that using convolutions, which are faster to train than RNNs.", "These convolutions uses masked filters, i.e. the center weight and also all weights right and/or below the center pixel are 0 (because they are current/future values and we only want to condition on the past).", "In most generative models, several layers are stacked, ultimately ending in three float values per pixel (RGB images, one value for grayscale images).", "PixelRNNs (including this implementation) traditionally end in a softmax over 255 values per pixel and channel (so 3*255 per RGB pixel).", "The following image shows the application of such a convolution with the softmax output (left) and the mask for a filter (right):  Blind spot  Using the mask on each convolutional filter effectively converts them into non-squared shapes (the green values in the image).", "Advantage: Using such non-squared convolutions prevents future values from leaking into present values.", "Disadvantage: Using such non-squared convolutions creates blind spots, i.e. for each pixel, some past values (diagonally top-right from it) cannot influence the value of that pixel.", "They combine horizontal (1xN) and vertical (Nx1) convolutions to prevent that.", "Gated convolutions  PixelRNNs via LSTMs so far created visually better images than PixelCNNs.", "They assume that one advantage of LSTMs is, that they (also) have multiplicative gates, while stacked convolutional layers only operate with summations.", "They alleviate that problem by adding gates to their convolutions:  Equation: output image = tanh(weights_1 * image) <element-wise product> sigmoid(weights_2 * image)  * is the convolutional operator.", "tanh(weights_1 * image) is a classical convolution with tanh activation function.", "sigmoid(weights_2 * image) are the gate values (0 = gate closed, 1 = gate open).", "weights_1 and weights_2 are learned.", "Conditional PixelCNNs  When generating images, they do not only want to condition the previous values, but also on a laten vector h that describes the image to generate.", "The new image distribution becomes: p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1, h).", "To implement that, they simply modify the previously mentioned gated convolution, adding h to it:  Equation: output image = tanh(weights_1 * image + weights_2 .", "h) <element-wise product> sigmoid(weights_3 * image + weights_4 .", "h)  .", "denotes here the matrix-vector multiplication.", "PixelCNN Autoencoder  The decoder in a standard autoencoder can be replaced by a PixelCNN, creating a PixelCNN-Autoencoder.", "Results  They achieve similar NLL-results as PixelRNN on CIFAR-10 and ImageNet, while training about twice as fast.", "Here, \"fast\" means that they used 32 GPUs for 60 hours.", "Using Conditional PixelCNNs on ImageNet (i.e. adding class information to each convolution) did not improve the NLL-score, but it did improve the image quality.", "They use a different neural network to create embeddings of human faces.", "Then they generate new faces based on these embeddings via PixelCNN.", "Their PixelCNN-Autoencoder generates significantly sharper (i.e. less blurry) images than a \"normal\" autoencoder."], "summary_text": "What  PixelRNN  PixelRNNs generate new images pixel by pixel (and row by row) via LSTMs (or other RNNs). Each pixel is therefore conditioned on the previously generated pixels. Training of PixelRNNs is slow due to the RNN-architecture (hard to parallelize). Previously PixelCNNs have been suggested, which use masked convolutions during training (instead of RNNs), but their image quality was worse. They suggest changes to PixelCNNs that improve the quality of the generated images (while still keeping them faster than RNNs). How  PixelRNNs split up the distribution p(image) into many conditional probabilities, one per pixel, each conditioned on all previous pixels: p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1). PixelCNNs implement that using convolutions, which are faster to train than RNNs. These convolutions uses masked filters, i.e. the center weight and also all weights right and/or below the center pixel are 0 (because they are current/future values and we only want to condition on the past). In most generative models, several layers are stacked, ultimately ending in three float values per pixel (RGB images, one value for grayscale images). PixelRNNs (including this implementation) traditionally end in a softmax over 255 values per pixel and channel (so 3*255 per RGB pixel). The following image shows the application of such a convolution with the softmax output (left) and the mask for a filter (right):  Blind spot  Using the mask on each convolutional filter effectively converts them into non-squared shapes (the green values in the image). Advantage: Using such non-squared convolutions prevents future values from leaking into present values. Disadvantage: Using such non-squared convolutions creates blind spots, i.e. for each pixel, some past values (diagonally top-right from it) cannot influence the value of that pixel. They combine horizontal (1xN) and vertical (Nx1) convolutions to prevent that. Gated convolutions  PixelRNNs via LSTMs so far created visually better images than PixelCNNs. They assume that one advantage of LSTMs is, that they (also) have multiplicative gates, while stacked convolutional layers only operate with summations. They alleviate that problem by adding gates to their convolutions:  Equation: output image = tanh(weights_1 * image) <element-wise product> sigmoid(weights_2 * image)  * is the convolutional operator. tanh(weights_1 * image) is a classical convolution with tanh activation function. sigmoid(weights_2 * image) are the gate values (0 = gate closed, 1 = gate open). weights_1 and weights_2 are learned. Conditional PixelCNNs  When generating images, they do not only want to condition the previous values, but also on a laten vector h that describes the image to generate. The new image distribution becomes: p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1, h). To implement that, they simply modify the previously mentioned gated convolution, adding h to it:  Equation: output image = tanh(weights_1 * image + weights_2 . h) <element-wise product> sigmoid(weights_3 * image + weights_4 . h)  . denotes here the matrix-vector multiplication. PixelCNN Autoencoder  The decoder in a standard autoencoder can be replaced by a PixelCNN, creating a PixelCNN-Autoencoder. Results  They achieve similar NLL-results as PixelRNN on CIFAR-10 and ImageNet, while training about twice as fast. Here, \"fast\" means that they used 32 GPUs for 60 hours. Using Conditional PixelCNNs on ImageNet (i.e. adding class information to each convolution) did not improve the NLL-score, but it did improve the image quality. They use a different neural network to create embeddings of human faces. Then they generate new faces based on these embeddings via PixelCNN. Their PixelCNN-Autoencoder generates significantly sharper (i.e. less blurry) images than a \"normal\" autoencoder.", "pdf_url": "http://arxiv.org/pdf/1606.05328", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/conditional_image_generation_with_pixelcnn_decoders.json"}
{"id": "8601517", "bin": "700_800", "summary_sentences": ["What  Current object detectors predict usually a set of bounding boxes, each having a classification score (\"how likely it is to be of an object class\") and regressed bounding box dimensions (\"where is the box and how high/wide is it\").", "These results are then filtered through NMS, suppressing all bounding boxes that overlap strongly with another box that has higher classification score.", "Sometimes the regressed dimensions are also iteratively refined (e.g. in two-stage detectors one can argue that there is one refinement after the RPN).", "The authors observe that this structure is problematic, as there is no uncertainty-related information regarding the regressed dimensions.", "For instance in NMS, the suppression works based on classification scores, but e.g. a too large bounding box might get a higher classification score than one with optimal fit, due to including more context and thereby making the classification more certain.", "The classification scores are also very non-monotonic with respect to the bounding box dimensions: Increasing the bounding box size might first lead to higher classification scores, then lower ones, then higher again.", "This makes iterative refinement hard or impossible.", "They propose IoU-Net, which predicts for each bounding box the expected IoU with the ground truth.", "That predicted IoU is then used in NMS as a replacement for the classification score, thereby selecting for the bounding boxes that the model thinks have highest IoU.", "The predicted IoU can also be used to perform iterative refinement, as it is more monotonic than the classification score.", "They propose \"Precise RoI Pooling\" (PrRoI Pooling), which is a version of RoI-Pooling/RoI-Align that does not suffer from quantization inaccuracies.", "How  Predicting IoUs  They feed each RoI's features through two fully connected layers to regress an IoU value for that RoI.", "They use one such branch per class.", "During training they augment the ground truth bounding boxes to generate examples with lower IoUs.", "Visualization:  Relationship between predicted IoUs and real IoUs (right) vs. classification scores and real IoUs (left):  IoU-guided NMS  This is essentially the same as classical NMS.", "They use the predicted IoU values to estimate which box to keep if two are sufficiently overlapping.", "When one box is suppressed, the remaining box's class score is updated to max(s_1, s_2), where s_1 and s_2 are the class scores of the two boxes.", "Optimization-based refinement of bounding boxes  The branch to predict IoUs is differentiable.", "They use this to iteratively improve predicted bounding boxes so that the predicted IoU is maximized.", "I.e. they backpropagate to the bounding box coordinates and change them according to the gradient.", "They scale up the gradients according to the bounding box size on the given axis, which is similar to optimizing in log-space.", "They add some early stopping criteria to not execute this indefinitely.", "Precise RoI Pooling  During bounding box refinement they use a more accurate (quantization free) RoI-Pooling.", "They use bilinear sampling to approximate the feature value of any continuous location within the feature map.", "Then they use integration to calculate the pooled value (makes it sound like they always use global pooling?).", "So while this method computes the integrals over all possible values within the given range, RoI Align only samples at N=4 locations per side.", "Visualization of the differences:  Results  They train and test on COCO.", "Their model runs at about 300ms per image on a Titan X.  IoU-guided NMS  IoU-guided NMS performs slightly better than Soft-NMS and significantly better than classical NMS.", "IoU-guides NMS shines especially for high IoU APs, improving over Soft-NMS by 1 to 3 percentage points.", "Optimization-based refinement  Using optimization-based refinement also improves APs by around 1 to 3 percentage points (over no refinement).", "Again, the improvement is most significant for high IoUs, reaching 5 to 6 percentage points better values.", "Joint Training  Training the IoU prediction branch jointly with a base network leads to slightly better features, improving APs by 0.4 to 0.6 points.", "Combining this with guided-NMS and optimization-based refinement, they improve the AP of a ResNet101+FPN baseline by 2.1 points."], "summary_text": "What  Current object detectors predict usually a set of bounding boxes, each having a classification score (\"how likely it is to be of an object class\") and regressed bounding box dimensions (\"where is the box and how high/wide is it\"). These results are then filtered through NMS, suppressing all bounding boxes that overlap strongly with another box that has higher classification score. Sometimes the regressed dimensions are also iteratively refined (e.g. in two-stage detectors one can argue that there is one refinement after the RPN). The authors observe that this structure is problematic, as there is no uncertainty-related information regarding the regressed dimensions. For instance in NMS, the suppression works based on classification scores, but e.g. a too large bounding box might get a higher classification score than one with optimal fit, due to including more context and thereby making the classification more certain. The classification scores are also very non-monotonic with respect to the bounding box dimensions: Increasing the bounding box size might first lead to higher classification scores, then lower ones, then higher again. This makes iterative refinement hard or impossible. They propose IoU-Net, which predicts for each bounding box the expected IoU with the ground truth. That predicted IoU is then used in NMS as a replacement for the classification score, thereby selecting for the bounding boxes that the model thinks have highest IoU. The predicted IoU can also be used to perform iterative refinement, as it is more monotonic than the classification score. They propose \"Precise RoI Pooling\" (PrRoI Pooling), which is a version of RoI-Pooling/RoI-Align that does not suffer from quantization inaccuracies. How  Predicting IoUs  They feed each RoI's features through two fully connected layers to regress an IoU value for that RoI. They use one such branch per class. During training they augment the ground truth bounding boxes to generate examples with lower IoUs. Visualization:  Relationship between predicted IoUs and real IoUs (right) vs. classification scores and real IoUs (left):  IoU-guided NMS  This is essentially the same as classical NMS. They use the predicted IoU values to estimate which box to keep if two are sufficiently overlapping. When one box is suppressed, the remaining box's class score is updated to max(s_1, s_2), where s_1 and s_2 are the class scores of the two boxes. Optimization-based refinement of bounding boxes  The branch to predict IoUs is differentiable. They use this to iteratively improve predicted bounding boxes so that the predicted IoU is maximized. I.e. they backpropagate to the bounding box coordinates and change them according to the gradient. They scale up the gradients according to the bounding box size on the given axis, which is similar to optimizing in log-space. They add some early stopping criteria to not execute this indefinitely. Precise RoI Pooling  During bounding box refinement they use a more accurate (quantization free) RoI-Pooling. They use bilinear sampling to approximate the feature value of any continuous location within the feature map. Then they use integration to calculate the pooled value (makes it sound like they always use global pooling?). So while this method computes the integrals over all possible values within the given range, RoI Align only samples at N=4 locations per side. Visualization of the differences:  Results  They train and test on COCO. Their model runs at about 300ms per image on a Titan X.  IoU-guided NMS  IoU-guided NMS performs slightly better than Soft-NMS and significantly better than classical NMS. IoU-guides NMS shines especially for high IoU APs, improving over Soft-NMS by 1 to 3 percentage points. Optimization-based refinement  Using optimization-based refinement also improves APs by around 1 to 3 percentage points (over no refinement). Again, the improvement is most significant for high IoUs, reaching 5 to 6 percentage points better values. Joint Training  Training the IoU prediction branch jointly with a base network leads to slightly better features, improving APs by 0.4 to 0.6 points. Combining this with guided-NMS and optimization-based refinement, they improve the AP of a ResNet101+FPN baseline by 2.1 points.", "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/acquisition_of_localization_confidence_for_accurate_od.json"}
{"id": "46342402", "bin": "700_800", "summary_sentences": ["What  They suggest a new stochastic optimization method, similar to the existing SGD, Adagrad or RMSProp.", "Stochastic optimization methods have to find parameters that minimize/maximize a stochastic function.", "A function is stochastic (non-deterministic), if the same set of parameters can generate different results.", "E.g. the loss of different mini-batches can differ, even when the parameters remain unchanged.", "Even for the same mini-batch the results can change due to e.g. dropout.", "Their method tends to converge faster to optimal parameters than the existing competitors.", "Their method can deal with non-stationary distributions (similar to e.g. SGD, Adadelta, RMSProp).", "Their method can deal with very sparse or noisy gradients (similar to e.g. Adagrad).", "How  Basic principle  Standard SGD just updates the parameters based on parameters = parameters - learningRate * gradient.", "Adam operates similar to that, but adds more \"cleverness\" to the rule.", "It assumes that the gradient values have means and variances and tries to estimate these values.", "Recall here that the function to optimize is stochastic, so there is some randomness in the gradients.", "The mean is also called \"the first moment\".", "The variance is also called \"the second (raw) moment\".", "Then an update rule very similar to SGD would be parameters = parameters - learningRate * means.", "They instead use the update rule parameters = parameters - learningRate * means/sqrt(variances).", "They call means/sqrt(variances) a 'Signal to Noise Ratio'.", "Basically, if the variance of a specific parameter's gradient is high, it is pretty unclear how it should be changend.", "So we choose a small step size in the update rule via learningRate * mean/sqrt(highValue).", "If the variance is low, it is easier to predict how far to \"move\", so we choose a larger step size via learningRate * mean/sqrt(lowValue).", "Exponential moving averages  In order to approximate the mean and variance values you could simply save the last T gradients and then average the values.", "That however is a pretty bad idea, because it can lead to high memory demands (e.g. for millions of parameters in CNNs).", "A simple average also has the disadvantage, that it would completely ignore all gradients before T and weight all of the last T gradients identically.", "In reality, you might want to give more weight to the last couple of gradients.", "Instead, they use an exponential moving average, which fixes both problems and simply updates the average at every timestep via the formula avg = alpha * avg + (1 - alpha) * avg.", "Let the gradient at timestep (batch) t be g, then we can approximate the mean and variance values using:  mean = beta1 * mean + (1 - beta1) * g  variance = beta2 * variance + (1 - beta2) * g^2.", "beta1 and beta2 are hyperparameters of the algorithm.", "Good values for them seem to be beta1=0.9 and beta2=0.999.", "At the start of the algorithm, mean and variance are initialized to zero-vectors.", "Bias correction  Initializing the mean and variance vectors to zero is an easy and logical step, but has the disadvantage that bias is introduced.", "E.g. at the first timestep, the mean of the gradient would be mean = beta1 * 0 + (1 - beta1) * g, with beta1=0.9 then: mean = 0.9 * g. So 0.9g, not g. Both the mean and the variance are biased (towards 0).", "This seems pretty harmless, but it can be shown that it lowers the convergence speed of the algorithm by quite a bit.", "So to fix this pretty they perform bias-corrections of the mean and the variance:  correctedMean = mean / (1-beta1^t) (where t is the timestep).", "correctedVariance = variance / (1-beta2^t).", "Both formulas are applied at every timestep after the exponential moving averages (they do not influence the next timestep)."], "summary_text": "What  They suggest a new stochastic optimization method, similar to the existing SGD, Adagrad or RMSProp. Stochastic optimization methods have to find parameters that minimize/maximize a stochastic function. A function is stochastic (non-deterministic), if the same set of parameters can generate different results. E.g. the loss of different mini-batches can differ, even when the parameters remain unchanged. Even for the same mini-batch the results can change due to e.g. dropout. Their method tends to converge faster to optimal parameters than the existing competitors. Their method can deal with non-stationary distributions (similar to e.g. SGD, Adadelta, RMSProp). Their method can deal with very sparse or noisy gradients (similar to e.g. Adagrad). How  Basic principle  Standard SGD just updates the parameters based on parameters = parameters - learningRate * gradient. Adam operates similar to that, but adds more \"cleverness\" to the rule. It assumes that the gradient values have means and variances and tries to estimate these values. Recall here that the function to optimize is stochastic, so there is some randomness in the gradients. The mean is also called \"the first moment\". The variance is also called \"the second (raw) moment\". Then an update rule very similar to SGD would be parameters = parameters - learningRate * means. They instead use the update rule parameters = parameters - learningRate * means/sqrt(variances). They call means/sqrt(variances) a 'Signal to Noise Ratio'. Basically, if the variance of a specific parameter's gradient is high, it is pretty unclear how it should be changend. So we choose a small step size in the update rule via learningRate * mean/sqrt(highValue). If the variance is low, it is easier to predict how far to \"move\", so we choose a larger step size via learningRate * mean/sqrt(lowValue). Exponential moving averages  In order to approximate the mean and variance values you could simply save the last T gradients and then average the values. That however is a pretty bad idea, because it can lead to high memory demands (e.g. for millions of parameters in CNNs). A simple average also has the disadvantage, that it would completely ignore all gradients before T and weight all of the last T gradients identically. In reality, you might want to give more weight to the last couple of gradients. Instead, they use an exponential moving average, which fixes both problems and simply updates the average at every timestep via the formula avg = alpha * avg + (1 - alpha) * avg. Let the gradient at timestep (batch) t be g, then we can approximate the mean and variance values using:  mean = beta1 * mean + (1 - beta1) * g  variance = beta2 * variance + (1 - beta2) * g^2. beta1 and beta2 are hyperparameters of the algorithm. Good values for them seem to be beta1=0.9 and beta2=0.999. At the start of the algorithm, mean and variance are initialized to zero-vectors. Bias correction  Initializing the mean and variance vectors to zero is an easy and logical step, but has the disadvantage that bias is introduced. E.g. at the first timestep, the mean of the gradient would be mean = beta1 * 0 + (1 - beta1) * g, with beta1=0.9 then: mean = 0.9 * g. So 0.9g, not g. Both the mean and the variance are biased (towards 0). This seems pretty harmless, but it can be shown that it lowers the convergence speed of the algorithm by quite a bit. So to fix this pretty they perform bias-corrections of the mean and the variance:  correctedMean = mean / (1-beta1^t) (where t is the timestep). correctedVariance = variance / (1-beta2^t). Both formulas are applied at every timestep after the exponential moving averages (they do not influence the next timestep).", "pdf_url": "https://arxiv.org/pdf/1412.6980", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/adam.json"}
{"id": "24928430", "bin": "700_800", "summary_sentences": ["The paper describes a combinatorial approach to embed trees into hyperbolic spaces without performing optimization.", "The resulting mechanism is analyzed to obtain dimensionality-precision tradeoffs.", "To embed any metric spaces in the hyperbolic spaces, a hyperbolic generalization of the multidimensional scaling (h-MDS) is proposed.", "Preliminaries  Hyperbolic Spaces  Have the “tree” like property ie the shortest path between a pair of points is almost the same as the path through the origin.", "Generally, Poincare ball model is used given its advantages like conformity to the Euclidean spaces.", "Fidelity Measures  Mean Average Precision - MAP  A local metric that ranks between distances of the immediate neighbors.", "Distortion  A global metric that depends on the underlying distances and not just the local relationship between distances.", "Combinatorial Construction for embedding hierarchies into Hyperbolic spaces  Embed the given graph G = (V, E) into a tree T.  Embed the tree T into the poincare ball Hd of dimensionality d.  Sarkar’s construction to embed points in a 2-d Poincare ball  Consider two points a and b (from the tree) where b is the parent of a.", "Assume that a is embedded as f(a) and b is embedded as f(b) and the children of a needs to be embedded.", "Reflect f(a) and f(b) across a geodesic such that f(a) is mapped to 0 (origin) while f(b) is mapped to some new point z.", "Children of a are placed at points yi which are equally placed around a circle of radius (er - 1) / (er + 1) and maximally seperated from z, where r is the scaling factor.", "Then all the points are reflected back across the geodesic so that all children are at a distance r from f(a).", "To embed the tree itself, place the root node at the origin, place its children around it in a circle, then place their children and so on.", "In this construct, precision scales logarithmically with the degree of the tree but linearly with the maximum path length.", "d-dimensional hyperbolic spaces  In the d-dimensional space, the points are embedded into hyperspheres (instead of circles).", "The number of children node that can be placed for a particular angle grows with the dimension.", "Increasing dimension helps with bushy trees (with high node degree).", "Hyperbolic multidimensional scaling (h-MDS)  Given the pairwise distance from a set of points in the hyperbolic space, how to recover the points?", "The corresponding problem in the Euclidean space is solved using MDS.", "A variant of MDS called as h-MDS is proposed.", "MDS makes a centering assumption that points have 0 mean.", "In h-MDS, a new mean (called as the pseudo-Euclidean mean) is introduced to enable recovery via matrix factorization.", "Instead of the Poincare model, the hyperboloid model is used (though the points can be mapped back and forth).", "pseudo-Euclidean Mean  A set of points can always be centered without affecting their pairwise distance by simply finding their mean and sending it to 0 via isometry  Recovery via matrix factorization  Given the pairwise distances, a new matrix Y is constructed by applying cosh on the pairwise distances.", "Running PCA on -Y recovers X up to rotation.", "Dimensionality Reduction with PGA (Principal Geodesic Analysis)  PGA is the counterpart of PCA in the hyperbolic spaces.", "First the Karcher mean of the given points is computed.", "All points xi are reflected so that their mean is 0 in the Poincare disk model.", "Combining that with Euclidean reflection formula and hyperbolic metrics leads to a non-convex loss function which can be optimized using gradient descent algorithm.", "Experiments  Datasets  Trees: fully balanced and phylogenic trees expressing genetic heritage.", "Tree-like hierarchy: WordNet hypernym and graph of Ph.D. advisor-advisee relationships.", "No-tree like disease relationships, proteins interactions etc  Results  Combinatorial construction outperforms approaches based on optimization in terms of both MAP and distortion.", "eg on WordNet, the combinatorial approach achieves a MAP of 0.989 with just 2 dimensions while the previous best was 0.87 with 200 dimensions."], "summary_text": "The paper describes a combinatorial approach to embed trees into hyperbolic spaces without performing optimization. The resulting mechanism is analyzed to obtain dimensionality-precision tradeoffs. To embed any metric spaces in the hyperbolic spaces, a hyperbolic generalization of the multidimensional scaling (h-MDS) is proposed. Preliminaries  Hyperbolic Spaces  Have the “tree” like property ie the shortest path between a pair of points is almost the same as the path through the origin. Generally, Poincare ball model is used given its advantages like conformity to the Euclidean spaces. Fidelity Measures  Mean Average Precision - MAP  A local metric that ranks between distances of the immediate neighbors. Distortion  A global metric that depends on the underlying distances and not just the local relationship between distances. Combinatorial Construction for embedding hierarchies into Hyperbolic spaces  Embed the given graph G = (V, E) into a tree T.  Embed the tree T into the poincare ball Hd of dimensionality d.  Sarkar’s construction to embed points in a 2-d Poincare ball  Consider two points a and b (from the tree) where b is the parent of a. Assume that a is embedded as f(a) and b is embedded as f(b) and the children of a needs to be embedded. Reflect f(a) and f(b) across a geodesic such that f(a) is mapped to 0 (origin) while f(b) is mapped to some new point z. Children of a are placed at points yi which are equally placed around a circle of radius (er - 1) / (er + 1) and maximally seperated from z, where r is the scaling factor. Then all the points are reflected back across the geodesic so that all children are at a distance r from f(a). To embed the tree itself, place the root node at the origin, place its children around it in a circle, then place their children and so on. In this construct, precision scales logarithmically with the degree of the tree but linearly with the maximum path length. d-dimensional hyperbolic spaces  In the d-dimensional space, the points are embedded into hyperspheres (instead of circles). The number of children node that can be placed for a particular angle grows with the dimension. Increasing dimension helps with bushy trees (with high node degree). Hyperbolic multidimensional scaling (h-MDS)  Given the pairwise distance from a set of points in the hyperbolic space, how to recover the points? The corresponding problem in the Euclidean space is solved using MDS. A variant of MDS called as h-MDS is proposed. MDS makes a centering assumption that points have 0 mean. In h-MDS, a new mean (called as the pseudo-Euclidean mean) is introduced to enable recovery via matrix factorization. Instead of the Poincare model, the hyperboloid model is used (though the points can be mapped back and forth). pseudo-Euclidean Mean  A set of points can always be centered without affecting their pairwise distance by simply finding their mean and sending it to 0 via isometry  Recovery via matrix factorization  Given the pairwise distances, a new matrix Y is constructed by applying cosh on the pairwise distances. Running PCA on -Y recovers X up to rotation. Dimensionality Reduction with PGA (Principal Geodesic Analysis)  PGA is the counterpart of PCA in the hyperbolic spaces. First the Karcher mean of the given points is computed. All points xi are reflected so that their mean is 0 in the Poincare disk model. Combining that with Euclidean reflection formula and hyperbolic metrics leads to a non-convex loss function which can be optimized using gradient descent algorithm. Experiments  Datasets  Trees: fully balanced and phylogenic trees expressing genetic heritage. Tree-like hierarchy: WordNet hypernym and graph of Ph.D. advisor-advisee relationships. No-tree like disease relationships, proteins interactions etc  Results  Combinatorial construction outperforms approaches based on optimization in terms of both MAP and distortion. eg on WordNet, the combinatorial approach achieves a MAP of 0.989 with just 2 dimensions while the previous best was 0.87 with 200 dimensions.", "pdf_url": "https://arxiv.org/pdf/1804.03329", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/representation-tradeoffs-for-hyperbolic-embeddings.json"}
{"id": "66883240", "bin": "700_800", "summary_sentences": ["Problem Statement  Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.", "VQA Challenge and Workshop  The authors organise an annual challenge and workshop to discuss the state-of-the-art methods and best practices in this domain.", "Interestingly, the second version is starting on 27th April 2017 (today).", "Benefits over tasks like image captioning:  Simple, n-gram statistics based methods are not sufficient.", "Requires the system to blend in different aspects of knowledge - object detection, activity recognition, commonsense reasoning etc.", "Since only short answers are expected, evaluation is easier.", "Dataset  Created a new dataset of 50000 realistic, abstract images.", "Used AMT to crowdsource the task of collecting questions and answers for MS COCO dataset (>200K images) and abstract images.", "Three questions per image and ten answers per question (along with their confidence) were collected.", "The entire dataset contains over 760K questions and 10M answers.", "The authors also performed an exhaustive analysis of the dataset to establish its diversity and to explore how the content of these question-answers differ from that of standard image captioning datasets.", "Highlights of data collection methodology  Emphasis on questions that require an image, and not just common sense, to be answered correctly.", "Workers were shown previous questions when writing new questions to increase diversity.", "Answers collected from multiple users to account for discrepancies in answers by humans.", "Two modalities supported:  Open-ended - produce the answer  multiple-choice - select from a set of options provided (18 options comprising of popular, plausible, random and ofc correct answer)  Highlights from data analysis  Most questions range from four to ten words while answers range from one to three words.", "Around 40% questions are “yes/no” questions.", "Significant (>80%) inter-human agreement for answers.", "The authors performed a study where human evaluators were asked to answer the questions without looking at the images.", "Further, they performed a study where evaluators were asked to label if a question could be answered using common sense and what was the youngest age group, they felt, could answer the question.", "The idea was to establish that a sufficient number of questions in the dataset required more than just common sense to answer.", "Baseline Models  random selection  prior (“yes”) - always answer as yes.", "per Q-type prior - pick the most popular answer per question type.", "nearest neighbor - find the k nearest neighbors for the given (image, question) pair.", "Methods  2-channel model (using vision and language models) followed by softmax over (K = 1000) most frequent answers.", "Image Channel  I - Used last hidden layer of VGGNet to obtain 4096-dim image embedding.", "norm I - : l2 normalized version of I.", "Question Channel  BoW Q - Bag-of-Words representation for the questions using the top 1000 words plus the top 1- first, second and third words of the questions.", "LSTM Q - Each word is encoded into 300-dim vectors using fully connected + tanh non-linearity.", "These embeddings are fed to an LSTM to obtain 1024d-dim embedding.", "Deeper LSTM Q - Same as LSTM Q but uses two hidden layers to obtain 2048-dim embedding.", "Multi-Layer Perceptron (MLP) - Combine image and question embeddings to obtain a single embedding.", "BoW Q + I method - concatenate BoW Q and I embeddings.", "LSTM Q + I, deeper LSTM Q + norm I methods - image embedding transformed to 1024-dim using a FC layer and tanh non-linearity followed by element-wise multiplication of image and question vectors.", "Pass combined embedding to an MLP - FC neural network with 2 hidden layers (1000 neurons and 0.5 dropout) with tanh, followed by softmax.", "Cross-entropy loss with VGGNet parameters frozen.", "Results  Deeper LSTM Q + norm I is the best model with 58.16% accuracy on open-ended dataset and 63.09% on multiple-choice but far behind the human evaluators (>80% and >90% respectively).", "The best model performs well for answers involving common visual objects but performs poorly for answers involving counts.", "Vision only model performs even worse than the model which always produces “yes” as the answer."], "summary_text": "Problem Statement  Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image. VQA Challenge and Workshop  The authors organise an annual challenge and workshop to discuss the state-of-the-art methods and best practices in this domain. Interestingly, the second version is starting on 27th April 2017 (today). Benefits over tasks like image captioning:  Simple, n-gram statistics based methods are not sufficient. Requires the system to blend in different aspects of knowledge - object detection, activity recognition, commonsense reasoning etc. Since only short answers are expected, evaluation is easier. Dataset  Created a new dataset of 50000 realistic, abstract images. Used AMT to crowdsource the task of collecting questions and answers for MS COCO dataset (>200K images) and abstract images. Three questions per image and ten answers per question (along with their confidence) were collected. The entire dataset contains over 760K questions and 10M answers. The authors also performed an exhaustive analysis of the dataset to establish its diversity and to explore how the content of these question-answers differ from that of standard image captioning datasets. Highlights of data collection methodology  Emphasis on questions that require an image, and not just common sense, to be answered correctly. Workers were shown previous questions when writing new questions to increase diversity. Answers collected from multiple users to account for discrepancies in answers by humans. Two modalities supported:  Open-ended - produce the answer  multiple-choice - select from a set of options provided (18 options comprising of popular, plausible, random and ofc correct answer)  Highlights from data analysis  Most questions range from four to ten words while answers range from one to three words. Around 40% questions are “yes/no” questions. Significant (>80%) inter-human agreement for answers. The authors performed a study where human evaluators were asked to answer the questions without looking at the images. Further, they performed a study where evaluators were asked to label if a question could be answered using common sense and what was the youngest age group, they felt, could answer the question. The idea was to establish that a sufficient number of questions in the dataset required more than just common sense to answer. Baseline Models  random selection  prior (“yes”) - always answer as yes. per Q-type prior - pick the most popular answer per question type. nearest neighbor - find the k nearest neighbors for the given (image, question) pair. Methods  2-channel model (using vision and language models) followed by softmax over (K = 1000) most frequent answers. Image Channel  I - Used last hidden layer of VGGNet to obtain 4096-dim image embedding. norm I - : l2 normalized version of I. Question Channel  BoW Q - Bag-of-Words representation for the questions using the top 1000 words plus the top 1- first, second and third words of the questions. LSTM Q - Each word is encoded into 300-dim vectors using fully connected + tanh non-linearity. These embeddings are fed to an LSTM to obtain 1024d-dim embedding. Deeper LSTM Q - Same as LSTM Q but uses two hidden layers to obtain 2048-dim embedding. Multi-Layer Perceptron (MLP) - Combine image and question embeddings to obtain a single embedding. BoW Q + I method - concatenate BoW Q and I embeddings. LSTM Q + I, deeper LSTM Q + norm I methods - image embedding transformed to 1024-dim using a FC layer and tanh non-linearity followed by element-wise multiplication of image and question vectors. Pass combined embedding to an MLP - FC neural network with 2 hidden layers (1000 neurons and 0.5 dropout) with tanh, followed by softmax. Cross-entropy loss with VGGNet parameters frozen. Results  Deeper LSTM Q + norm I is the best model with 58.16% accuracy on open-ended dataset and 63.09% on multiple-choice but far behind the human evaluators (>80% and >90% respectively). The best model performs well for answers involving common visual objects but performs poorly for answers involving counts. Vision only model performs even worse than the model which always produces “yes” as the answer.", "pdf_url": "https://arxiv.org/pdf/1505.00468v6", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/vqa-visual-question-answering.json"}
{"id": "10374612", "bin": "700_800", "summary_sentences": ["Leases: An efficient fault-tolerant mechanism for distributed file cache consistency – Gray & Cheriton 1989  This paper introduced the leasing model for distributed systems.", "Leases are conceptually very straightforward and bring a surprising number of benefits for such a simple mechanism.", "Also in this paper you’ll find the simple formulas that can help you figure out the optimum lease term for your workloads.", "Caching introduces the problem of ensuring consistency between the cached data and its primary location of storage.", "By consistent, we mean that the behaviour is equivalent to their being only a single (uncached) copy of the data except for the performance benefit of the cache.", "Now that’s one of the most straightforward definitions of consistent you’re likely to find!", "A distributed system… can experience partial failures: a host may crash or messages may be lost.", "Existing approaches to consistency for file caches fall into two categories: those that assume reliable broadcast, and so do not tolerate communication failures, and those that require a consistency check for every read, and so fail to deliver good performance.", "Those were simpler times ;) Leases are a very elegant solution to these problems.", "A lease is a contract that gives its holder specified rights over property for a limited period of time.", "In the context of caching, a lease grants to its holder control over writes to the covered datum during the term of the lease, such that the server must obtain the approval of the leaseholder before the datum may be written….", "A cache using leases requires a valid lease on the datum (in addition to holding the datum) before it returns the datum in response to a read, or modifies the datum in response to a write.", "From a performance perspective, one big advantage of leases is that once a lease is held, subsequent reads can return straightaway.", "Leases are also very easy to implement on the server-side: using short leases, you can always just wait for the lease time to expire to get back to a known state.", "Short lease terms have several advantages.", "One is that they minimize the delay resulting from client and server failures (and partitioning communication failures).", "When the server cannot communicate with a client, the server must delay writes to a file for which the failed client holds a lease until that lease has expired.", "Short lease terms also minimize the overhead due to false sharing – where a client wants to write to a file that is covered by a lease held by another client, when in fact that client is no longer using it.", "Key to the overall performance of the system is choosing an appropriate lease term:  The choice of lease term is based on the trade-off between minimizing the lease extension overhead versus minimizing false sharing.", "Given a read rate of R, write rate of W, and a file shared between S caches then the lease benefit factor B is given by B = 2R/SW.", "And you’ll be getting benefit from a leasing scheme so long as the amount of time a client holds a lease (without renewing) is > 1/(R(B-1)).", "Leases have very nice fault-tolerant properties:  Leases ensure consistency provided that the hosts and network do not suffer certain Byzantine failures including clock failure.", "More specifically, consistency is maintained in spite of message loss (including partition), and client or server failures (assuming writes are persistent at the server across a crash).", "Moreover, availability is not reduced by the caches because an unreachable client at most briefly delays write access by other clients.", "In a forward-looking statement in the conclusion, the authors state:  Lease appear well-suited to large-scale distributed systems.", "The improvement in response time that they offer is more significant for the faster processors and higher delay networks.", "In their 2013 “ Scaling memcache at Facebook ” paper, Nishtala et al. discuss the introduction of a lease mechanism to cope with ‘thundering herds.’ Without leases, all of the cache misses resulted in a peak database query rate of 17K/s.", "With leases,the peak database query rate was 1.3K/s."], "summary_text": "Leases: An efficient fault-tolerant mechanism for distributed file cache consistency – Gray & Cheriton 1989  This paper introduced the leasing model for distributed systems. Leases are conceptually very straightforward and bring a surprising number of benefits for such a simple mechanism. Also in this paper you’ll find the simple formulas that can help you figure out the optimum lease term for your workloads. Caching introduces the problem of ensuring consistency between the cached data and its primary location of storage. By consistent, we mean that the behaviour is equivalent to their being only a single (uncached) copy of the data except for the performance benefit of the cache. Now that’s one of the most straightforward definitions of consistent you’re likely to find! A distributed system… can experience partial failures: a host may crash or messages may be lost. Existing approaches to consistency for file caches fall into two categories: those that assume reliable broadcast, and so do not tolerate communication failures, and those that require a consistency check for every read, and so fail to deliver good performance. Those were simpler times ;) Leases are a very elegant solution to these problems. A lease is a contract that gives its holder specified rights over property for a limited period of time. In the context of caching, a lease grants to its holder control over writes to the covered datum during the term of the lease, such that the server must obtain the approval of the leaseholder before the datum may be written…. A cache using leases requires a valid lease on the datum (in addition to holding the datum) before it returns the datum in response to a read, or modifies the datum in response to a write. From a performance perspective, one big advantage of leases is that once a lease is held, subsequent reads can return straightaway. Leases are also very easy to implement on the server-side: using short leases, you can always just wait for the lease time to expire to get back to a known state. Short lease terms have several advantages. One is that they minimize the delay resulting from client and server failures (and partitioning communication failures). When the server cannot communicate with a client, the server must delay writes to a file for which the failed client holds a lease until that lease has expired. Short lease terms also minimize the overhead due to false sharing – where a client wants to write to a file that is covered by a lease held by another client, when in fact that client is no longer using it. Key to the overall performance of the system is choosing an appropriate lease term:  The choice of lease term is based on the trade-off between minimizing the lease extension overhead versus minimizing false sharing. Given a read rate of R, write rate of W, and a file shared between S caches then the lease benefit factor B is given by B = 2R/SW. And you’ll be getting benefit from a leasing scheme so long as the amount of time a client holds a lease (without renewing) is > 1/(R(B-1)). Leases have very nice fault-tolerant properties:  Leases ensure consistency provided that the hosts and network do not suffer certain Byzantine failures including clock failure. More specifically, consistency is maintained in spite of message loss (including partition), and client or server failures (assuming writes are persistent at the server across a crash). Moreover, availability is not reduced by the caches because an unreachable client at most briefly delays write access by other clients. In a forward-looking statement in the conclusion, the authors state:  Lease appear well-suited to large-scale distributed systems. The improvement in response time that they offer is more significant for the faster processors and higher delay networks. In their 2013 “ Scaling memcache at Facebook ” paper, Nishtala et al. discuss the introduction of a lease mechanism to cope with ‘thundering herds.’ Without leases, all of the cache misses resulted in a peak database query rate of 17K/s. With leases,the peak database query rate was 1.3K/s.", "pdf_url": "http://web.stanford.edu/class/cs240/readings/89-leases.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/leases-an-efficient-fault-tolerant-mechanism-for-distributed-file-cache-consistency.json"}
{"id": "84030160", "bin": "800_900", "summary_sentences": ["Challenging common assumptions in the unsupervised learning of disentangled representations Locatello et al., ICML’19  Today’s paper choice won a best paper award at ICML’19.", "The ‘common assumptions’ that the paper challenges seem to be: “unsupervised learning of disentangled representations is possible, and useful!”  The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms.", "In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions.", "What exactly is a ‘disentangled representation’ and why might we want one?", "Put the ‘disentangled’ part to one side for a moment, and let’s start out by revisiting what we mean by a representation.", "Given a real-world observation  (e.g. of an image or video), a representation  is a transformation of  (typically to a lower dimensional space in order to be useful) that somehow preserves the salient information in the  so that we can still use  to extract useful information about the input (e.g.", "for building classifiers).", "As a trivial example, suppose we had real world observations consisting of 1000 points sampled from a straight line, a good lower-valued representation would be a (gradient, intercept) tuple.", "Of course real-world examples are much more complex than this!", "A disentangled representation is a representation with a compact and interpretable structure, which captures the essence of the input independent of the task the representation is ultimately going to be used for.", "That’s quite tricky – even in my contrived straight line example what looked to be a great representation would be useless if the task turned out to be calculating the area of the the rectangle enclosed by the points in the observation.", "While there is no single formalized notion of disentanglement (yet) which is widely accepted, the key intuition is that a disentangled representation should separate the distinct information factors of variation in the data.", "A change in a single underlying factor of variation should lead to a change in a single factor in the learned representation.", "The state of the art for representation learning centres around Variational Autoencoders, using one deep neural network to learn a representation, and another one to attempt to reconstruct the original input from that representation.", "The representation  is usually taken as the mean of the approximate posterior distribution of the first (encoding) network.", "In theory, disentanglement is impossible  We theoretically prove that (perhaps unsurprisingly) the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases both on the considered learning approaches and the data sets.", "The full proof is giving in appendix A (missing from my copy of the pdf), but it boils down to this:  My layman’s interpretation is this: given all the possible ways we could   decompose the input into factors, whatever representation we ultimately choose there is some other representation in which a change to a single dimension in the first impacts all the dimensions of the second (they are entangled).", "There’s no way for an unsupervised method to distinguish between these two equivalent generative models, and thus the resulting learned representation must be entangled with at least one of them.", "After observing  , we can construct many generative models which have the same marginal distribution of  .", "Any one of these models could be the true causal generative model for the data, and the right model cannot be identified given only the distribution of  .", "For a wonderful demonstration of this in lower dimensions, see ‘ Same stats, different graphs ’.", "In practice, might we be able to learn disentangled representations anyway?", "While Theorem 1 shows that unsupervised disentanglement learning is fundamentally impossible for arbitrary generative models, this does not necessarily mean that it is an impossible endeavour in practice.", "After all, real world generative models may have a certain structure that could be exploited through suitably chosen inductive biases.", "But, the authors argue, you should make explicit the inductive biases you are selecting.", "To investigate all this the authors take six recent unsupervised disentanglement methods, train them over seven different data sets, and evaluate them using six different disentanglement measures.", "The result is  a corpus of more than 10,000 trained models.", "The library used to train and evaluate these models, disentanglement_lib has been made available at  [url]"], "summary_text": "Challenging common assumptions in the unsupervised learning of disentangled representations Locatello et al., ICML’19  Today’s paper choice won a best paper award at ICML’19. The ‘common assumptions’ that the paper challenges seem to be: “unsupervised learning of disentangled representations is possible, and useful!”  The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. What exactly is a ‘disentangled representation’ and why might we want one? Put the ‘disentangled’ part to one side for a moment, and let’s start out by revisiting what we mean by a representation. Given a real-world observation  (e.g. of an image or video), a representation  is a transformation of  (typically to a lower dimensional space in order to be useful) that somehow preserves the salient information in the  so that we can still use  to extract useful information about the input (e.g. for building classifiers). As a trivial example, suppose we had real world observations consisting of 1000 points sampled from a straight line, a good lower-valued representation would be a (gradient, intercept) tuple. Of course real-world examples are much more complex than this! A disentangled representation is a representation with a compact and interpretable structure, which captures the essence of the input independent of the task the representation is ultimately going to be used for. That’s quite tricky – even in my contrived straight line example what looked to be a great representation would be useless if the task turned out to be calculating the area of the the rectangle enclosed by the points in the observation. While there is no single formalized notion of disentanglement (yet) which is widely accepted, the key intuition is that a disentangled representation should separate the distinct information factors of variation in the data. A change in a single underlying factor of variation should lead to a change in a single factor in the learned representation. The state of the art for representation learning centres around Variational Autoencoders, using one deep neural network to learn a representation, and another one to attempt to reconstruct the original input from that representation. The representation  is usually taken as the mean of the approximate posterior distribution of the first (encoding) network. In theory, disentanglement is impossible  We theoretically prove that (perhaps unsurprisingly) the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases both on the considered learning approaches and the data sets. The full proof is giving in appendix A (missing from my copy of the pdf), but it boils down to this:  My layman’s interpretation is this: given all the possible ways we could   decompose the input into factors, whatever representation we ultimately choose there is some other representation in which a change to a single dimension in the first impacts all the dimensions of the second (they are entangled). There’s no way for an unsupervised method to distinguish between these two equivalent generative models, and thus the resulting learned representation must be entangled with at least one of them. After observing  , we can construct many generative models which have the same marginal distribution of  . Any one of these models could be the true causal generative model for the data, and the right model cannot be identified given only the distribution of  . For a wonderful demonstration of this in lower dimensions, see ‘ Same stats, different graphs ’. In practice, might we be able to learn disentangled representations anyway? While Theorem 1 shows that unsupervised disentanglement learning is fundamentally impossible for arbitrary generative models, this does not necessarily mean that it is an impossible endeavour in practice. After all, real world generative models may have a certain structure that could be exploited through suitably chosen inductive biases. But, the authors argue, you should make explicit the inductive biases you are selecting. To investigate all this the authors take six recent unsupervised disentanglement methods, train them over seven different data sets, and evaluate them using six different disentanglement measures. The result is  a corpus of more than 10,000 trained models. The library used to train and evaluate these models, disentanglement_lib has been made available at  [url]", "pdf_url": "http://proceedings.mlr.press/v97/locatello19a/locatello19a.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/challenging-common-assumptions-in-the-unsupervised-learning-of-disentangled-representations.json"}
{"id": "61558513", "bin": "800_900", "summary_sentences": ["The paper introduces a query reformulation system that rewrites a query to maximise the number of “relevant” documents that are extracted from a given black box search engine.", "A Reinforcement Learning (RL) agent selects the terms that are to be added to the reformulated query and the rewards are decided on the basis of document recall.", "Implementation  Key Aspect  The underlying problem is as follows: when the end user makes a query to a search engine, the engine often relies on word matching techniques to perform retrieval.", "This means relevant documents could be missed if there is no exactly matching words between the query and the document.", "This problem can be handled at two levels: First, the search engine itself takes care of query semantics.", "Alternatively, we assume the search engine to be dumb and instead have a system in place that can improve the original queries (automatic query reformulation).", "The paper takes the latter approach and expands the original query by adding terms from the set of retrieved documents (pseudo relevance feedback).", "Datasets  TREC - Complex Answer Retrieval (TREC-CAR)  Jeopardy Q&A dataset  Microsoft Academic (MSA) dataset - created by the authors using papers crawled from Microsoft Academic API  Framework  Query Reformulation task is modeled as an RL problem where:  Environment is the search engine.", "Actions are whether a word is to be added to the query or not and if yes, then what word is added.", "Reward is the retrieval accuracy.", "The input to the system is a query q0 consisting of a sequence of words w1, …, wn and a candidate term ti with some context words.", "Candidate terms are all the terms that appear in the original query and the documents retrieved using the query.", "The words are mapped to vectors and then a fixed size representation is obtained for the sequence using CNN’s or RNNs.", "Similarly, a representation is obtained for the candidate words by feeding them and their context words to the CNN or RNNs.", "Finally, a sigmoidal score is computed for all the candidate words.", "An RNN sequentially applies this model to emit query words till an end token is emitted.", "Vocabulary is used only from the extracted documents and not the entire vocabulary set, to keep the inference fast.", "Training  The model is trained using REINFORCE algorithm which minimizes the Ca = (R − R~) * sum(log(P(t|q))) where R~ is the baseline.", "Value network minimises Cb = &\\alpha(||R-R~||2)  Ca and Cb are minimised using SGD.", "An entropy regulation term is added to prevent the probability distribution from reaching the peak.", "Experiments  Baseline Methods  Raw - Original query is fed to the search engine without any modification.", "Pseudo-Relevance Feedback (PRF-TFIDF) - The query is expanded using the top-N TF-IDF terms.", "PRF-Relevance Model (PRF-RM) - Probability of adding token t to the query q0 is given by P(t|q0) = (1 − λ)P′(t|q0) + λ sum (P(d)P(t|d)P(q0|d))  Proposed Methods  Supervised Learning  Assumes that the query words contribute indepently to the query retrival performace.", "(Too strong an assumption).", "A term is marked as relevant if (R(new_query) - R(old_query))/R(old_query) > 0.005  Reinforcement Learning  RL-RNN/CNN - RL Framework + RNN/CNN to encode the input features.", "RL-RNN-SEQ - Add a sequential generator.", "Metrics  Recall@K  Precision@K  Mean Average Precision@K  Reward - The paper uses Recall@K as a reward when training the RL-based models with the argument that the “metric has shown to be effective in improving the other metrics as well”, without any justification though.", "SL-Oracle - classifier that perfectly selects terms that will increase performance based on the supervised learning approach.", "RL-Oracle - Produces a conservative upper-bound for the performance of the RL Agent.", "It splits the test data into N subsets and trains an RL agent for each subset.", "Then, the reward is averaged over all the N subsets.", "Observations  Reformulation based methods > original query  RL methods > Supervised methods > unsupervised methods  RL-RNN-SEQ performs slightly worse than RL-RNN but is much faster (as it produces shorter queries).", "RL-based model benefits from more candidate terms while the classical PRF method quickly saturates.", "Comments  Interestingly, for each raw query, they carried out the reformulation step just once and not multiple times.", "The number of times a query is reformulated could also have become a part of the RL framework."], "summary_text": "The paper introduces a query reformulation system that rewrites a query to maximise the number of “relevant” documents that are extracted from a given black box search engine. A Reinforcement Learning (RL) agent selects the terms that are to be added to the reformulated query and the rewards are decided on the basis of document recall. Implementation  Key Aspect  The underlying problem is as follows: when the end user makes a query to a search engine, the engine often relies on word matching techniques to perform retrieval. This means relevant documents could be missed if there is no exactly matching words between the query and the document. This problem can be handled at two levels: First, the search engine itself takes care of query semantics. Alternatively, we assume the search engine to be dumb and instead have a system in place that can improve the original queries (automatic query reformulation). The paper takes the latter approach and expands the original query by adding terms from the set of retrieved documents (pseudo relevance feedback). Datasets  TREC - Complex Answer Retrieval (TREC-CAR)  Jeopardy Q&A dataset  Microsoft Academic (MSA) dataset - created by the authors using papers crawled from Microsoft Academic API  Framework  Query Reformulation task is modeled as an RL problem where:  Environment is the search engine. Actions are whether a word is to be added to the query or not and if yes, then what word is added. Reward is the retrieval accuracy. The input to the system is a query q0 consisting of a sequence of words w1, …, wn and a candidate term ti with some context words. Candidate terms are all the terms that appear in the original query and the documents retrieved using the query. The words are mapped to vectors and then a fixed size representation is obtained for the sequence using CNN’s or RNNs. Similarly, a representation is obtained for the candidate words by feeding them and their context words to the CNN or RNNs. Finally, a sigmoidal score is computed for all the candidate words. An RNN sequentially applies this model to emit query words till an end token is emitted. Vocabulary is used only from the extracted documents and not the entire vocabulary set, to keep the inference fast. Training  The model is trained using REINFORCE algorithm which minimizes the Ca = (R − R~) * sum(log(P(t|q))) where R~ is the baseline. Value network minimises Cb = &\\alpha(||R-R~||2)  Ca and Cb are minimised using SGD. An entropy regulation term is added to prevent the probability distribution from reaching the peak. Experiments  Baseline Methods  Raw - Original query is fed to the search engine without any modification. Pseudo-Relevance Feedback (PRF-TFIDF) - The query is expanded using the top-N TF-IDF terms. PRF-Relevance Model (PRF-RM) - Probability of adding token t to the query q0 is given by P(t|q0) = (1 − λ)P′(t|q0) + λ sum (P(d)P(t|d)P(q0|d))  Proposed Methods  Supervised Learning  Assumes that the query words contribute indepently to the query retrival performace. (Too strong an assumption). A term is marked as relevant if (R(new_query) - R(old_query))/R(old_query) > 0.005  Reinforcement Learning  RL-RNN/CNN - RL Framework + RNN/CNN to encode the input features. RL-RNN-SEQ - Add a sequential generator. Metrics  Recall@K  Precision@K  Mean Average Precision@K  Reward - The paper uses Recall@K as a reward when training the RL-based models with the argument that the “metric has shown to be effective in improving the other metrics as well”, without any justification though. SL-Oracle - classifier that perfectly selects terms that will increase performance based on the supervised learning approach. RL-Oracle - Produces a conservative upper-bound for the performance of the RL Agent. It splits the test data into N subsets and trains an RL agent for each subset. Then, the reward is averaged over all the N subsets. Observations  Reformulation based methods > original query  RL methods > Supervised methods > unsupervised methods  RL-RNN-SEQ performs slightly worse than RL-RNN but is much faster (as it produces shorter queries). RL-based model benefits from more candidate terms while the classical PRF method quickly saturates. Comments  Interestingly, for each raw query, they carried out the reformulation step just once and not multiple times. The number of times a query is reformulated could also have become a part of the RL framework.", "pdf_url": "https://arxiv.org/pdf/1704.04572", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/task-oriented-query-reformulation-with-reinforcement-learning.json"}
{"id": "187460", "bin": "800_900", "summary_sentences": ["Helping Developers Help Themselves: Automatic Decomposition of Code Review Changes – Barnett et al. 2015  Earlier this week we saw that pull requests with well organised commits are strongly preferred by integrators .", "Unfortunately, developers often make changes that incorporate multiple bug fixes, feature additions, refactorings, etc..", "These result in changes that are both large and only loosely related, if at all, leading to difficulty in understanding.", "Rounding out this week of papers from ICSE ’15, Barnett et al. from Microsoft developed a tool called ClusterChanges which decomposes changesets into independent parts.", "In a study, developers found the partioning to be a helpful aid during code reviews.", "… we built a prototype graphical tool and used it to investigate changesets submitted for review in Bing and Office at Microsoft.", "Our quantitative evaluation shows that over 40% of changes submitted for review at Microsoft can be potentially decomposed into multiple partitions, indicating a high potential for use.", "The basic approach to identifying related changes is to take the diff-regions produced by a standard diff tool comparing before and after versions of files, and then group those diff-regions together based on definition-and-use relationships.", "We use the def-use relationship as the primary organizing principle for clustering diff-regions.", "Programmers often introduce interesting functional changes to code by introducing or modifying definitions along with their uses.", "ClusterChanges finds definitions (of types, fields, and methods) that have been changed within a diff-region, and the uses of that definition changed within diff-regions.", "Diff-regions f1 and f2 are then grouped into the same partition (RelatedDiffs) if any one of the following conditions is true:  f1 and f2 are both within the same enclosing method, or  there are changes to the definition of some element in f1 and corresponding changes in the use of that element in f2  f1 and f2 both contain a change to the use of some element, and that element is defined within the changeset, but not itself changed  We group diff-regions in the same method together because a) in practice, we observe that changes to the same method are often related, and b) in prior research, we observed that reviewers usually review methods atomically (i.e., they rarely review different diffregions in a method separately).", "Given these relations we create a partitioning over the set of diff-regions by computing the reflexive, symmetric and transitive closure of RelatedDiffs.", "The result of this process is a set of trivial partitions that are fully enclosed within a single method, or where there is only one diff-region and it is outside of a method, and a set of non-trivial partitions (everything else).", "The ClusterChanges tool then displays these partitions graphically:  ClusterChanges was applied to a randomly selected set of 1000 changesets submitted for review in the development of Microsoft Office 2013.", "While the most common case are changesets containing just one non-trivial partition, this still makes up only 45%.", "Nearly 42% of all changes contain more than one non-trivial partition.", "In addition, the proportion of changed methods that end up in non-trivial partitions is 66% on average per review.", "To the degree that CLUSTERCHANGES correctly identifies non-trivial partitions, this indicates that i) a large proportion of changesets can be decomposed into multiple independent changes, and ii) our decomposition covers a large fraction of changed methods in a review.", "Looking at changesets with lots of partitions, the authors found that many of these could be further consolidated by an enhancement to the tool that also considered:  (a) annotating several methods with common C# attributes such as Serializable or Obsolete, (b) a common refactoring (e.g. addition of a log message or variable renaming) across a large number of methods, and (c) relationships between overridden methods and their implementations.", "A user study was conducted in which the developers responsible for the changesets were asked if they agreed with the automated decomposition.", "Of the 20 participants, 16 said that our non-trivial partitions were both correct and complete, i.e., the non-trivial partitions were indeed independent, the diff-regions within each partition were related and there were no missing conceptual groups… most developers agree with our automatic partitioning and believe the decomposition is useful for reviewers to understand their changes better (some even asked for the prototype to use on their own reviews going forward).", "With these promising early results, the authors will now be moving on to do further studies with code reviewers."], "summary_text": "Helping Developers Help Themselves: Automatic Decomposition of Code Review Changes – Barnett et al. 2015  Earlier this week we saw that pull requests with well organised commits are strongly preferred by integrators . Unfortunately, developers often make changes that incorporate multiple bug fixes, feature additions, refactorings, etc.. These result in changes that are both large and only loosely related, if at all, leading to difficulty in understanding. Rounding out this week of papers from ICSE ’15, Barnett et al. from Microsoft developed a tool called ClusterChanges which decomposes changesets into independent parts. In a study, developers found the partioning to be a helpful aid during code reviews. … we built a prototype graphical tool and used it to investigate changesets submitted for review in Bing and Office at Microsoft. Our quantitative evaluation shows that over 40% of changes submitted for review at Microsoft can be potentially decomposed into multiple partitions, indicating a high potential for use. The basic approach to identifying related changes is to take the diff-regions produced by a standard diff tool comparing before and after versions of files, and then group those diff-regions together based on definition-and-use relationships. We use the def-use relationship as the primary organizing principle for clustering diff-regions. Programmers often introduce interesting functional changes to code by introducing or modifying definitions along with their uses. ClusterChanges finds definitions (of types, fields, and methods) that have been changed within a diff-region, and the uses of that definition changed within diff-regions. Diff-regions f1 and f2 are then grouped into the same partition (RelatedDiffs) if any one of the following conditions is true:  f1 and f2 are both within the same enclosing method, or  there are changes to the definition of some element in f1 and corresponding changes in the use of that element in f2  f1 and f2 both contain a change to the use of some element, and that element is defined within the changeset, but not itself changed  We group diff-regions in the same method together because a) in practice, we observe that changes to the same method are often related, and b) in prior research, we observed that reviewers usually review methods atomically (i.e., they rarely review different diffregions in a method separately). Given these relations we create a partitioning over the set of diff-regions by computing the reflexive, symmetric and transitive closure of RelatedDiffs. The result of this process is a set of trivial partitions that are fully enclosed within a single method, or where there is only one diff-region and it is outside of a method, and a set of non-trivial partitions (everything else). The ClusterChanges tool then displays these partitions graphically:  ClusterChanges was applied to a randomly selected set of 1000 changesets submitted for review in the development of Microsoft Office 2013. While the most common case are changesets containing just one non-trivial partition, this still makes up only 45%. Nearly 42% of all changes contain more than one non-trivial partition. In addition, the proportion of changed methods that end up in non-trivial partitions is 66% on average per review. To the degree that CLUSTERCHANGES correctly identifies non-trivial partitions, this indicates that i) a large proportion of changesets can be decomposed into multiple independent changes, and ii) our decomposition covers a large fraction of changed methods in a review. Looking at changesets with lots of partitions, the authors found that many of these could be further consolidated by an enhancement to the tool that also considered:  (a) annotating several methods with common C# attributes such as Serializable or Obsolete, (b) a common refactoring (e.g. addition of a log message or variable renaming) across a large number of methods, and (c) relationships between overridden methods and their implementations. A user study was conducted in which the developers responsible for the changesets were asked if they agreed with the automated decomposition. Of the 20 participants, 16 said that our non-trivial partitions were both correct and complete, i.e., the non-trivial partitions were indeed independent, the diff-regions within each partition were related and there were no missing conceptual groups… most developers agree with our automatic partitioning and believe the decomposition is useful for reviewers to understand their changes better (some even asked for the prototype to use on their own reviews going forward). With these promising early results, the authors will now be moving on to do further studies with code reviewers.", "pdf_url": "http://research.microsoft.com/pubs/238937/barnett2015hdh.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/helping-developers-help-themselves-automatic-decomposition-of-code-review-changes.json"}
{"id": "79471009", "bin": "800_900", "summary_sentences": ["Standard Deep Learning networks are not suitable for continual learning setting as the change in the data distribution leads to catastrophic forgetting.", "The paper proposes Memory-based Parameter Adaptation (MbPA), a technique that augments a standard neural network with an episodic memory (containing examples from the previous tasks).", "This episodic memory allows for rapid acquisition of new knowledge (corresponding to the current task) while preserving performance on the previous tasks.", "Architecture  MbPA consists of 3 components:  Embedding Network f  Memory M  Output network g  f and g are parametric components while M is a non-parametric component.", "M is a dynamically sized dictionary where the key represents the output of the embedding network and the value represents the desired output for a given input (input to the model).", "When a new training tuple (xj, yj) is fed as input to the model, a key-value pair (hj, vj) is added to the memory.", "hj = f(xj)  The memory has a fixed size and acts as a circular buffer.", "When it gets filled up, earlier examples are dropped.", "When accessing the memory using a key hkey, the k-nearest neighbours (in terms of distance from the given key) are retrieved.", "Training Phase  During the training phase, the memory is only used to store the input examples and does not interfere with the training procedure.", "Testing Phase  During testing, the memory is used to adapt the parameters of the output network g while the embedding network f remains the same.", "Given the input x, obtain the embedding corresponding to x and using that as the key, retrieve the k-nearest neighbours from the memory.", "Each retrived neighbour is a tuple of the form (hk, vk, wk) where wk is propotional to the closeness between the input query and the key corresponding to the retrived example.", "The collection of all the retrieved examples are referred to as the context C.  The parameters of the output network g are adapted from θ to θx where θx = θ + δM(x, θ)  δM(x, θ) is referred to as the contextual update of parameters of the output network.", "Interpretation of MbPA  MbPA can be interpreted as decreasing the weighted average of negative log likelihood over the retrieved neighbours in the context C.  The expression corresponding to  δM(x, θ) can be obtained by performing gradient descent to minimise the max a posterior over the context C.  The a posterior expression can be written as a sum of two terms - one corresponding to a weighted likelihood of data in the context C and the other corresponding to a regularisation term to prevent overfitting the data.", "This idea can be thought of as a generalisation of attention.", "Attention can be viewed as fitting a constant function over the neighbourhood of memories while MbPA fits a more general function which is parameterised by the output network of the given model.", "Refer appendix E in the paper for further details.", "Experiments  MbPA aims to solve the fundamental problem of enabling the model to deal with changes in data distribution.", "In that sense, it is evaluated on a wide range of settings: continual learning, incremental learning, unbalanced datasets and change in data distribution at test time.", "Continual Learning:  In this setting, the model encounters a sequence of tasks and cannot revisit a previous task.", "Permuted MNIST dataset was used.", "The key takeaway is that once a task is catastrophically forgotten, only a few gradient updates on a carefully selected data, are sufficient to recover the performance.", "Incremental Learning:  In this setting, the model is trained on a subset of classes and then introduced to novel, unseen classes.", "The model is tested to see if it can incorporate the new knowledge while retaining the knowledge about the previous classes.", "Imagenet dataset with Resnet V1 model is used.", "It is first pretrained on 500 classes and then fine-tuned to see how quickly could it adapt to new classes.", "Unbalanced Dataset:  This setting is similar to the incremental learning setting with the key difference that once the model has been trained on a part of the dataset and is to be finetuned to acquire new knowledge, the dataset used for finetuning is much smaller than the initial dataset thus creating the effect of unbalanced datasets.", "Language Modelling:  MbPA is used to adapt to the shift in the word distribution that is common to language modelling tasks.", "PTB and WikiText datasets were used.", "MbPA exhibits strong performance on all these tasks showing that the memory-based parameter adaption technique is effective across a range of tasks in supervised learning."], "summary_text": "Standard Deep Learning networks are not suitable for continual learning setting as the change in the data distribution leads to catastrophic forgetting. The paper proposes Memory-based Parameter Adaptation (MbPA), a technique that augments a standard neural network with an episodic memory (containing examples from the previous tasks). This episodic memory allows for rapid acquisition of new knowledge (corresponding to the current task) while preserving performance on the previous tasks. Architecture  MbPA consists of 3 components:  Embedding Network f  Memory M  Output network g  f and g are parametric components while M is a non-parametric component. M is a dynamically sized dictionary where the key represents the output of the embedding network and the value represents the desired output for a given input (input to the model). When a new training tuple (xj, yj) is fed as input to the model, a key-value pair (hj, vj) is added to the memory. hj = f(xj)  The memory has a fixed size and acts as a circular buffer. When it gets filled up, earlier examples are dropped. When accessing the memory using a key hkey, the k-nearest neighbours (in terms of distance from the given key) are retrieved. Training Phase  During the training phase, the memory is only used to store the input examples and does not interfere with the training procedure. Testing Phase  During testing, the memory is used to adapt the parameters of the output network g while the embedding network f remains the same. Given the input x, obtain the embedding corresponding to x and using that as the key, retrieve the k-nearest neighbours from the memory. Each retrived neighbour is a tuple of the form (hk, vk, wk) where wk is propotional to the closeness between the input query and the key corresponding to the retrived example. The collection of all the retrieved examples are referred to as the context C.  The parameters of the output network g are adapted from θ to θx where θx = θ + δM(x, θ)  δM(x, θ) is referred to as the contextual update of parameters of the output network. Interpretation of MbPA  MbPA can be interpreted as decreasing the weighted average of negative log likelihood over the retrieved neighbours in the context C.  The expression corresponding to  δM(x, θ) can be obtained by performing gradient descent to minimise the max a posterior over the context C.  The a posterior expression can be written as a sum of two terms - one corresponding to a weighted likelihood of data in the context C and the other corresponding to a regularisation term to prevent overfitting the data. This idea can be thought of as a generalisation of attention. Attention can be viewed as fitting a constant function over the neighbourhood of memories while MbPA fits a more general function which is parameterised by the output network of the given model. Refer appendix E in the paper for further details. Experiments  MbPA aims to solve the fundamental problem of enabling the model to deal with changes in data distribution. In that sense, it is evaluated on a wide range of settings: continual learning, incremental learning, unbalanced datasets and change in data distribution at test time. Continual Learning:  In this setting, the model encounters a sequence of tasks and cannot revisit a previous task. Permuted MNIST dataset was used. The key takeaway is that once a task is catastrophically forgotten, only a few gradient updates on a carefully selected data, are sufficient to recover the performance. Incremental Learning:  In this setting, the model is trained on a subset of classes and then introduced to novel, unseen classes. The model is tested to see if it can incorporate the new knowledge while retaining the knowledge about the previous classes. Imagenet dataset with Resnet V1 model is used. It is first pretrained on 500 classes and then fine-tuned to see how quickly could it adapt to new classes. Unbalanced Dataset:  This setting is similar to the incremental learning setting with the key difference that once the model has been trained on a part of the dataset and is to be finetuned to acquire new knowledge, the dataset used for finetuning is much smaller than the initial dataset thus creating the effect of unbalanced datasets. Language Modelling:  MbPA is used to adapt to the shift in the word distribution that is common to language modelling tasks. PTB and WikiText datasets were used. MbPA exhibits strong performance on all these tasks showing that the memory-based parameter adaption technique is effective across a range of tasks in supervised learning.", "pdf_url": "https://arxiv.org/pdf/1802.10542", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/memory-based-parameter-adaption.json"}
{"id": "39353178", "bin": "800_900", "summary_sentences": ["In the search for more general AI agents, we must eventually abandon the practice of handcrafting reward functions in RL.", "As of right now, state-of-the-art RL agents still require a “good” reward function that helps the agent learn complex behaviors and, by extension, an approximation of the optimal policy.", "For example, consider the popular pendulum task.", "The goal of the agent is to swing up and balance the pendulum.", "If the reward function is simply a +1 for having balanced the pendulum at the end of the episode (achieved the goal), or a -1 if the agent failed to balance the pendulum (failed to achieve the goal), the agent would never learn anything.", "With positive rewards arriving so incredibly infrequently, the agent would not have any motivation to explore different swing up behaviors, since every action it tries would seem equally bad.", "Hence, a more complex reward function is necessary that encourages the agent to develop intermediate “sub-behaviors” which allow it to achieve its goal.", "One solution to this problem as presented by the authors is focusing on the acquisition of skills, or options, that provide the agent with the ability to use composition to carry out heirarchical planning-tasks.", "The authors claim that agents should have a sophisticated internal motivational system that should not have to be redesigned for different problems.", "Options are closed-loop “mini”-policies for taking action over a period of time.", "The authors present a learning framework based on semi-Markov Decision Processes , which are used for adding temporal abstractions to RL.", "This framework utilizes the saliency of certain events that occur in the environment to generate intrinsic reward signals that stimulate curiosity within the agent.", "Eventually, the agent will “lose interest” in the event, as it loses its novelty (a.k.a.", "boredom), but retain knowledge of the interaction.", "Extrinsic reward signals are present and are generated by accomplishing goals.", "The authors demonstrated their framework on a small world experiment, where an agent in a grid-world was able to interact with a few objects.", "Some of the options it could learn involved turning a light switch on and off, kicking a ball, or making a bell ring.", "Notes  The policies of many options are updated simultaneously during an agent’s interaction with the environment.", "If an option could have produced the current action in the current state, its policy can be updated as well.", "In general, options have to be provided by the system designer.", "The state that could lead to the execution of an option, the option’s terminating condition, and the reward function that evaluates the option’s performance is required.", "It is desirable to automate the discovery of new options.", "This paper is a good starting point for looking into the area of RL that deals with the reward-function problem.", "The concept of developing a motivational system that is shared across tasks is one that has not really been explored/is not prevalent today.", "Going back to my example of the pendulum task- an RL agent would require an internal motivational system that understood physics in order to explore the environment effectively and receive a reward from the salient event of balancing the pendulum.", "If the agent could understand from prior experiences what balancing means (i.e., the agent utilizes a learned model of physics to generate some type of understanding of the physical properties of an object in a balanced state), then the agent could motivate itself to select specific sequences of actions that bring the pendulum closer to a balanced state.", "Therefore, the system designer could use a much simpler and less informative reward signal without having to do almost any hand-crafting.", "An interesting experiment to test this would be to train an RL agent on a number of different tasks that involve balancing, and then to use transfer learning (sharing network weights?)", "to have the agent solve the pendulum task.", "Or, to use a form of (differentiable) memory to store an approximately optimal set of sequences of actions related to balancing objects on a number of tasks.", "The idea is to capture the learned “skills” and transfer them to new tasks.", "The agent could use these experiences to accelerate learning on a novel balancing task.", "If the memory is associative, you could also store multiple physical skills beyond just balancing.", "From a practical standpoint, the prior experience would need to influence the Q-values of a given state and action through some sort of “bonus”."], "summary_text": "In the search for more general AI agents, we must eventually abandon the practice of handcrafting reward functions in RL. As of right now, state-of-the-art RL agents still require a “good” reward function that helps the agent learn complex behaviors and, by extension, an approximation of the optimal policy. For example, consider the popular pendulum task. The goal of the agent is to swing up and balance the pendulum. If the reward function is simply a +1 for having balanced the pendulum at the end of the episode (achieved the goal), or a -1 if the agent failed to balance the pendulum (failed to achieve the goal), the agent would never learn anything. With positive rewards arriving so incredibly infrequently, the agent would not have any motivation to explore different swing up behaviors, since every action it tries would seem equally bad. Hence, a more complex reward function is necessary that encourages the agent to develop intermediate “sub-behaviors” which allow it to achieve its goal. One solution to this problem as presented by the authors is focusing on the acquisition of skills, or options, that provide the agent with the ability to use composition to carry out heirarchical planning-tasks. The authors claim that agents should have a sophisticated internal motivational system that should not have to be redesigned for different problems. Options are closed-loop “mini”-policies for taking action over a period of time. The authors present a learning framework based on semi-Markov Decision Processes , which are used for adding temporal abstractions to RL. This framework utilizes the saliency of certain events that occur in the environment to generate intrinsic reward signals that stimulate curiosity within the agent. Eventually, the agent will “lose interest” in the event, as it loses its novelty (a.k.a. boredom), but retain knowledge of the interaction. Extrinsic reward signals are present and are generated by accomplishing goals. The authors demonstrated their framework on a small world experiment, where an agent in a grid-world was able to interact with a few objects. Some of the options it could learn involved turning a light switch on and off, kicking a ball, or making a bell ring. Notes  The policies of many options are updated simultaneously during an agent’s interaction with the environment. If an option could have produced the current action in the current state, its policy can be updated as well. In general, options have to be provided by the system designer. The state that could lead to the execution of an option, the option’s terminating condition, and the reward function that evaluates the option’s performance is required. It is desirable to automate the discovery of new options. This paper is a good starting point for looking into the area of RL that deals with the reward-function problem. The concept of developing a motivational system that is shared across tasks is one that has not really been explored/is not prevalent today. Going back to my example of the pendulum task- an RL agent would require an internal motivational system that understood physics in order to explore the environment effectively and receive a reward from the salient event of balancing the pendulum. If the agent could understand from prior experiences what balancing means (i.e., the agent utilizes a learned model of physics to generate some type of understanding of the physical properties of an object in a balanced state), then the agent could motivate itself to select specific sequences of actions that bring the pendulum closer to a balanced state. Therefore, the system designer could use a much simpler and less informative reward signal without having to do almost any hand-crafting. An interesting experiment to test this would be to train an RL agent on a number of different tasks that involve balancing, and then to use transfer learning (sharing network weights?) to have the agent solve the pendulum task. Or, to use a form of (differentiable) memory to store an approximately optimal set of sequences of actions related to balancing objects on a number of tasks. The idea is to capture the learned “skills” and transfer them to new tasks. The agent could use these experiences to accelerate learning on a novel balancing task. If the memory is associative, you could also store multiple physical skills beyond just balancing. From a practical standpoint, the prior experience would need to influence the Q-values of a given state and action through some sort of “bonus”.", "pdf_url": "http://www.cs.cornell.edu/~helou/IMRL.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/intrinsically-motivated-rl.json"}
{"id": "11006232", "bin": "800_900", "summary_sentences": ["A few useful things to know about machine learning – Domingos 2012  Developing successful machine learning applications requires a substantial amount of ‘black art’ that is hard to find in textbooks  This paper looks at twelve key lessons including pitfalls to avoid, important issues to focus on, and answers to common questions.", "The paper was published in 2012, and since then the excellent ‘ Data Science for Business ‘ book by Provost and Fawcett has been released by O’Reilly.", "Now much of the wisdom from this paper can indeed be found in a textbook!", "If you enjoy this paper, I highly recommend the book as well.", "According to the paper, the key to not getting lost in the huge space of learning algorithms is to understand that they are composed of three elements: a representation model (e.g. k-nearest neighbour, naive bayes, decision trees); an evaluation function (scoring function) to tell good from bad; and an optimization technique to search for the highest scoring classifier.", "Most textbooks are organized by representation, and it’s easy to overlook the fact that the other components are equally important.", "You want your machine learning to work well (generalize) outside of the examples in the training set you gave it.", "The most common mistake among machine learning beginners is to test on the training data and have the illusion of success… in the early days of machine learning, the need to keep training and test data separate was not widely appreciated.", "(Of course, if you started your journey as a ‘machine learning beginner’ by taking Andrew Ng’s online course you won’t be falling into this trap!).", "Data alone is not enough, you also need to supply some domain knowledge to help guide the process.", "Machine learning is not magic; it can’t get something from nothing.", "What it does is get more from less.", "Programming, like all engineering, is a lot of work: we have to build everything from scratch.", "Learning is more like farming, which lets nature do most of the work.", "Farmers combine seeds with nutrients to grow crops.", "Learners combine knowledge with data to grow programs.", "Overfitting is a well-known problem in machine learning, and cross-validation can help to combat this.", "Nevertheless, you should be skeptical of claims that a particular technique ‘solves’ the overfitting problem.", "It’s easy to avoid overfitting (variance), by falling into the opposite problem of underfitting (bias).", "Simultaneously avoiding both requires learning a perfect classifier, and short of knowing it in advance there is no single technique that will always do best.", "The ‘curse of dimensionality’ refers to the fact that many algorithms that work in low dimensions become intractable when the input is high-dimensional.", "…the similarity-based reasoning that machine learning algorithms depend on (explicitly or implicitly) breaks down in high dimensions  Fortunately many problem domains are non-uniform giving an effectively lower dimension, or algorithms for explicitly reducing the dimensionality can be used.", "The most important factor in the success of a machine learning project is the features used.", "If the raw data is not in a form that is amenable to learning, you may be able to construct features from it that are:  First-timers are often surprised by how little time in a machine learning project is spent actually doing machine learning.", "But it makes sense if you consider how time consuming it is to gather data, integrate it, clean it and pre-process it, and how much trial and error can go into feature design.", "What’s better?", "Smart algorithms or lots of data:  As a rule of thumb, a dumb algorithm with lots and lots of data beats a clever one with modest amounts of it.", "This brings up the problem of scalability:  In most of computer science, the two main limited resources are time and memory.", "In machine learning there is a third one: training data.", "Which one is the bottleneck has changed from decade to decade.", "Why have just one learner though, when you can have many, ensembles of learners do best.", "…researchers noticed that if instead of selecting the best variation found, we combine many variations, the results are better – often much better – and at little extra effort for the user  The winner of the ‘ Netflix prize ‘ was a stacked ensemble of over 100 learners.", "There’s plenty more folk wisdom in the paper, so do check it out if this has piqued your interest."], "summary_text": "A few useful things to know about machine learning – Domingos 2012  Developing successful machine learning applications requires a substantial amount of ‘black art’ that is hard to find in textbooks  This paper looks at twelve key lessons including pitfalls to avoid, important issues to focus on, and answers to common questions. The paper was published in 2012, and since then the excellent ‘ Data Science for Business ‘ book by Provost and Fawcett has been released by O’Reilly. Now much of the wisdom from this paper can indeed be found in a textbook! If you enjoy this paper, I highly recommend the book as well. According to the paper, the key to not getting lost in the huge space of learning algorithms is to understand that they are composed of three elements: a representation model (e.g. k-nearest neighbour, naive bayes, decision trees); an evaluation function (scoring function) to tell good from bad; and an optimization technique to search for the highest scoring classifier. Most textbooks are organized by representation, and it’s easy to overlook the fact that the other components are equally important. You want your machine learning to work well (generalize) outside of the examples in the training set you gave it. The most common mistake among machine learning beginners is to test on the training data and have the illusion of success… in the early days of machine learning, the need to keep training and test data separate was not widely appreciated. (Of course, if you started your journey as a ‘machine learning beginner’ by taking Andrew Ng’s online course you won’t be falling into this trap!). Data alone is not enough, you also need to supply some domain knowledge to help guide the process. Machine learning is not magic; it can’t get something from nothing. What it does is get more from less. Programming, like all engineering, is a lot of work: we have to build everything from scratch. Learning is more like farming, which lets nature do most of the work. Farmers combine seeds with nutrients to grow crops. Learners combine knowledge with data to grow programs. Overfitting is a well-known problem in machine learning, and cross-validation can help to combat this. Nevertheless, you should be skeptical of claims that a particular technique ‘solves’ the overfitting problem. It’s easy to avoid overfitting (variance), by falling into the opposite problem of underfitting (bias). Simultaneously avoiding both requires learning a perfect classifier, and short of knowing it in advance there is no single technique that will always do best. The ‘curse of dimensionality’ refers to the fact that many algorithms that work in low dimensions become intractable when the input is high-dimensional. …the similarity-based reasoning that machine learning algorithms depend on (explicitly or implicitly) breaks down in high dimensions  Fortunately many problem domains are non-uniform giving an effectively lower dimension, or algorithms for explicitly reducing the dimensionality can be used. The most important factor in the success of a machine learning project is the features used. If the raw data is not in a form that is amenable to learning, you may be able to construct features from it that are:  First-timers are often surprised by how little time in a machine learning project is spent actually doing machine learning. But it makes sense if you consider how time consuming it is to gather data, integrate it, clean it and pre-process it, and how much trial and error can go into feature design. What’s better? Smart algorithms or lots of data:  As a rule of thumb, a dumb algorithm with lots and lots of data beats a clever one with modest amounts of it. This brings up the problem of scalability:  In most of computer science, the two main limited resources are time and memory. In machine learning there is a third one: training data. Which one is the bottleneck has changed from decade to decade. Why have just one learner though, when you can have many, ensembles of learners do best. …researchers noticed that if instead of selecting the best variation found, we combine many variations, the results are better – often much better – and at little extra effort for the user  The winner of the ‘ Netflix prize ‘ was a stacked ensemble of over 100 learners. There’s plenty more folk wisdom in the paper, so do check it out if this has piqued your interest.", "pdf_url": "http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/a-few-useful-things-to-know-about-machine-learning.json"}
{"id": "38779502", "bin": "800_900", "summary_sentences": ["What  They suggest a variation of Faster R-CNN architectures that also tracks objects in videos (additionally to detecting them, i.e. additionally to bounding box detection).", "The model does detection and tracking in one forward pass.", "The \"tracking\" here is rather primitive: It only signals where an object frame t will likely be in frame t+x.", "That information can then be used to compute chains of bounding boxes over time.", "How  Architecture  They base their model on R-FCN, which is similar to Faster R-CNN.", "They have two images/frames (t and t+x) as inputs.", "Same as in Faster R-CNN:  They apply a base network (e.g. ResNet) to both of them.", "They use an RPN to locate RoIs in the feature maps.", "They use RoI-Pooling to extract and pool each RoI's features.", "They apply classification (object class) and regression (object location/dimensions) to each RoI.", "They compute correlations between the feature maps of both frames.", "They then extract and pool each RoI from the feature and correlation maps of from t.  They then apply a tracking branch, which predicts the new position (x, y) and dimensions (height, width) of the RoI in frame t+x.", "That prediction can be used to find a matching bounding box in frame t+x, which again can be used to track objects over time.", "During training, they use a smooth L1 loss for the tracking branch.", "Visualization of the architecture:  Visualization of the tracking process:  Correlation  Correlations are estimated between the feature maps of frame t and t+x.", "In the simplest form, correlation is measured by extracting some point (i,j) from the feature maps t and t+x (resulting in two vectors) and then computing the scalar product (resulting in a scalar).", "They use a more complex correlation, which compares point (i,j) in frame t not only to (i,j) in t+x, but to (i+d,j+q), where d and q come from an interval.", "I.e. they compare to a neighbourhood in frame t+x.", "They use d=8 (same for q), resulting in 64 correlation values per (i,j).", "Furthermore, they compute correlations for multiple scales (i.e. early and late in the base network).", "(With striding so that the correlation maps end up having the same sizes.)", "Tubelets  The previous architecture only regresses the new location of a bounding box in frame t+x.", "From that information they derive tubelets, tracks of bounding boxes over multiple frames (i.e. objects tracked over time).", "Core of the method to do that are scores between pairs of bounding boxes (between frame t and t+x).", "The scores are computed per class, i.e. no score between two bounding boxes of different classes.", "Each score has three components:  (a) Probability of the bounding box in frame t having the specified class,  (b) Probability of the bounding box in frame t+x having the specified class,  (c) A flag whether the bounding box in frame t+x matches the expected future position of the bounding box in frame t. The future position is estimated using the new tracking branch.", "The flag has a value of 1 if the IoU between future bounding box and expectation is >0.5.", "Once all scores are computed, the Viterbi algorithm can be used to compute optimal paths over the frames, leading to tracked objects, which are the tubelets.", "After generating a tubelet, they increase the object detection scores of the respective class of all bounding boxes in the tubelet.", "(Because future frames are now giving support that a bounding box really does have a specific object class.)", "Results  They train on ImageNet VID dataset (after first training on standard ImageNet).", "Training with the tracking branch improves raw object detection by about 1.6 points mAP.", "Predicting on two frames (e.g. t and t+1) with tracking improves object detection scores for some classes significantly (e.g.", "+9 points for rabbits).", "Adding a significant gap between the frames (t and t+10) is still enough to improve object detection scores (a bit less though).", "Results overview:  Their tracking branch barely adds runtime to the model.", "Only the tubelet generation adds significant time.", "They modify it (somehow, not described) to a non-causal(?)", "version, which runs faster.", "Additional runtime per frame with tracking is then 14ms."], "summary_text": "What  They suggest a variation of Faster R-CNN architectures that also tracks objects in videos (additionally to detecting them, i.e. additionally to bounding box detection). The model does detection and tracking in one forward pass. The \"tracking\" here is rather primitive: It only signals where an object frame t will likely be in frame t+x. That information can then be used to compute chains of bounding boxes over time. How  Architecture  They base their model on R-FCN, which is similar to Faster R-CNN. They have two images/frames (t and t+x) as inputs. Same as in Faster R-CNN:  They apply a base network (e.g. ResNet) to both of them. They use an RPN to locate RoIs in the feature maps. They use RoI-Pooling to extract and pool each RoI's features. They apply classification (object class) and regression (object location/dimensions) to each RoI. They compute correlations between the feature maps of both frames. They then extract and pool each RoI from the feature and correlation maps of from t.  They then apply a tracking branch, which predicts the new position (x, y) and dimensions (height, width) of the RoI in frame t+x. That prediction can be used to find a matching bounding box in frame t+x, which again can be used to track objects over time. During training, they use a smooth L1 loss for the tracking branch. Visualization of the architecture:  Visualization of the tracking process:  Correlation  Correlations are estimated between the feature maps of frame t and t+x. In the simplest form, correlation is measured by extracting some point (i,j) from the feature maps t and t+x (resulting in two vectors) and then computing the scalar product (resulting in a scalar). They use a more complex correlation, which compares point (i,j) in frame t not only to (i,j) in t+x, but to (i+d,j+q), where d and q come from an interval. I.e. they compare to a neighbourhood in frame t+x. They use d=8 (same for q), resulting in 64 correlation values per (i,j). Furthermore, they compute correlations for multiple scales (i.e. early and late in the base network). (With striding so that the correlation maps end up having the same sizes.) Tubelets  The previous architecture only regresses the new location of a bounding box in frame t+x. From that information they derive tubelets, tracks of bounding boxes over multiple frames (i.e. objects tracked over time). Core of the method to do that are scores between pairs of bounding boxes (between frame t and t+x). The scores are computed per class, i.e. no score between two bounding boxes of different classes. Each score has three components:  (a) Probability of the bounding box in frame t having the specified class,  (b) Probability of the bounding box in frame t+x having the specified class,  (c) A flag whether the bounding box in frame t+x matches the expected future position of the bounding box in frame t. The future position is estimated using the new tracking branch. The flag has a value of 1 if the IoU between future bounding box and expectation is >0.5. Once all scores are computed, the Viterbi algorithm can be used to compute optimal paths over the frames, leading to tracked objects, which are the tubelets. After generating a tubelet, they increase the object detection scores of the respective class of all bounding boxes in the tubelet. (Because future frames are now giving support that a bounding box really does have a specific object class.) Results  They train on ImageNet VID dataset (after first training on standard ImageNet). Training with the tracking branch improves raw object detection by about 1.6 points mAP. Predicting on two frames (e.g. t and t+1) with tracking improves object detection scores for some classes significantly (e.g. +9 points for rabbits). Adding a significant gap between the frames (t and t+10) is still enough to improve object detection scores (a bit less though). Results overview:  Their tracking branch barely adds runtime to the model. Only the tubelet generation adds significant time. They modify it (somehow, not described) to a non-causal(?) version, which runs faster. Additional runtime per frame with tracking is then 14ms.", "pdf_url": "https://arxiv.org/pdf/1710.03958", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/detect_to_track_and_track_to_detect.json"}
{"id": "85123331", "bin": "800_900", "summary_sentences": ["Scala Actors: Unifying thread-based and event-based programming – Haller & Odersky 2008  Yesterday we saw a Haskell-based approach to unifying events and threads , today’s paper shows how to apply some of those same ideas on top of the JVM using Scala.", "There is an impedance mismatch between message-passing concurrency and virtual machines, such as the JVM.", "VMs usually map their threads to heavyweight OS processes.", "Without a lightweight process abstraction, users are often forced to write parts of concurrent applications in an event-driven style which obscures control flow, and increases the burden on the programmer.", "In this paper we show how thread-based and event-based programming can be unified under a single actor abstraction.", "Using advanced abstraction mechanisms of the Scala programming language, we implement our approach on unmodified JVMs.", "What is an actor?", "An actor is a concurrent process that communicates with other actors by exchanging messages.", "Communication is asynchronous; messages are buffered in an actor’s mailbox.", "An actor may respond to an asynchronous message by creating new actors, sending messages to known actors (including itself), or changing its behavior.", "The behavior specifies how the actor responds to the next message that it receives.", "Some of the authors we read earlier in the week would probably put actors in the ‘event-based’ category.", "As Haller and Odersky point out though, they do avoid the inversion of control issue with out and out event-driven systems.", "They call it ‘message-based’ concurrency – the same term used in the original ‘dual’ paper we looked at on Monday.", "An actor can suspend with a full thread stack (receive) or it can suspend with just a continuation closure (react).", "The first form of suspension corresponds to thread-based, the second form to event-based programming.", "The new system combines the benefits of both models…  Complexity is reduced for the programmer since:  Accessing an actor’s mailbox is race-free by design  Message-passing with pattern matching is often more convenient than explicit thread-based synchronization  Actors are lightweight, supported fine-grained concurrency models without the need for explicit management of thread pools  Actors interoperate naturally with normal JVM threads (that are treated as actors).", "The scheme is implemented in the Scala Actors library.", "… it might seem that Scala is a language specialized for actor concurrency.", "In fact, this is not true.", "Scala only assumes the basic thread model of the underlying host.", "The implementation executes multiple actors on multiple threads:  The basic idea of our implementation is to use a thread pool to execute actors, and to resize the thread pool whenever it is necessary to support general thread operations.", "If actors use only operations of the event-based model, the size of the thread pool can be fixed.", "This is different if some of the actors use blocking operations such as receive or system I/O.", "In the case where every worker thread is occupied by a blocked actor and there are pending tasks, the thread pool has to grow.", "Combinators are introduced to enable actor composition.", "andThen for example permits sequential composition in a manner reminiscent of the CPS Monad of Li and Zdancewic .", "awaitPing andThen sendPong  There are several examples of actors given in the paper.", "But before you dive in too deep, it’s worth knowing that the Scala Actors library was deprecated in Scala 2.11 in favour of Akka .", "See the Akka documentation on Actors for an overview of the currently supported actor model.", "The basic actor notions apply equally in both of course.", "Section 6 of the paper gives a short discussion of the benefits of the actor model when building web applications.", "Compared to a purely event-based approach, users are relieved from writing their own ad hoc thread pooling code.", "Since the internal thread pool can be global to the web application server, the thread pool controller can leverage more information for its decisions.", "Finally, accesses to an actor’s mailbox are race-free.", "Therefore, resources such as user profiles can be protected by modeling them as (thread-less) actors.", "It seems fitting to end with a tribute to Erlang:  Our library was inspired to a large extent by Erlang’s elegant programming model.", "Erlang is a dynamically-typed functional programming language designed for programming real-time control systems.", "The combination of lightweight isolated processes, asynchronous message passing with pattern matching, and controlled error propagation has been proven to be very effective.", "One of our main contributions lies in the integration of Erlang’s programming model into a full-fledged object-oriented and functional language."], "summary_text": "Scala Actors: Unifying thread-based and event-based programming – Haller & Odersky 2008  Yesterday we saw a Haskell-based approach to unifying events and threads , today’s paper shows how to apply some of those same ideas on top of the JVM using Scala. There is an impedance mismatch between message-passing concurrency and virtual machines, such as the JVM. VMs usually map their threads to heavyweight OS processes. Without a lightweight process abstraction, users are often forced to write parts of concurrent applications in an event-driven style which obscures control flow, and increases the burden on the programmer. In this paper we show how thread-based and event-based programming can be unified under a single actor abstraction. Using advanced abstraction mechanisms of the Scala programming language, we implement our approach on unmodified JVMs. What is an actor? An actor is a concurrent process that communicates with other actors by exchanging messages. Communication is asynchronous; messages are buffered in an actor’s mailbox. An actor may respond to an asynchronous message by creating new actors, sending messages to known actors (including itself), or changing its behavior. The behavior specifies how the actor responds to the next message that it receives. Some of the authors we read earlier in the week would probably put actors in the ‘event-based’ category. As Haller and Odersky point out though, they do avoid the inversion of control issue with out and out event-driven systems. They call it ‘message-based’ concurrency – the same term used in the original ‘dual’ paper we looked at on Monday. An actor can suspend with a full thread stack (receive) or it can suspend with just a continuation closure (react). The first form of suspension corresponds to thread-based, the second form to event-based programming. The new system combines the benefits of both models…  Complexity is reduced for the programmer since:  Accessing an actor’s mailbox is race-free by design  Message-passing with pattern matching is often more convenient than explicit thread-based synchronization  Actors are lightweight, supported fine-grained concurrency models without the need for explicit management of thread pools  Actors interoperate naturally with normal JVM threads (that are treated as actors). The scheme is implemented in the Scala Actors library. … it might seem that Scala is a language specialized for actor concurrency. In fact, this is not true. Scala only assumes the basic thread model of the underlying host. The implementation executes multiple actors on multiple threads:  The basic idea of our implementation is to use a thread pool to execute actors, and to resize the thread pool whenever it is necessary to support general thread operations. If actors use only operations of the event-based model, the size of the thread pool can be fixed. This is different if some of the actors use blocking operations such as receive or system I/O. In the case where every worker thread is occupied by a blocked actor and there are pending tasks, the thread pool has to grow. Combinators are introduced to enable actor composition. andThen for example permits sequential composition in a manner reminiscent of the CPS Monad of Li and Zdancewic . awaitPing andThen sendPong  There are several examples of actors given in the paper. But before you dive in too deep, it’s worth knowing that the Scala Actors library was deprecated in Scala 2.11 in favour of Akka . See the Akka documentation on Actors for an overview of the currently supported actor model. The basic actor notions apply equally in both of course. Section 6 of the paper gives a short discussion of the benefits of the actor model when building web applications. Compared to a purely event-based approach, users are relieved from writing their own ad hoc thread pooling code. Since the internal thread pool can be global to the web application server, the thread pool controller can leverage more information for its decisions. Finally, accesses to an actor’s mailbox are race-free. Therefore, resources such as user profiles can be protected by modeling them as (thread-less) actors. It seems fitting to end with a tribute to Erlang:  Our library was inspired to a large extent by Erlang’s elegant programming model. Erlang is a dynamically-typed functional programming language designed for programming real-time control systems. The combination of lightweight isolated processes, asynchronous message passing with pattern matching, and controlled error propagation has been proven to be very effective. One of our main contributions lies in the integration of Erlang’s programming model into a full-fledged object-oriented and functional language.", "pdf_url": "http://www.sciencedirect.com/science/article/pii/S0304397508006695/pdf?md5=9c06db85926974f6a6f934e60ad7e856&pid=1-s2.0-S0304397508006695-main.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/scala-actors-unifying-thread-based-and-event-based-programming.json"}
{"id": "84548548", "bin": "800_900", "summary_sentences": ["Darwinian data structure selection Basios et al., FSE’18  GraphIt may have caught your attention for the success of its approach, but I suspect for many readers it’s not something you’ll be immediately applying.", "Darwinian Data Structures (DDSs) on the other hand looks to be of immediate interest to many Java and C++ projects (and generalises beyond those languages).", "What I would have called an ADT (e.g., a List), the authors call Darwinian Data Structures.", "The ‘Darwinian’ part comes from the fact that ADTs have multiple concrete implementations, and Artemis, “a multi-objective, cloud-based search-based optimisation framework” finds the best implementation class (e.g. ArrayList, LinkedList) for your specific use case.", "It does this using the NSGA-II genetic algorithm-based optimiser in the current implementation.", "In brief, Artemis finds the places in your code where you are using an ADT, and explores the possible concrete instantiation space for those ADTs using your test suite as a guide to performance.", "Then it outputs the transformed source.", "You might be wondering whether e.g. LinkedList vs ArrayList makes that big a difference in most real world projects:  Artemis achieves substantial performance improvements for every project in 5 Java projects from DaCapo benchmark, 8 popular projects, and 30 uniformly sampled projects from GitHub.", "For execution time, CPU usage, and memory consumption, Artemis finds at least one solution that improves all measures for 86% (37/43) of the projects.", "The median improvement across the best solutions is 4.8%, 10.1%, and 5.1% for runtime, memory, and CPU usage.", "For example, consider this code from google-http-java-client, which currently uses ArrayList :  Switching to LinkedList and comparing performance over the same test set for 30 runs, we get a median 46% reduction in execution time.", "We are interested not just in searching the space of Darwinian data structures, but also tuning them via their constructor parameters.", "For example, choosing an appropriate initial capacity size for an ArrayList.", "End-to-end Artemis works like this:  There’s a one-off up-front exercise to analyse the collections library / libraries of interest and build a dictionary that describes the search space.", "Then given the source code and test suite of a project Artemis explores the AST to find uses of DDSs, outputting a templated version of the source code with replacement points for each usage.", "A search algorithm is then used to find the best choice in each location, with the test suite being used to judge performance.", "Finding candidate program points for DDS substitution  Given an AST, it’s easy to find declarations using the abstract data type (e.g. List), but in the code bases under study the authors also found many cases where programmers had over-specified, using a concrete type for variable and parameter type declarations, e.g.", "Artemis will apply further transformations to replace these with the abstract type instead, thus permitting DDS exploration.", "Many programs make extensive use of collection types, resulting in a very large overall search space.", "Artemis profiles the input program while running the test suite to identify the highest value points in the program to explore and thus prunes the search space.", "Profiling is done using the JConsole profiler.", "Searching for the best parameters  The overall search space for a given DDS consists of all the possible concrete implementation types, together with the parameter spaces for their respective constructor arguments.", "For each generation, NSGA-II applies tournament selection, followed by a uniform crossover and a uniform mutation operation.", "In our experiments, we designed fitness functions to capture execution time, memory consumption, and CPU usage.", "After fitness evaluation, Artemis applies standard non-dominated selection to form the next generation.", "Artemis repeats this process until the solutions in a generation converge.", "At this point, Artemis returns all non-dominated solutions in the final population.", "In the evaluation, the initial population size is set to 30, with a limit of 900 function evaluations.", "To assess fitness Artemis relies on running the test suite.", "Therefore the results will only apply to production use cases to the extent that your test suite mirrors production usage.", "Even though performance test suites are a more appropriate and logical choice for evaluating the non-functional properties of the program, most real world programs in GitHub do not provide a performance suite.", "For this reason, we use the regression test suites to evaluate the non-functional properties of the GitHub projects of this study whenever a performance test suite is not available.", "Test suite execution time is measured using the Maven Surefire plugin, with profiling done by JConsole.", "Each generated solution is run for at least 30 simulations, with start-up/ JVM warm-up runs not counting towards this total.", "(See ‘[Virtual machine warmup blows hot and cold](  [url]"], "summary_text": "Darwinian data structure selection Basios et al., FSE’18  GraphIt may have caught your attention for the success of its approach, but I suspect for many readers it’s not something you’ll be immediately applying. Darwinian Data Structures (DDSs) on the other hand looks to be of immediate interest to many Java and C++ projects (and generalises beyond those languages). What I would have called an ADT (e.g., a List), the authors call Darwinian Data Structures. The ‘Darwinian’ part comes from the fact that ADTs have multiple concrete implementations, and Artemis, “a multi-objective, cloud-based search-based optimisation framework” finds the best implementation class (e.g. ArrayList, LinkedList) for your specific use case. It does this using the NSGA-II genetic algorithm-based optimiser in the current implementation. In brief, Artemis finds the places in your code where you are using an ADT, and explores the possible concrete instantiation space for those ADTs using your test suite as a guide to performance. Then it outputs the transformed source. You might be wondering whether e.g. LinkedList vs ArrayList makes that big a difference in most real world projects:  Artemis achieves substantial performance improvements for every project in 5 Java projects from DaCapo benchmark, 8 popular projects, and 30 uniformly sampled projects from GitHub. For execution time, CPU usage, and memory consumption, Artemis finds at least one solution that improves all measures for 86% (37/43) of the projects. The median improvement across the best solutions is 4.8%, 10.1%, and 5.1% for runtime, memory, and CPU usage. For example, consider this code from google-http-java-client, which currently uses ArrayList :  Switching to LinkedList and comparing performance over the same test set for 30 runs, we get a median 46% reduction in execution time. We are interested not just in searching the space of Darwinian data structures, but also tuning them via their constructor parameters. For example, choosing an appropriate initial capacity size for an ArrayList. End-to-end Artemis works like this:  There’s a one-off up-front exercise to analyse the collections library / libraries of interest and build a dictionary that describes the search space. Then given the source code and test suite of a project Artemis explores the AST to find uses of DDSs, outputting a templated version of the source code with replacement points for each usage. A search algorithm is then used to find the best choice in each location, with the test suite being used to judge performance. Finding candidate program points for DDS substitution  Given an AST, it’s easy to find declarations using the abstract data type (e.g. List), but in the code bases under study the authors also found many cases where programmers had over-specified, using a concrete type for variable and parameter type declarations, e.g. Artemis will apply further transformations to replace these with the abstract type instead, thus permitting DDS exploration. Many programs make extensive use of collection types, resulting in a very large overall search space. Artemis profiles the input program while running the test suite to identify the highest value points in the program to explore and thus prunes the search space. Profiling is done using the JConsole profiler. Searching for the best parameters  The overall search space for a given DDS consists of all the possible concrete implementation types, together with the parameter spaces for their respective constructor arguments. For each generation, NSGA-II applies tournament selection, followed by a uniform crossover and a uniform mutation operation. In our experiments, we designed fitness functions to capture execution time, memory consumption, and CPU usage. After fitness evaluation, Artemis applies standard non-dominated selection to form the next generation. Artemis repeats this process until the solutions in a generation converge. At this point, Artemis returns all non-dominated solutions in the final population. In the evaluation, the initial population size is set to 30, with a limit of 900 function evaluations. To assess fitness Artemis relies on running the test suite. Therefore the results will only apply to production use cases to the extent that your test suite mirrors production usage. Even though performance test suites are a more appropriate and logical choice for evaluating the non-functional properties of the program, most real world programs in GitHub do not provide a performance suite. For this reason, we use the regression test suites to evaluate the non-functional properties of the GitHub projects of this study whenever a performance test suite is not available. Test suite execution time is measured using the Maven Surefire plugin, with profiling done by JConsole. Each generated solution is run for at least 30 simulations, with start-up/ JVM warm-up runs not counting towards this total. (See ‘[Virtual machine warmup blows hot and cold](  [url]", "pdf_url": "https://arxiv.org/pdf/1706.03232", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/darwinian-data-structure-selection.json"}
{"id": "11418476", "bin": "800_900", "summary_sentences": ["Welcome to another paper review!", "DialogueRNN is a method that aims to perform emotion classification of utterances in the context of a conversation.", "Many applications can benefit from such type of analysis such as understanding the emotional context and interchange in debates and social media threads.", "Previous methods do not pay attention to individuals’ emotional states.", "The proposed emotion detection model considers individual speakers by focusing on three different aspects: the speaker, the context of preceding utterances, and the emotion from preceding utterances.", "The idea is that these three aspects are important to accurately predict the emotion of the utterance.", "Model  Utterances for a party (i.e., individual) are represented through textual features obtained from a convolutional neural network.", "As utterances come in a multimodal setting, audio and visual features are also extracted using 3D-CNN and openSMILE, respectively.", "The network is trained at the utterance level with the target emotion labels.", "The proposed model (called DialogueRNN), illustrated in the figure above, determines the final emotion of the utterance through the following factors:  Party state — models the parties’ emotion dynamics through the conversations.", "The basic idea behind the party state is to ensure that the model is aware of the speaker of each utterance in the conversation.", "Global state — models the context of an utterance in the dialogue, given by jointly encoding preceding utterances and the party state.", "Note that attention mechanism is applied to the global state to provide improved context representation.", "This state basically serves as the speaker-specific utterance representation.", "Emotion representation — inferred through party state and preceding speaker’s states as context (global state).", "This representation is used to perform the final emotion classification via a softmax layer.", "Each component of the architecture is modeled by a gated recurrent unit (GRU).", "It’s important to note that during training, the speaker state is updated using the current utterance along with its context, which is nothing less than the preceding global states applied an attention mechanism.", "The role of the attention mechanism is that it assigns higher attention scores to the utterances that are emotionally relevant to the current utterance.", "Overall, the speaker update encodes — via the Party GRU (shown in blue) — the information on the current utterance along with its context from the Global GRU (shown in green).", "All this information is important for performing the final emotion classification, which is performed by the emotion GRU (shown in maroon).", "Note that the current emotion classification also relies on the previous emotion-relevant information as well.", "Variants  Several variants of the DialogueRNN model are proposed and compared in this study:  DialogueRNN_l — considers an extra listener state (defined at the end of this post) while a speaker utters.", "BiDialogueRNN — a bidirectional RNN architecture is used instead  DialogueRNN+Att — attention is applied over all surrounding emotion representations  BiDialogueRNN+Att — similar to the previous model but considers a bidirectional RNN instead  Other baselines are also proposed which you can refer to in the paper.", "Results  Two datasets are used for all experiments: IEMOCAP and AVEC .", "Both datasets contain interactions between multiple parties.", "From the table below, we can observe that DialogueRNN (highlighted in green) outperforms all baselines and the state-of-the-art model (CMN) on both datasets.", "Note that these results are only using the text modality.", "We can also observe in the table above that the listener component (model highlighted in orange) doesn’t improve the model’s performance.", "In general, the other variants were found to perform well, especially the BiDialogueRNN+Att, which in general produced the better results.", "As shown in the table below, the proposed model, DialogueRNN, also significantly outperforms other models in the multimodal setting (using a fusion of modalities).", "As a case study, we can observe from the attention figure below that DialogueRNN correctly anticipates the emotion of frustration (labeled Turn 44) using the preceding context (41 and 42).", "For the CMN model, this was found not to be the case.", "An important ablation study was conducted to observe the importance of Emotion GRU and Party State components.", "We can see from the table below that the absence of part state decreases performance.", "In fact, it can be observed that the party state seems to be more important than Emotion GRU.", "Listener update changes the state of the listener based on the current speaker utterance.", "Visual cues are used to represent this information.", "However, authors found via the experiments that this update has no effect in a conversation as a silent party has no influence in a conversation.", "Reference  DialogueRNN: An Attentive RNN for Emotion Detection in Conversations — [ Paper ] | [ Code ]"], "summary_text": "Welcome to another paper review! DialogueRNN is a method that aims to perform emotion classification of utterances in the context of a conversation. Many applications can benefit from such type of analysis such as understanding the emotional context and interchange in debates and social media threads. Previous methods do not pay attention to individuals’ emotional states. The proposed emotion detection model considers individual speakers by focusing on three different aspects: the speaker, the context of preceding utterances, and the emotion from preceding utterances. The idea is that these three aspects are important to accurately predict the emotion of the utterance. Model  Utterances for a party (i.e., individual) are represented through textual features obtained from a convolutional neural network. As utterances come in a multimodal setting, audio and visual features are also extracted using 3D-CNN and openSMILE, respectively. The network is trained at the utterance level with the target emotion labels. The proposed model (called DialogueRNN), illustrated in the figure above, determines the final emotion of the utterance through the following factors:  Party state — models the parties’ emotion dynamics through the conversations. The basic idea behind the party state is to ensure that the model is aware of the speaker of each utterance in the conversation. Global state — models the context of an utterance in the dialogue, given by jointly encoding preceding utterances and the party state. Note that attention mechanism is applied to the global state to provide improved context representation. This state basically serves as the speaker-specific utterance representation. Emotion representation — inferred through party state and preceding speaker’s states as context (global state). This representation is used to perform the final emotion classification via a softmax layer. Each component of the architecture is modeled by a gated recurrent unit (GRU). It’s important to note that during training, the speaker state is updated using the current utterance along with its context, which is nothing less than the preceding global states applied an attention mechanism. The role of the attention mechanism is that it assigns higher attention scores to the utterances that are emotionally relevant to the current utterance. Overall, the speaker update encodes — via the Party GRU (shown in blue) — the information on the current utterance along with its context from the Global GRU (shown in green). All this information is important for performing the final emotion classification, which is performed by the emotion GRU (shown in maroon). Note that the current emotion classification also relies on the previous emotion-relevant information as well. Variants  Several variants of the DialogueRNN model are proposed and compared in this study:  DialogueRNN_l — considers an extra listener state (defined at the end of this post) while a speaker utters. BiDialogueRNN — a bidirectional RNN architecture is used instead  DialogueRNN+Att — attention is applied over all surrounding emotion representations  BiDialogueRNN+Att — similar to the previous model but considers a bidirectional RNN instead  Other baselines are also proposed which you can refer to in the paper. Results  Two datasets are used for all experiments: IEMOCAP and AVEC . Both datasets contain interactions between multiple parties. From the table below, we can observe that DialogueRNN (highlighted in green) outperforms all baselines and the state-of-the-art model (CMN) on both datasets. Note that these results are only using the text modality. We can also observe in the table above that the listener component (model highlighted in orange) doesn’t improve the model’s performance. In general, the other variants were found to perform well, especially the BiDialogueRNN+Att, which in general produced the better results. As shown in the table below, the proposed model, DialogueRNN, also significantly outperforms other models in the multimodal setting (using a fusion of modalities). As a case study, we can observe from the attention figure below that DialogueRNN correctly anticipates the emotion of frustration (labeled Turn 44) using the preceding context (41 and 42). For the CMN model, this was found not to be the case. An important ablation study was conducted to observe the importance of Emotion GRU and Party State components. We can see from the table below that the absence of part state decreases performance. In fact, it can be observed that the party state seems to be more important than Emotion GRU. Listener update changes the state of the listener based on the current speaker utterance. Visual cues are used to represent this information. However, authors found via the experiments that this update has no effect in a conversation as a silent party has no influence in a conversation. Reference  DialogueRNN: An Attentive RNN for Emotion Detection in Conversations — [ Paper ] | [ Code ]", "pdf_url": "https://arxiv.org/pdf/1811.00405", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/dialoguernn-emotion-classification-in-conversation-1e389d035aff.json"}
{"id": "75937139", "bin": "800_900", "summary_sentences": ["Convolutional Neural Networks are extremely good feature extractors in the sense that features extracted for one task (say image classification) can be easily transferred to another task (say image segmentation).", "Existing unsupervised approaches do not aim to learn discriminative features and supervised approaches for discriminative features do not scale well.", "The paper presents an approach to learn features in an unsupervised setting by using a set of target representations called as Noise As Target (NAT) which acts as a kind of proxy supervising signal.", "Approach  Unsupervised Setting  Given a collection of image X (x1, x2, …, xn), we want to learn a parameterized mapping f such that f(xi) gives the features of image xi.", "We would jointly learn the target vectors yi (more on it later).", "Loss Function  Squared L2 norm is used as the distance measure while making sure that final activations are unit normalized.", "Fixed Target Representation  In the setting of the problem where we are learning both the features and the target representation, a trivial solution would be the one where all the input images map to the same target and are assigned the same representation.", "No discriminative features are learned in this case.", "To avoid such situations, a set of k predefined target representations are chosen and each image is mapped to one of these k representations (based on the features).", "There is an assumption that k > n so that each image is assigned a different target.", "One simple choice of target representation is the standard one-hot vector which implies that all the class (and by extension, the associated images) are orthogonal and equidistant from each other.", "But this is not a reasonable approximation as not all the image pairs are equally similar or dissimilar.", "Instead, the target vectors are uniformly sampled from a d-dimensional unit sphere, where d is the dimensionality of the feature representation.", "That is, the idea is to map the features to the manifold of the d-dimensional L2 sphere by using the K predefined representations as for the discrete approximation of the manifold.", "Since each data point (image) is mapped to a new point on the manifold, the algorithm is suited for online training as well.", "Optimisation  For the training, the number of target K is reduced to the number of images n and an assignment matrix P is learned which ensures that the mapping between the image to target is 1-to-1.", "The resulting optimisation equation can be solved using the Hungarian Algorithm but at a high-cost O(n^3).", "An optimisation is to take a batch of b images and update the square matrix PB for dimension bXb (made of the images and their corresponding targets).", "This reduces the overall complexity of O(nb^2).", "Other optimisation techniques, that are common to supervised learning, like batch norm used in this setting as well.", "Implementation Detail  Used AlexNet with NATs to train the unsupervised model.", "An MLP is trained on these features to learn the classifier.", "Standard preprocessing techniques like random cropping/flipping are used.", "Experimental Details  Dataset  ImageNet for training the AlexNet architecture with the proposed approach.", "Pascal VOC 2007 for transfer learning experiments.", "Baselines  Unsupervised approaches like autoencoder, GAN, BiGAN  Self-supervised  SOTA models using hand-made features SIFT with Fisher Vector.", "Observation  Using squared loss instead of softmax does not deteriorate the performance too much.", "The authors compare the effect of using discrete vs continuous target representations for transfer learning.", "For the discrete representation, elements of the canonical basis of a k-dimensional space (k=1000, 10000, 100000) are used.", "Experiments demonstrate that d-dimensional continuous vectors perform much better than the discrete vectors.", "While training the unsupervised network, its features were extracted after every 20 iterations to evaluate the performance on transfer learning task.", "The test accuracy increases up to around 100 iterations then saturate.", "Comparing the visualization of the first convolutional layer filters (for AlexNet with and without supervision) shows that while unsupervised filters are less sharp, they maintain the edge and orientation information.", "The proposed unsupervised method outperforms all the unsupervised baselines and is competitive with respect to the supervised baseline.", "But it is still far behind the model using handcrafted features.", "For transfer learning, on Pascal VOC, the proposed approach beats the supervised baseline and works at par with the supervised approach.", "Notes  The paper proposed a simple unsupervised framework for learning discriminative features without having to rely on proxy tasks like image generation and without having to make an assumption about the input domain.", "The key aspect of the proposed approach is that each image is assigned to a unique point in the d-dimensional manifold which means 2 images could be very close to each other on the manifold while being quite distinct in reality.", "It is interesting to see that such a simple strategy is able to give such good results."], "summary_text": "Convolutional Neural Networks are extremely good feature extractors in the sense that features extracted for one task (say image classification) can be easily transferred to another task (say image segmentation). Existing unsupervised approaches do not aim to learn discriminative features and supervised approaches for discriminative features do not scale well. The paper presents an approach to learn features in an unsupervised setting by using a set of target representations called as Noise As Target (NAT) which acts as a kind of proxy supervising signal. Approach  Unsupervised Setting  Given a collection of image X (x1, x2, …, xn), we want to learn a parameterized mapping f such that f(xi) gives the features of image xi. We would jointly learn the target vectors yi (more on it later). Loss Function  Squared L2 norm is used as the distance measure while making sure that final activations are unit normalized. Fixed Target Representation  In the setting of the problem where we are learning both the features and the target representation, a trivial solution would be the one where all the input images map to the same target and are assigned the same representation. No discriminative features are learned in this case. To avoid such situations, a set of k predefined target representations are chosen and each image is mapped to one of these k representations (based on the features). There is an assumption that k > n so that each image is assigned a different target. One simple choice of target representation is the standard one-hot vector which implies that all the class (and by extension, the associated images) are orthogonal and equidistant from each other. But this is not a reasonable approximation as not all the image pairs are equally similar or dissimilar. Instead, the target vectors are uniformly sampled from a d-dimensional unit sphere, where d is the dimensionality of the feature representation. That is, the idea is to map the features to the manifold of the d-dimensional L2 sphere by using the K predefined representations as for the discrete approximation of the manifold. Since each data point (image) is mapped to a new point on the manifold, the algorithm is suited for online training as well. Optimisation  For the training, the number of target K is reduced to the number of images n and an assignment matrix P is learned which ensures that the mapping between the image to target is 1-to-1. The resulting optimisation equation can be solved using the Hungarian Algorithm but at a high-cost O(n^3). An optimisation is to take a batch of b images and update the square matrix PB for dimension bXb (made of the images and their corresponding targets). This reduces the overall complexity of O(nb^2). Other optimisation techniques, that are common to supervised learning, like batch norm used in this setting as well. Implementation Detail  Used AlexNet with NATs to train the unsupervised model. An MLP is trained on these features to learn the classifier. Standard preprocessing techniques like random cropping/flipping are used. Experimental Details  Dataset  ImageNet for training the AlexNet architecture with the proposed approach. Pascal VOC 2007 for transfer learning experiments. Baselines  Unsupervised approaches like autoencoder, GAN, BiGAN  Self-supervised  SOTA models using hand-made features SIFT with Fisher Vector. Observation  Using squared loss instead of softmax does not deteriorate the performance too much. The authors compare the effect of using discrete vs continuous target representations for transfer learning. For the discrete representation, elements of the canonical basis of a k-dimensional space (k=1000, 10000, 100000) are used. Experiments demonstrate that d-dimensional continuous vectors perform much better than the discrete vectors. While training the unsupervised network, its features were extracted after every 20 iterations to evaluate the performance on transfer learning task. The test accuracy increases up to around 100 iterations then saturate. Comparing the visualization of the first convolutional layer filters (for AlexNet with and without supervision) shows that while unsupervised filters are less sharp, they maintain the edge and orientation information. The proposed unsupervised method outperforms all the unsupervised baselines and is competitive with respect to the supervised baseline. But it is still far behind the model using handcrafted features. For transfer learning, on Pascal VOC, the proposed approach beats the supervised baseline and works at par with the supervised approach. Notes  The paper proposed a simple unsupervised framework for learning discriminative features without having to rely on proxy tasks like image generation and without having to make an assumption about the input domain. The key aspect of the proposed approach is that each image is assigned to a unique point in the d-dimensional manifold which means 2 images could be very close to each other on the manifold while being quite distinct in reality. It is interesting to see that such a simple strategy is able to give such good results.", "pdf_url": "https://arxiv.org/pdf/1704.05310", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/unsupervised-learning-by-predicting-noise.json"}
{"id": "73414525", "bin": "800_900", "summary_sentences": ["This paper presents a neural network architecture that can take as input a question and a sequence of facts expressed in natural language (i.e. a sequence of words) and produce its output the answer to that question.", "The main components of the architecture are as follows:  * The question (q) and the facts (f_1, ... , f_K) are each individually transformed into a fixed size vector using the same GRU RNN (with the last hidden layer serving as the vector representation).", "* These vectors are each passed through \"reasoning layers\", where each layer transforms the question q and the facts f_k into a new vector representation.", "This is done by feeding each question fact pair (q,f_k) to a neural network that outputs a new representation for the fact f_k (which replaces its old representation in the layer), as well as a new representation for the question.", "All K new question representations are then pooled to obtain a single question representation that replace the old one in the layer.", "* The last reasoning layer is either fed to a softmax layer for binary questions, or to a scoring layer for questions with multiple and varying candidate answers.", "This so-called Neural Reasoner can be trained by backpropagation, in an end-to-end, supervised way.", "The authors also suggest the use of auxiliary tasks, to improve results.", "The first (\"original\") adds an autoencoder reconstuction cost, that reproduces the question and facts from its first layer encoding.", "The second (\"abstract\") instead reconstructs a more abstract version of the sentences (e.g. \"The triangle is above the pink rectangle.\"", "becomes \"x is above y\").", "Importantly, while the Neural Reasoner framework is presented in this paper as covering many different variants, the version that is experimentally tested is one where the fact representations f_k are actually left unchanged throughout the reasoning layers, with only the question representation being changed.", "The paper presents experiments on two synthetic reasoning tasks and report performances that compare favorably with previously published alternatives (based on the general Memory Network architecture).", "The experiments also show that the auxiliary tasks can substantially improve the performance of the model   #### My two cents  The proposed Neural Reasoner framework is actually very close to work published on arXiv at about the same time on End-to-End Memory Networks  [ref] .", "In fact, the version tested in the paper, with unchanged fact representations throughout layers, is extremely close to End-to-End Memory Networks.", "That said, there are also lots of differences.", "For instance, this paper proposes the use of multilayer networks within each Reasoning Layer, to produce updated question representations.", "In fact, experiments suggest that using several layers can be very beneficial for the path finding task.", "The sentence representation at the first layer is also different, being based on a non-linear RNN instead of being based on linear operations on embeddings as in Memory Networks.", "The most interesting aspect of this paper to me is probably the demonstration that the use of an auxiliary task such as \"original\", which is unsupervised, can substantially improve the performance, again for the path finding task.", "That is, to me, probably the most exciting direction of future research that this paper highlights as promising.", "I also liked how the model is presented.", "It didn't take me much time to understand the model, and I actually found it easier to absorb than the Memory Network model, despite both being very similar.", "I think this model is indeed a bit simpler than Memory Networks, which is a good thing.", "It also suggests a different approach to the problem, one where the facts representations are also updated during forward propagation, not just the question's representation (which is the version initially described in the paper...", "I hope experiments on that variant are eventually presented).", "It's unfortunate that the authors only performed experiments on 2 of the 20 synthetic question-answering tasks.", "I hope a future version of this work can report results on the full benchmark and directly compare with End-to-End Memory Networks.", "I was also unable to find out which of the question representation pooling mechanism (section 3.2.2) was used in the experiments.", "Perhaps the authors forgot to state it?", "Overall, a pretty interesting paper that open different doors towards reasoning with neural networks."], "summary_text": "This paper presents a neural network architecture that can take as input a question and a sequence of facts expressed in natural language (i.e. a sequence of words) and produce its output the answer to that question. The main components of the architecture are as follows:  * The question (q) and the facts (f_1, ... , f_K) are each individually transformed into a fixed size vector using the same GRU RNN (with the last hidden layer serving as the vector representation). * These vectors are each passed through \"reasoning layers\", where each layer transforms the question q and the facts f_k into a new vector representation. This is done by feeding each question fact pair (q,f_k) to a neural network that outputs a new representation for the fact f_k (which replaces its old representation in the layer), as well as a new representation for the question. All K new question representations are then pooled to obtain a single question representation that replace the old one in the layer. * The last reasoning layer is either fed to a softmax layer for binary questions, or to a scoring layer for questions with multiple and varying candidate answers. This so-called Neural Reasoner can be trained by backpropagation, in an end-to-end, supervised way. The authors also suggest the use of auxiliary tasks, to improve results. The first (\"original\") adds an autoencoder reconstuction cost, that reproduces the question and facts from its first layer encoding. The second (\"abstract\") instead reconstructs a more abstract version of the sentences (e.g. \"The triangle is above the pink rectangle.\" becomes \"x is above y\"). Importantly, while the Neural Reasoner framework is presented in this paper as covering many different variants, the version that is experimentally tested is one where the fact representations f_k are actually left unchanged throughout the reasoning layers, with only the question representation being changed. The paper presents experiments on two synthetic reasoning tasks and report performances that compare favorably with previously published alternatives (based on the general Memory Network architecture). The experiments also show that the auxiliary tasks can substantially improve the performance of the model   #### My two cents  The proposed Neural Reasoner framework is actually very close to work published on arXiv at about the same time on End-to-End Memory Networks  [ref] . In fact, the version tested in the paper, with unchanged fact representations throughout layers, is extremely close to End-to-End Memory Networks. That said, there are also lots of differences. For instance, this paper proposes the use of multilayer networks within each Reasoning Layer, to produce updated question representations. In fact, experiments suggest that using several layers can be very beneficial for the path finding task. The sentence representation at the first layer is also different, being based on a non-linear RNN instead of being based on linear operations on embeddings as in Memory Networks. The most interesting aspect of this paper to me is probably the demonstration that the use of an auxiliary task such as \"original\", which is unsupervised, can substantially improve the performance, again for the path finding task. That is, to me, probably the most exciting direction of future research that this paper highlights as promising. I also liked how the model is presented. It didn't take me much time to understand the model, and I actually found it easier to absorb than the Memory Network model, despite both being very similar. I think this model is indeed a bit simpler than Memory Networks, which is a good thing. It also suggests a different approach to the problem, one where the facts representations are also updated during forward propagation, not just the question's representation (which is the version initially described in the paper... I hope experiments on that variant are eventually presented). It's unfortunate that the authors only performed experiments on 2 of the 20 synthetic question-answering tasks. I hope a future version of this work can report results on the full benchmark and directly compare with End-to-End Memory Networks. I was also unable to find out which of the question representation pooling mechanism (section 3.2.2) was used in the experiments. Perhaps the authors forgot to state it? Overall, a pretty interesting paper that open different doors towards reasoning with neural networks.", "pdf_url": "http://arxiv.org/pdf/1508.05508", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/pengllw15.json"}
{"id": "18521109", "bin": "800_900", "summary_sentences": ["This paper investigates different paradigms for learning how to answer natural language queries through various forms of feedback.", "Most interestingly, it investigates whether a model can learn to answer correctly questions when the feedback is presented purely in the form of a sentence (e.g. \"Yes, that's right\", \"Yes, that's correct\", \"No, that's incorrect\", etc.).", "This later form of feedback is particularly hard to leverage, since the model has to somehow learn that the word \"Yes\" is a sign of a positive feedback, but not the word \"No\".", "Normally, we'd trained a model to directly predict the correct answer to questions based on feedback provided by an expert that always answers correctly.", "\"Imitating\" this expert just corresponds to regular supervised learning.", "The paper however explores other variations on this learning scenario.", "Specifically, they consider 3 dimensions of variations.", "The first dimension of variation is who is providing the answers.", "Instead of an expert (who is always right), the paper considers the case where the model is instead observing a different, \"imperfect\" expert whose answers come from a fixed policy that answers correctly only a fraction of the time (the paper looked at 0.5, 0.1 and 0.01).", "Note that the paper refers to these answers as coming from \"the learner\" (which should be the model), but since the policy is fixed and actually doesn't depend on the model, I think one can also think of it as coming from another agent, which I'll refer to as the imperfect expert (I think this is also known as \"off policy learning\" in the RL world).", "The second dimension of variation on the learning scenario that is explored is in the nature of the \"supervision type\" (i.e. nature of the labels).", "There are 10 of them (see Figure 1 for a nice illustration).", "In addition to the real expert's answers only (Type 1), the paper considers other types that instead involve the imperfect expert and fall in one of the two categories below:  1.", "Explicit positive / negative rewards based on whether the imperfect expert's answer is correct.", "2.", "Various forms of natural language responses to the imperfect expert's answers, which vary from worded positive/negative feedback, to hints, to mentions of the supporting fact for the correct answer.", "Also, mixtures of the above are considered.", "Finally, the third dimension of variation is how the model learns from the observed data.", "In addition to the regular supervised learning approach of imitating the observed answers (whether it's from the real expert or the imperfect expert), two other distinct approaches are considered, each inspired by the two categories of feedback mentioned above:  1.", "Reward-based imitation: this simply corresponds to ignoring answers from the imperfect expert for which the reward is not positive (as for when the answers come from the regular expert, they are always used I believe).", "2.", "Forward prediction: this consists in predicting the natural language feedback to the answer of the imperfect expert.", "This is essentially treated as a classification problem over possible feedback (with negative sampling, since there are many possible feedback responses), that leverages a soft-attention architecture over the answers the expert could have given, which is also informed by the actual answer that was given (see Equation 2).", "Also, a mixture of both of these learning approaches is considered.", "The paper thoroughly explores experimentally all these dimensions, on two question-answering datasets (single supporting fact bAbI dataset and MovieQA).", "The neural net model architectures used are all based on memory networks.", "Without much surprise, imitating the true expert performs best.", "But quite surprisingly, forward prediction leveraging only natural language feedback to an imperfect expert often performs competitively compared to reward-based imitation.", "#### My two cents This is a very thought provoking paper!", "I very much like the idea of exploring how a model could learn a task based on instructions in natural language.", "This makes me think of this work  [ref]  on using zero-shot learning to learn a model that can produce a visual classifier based on a description of what must be recognized.", "Another component that is interesting here is studying how a model can learn without knowing a priori whether a feedback is positive or negative.", "This sort of makes me think of [this work]( [url]"], "summary_text": "This paper investigates different paradigms for learning how to answer natural language queries through various forms of feedback. Most interestingly, it investigates whether a model can learn to answer correctly questions when the feedback is presented purely in the form of a sentence (e.g. \"Yes, that's right\", \"Yes, that's correct\", \"No, that's incorrect\", etc.). This later form of feedback is particularly hard to leverage, since the model has to somehow learn that the word \"Yes\" is a sign of a positive feedback, but not the word \"No\". Normally, we'd trained a model to directly predict the correct answer to questions based on feedback provided by an expert that always answers correctly. \"Imitating\" this expert just corresponds to regular supervised learning. The paper however explores other variations on this learning scenario. Specifically, they consider 3 dimensions of variations. The first dimension of variation is who is providing the answers. Instead of an expert (who is always right), the paper considers the case where the model is instead observing a different, \"imperfect\" expert whose answers come from a fixed policy that answers correctly only a fraction of the time (the paper looked at 0.5, 0.1 and 0.01). Note that the paper refers to these answers as coming from \"the learner\" (which should be the model), but since the policy is fixed and actually doesn't depend on the model, I think one can also think of it as coming from another agent, which I'll refer to as the imperfect expert (I think this is also known as \"off policy learning\" in the RL world). The second dimension of variation on the learning scenario that is explored is in the nature of the \"supervision type\" (i.e. nature of the labels). There are 10 of them (see Figure 1 for a nice illustration). In addition to the real expert's answers only (Type 1), the paper considers other types that instead involve the imperfect expert and fall in one of the two categories below:  1. Explicit positive / negative rewards based on whether the imperfect expert's answer is correct. 2. Various forms of natural language responses to the imperfect expert's answers, which vary from worded positive/negative feedback, to hints, to mentions of the supporting fact for the correct answer. Also, mixtures of the above are considered. Finally, the third dimension of variation is how the model learns from the observed data. In addition to the regular supervised learning approach of imitating the observed answers (whether it's from the real expert or the imperfect expert), two other distinct approaches are considered, each inspired by the two categories of feedback mentioned above:  1. Reward-based imitation: this simply corresponds to ignoring answers from the imperfect expert for which the reward is not positive (as for when the answers come from the regular expert, they are always used I believe). 2. Forward prediction: this consists in predicting the natural language feedback to the answer of the imperfect expert. This is essentially treated as a classification problem over possible feedback (with negative sampling, since there are many possible feedback responses), that leverages a soft-attention architecture over the answers the expert could have given, which is also informed by the actual answer that was given (see Equation 2). Also, a mixture of both of these learning approaches is considered. The paper thoroughly explores experimentally all these dimensions, on two question-answering datasets (single supporting fact bAbI dataset and MovieQA). The neural net model architectures used are all based on memory networks. Without much surprise, imitating the true expert performs best. But quite surprisingly, forward prediction leveraging only natural language feedback to an imperfect expert often performs competitively compared to reward-based imitation. #### My two cents This is a very thought provoking paper! I very much like the idea of exploring how a model could learn a task based on instructions in natural language. This makes me think of this work  [ref]  on using zero-shot learning to learn a model that can produce a visual classifier based on a description of what must be recognized. Another component that is interesting here is studying how a model can learn without knowing a priori whether a feedback is positive or negative. This sort of makes me think of [this work]( [url]", "pdf_url": "http://arxiv.org/pdf/1604.06045", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/weston16.json"}
{"id": "8784223", "bin": "800_900", "summary_sentences": ["The paper presents a very interesting approach for learning independent (inverse) data transformation from a set of transformed data points in an unsupervised manner.", "Formulation  We start with a given data distribution P (say the MNIST dataset) where each x ε Rd.", "Consider N transformations M1, …, MN (functions that map input x to transformed input x’).", "Note that N need not be known before hand.", "These transformations can be thought of as independent (from other transformations) causal mechanisms.", "Applying these transformation would give N new distributions Q1, …, QN.", "These individual distributions are combined to form a single transformed distribution Q which contains the union of samples from the individual distributions.", "At training time, two datasets are created.", "One dataset corresponds to untransformed objects (sampled from P), referred to as DP.", "The other dataset corresponds to samples from the transformed distribution Q and is referred to as DQ.", "Note that all the samples in DP and DQ are sampled independently and no supervising information is needed.", "A series of N’ parametric models, called as experts, are initialized and would be trained to learn the different mechanisms.", "For simplicity, assume that N = N’.", "If N > N’, some experts would learn more than one transformation or certain transformations would not be learnt.", "If N < N’, some experts would not learn anything or some experts would learn the same distribution.", "All of these cases can be diagnosed and corrected by changing the number of experts.", "The experts are trained with the goal of maximizing an objective parameter c: Rd to R. c takes high values on the support of P and low values outside.", "During training, an example xQ (from DQ) is fed to all the experts at the same time.", "Each expert produces a value cj = c(Ej(xQ))  The winning expert is the one whose output is the max among all the outputs.", "Its parameters are updated to maximise its output while the other experts are not updated.", "This forces the best performing model to become even better and hence specialize.", "The objective c comes from adversarial training where a discriminator network discriminates between the untransformed input and the output of the experts.", "Each expert can be thought of as a GAN that conditions on the input xQ (and not on a noise vector).", "The output of the different experts is fed to the discriminator which provides both a selection mechanism and the gradients for training the experts.", "Experiments  Experiments are performed on the MNIST dataset using the transformations like translation along 4 directions and along 4 diagonals, contrast shift and inversion.", "The discriminator is further trained against the output of all the losing experts thereby furthering strengthing the winning expert.", "Approximate Identity Initialization  The experts are initialized randomly and then pretrained to approximate the identity function by training with identical input-output pairs.", "This ensures that the experts start from a similar level.", "In practice, it seems necessary for the success of the proposed approach.", "Observations  During the initial phase, there is a heavy competition between the experts and eventually different winners emerge for different transformations.", "The approximate quality of reconstructed output was also evaluated using a downstream task.", "3 type of inputs were created:  Untransformed images  Transformed images  Transformed images a being processed by experts.", "These inputs are fed to a pretrained MNISTN classifier.", "The classifier performs poorly on the transformed images while the performance for images processed by experts quickly catches up with the performance on untransformed images.", "The experts Ei generalize on the data points from a different dataset as well.", "To test the generalisation capabilities of the expert, a sample of data from the omniglot dataset is transformed and fed to experts (which are trained only on MNIST).", "Each expert consistently applies the same transformation even though the inputs are outside the training domain.", "This suggests that the experts have generalized to different transformations irrespective of the underlying dataset.", "Comments  The experiments are quite limited in terms of complexity of dataset and complexity of transformation but it provides evidence for a promising connection between deep learning and causality.", "Appendix mentions that in case there are too many experts, for most of the tasks, only one model specialises and the extra experts do not specialize at all.", "This is interesting as there is no explicit regularisation penalty which prevents the emergence of multiple experts per task."], "summary_text": "The paper presents a very interesting approach for learning independent (inverse) data transformation from a set of transformed data points in an unsupervised manner. Formulation  We start with a given data distribution P (say the MNIST dataset) where each x ε Rd. Consider N transformations M1, …, MN (functions that map input x to transformed input x’). Note that N need not be known before hand. These transformations can be thought of as independent (from other transformations) causal mechanisms. Applying these transformation would give N new distributions Q1, …, QN. These individual distributions are combined to form a single transformed distribution Q which contains the union of samples from the individual distributions. At training time, two datasets are created. One dataset corresponds to untransformed objects (sampled from P), referred to as DP. The other dataset corresponds to samples from the transformed distribution Q and is referred to as DQ. Note that all the samples in DP and DQ are sampled independently and no supervising information is needed. A series of N’ parametric models, called as experts, are initialized and would be trained to learn the different mechanisms. For simplicity, assume that N = N’. If N > N’, some experts would learn more than one transformation or certain transformations would not be learnt. If N < N’, some experts would not learn anything or some experts would learn the same distribution. All of these cases can be diagnosed and corrected by changing the number of experts. The experts are trained with the goal of maximizing an objective parameter c: Rd to R. c takes high values on the support of P and low values outside. During training, an example xQ (from DQ) is fed to all the experts at the same time. Each expert produces a value cj = c(Ej(xQ))  The winning expert is the one whose output is the max among all the outputs. Its parameters are updated to maximise its output while the other experts are not updated. This forces the best performing model to become even better and hence specialize. The objective c comes from adversarial training where a discriminator network discriminates between the untransformed input and the output of the experts. Each expert can be thought of as a GAN that conditions on the input xQ (and not on a noise vector). The output of the different experts is fed to the discriminator which provides both a selection mechanism and the gradients for training the experts. Experiments  Experiments are performed on the MNIST dataset using the transformations like translation along 4 directions and along 4 diagonals, contrast shift and inversion. The discriminator is further trained against the output of all the losing experts thereby furthering strengthing the winning expert. Approximate Identity Initialization  The experts are initialized randomly and then pretrained to approximate the identity function by training with identical input-output pairs. This ensures that the experts start from a similar level. In practice, it seems necessary for the success of the proposed approach. Observations  During the initial phase, there is a heavy competition between the experts and eventually different winners emerge for different transformations. The approximate quality of reconstructed output was also evaluated using a downstream task. 3 type of inputs were created:  Untransformed images  Transformed images  Transformed images a being processed by experts. These inputs are fed to a pretrained MNISTN classifier. The classifier performs poorly on the transformed images while the performance for images processed by experts quickly catches up with the performance on untransformed images. The experts Ei generalize on the data points from a different dataset as well. To test the generalisation capabilities of the expert, a sample of data from the omniglot dataset is transformed and fed to experts (which are trained only on MNIST). Each expert consistently applies the same transformation even though the inputs are outside the training domain. This suggests that the experts have generalized to different transformations irrespective of the underlying dataset. Comments  The experiments are quite limited in terms of complexity of dataset and complexity of transformation but it provides evidence for a promising connection between deep learning and causality. Appendix mentions that in case there are too many experts, for most of the tasks, only one model specialises and the extra experts do not specialize at all. This is interesting as there is no explicit regularisation penalty which prevents the emergence of multiple experts per task.", "pdf_url": "https://arxiv.org/pdf/1712.00961", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/learning-independent-causal-mechanisms.json"}
{"id": "87827594", "bin": "800_900", "summary_sentences": ["This paper can be thought as proposing a variational autoencoder applied to a form of meta-learning, i.e. where the input is not a single input but a dataset of inputs.", "For this, in addition to having to learn an approximate inference network over the latent variable $z_i$ for each input $x_i$ in an input dataset $D$, approximate inference is also learned over a latent variable $c$ that is global to the dataset $D$.", "By using Gaussian distributions for $z_i$ and $c$, the reparametrization trick can be used to train the variational autoencoder.", "The generative model factorizes as  $p(D=(x_1,\\dots,x_N), (z_1,\\dots,z_N), c) = p(c) \\prod_i p(z_i|c) p(x_i|z_i,c)$  and learning is based on the following variational posterior decomposition:  $q((z_1,\\dots,z_N), c|D=(x_1,\\dots,x_N)) = q(c|D) \\prod_i q(z_i|x_i,c)$.", "Moreover, latent variable $z_i$ is decomposed into multiple ($L$) layers $z_i = (z_{i,1}, \\dots, z_{i,L})$.", "Each layer in the generative model is directly connected to the input.", "The layers are generated from $z_{i,L}$ to $z_{i,1}$, each layer being conditioned on the previous (see Figure 1 *Right* for the graphical model), with the approximate posterior following a similar decomposition.", "The architecture for the approximate inference network $q(c|D)$ first maps all inputs $x_i\\in D$ into a vector representation, then performs mean pooling of these representations to obtain a single vector, followed by a few more layers to produce the parameters of the Gaussian distribution over $c$.", "Training is performed by stochastic gradient descent, over minibatches of datasets (i.e. multiple sets $D$).", "The model has multiple applications, explored in the experiments.", "One is of summarizing a dataset $D$ into a smaller subset $S\\in D$.", "This is done by initializing $S\\leftarrow D$ and greedily removing elements of $S$, each time minimizing the KL divergence between $q(c|D)$ and $q(c|S)$ (see the experiments on a synthetic Spatial MNIST problem of section 5.3).", "Another application is few-shot classification, where very few examples of a number of classes are given, and a new test example $x'$ must be assigned to one of these classes.", "Classification is performed by treating the small set of examples of each class $k$ as its own dataset $D_k$.", "Then, test example $x$ is classified into class $k$ for which the KL divergence between $q(c|x')$ and $q(c|D_k)$ is smallest.", "Positive results are reported when training on OMNIGLOT classes and testing on either the MNIST classes or unseen OMNIGLOT datasets, when compared to a 1-nearest neighbor classifier based on the raw input or on a representation learned by a regular autoencoder.", "Finally, another application is that of generating new samples from an input dataset of examples.", "The approximate posterior is used to compute $q(c|D)$.", "Then, $c$ is assigned to its posterior mean, from which a value for the hidden layers $z$ and finally a sample $x$ can be generated.", "It is shown that this procedure produces convincing samples that are visually similar from those in the input set $D$.", "**My two cents**  Another really nice example of deep learning applied to a form of meta-learning, i.e. learning a model that is trained to take *new* datasets as input and generalize even if confronted to datasets coming from an unseen data distribution.", "I'm particularly impressed by the many tasks explored successfully with the same approach: few-shot classification and generative sampling, as well as a form of summarization (though this last probably isn't really meta-learning).", "Overall, the approach is quite elegant and appealing.", "The very simple, synthetic experiments of section 5.1 and 5.2 are also interesting.", "Section 5.2 presents the notion of a *prior-interpolation layer*, which is well motivated but seems to be used only in that section.", "I wonder how important it is, outside of the specific case of section 5.2.", "Overall, very excited by this work, which further explores the theme of meta-learning in an interesting way."], "summary_text": "This paper can be thought as proposing a variational autoencoder applied to a form of meta-learning, i.e. where the input is not a single input but a dataset of inputs. For this, in addition to having to learn an approximate inference network over the latent variable $z_i$ for each input $x_i$ in an input dataset $D$, approximate inference is also learned over a latent variable $c$ that is global to the dataset $D$. By using Gaussian distributions for $z_i$ and $c$, the reparametrization trick can be used to train the variational autoencoder. The generative model factorizes as  $p(D=(x_1,\\dots,x_N), (z_1,\\dots,z_N), c) = p(c) \\prod_i p(z_i|c) p(x_i|z_i,c)$  and learning is based on the following variational posterior decomposition:  $q((z_1,\\dots,z_N), c|D=(x_1,\\dots,x_N)) = q(c|D) \\prod_i q(z_i|x_i,c)$. Moreover, latent variable $z_i$ is decomposed into multiple ($L$) layers $z_i = (z_{i,1}, \\dots, z_{i,L})$. Each layer in the generative model is directly connected to the input. The layers are generated from $z_{i,L}$ to $z_{i,1}$, each layer being conditioned on the previous (see Figure 1 *Right* for the graphical model), with the approximate posterior following a similar decomposition. The architecture for the approximate inference network $q(c|D)$ first maps all inputs $x_i\\in D$ into a vector representation, then performs mean pooling of these representations to obtain a single vector, followed by a few more layers to produce the parameters of the Gaussian distribution over $c$. Training is performed by stochastic gradient descent, over minibatches of datasets (i.e. multiple sets $D$). The model has multiple applications, explored in the experiments. One is of summarizing a dataset $D$ into a smaller subset $S\\in D$. This is done by initializing $S\\leftarrow D$ and greedily removing elements of $S$, each time minimizing the KL divergence between $q(c|D)$ and $q(c|S)$ (see the experiments on a synthetic Spatial MNIST problem of section 5.3). Another application is few-shot classification, where very few examples of a number of classes are given, and a new test example $x'$ must be assigned to one of these classes. Classification is performed by treating the small set of examples of each class $k$ as its own dataset $D_k$. Then, test example $x$ is classified into class $k$ for which the KL divergence between $q(c|x')$ and $q(c|D_k)$ is smallest. Positive results are reported when training on OMNIGLOT classes and testing on either the MNIST classes or unseen OMNIGLOT datasets, when compared to a 1-nearest neighbor classifier based on the raw input or on a representation learned by a regular autoencoder. Finally, another application is that of generating new samples from an input dataset of examples. The approximate posterior is used to compute $q(c|D)$. Then, $c$ is assigned to its posterior mean, from which a value for the hidden layers $z$ and finally a sample $x$ can be generated. It is shown that this procedure produces convincing samples that are visually similar from those in the input set $D$. **My two cents**  Another really nice example of deep learning applied to a form of meta-learning, i.e. learning a model that is trained to take *new* datasets as input and generalize even if confronted to datasets coming from an unseen data distribution. I'm particularly impressed by the many tasks explored successfully with the same approach: few-shot classification and generative sampling, as well as a form of summarization (though this last probably isn't really meta-learning). Overall, the approach is quite elegant and appealing. The very simple, synthetic experiments of section 5.1 and 5.2 are also interesting. Section 5.2 presents the notion of a *prior-interpolation layer*, which is well motivated but seems to be used only in that section. I wonder how important it is, outside of the specific case of section 5.2. Overall, very excited by this work, which further explores the theme of meta-learning in an interesting way.", "pdf_url": "http://arxiv.org/pdf/1606.02185v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/1606.02185.json"}
{"id": "45538669", "bin": "800_900", "summary_sentences": ["What  They describe a strategy to train CNNs in unsupervised fashion.", "The method is based on iterating k-means clustering in feature space, followed by training the CNN to predict the cluster labels.", "The method achieves sizeable improvements over the state of the art in unsupervised learning on ImageNet.", "How  Method  They start with a CNN (not pretrained)  They then iterate the following steps:  They apply the CNN to their dataset, converting each image to a feature vector.", "(At the start, this will be mostly noise.)", "They apply k-means to these feature vectors.", "They use the resulting clustering as (pseudo-)labels for the images.", "They train the CNN for some batches to predict these (pseudo-)labels.", "They start again at (1).", "Visualization:  Avoiding trivial solutions  Empty Clusters  While k-means is iterating, some clusters may become empty.", "If that happens, they move the centroid of such a cluster (e.g. call it \"cluster A\" here) to another cluster (e.g.", "\"cluster B\"), where B must have associated feature vectors.", "After moving, they perturb the centroid of cluster A's location by a small amount.", "They then split the feature vectors of cluster B between A and B.", "Trivial Parameterization  If one cluster becomes dominating, i.e. if most of the feature vectors are associated to it, the CNN can start to learn trivial parameterizations that only focus on differentiating between that one cluster and everything else, because.", "They avoid this by sampling images based on a uniform distribution over the (pseudo-)labels.", "Other stuff  Their test their methods mainly with AlexNet.", "Also a bit with VGG.", "(Both modified to use BN.)", "They update the clusters once per epoch on ImageNet.", "They train on ImageNet for 500 epochs.", "They apply a Sobel-based filter before their models (i.e. they do not get colors as inputs).", "Results  ImageNet  They measure the normalized mutual information between cluster assignments and ImageNet labels.", "This mutual information increases over time, indicating that the cluster assignments matches more and more the labels (which were not used during training).", "They measure the normalized mutual information of cluster assignments between each pair of two consecutive epochs.", "This mutual information increases over time, indicating that the clustering becomes more stable.", "They measure the achieved mAP on Pascal VOC 2007 (I guess they use the model as pretrained weights?)", "based on the number of clusters in k-means.", "They achieve best results with 10,000 clusters.", "Accuracy per layer  After training they place a linear classifier on top of their model and train it on the labels.", "They do repeat this for each layer to get the accuracy per layer.", "They significantly outperform all competing methods (by several percentage points, depending on layer).", "At conv5 (max conv layer in AlexNet) they are beaten by supervised learning by ~12 points (50.5 vs 38.2).", "The difference is lower for conv3 (44.2 vs 41.0).", "They also compare models pretrained on ImageNet, with the linear classifier fine-tuned on the Places dataset.", "In this case they can keep up more with the model pretrained in supervised fashion (38.7 vs 34.7 at conv5, 39.4 vs 39.8 at conv4).", "Stats:  YFCC100M  They train on ImageNet and then search on YFCC100M for images leading to high activations in filters.", "Images with high activations and generated \"ideal\" images for that filter:  Images with high activations for filters in layer conv5:  They note that some filters in conv5 seem to just re-learn patterns that are already covered by filters in lower layers.", "Pascal VOC  They pretrain on ImageNet and then finetune on Pascal VOC for classification, object detection and segmentation.", "They are still behind a model pretrained with labels.", "Depending on the task between ~1 (detection), ~3 (segmentation) or ~6 (classification) points.", "They outperform other unsupervised methods by several points.", "They outperform a randomly initialized (i.e. not pretrained) model significantly.", "(This is even more the case if only the last fully connected layers are finetuned, not the convolutional layers.)", "They train with VGG16 instead of AlexNet and can improve their accuracy by an amount comparable to when the same model switch is done for the supervised training."], "summary_text": "What  They describe a strategy to train CNNs in unsupervised fashion. The method is based on iterating k-means clustering in feature space, followed by training the CNN to predict the cluster labels. The method achieves sizeable improvements over the state of the art in unsupervised learning on ImageNet. How  Method  They start with a CNN (not pretrained)  They then iterate the following steps:  They apply the CNN to their dataset, converting each image to a feature vector. (At the start, this will be mostly noise.) They apply k-means to these feature vectors. They use the resulting clustering as (pseudo-)labels for the images. They train the CNN for some batches to predict these (pseudo-)labels. They start again at (1). Visualization:  Avoiding trivial solutions  Empty Clusters  While k-means is iterating, some clusters may become empty. If that happens, they move the centroid of such a cluster (e.g. call it \"cluster A\" here) to another cluster (e.g. \"cluster B\"), where B must have associated feature vectors. After moving, they perturb the centroid of cluster A's location by a small amount. They then split the feature vectors of cluster B between A and B. Trivial Parameterization  If one cluster becomes dominating, i.e. if most of the feature vectors are associated to it, the CNN can start to learn trivial parameterizations that only focus on differentiating between that one cluster and everything else, because. They avoid this by sampling images based on a uniform distribution over the (pseudo-)labels. Other stuff  Their test their methods mainly with AlexNet. Also a bit with VGG. (Both modified to use BN.) They update the clusters once per epoch on ImageNet. They train on ImageNet for 500 epochs. They apply a Sobel-based filter before their models (i.e. they do not get colors as inputs). Results  ImageNet  They measure the normalized mutual information between cluster assignments and ImageNet labels. This mutual information increases over time, indicating that the cluster assignments matches more and more the labels (which were not used during training). They measure the normalized mutual information of cluster assignments between each pair of two consecutive epochs. This mutual information increases over time, indicating that the clustering becomes more stable. They measure the achieved mAP on Pascal VOC 2007 (I guess they use the model as pretrained weights?) based on the number of clusters in k-means. They achieve best results with 10,000 clusters. Accuracy per layer  After training they place a linear classifier on top of their model and train it on the labels. They do repeat this for each layer to get the accuracy per layer. They significantly outperform all competing methods (by several percentage points, depending on layer). At conv5 (max conv layer in AlexNet) they are beaten by supervised learning by ~12 points (50.5 vs 38.2). The difference is lower for conv3 (44.2 vs 41.0). They also compare models pretrained on ImageNet, with the linear classifier fine-tuned on the Places dataset. In this case they can keep up more with the model pretrained in supervised fashion (38.7 vs 34.7 at conv5, 39.4 vs 39.8 at conv4). Stats:  YFCC100M  They train on ImageNet and then search on YFCC100M for images leading to high activations in filters. Images with high activations and generated \"ideal\" images for that filter:  Images with high activations for filters in layer conv5:  They note that some filters in conv5 seem to just re-learn patterns that are already covered by filters in lower layers. Pascal VOC  They pretrain on ImageNet and then finetune on Pascal VOC for classification, object detection and segmentation. They are still behind a model pretrained with labels. Depending on the task between ~1 (detection), ~3 (segmentation) or ~6 (classification) points. They outperform other unsupervised methods by several points. They outperform a randomly initialized (i.e. not pretrained) model significantly. (This is even more the case if only the last fully connected layers are finetuned, not the convolutional layers.) They train with VGG16 instead of AlexNet and can improve their accuracy by an amount comparable to when the same model switch is done for the supervised training.", "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/deep_clustering_for_unsupervised_learning_of_visual_features.json"}
{"id": "56196", "bin": "800_900", "summary_sentences": ["The paper proposes an approach for learning neural networks (modules) that can be combined in different ways to solve different tasks (combinatorial generalization).", "The proposed model is called as BOUNCEGRAD.", "Link to the code  Setup  Focuses on supervised learning.", "Task distribution p(T).", "Each task is a joint distribution pT(x, y) over (x, y) data pairs.", "Given data from m meta-training tasks, and a meta-test task, find a hypothesis h which performs well on the unseen data drawn from the meta-test task.", "Structured Hypothesis  Given a compositional scheme C, a set of modules F1, …, Fk (represented as a whole by F) and the set of their respective parameters θ1, …, θk (represented as a whole by θ), (C, F, θ) represents the set of possible functional input-output mappings.", "These mappings form the hypothesis space.", "A structured hypothesis model is specified by what modules to use and their parametric forms (but not the values).", "Examples of compositional schemes  Choosing a single module for the task at hand.", "Fixed compositional structure but different modules selected every time.", "Weight ensemble (maybe using attention mechanism)  General function composition tree  Phases  Offline Meta Learning Phase:  Take training and validation dataset for the first k tasks and generate a parameterization for each module θ1, …, θk.", "The hypothesis (or composition) to use comes from the online meta-test learning phase.", "In this stage, find the best θ given a structure.", "Online Meta-test Learning Phase  Given a hypothesis space and θ, the output is a compositional form (or hypothesis) that specifies how to compose the models.", "In this stage, find the best structure, given a hypothesis space and θ.", "Learning Algorithm  During Meta-test learning phase, simulated annealing is used to find the optimal structure, with temperature T decreased over time.", "During meta-learning phrase, the actual objective function is replaced by a surrogate, smooth objective function (during the search step) to avoid local minima.", "Once a structure has been picked, any gradient descent based approach can be used to optimize the modules.", "Basically the state of optimization process comprises of the parameters and the temperature.", "Together, they are used to induce a distribution over the structures.", "Given a structure, θ is optimized and T is annealed over time.", "The learning procedure can be improved upon by performing parameter tuning during the online (meta-test learning) phase as well.", "the resulting approach is referred to as MOMA - MOdular MAml.", "Experiments  Approaches  Pooled - Single network using combined data of all the tasks.", "MAML - Single network using MAML  BOUNCEGRAD - Modular Network without MAML adaptation in online learning.", "MOMA - BOUNCEGRAD with MAML adaptation in online learning.", "Domains  Simple Functional Relationships  Sine-function prediction problem  In general, MOMA outperforms other models.", "With a small amount of online training data, BOUNCEGRAD outperforms other models as it has a better structural prior.", "Predicting next frame of a kinematic skeleton (motion capture data)  11 different objects (with different shapes) on 4 surfaces with different friction properties.", "2 meta-learning scenarios are considered.", "In the first case, the object-surface combination in the test case was present in some meta-training tasks and in the other case, it was not present.", "For previously seen combinations, MOMA performs the best followed by BOUNCEGRAD and MAML.", "For unseen combinations, all the 3 are equally good.", "Compositional scheme is the attention mechanism.", "An interesting result is that the modules seem to specialize (and activate more often) based on the shape of the object.", "Predicting next frame of a kinematic selection (using motion capture data)  Composition Structure - generating kinematics subtrees for each body part (2 legs, 2 arms, 2 torsi).", "Again 2 setups are used - one where all activities in the training and the meta-test task are shared while the other setup where the activities are not shared.", "For known activities MOMA and BOUNCEGRAD perform the best while for unknown activities, MOMS performs the best.", "Notes  While the approach is interesting, maybe a more suitable set of tasks (from the point of composition) would be more convincing.", "It would be useful to see the computational tradeoff between MAML, BOUNCEGRAD, and MOMA."], "summary_text": "The paper proposes an approach for learning neural networks (modules) that can be combined in different ways to solve different tasks (combinatorial generalization). The proposed model is called as BOUNCEGRAD. Link to the code  Setup  Focuses on supervised learning. Task distribution p(T). Each task is a joint distribution pT(x, y) over (x, y) data pairs. Given data from m meta-training tasks, and a meta-test task, find a hypothesis h which performs well on the unseen data drawn from the meta-test task. Structured Hypothesis  Given a compositional scheme C, a set of modules F1, …, Fk (represented as a whole by F) and the set of their respective parameters θ1, …, θk (represented as a whole by θ), (C, F, θ) represents the set of possible functional input-output mappings. These mappings form the hypothesis space. A structured hypothesis model is specified by what modules to use and their parametric forms (but not the values). Examples of compositional schemes  Choosing a single module for the task at hand. Fixed compositional structure but different modules selected every time. Weight ensemble (maybe using attention mechanism)  General function composition tree  Phases  Offline Meta Learning Phase:  Take training and validation dataset for the first k tasks and generate a parameterization for each module θ1, …, θk. The hypothesis (or composition) to use comes from the online meta-test learning phase. In this stage, find the best θ given a structure. Online Meta-test Learning Phase  Given a hypothesis space and θ, the output is a compositional form (or hypothesis) that specifies how to compose the models. In this stage, find the best structure, given a hypothesis space and θ. Learning Algorithm  During Meta-test learning phase, simulated annealing is used to find the optimal structure, with temperature T decreased over time. During meta-learning phrase, the actual objective function is replaced by a surrogate, smooth objective function (during the search step) to avoid local minima. Once a structure has been picked, any gradient descent based approach can be used to optimize the modules. Basically the state of optimization process comprises of the parameters and the temperature. Together, they are used to induce a distribution over the structures. Given a structure, θ is optimized and T is annealed over time. The learning procedure can be improved upon by performing parameter tuning during the online (meta-test learning) phase as well. the resulting approach is referred to as MOMA - MOdular MAml. Experiments  Approaches  Pooled - Single network using combined data of all the tasks. MAML - Single network using MAML  BOUNCEGRAD - Modular Network without MAML adaptation in online learning. MOMA - BOUNCEGRAD with MAML adaptation in online learning. Domains  Simple Functional Relationships  Sine-function prediction problem  In general, MOMA outperforms other models. With a small amount of online training data, BOUNCEGRAD outperforms other models as it has a better structural prior. Predicting next frame of a kinematic skeleton (motion capture data)  11 different objects (with different shapes) on 4 surfaces with different friction properties. 2 meta-learning scenarios are considered. In the first case, the object-surface combination in the test case was present in some meta-training tasks and in the other case, it was not present. For previously seen combinations, MOMA performs the best followed by BOUNCEGRAD and MAML. For unseen combinations, all the 3 are equally good. Compositional scheme is the attention mechanism. An interesting result is that the modules seem to specialize (and activate more often) based on the shape of the object. Predicting next frame of a kinematic selection (using motion capture data)  Composition Structure - generating kinematics subtrees for each body part (2 legs, 2 arms, 2 torsi). Again 2 setups are used - one where all activities in the training and the meta-test task are shared while the other setup where the activities are not shared. For known activities MOMA and BOUNCEGRAD perform the best while for unknown activities, MOMS performs the best. Notes  While the approach is interesting, maybe a more suitable set of tasks (from the point of composition) would be more convincing. It would be useful to see the computational tradeoff between MAML, BOUNCEGRAD, and MOMA.", "pdf_url": "https://arxiv.org/pdf/1806.10166", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/modular-meta-learning.json"}
{"id": "31303819", "bin": "800_900", "summary_sentences": ["Problem Statement  A common way of measuring image similarity is to embed them into feature spaces where distance acts as a proxy for similarity.", "But this feature space can capture one (or a weighted combination) of the many possible notions of similarity.", "What if contracting notions of similarity could be captured at the same time - in terms of semantically distinct subspaces.", "The paper proposes a new architecture called as Conditional Similarity Networks (CSNs) which learns a disentangled embedding such that the features, for different notions of similarity, are encoded into separate dimensions.", "It jointly learns masks (or feature extractors) that select and reweights relevant dimensions to induce a subspace that encodes a specific notion of similarity.", "Conditional Similarity Networks  Given an image, x, learn a non-linear feature embedding f(x) such that for any 2 images x1 and x2, the euclidean distance between f(x1) and f(x2) reflects their similarity.", "Conditional Similarity Triplets  Given a triplet of images (x1, x2, x3) and a condition c (the notion of similarity), an oracle (say crowd) is used to determmine if x1 is more similar to x2 or x3 as per the given criteria c.  In general, for images i, j, l, the triplet t is ordered {i, j, l | c} if i is more similar to j than l.  Learning From Triplets  Define a loss function LT() to model the similarity structure over the triplets.", "LT(i, j, l) = max{0, D(i, j) - D(i, l) + h} where D is the euclidean distance function and h is the similarity scalar margin to prevent trivial solutions.", "To model conditional similarities, masks m are defined as m = σ(β) where σ is the RELU unit and β is a set of parameters to be learnt.", "mc denotes the selection of the c-th mask column from feature vector.", "It thus acts as an element-wise gating function which selects the relevant dimensions of the embedding to attend to a particular similarity concept.", "The euclidean function D now computes the masked distance (f(i, c)mc) between the two given images.", "Two regularising terms are also added - L2 norm for D and L1 norm for m.  Experiments  Datasets  Fonts dataset by Bernhardsson  3.1 million 64 by 64-pixel grey scale images.", "Zappos50k shoe dataset  Contains 50,000 images of individual richly annotated shoes.", "Characteristics of interest:  Type of the shoes (i.e., shoes, boots, sandals or slippers)  Suggested gender of the shoes (i.e., for women, men, girls or boys)  Height of the shoes’ heels (0 to 5 inches)  Closing mechanism of the shoes (buckle, pull on, slip on, hook and loop or laced up)  Models  Initial model for the experiments is a ConvNet pre-trained on ImageNet  Standard Triplet Network  Learn from all available triplets jointly as if they have the same notion of similarity.", "Set of Task Specific Triplet Networks  Train n separate triplet networks such that each is trained on a single notion of similarity.", "Needs far more parameters and compute.", "Conditional Similarity Networks - fixed disjoint masks  In this version, only the convolutional filters and the embedding is learnt and masks are predefined to be disjoint.", "Aims to learn a fully disjoint embedding.", "Conditional Similarity Networks - learned masks  Learns all the components - conv filters, embedding and the masks.", "Refer paper for details on hyperparameters.", "Results  Visual exploration of the learned subspaces (t-sne visualisation) show that network successfully disentangles different features in the embedded vector space.", "The learned masks are very sparse and share dimensions.", "This shows that CSNs may learn to only use the required number of dimensions thereby doing away with the need of picking the right size of embedding.", "Order of performance:  CSNs with learned masks > CSNs with fixed masks > Task-specific networks > standard triplet network.", "Though CSNs with learned masks require more training data.", "CSNs also outperform Standard Triplet Network when used as off the shelf features for (brand) classification task and is very close to the performance of ResNet trained on ImageNet.", "This shows that while CSN retained most of the information in the original network, the training mechanism of Standard Triplet Network hurts the underlying conv features and their generalising capability"], "summary_text": "Problem Statement  A common way of measuring image similarity is to embed them into feature spaces where distance acts as a proxy for similarity. But this feature space can capture one (or a weighted combination) of the many possible notions of similarity. What if contracting notions of similarity could be captured at the same time - in terms of semantically distinct subspaces. The paper proposes a new architecture called as Conditional Similarity Networks (CSNs) which learns a disentangled embedding such that the features, for different notions of similarity, are encoded into separate dimensions. It jointly learns masks (or feature extractors) that select and reweights relevant dimensions to induce a subspace that encodes a specific notion of similarity. Conditional Similarity Networks  Given an image, x, learn a non-linear feature embedding f(x) such that for any 2 images x1 and x2, the euclidean distance between f(x1) and f(x2) reflects their similarity. Conditional Similarity Triplets  Given a triplet of images (x1, x2, x3) and a condition c (the notion of similarity), an oracle (say crowd) is used to determmine if x1 is more similar to x2 or x3 as per the given criteria c.  In general, for images i, j, l, the triplet t is ordered {i, j, l | c} if i is more similar to j than l.  Learning From Triplets  Define a loss function LT() to model the similarity structure over the triplets. LT(i, j, l) = max{0, D(i, j) - D(i, l) + h} where D is the euclidean distance function and h is the similarity scalar margin to prevent trivial solutions. To model conditional similarities, masks m are defined as m = σ(β) where σ is the RELU unit and β is a set of parameters to be learnt. mc denotes the selection of the c-th mask column from feature vector. It thus acts as an element-wise gating function which selects the relevant dimensions of the embedding to attend to a particular similarity concept. The euclidean function D now computes the masked distance (f(i, c)mc) between the two given images. Two regularising terms are also added - L2 norm for D and L1 norm for m.  Experiments  Datasets  Fonts dataset by Bernhardsson  3.1 million 64 by 64-pixel grey scale images. Zappos50k shoe dataset  Contains 50,000 images of individual richly annotated shoes. Characteristics of interest:  Type of the shoes (i.e., shoes, boots, sandals or slippers)  Suggested gender of the shoes (i.e., for women, men, girls or boys)  Height of the shoes’ heels (0 to 5 inches)  Closing mechanism of the shoes (buckle, pull on, slip on, hook and loop or laced up)  Models  Initial model for the experiments is a ConvNet pre-trained on ImageNet  Standard Triplet Network  Learn from all available triplets jointly as if they have the same notion of similarity. Set of Task Specific Triplet Networks  Train n separate triplet networks such that each is trained on a single notion of similarity. Needs far more parameters and compute. Conditional Similarity Networks - fixed disjoint masks  In this version, only the convolutional filters and the embedding is learnt and masks are predefined to be disjoint. Aims to learn a fully disjoint embedding. Conditional Similarity Networks - learned masks  Learns all the components - conv filters, embedding and the masks. Refer paper for details on hyperparameters. Results  Visual exploration of the learned subspaces (t-sne visualisation) show that network successfully disentangles different features in the embedded vector space. The learned masks are very sparse and share dimensions. This shows that CSNs may learn to only use the required number of dimensions thereby doing away with the need of picking the right size of embedding. Order of performance:  CSNs with learned masks > CSNs with fixed masks > Task-specific networks > standard triplet network. Though CSNs with learned masks require more training data. CSNs also outperform Standard Triplet Network when used as off the shelf features for (brand) classification task and is very close to the performance of ResNet trained on ImageNet. This shows that while CSN retained most of the information in the original network, the training mechanism of Standard Triplet Network hurts the underlying conv features and their generalising capability", "pdf_url": "https://arxiv.org/pdf/1603.07810.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/conditional-similarity-networks.json"}
{"id": "15062404", "bin": "800_900", "summary_sentences": ["What  They propose a method for classifiers to estimate how hard/difficult an example is (for the specific classifier).", "They call that \"realistic predictors\" (in the sense of \"being realistic with respect to one's own abilities\").", "Their method allows them to  focus training on hard examples,  reject too difficult inputs,  based on point (2) guarantee average accuracies by rejecting inputs with certain thresholds of diffculty-estimates.", "(This is useful e.g. for some safety critical systems, where no decision might be better than a wrong decision.", "It can also be used for combinations of models and humans, where a model would automate easy tasks and a few humans deal with the remaining hard tasks.)", "How  Architecture  They have a given classifier F.  They need per example i a hardness estimate s_i denoting how difficult that input is for F.  They predict that value with HP-Net (hardness predictor), a network completely separate from F.  The estimated hardness values s_i influence the training of F for hard negative mining.", "The predictions of F are used during the training of HP-Net.", "Visualization:  Loss  For F (classifier):  F predicts for multiple classes a probability value (after softmax).", "They use a crossentropy loss weighted by the hardness:  Here, s_i is the hardness if the i-th example and p_i^c is the probability predicted by F for the correct class of the i-th example.", "This increases the loss for hard examples (high s_i value) and decreases it for easy examples (low s_i).", "For HP-Net (hardness predictor):  HP-Net predicts the inverse of the probability that F is going to predict for the correct class.", "So if F predicts for the correct class a high probability, HP-Net is trained to predict a low hardness value and vice versa.", "They use a binary crossentropy loss between p_i^c (F's prediction for the correct class) and s_i (hardness):  Training Schedule and Refinement  The full training schedules happens in the following steps.", "Train F and HP-Net jointly on a training set.", "Do that training iteratively (one batch for F, then one batch for HP-Net).", "Otherwise the networks would not converge for them.", "Use HP-Net to eliminate hard examples from the dataset.", "(E.g. the hardest 5%.)", "Train a new F on reduced dataset, result is F' (with higher accuracy for these easier examples).", "Reuse HP-Net from (1) during this training, keep its weights fixed.", "Output pair (F', HP-Net).", "Then, at test time examples above a certain hardness threshold are rejected (similar to step 2) and F' is applied to the remaining examples.", "Visualization:  Results  They test on MNIST, MIT67 and ImageNet.", "They test for both F and HP-Net the networks VGG, ResNet, kerasNet and LeNet5.", "(They slightly modify the heads for HP-Net.)", "They test with and without weight sharing between F and HP-Net.", "They observe that hardness scores predicted by HP-Net start with high values and then shift towards lower values during training.", "(As expected.)", "They achieve significantly worse accuracies if using weight sharing between F and HP-Net.", "This indicates that the two networks solve fairly different tasks.", "They achieve the highest accuracies when using the same network architectures for F and HP-Net (e.g. both ResNet).", "This indicates that the HP-Net has to operate in a similar way to F in order to predict its predictions.", "On ImageNet  They get almost 1 percentage point higher accuracy, even when not rejecting any examples.", "This is likely due to the indirect hard negative mining in the loss of F.  Rejecting the 5% most difficult examples results in 0.7 points higher accuracy (on the remaining ones).", "At 10% rejection rate they get 1.3 higher accuracy.", "One can reject examples based on the hardness score or based on the highest class probability predicted by F. Doing it based on the hardness score performs marginally better (+0.1 points) for high rejection rates and decently better (+0.7 points) for a rejection rate of 5%.", "Refining on the reduced dataset (from F to F') gives around 0.1 points higher accuracy.", "To reach with VGG the accuracy of ResNet at 2% rejection rate, one has to set VGG's rejection rate to 10%."], "summary_text": "What  They propose a method for classifiers to estimate how hard/difficult an example is (for the specific classifier). They call that \"realistic predictors\" (in the sense of \"being realistic with respect to one's own abilities\"). Their method allows them to  focus training on hard examples,  reject too difficult inputs,  based on point (2) guarantee average accuracies by rejecting inputs with certain thresholds of diffculty-estimates. (This is useful e.g. for some safety critical systems, where no decision might be better than a wrong decision. It can also be used for combinations of models and humans, where a model would automate easy tasks and a few humans deal with the remaining hard tasks.) How  Architecture  They have a given classifier F.  They need per example i a hardness estimate s_i denoting how difficult that input is for F.  They predict that value with HP-Net (hardness predictor), a network completely separate from F.  The estimated hardness values s_i influence the training of F for hard negative mining. The predictions of F are used during the training of HP-Net. Visualization:  Loss  For F (classifier):  F predicts for multiple classes a probability value (after softmax). They use a crossentropy loss weighted by the hardness:  Here, s_i is the hardness if the i-th example and p_i^c is the probability predicted by F for the correct class of the i-th example. This increases the loss for hard examples (high s_i value) and decreases it for easy examples (low s_i). For HP-Net (hardness predictor):  HP-Net predicts the inverse of the probability that F is going to predict for the correct class. So if F predicts for the correct class a high probability, HP-Net is trained to predict a low hardness value and vice versa. They use a binary crossentropy loss between p_i^c (F's prediction for the correct class) and s_i (hardness):  Training Schedule and Refinement  The full training schedules happens in the following steps. Train F and HP-Net jointly on a training set. Do that training iteratively (one batch for F, then one batch for HP-Net). Otherwise the networks would not converge for them. Use HP-Net to eliminate hard examples from the dataset. (E.g. the hardest 5%.) Train a new F on reduced dataset, result is F' (with higher accuracy for these easier examples). Reuse HP-Net from (1) during this training, keep its weights fixed. Output pair (F', HP-Net). Then, at test time examples above a certain hardness threshold are rejected (similar to step 2) and F' is applied to the remaining examples. Visualization:  Results  They test on MNIST, MIT67 and ImageNet. They test for both F and HP-Net the networks VGG, ResNet, kerasNet and LeNet5. (They slightly modify the heads for HP-Net.) They test with and without weight sharing between F and HP-Net. They observe that hardness scores predicted by HP-Net start with high values and then shift towards lower values during training. (As expected.) They achieve significantly worse accuracies if using weight sharing between F and HP-Net. This indicates that the two networks solve fairly different tasks. They achieve the highest accuracies when using the same network architectures for F and HP-Net (e.g. both ResNet). This indicates that the HP-Net has to operate in a similar way to F in order to predict its predictions. On ImageNet  They get almost 1 percentage point higher accuracy, even when not rejecting any examples. This is likely due to the indirect hard negative mining in the loss of F.  Rejecting the 5% most difficult examples results in 0.7 points higher accuracy (on the remaining ones). At 10% rejection rate they get 1.3 higher accuracy. One can reject examples based on the hardness score or based on the highest class probability predicted by F. Doing it based on the hardness score performs marginally better (+0.1 points) for high rejection rates and decently better (+0.7 points) for a rejection rate of 5%. Refining on the reduced dataset (from F to F') gives around 0.1 points higher accuracy. To reach with VGG the accuracy of ResNet at 2% rejection rate, one has to set VGG's rejection rate to 10%.", "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/towards_realistic_predictors.json"}
{"id": "52208833", "bin": "900_1000", "summary_sentences": ["Two pretraining objectives that have been successful for pretraining neural networks used in transfer learning NLP are autoregressive (AR) language modeling and autoencoding (AE).", "Autoregressive language modeling is not able to model deep bidirectional context which has recently been found to be effective in several downstream NLP tasks such as sentiment analysis and question answering.", "On the other hand, autoencoding based pretraining aims to reconstruct original data from corrupted data.", "A popular example of such modeling is used in BERT, an effective state-of-the-art technique used to address several NLP tasks.", "One advantage of models like BERT is that bidirectional contexts can be used in the reconstruction process, something that AR language modeling lacks.", "However, BERT partially masks the input (i.e. tokens) during pretraining which results in a pre-training-finetune discrepancy.", "In addition, BERT assumes independence on predicted tokens, something which AR models allow for via the product rule which is used to factorize the joint probability of predicted tokens.", "This could potentially help with the pretrain-finetune discrepancy found in BERT.", "The proposed model (XLNet) borrows ideas from the two types of language pretraining objectives (AR and AE) while avoiding their limitations.", "The XLNet model  XLNet makes use of a permutation operation during training time that allows context to consists of tokens from both left and right, capturing the bidirectional context, making it a generalized order-aware AR language model.", "During pretraining, XLNet adopts the segment recurrent mechanism and relative encoding scheme proposed in Transformer-XL .", "Essentially, the novel permutation language modeling objective (see paper for extra details) allows sharing of the model parameters across all the permuted factorization orders.", "This enables the AR model to properly and effectively capture bidirectional context while avoiding the independence assumption and pretrain-finetune discrepancy that BERT is subject to.", "Simply put it, XLNet keeps the original sequence order, uses positional encodings, and relies on a special attention mask in Transformers to achieve the said permutation of the factorization order.", "In other words, the original Transformer architecture is modified and re-parameterized to avoid issues such as target ambiguity and pretrain-finetune discrepancy.", "The core change happens in the hidden representation layers (see paper for details).", "XLNet is based on the Transformer-XL which it uses as the main pretraining framework.", "Obviously, for the proposed permutation operation to work, a few modifications are proposed, which enforce the proper reuse of the hidden states from previous segments.", "Some design ideas from BERT are also used to perform partial prediction and support certain tasks that consist of multiple segments like question and context paragraph in question answering.", "From the following examples below, we can observe that both BERT and XLNet compute the objective differently.", "In general, XLNet captures more important dependencies between prediction targets, such as (New, York), which BERT omits.", "XLNet also proves to cover more dependencies as compared to GPT and ELMo .", "Overall, XLNet makes a compelling case for bridging the gap between language modeling and pretraining, all achieved by leveraging AR modeling and borrowing techniques from previous methods like BERT and Transformer-XL.", "More importantly, XLNet aims to address the pretrain-finetune discrepancy, which means language models can potentially improve downstream tasks through this useful generalization.", "Experiments  Several sources like BooksCorpus, English Wikipedia, Giga5, and Common Crawl are combined and used for pretraining.", "Tokenization is achieved with SentencePiece.", "The same architecture hyperparameters as BERT-Large are used in XLNet-Large and trained on 512 TPU v3 chips for 500K epochs with an Adam optimizer.", "A linear learning rate decay and a batch size of 2048 are used, all leading to roughly 2.5 days of training.", "XLNet-Large was not able to leverage the additional data scale, so XLNet-Base (analogous to BERT-Base) was used to conduct a fair comparison with BERT.", "This also means that only BooksCorpus and English Wikipedia were used for pretraining.", "Results  RACE involves a reading comprehension dataset that is used to test the question answering and long text understanding capabilities of the model.", "As shown in the table below, XLNet outperforms (in terms of accuracy) both GPT and BERT pretraining models.", "SQuAD and NewsQA are also popular reading comprehension datasets which consist of two tasks.", "Specifically, XLNet jointly trains on SQuAD 2.0 and NewsQA and obtains state-of-the-art performance on this task, outperforming BERT even on the dev set (see results below).", "XLNet now holds state-of-the-art (SoTA) results on several text classification benchmarks such as IMDB and DBpedia (see results below).", "GLUE consists of 9 natural language understanding tasks.", "Using XLNet, multiple settings such as single-task and multi-task, as well as single models and ensembles are tested on GLUE.", "In the end, a multi-task ensemble XLNet achieves SoTA results on 7 out of 9 tasks.", "XLNet outperforms BERT on the different datasets as seen in the table below.", "An interesting ablation study and extended results are provided in the paper to justify some of the design choices made in XLNet and how it compares with other models such as BERT and GPT.", "XLNet: Generalized Autoregressive Pretraining for Language Understanding — (Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le)"], "summary_text": "Two pretraining objectives that have been successful for pretraining neural networks used in transfer learning NLP are autoregressive (AR) language modeling and autoencoding (AE). Autoregressive language modeling is not able to model deep bidirectional context which has recently been found to be effective in several downstream NLP tasks such as sentiment analysis and question answering. On the other hand, autoencoding based pretraining aims to reconstruct original data from corrupted data. A popular example of such modeling is used in BERT, an effective state-of-the-art technique used to address several NLP tasks. One advantage of models like BERT is that bidirectional contexts can be used in the reconstruction process, something that AR language modeling lacks. However, BERT partially masks the input (i.e. tokens) during pretraining which results in a pre-training-finetune discrepancy. In addition, BERT assumes independence on predicted tokens, something which AR models allow for via the product rule which is used to factorize the joint probability of predicted tokens. This could potentially help with the pretrain-finetune discrepancy found in BERT. The proposed model (XLNet) borrows ideas from the two types of language pretraining objectives (AR and AE) while avoiding their limitations. The XLNet model  XLNet makes use of a permutation operation during training time that allows context to consists of tokens from both left and right, capturing the bidirectional context, making it a generalized order-aware AR language model. During pretraining, XLNet adopts the segment recurrent mechanism and relative encoding scheme proposed in Transformer-XL . Essentially, the novel permutation language modeling objective (see paper for extra details) allows sharing of the model parameters across all the permuted factorization orders. This enables the AR model to properly and effectively capture bidirectional context while avoiding the independence assumption and pretrain-finetune discrepancy that BERT is subject to. Simply put it, XLNet keeps the original sequence order, uses positional encodings, and relies on a special attention mask in Transformers to achieve the said permutation of the factorization order. In other words, the original Transformer architecture is modified and re-parameterized to avoid issues such as target ambiguity and pretrain-finetune discrepancy. The core change happens in the hidden representation layers (see paper for details). XLNet is based on the Transformer-XL which it uses as the main pretraining framework. Obviously, for the proposed permutation operation to work, a few modifications are proposed, which enforce the proper reuse of the hidden states from previous segments. Some design ideas from BERT are also used to perform partial prediction and support certain tasks that consist of multiple segments like question and context paragraph in question answering. From the following examples below, we can observe that both BERT and XLNet compute the objective differently. In general, XLNet captures more important dependencies between prediction targets, such as (New, York), which BERT omits. XLNet also proves to cover more dependencies as compared to GPT and ELMo . Overall, XLNet makes a compelling case for bridging the gap between language modeling and pretraining, all achieved by leveraging AR modeling and borrowing techniques from previous methods like BERT and Transformer-XL. More importantly, XLNet aims to address the pretrain-finetune discrepancy, which means language models can potentially improve downstream tasks through this useful generalization. Experiments  Several sources like BooksCorpus, English Wikipedia, Giga5, and Common Crawl are combined and used for pretraining. Tokenization is achieved with SentencePiece. The same architecture hyperparameters as BERT-Large are used in XLNet-Large and trained on 512 TPU v3 chips for 500K epochs with an Adam optimizer. A linear learning rate decay and a batch size of 2048 are used, all leading to roughly 2.5 days of training. XLNet-Large was not able to leverage the additional data scale, so XLNet-Base (analogous to BERT-Base) was used to conduct a fair comparison with BERT. This also means that only BooksCorpus and English Wikipedia were used for pretraining. Results  RACE involves a reading comprehension dataset that is used to test the question answering and long text understanding capabilities of the model. As shown in the table below, XLNet outperforms (in terms of accuracy) both GPT and BERT pretraining models. SQuAD and NewsQA are also popular reading comprehension datasets which consist of two tasks. Specifically, XLNet jointly trains on SQuAD 2.0 and NewsQA and obtains state-of-the-art performance on this task, outperforming BERT even on the dev set (see results below). XLNet now holds state-of-the-art (SoTA) results on several text classification benchmarks such as IMDB and DBpedia (see results below). GLUE consists of 9 natural language understanding tasks. Using XLNet, multiple settings such as single-task and multi-task, as well as single models and ensembles are tested on GLUE. In the end, a multi-task ensemble XLNet achieves SoTA results on 7 out of 9 tasks. XLNet outperforms BERT on the different datasets as seen in the table below. An interesting ablation study and extended results are provided in the paper to justify some of the design choices made in XLNet and how it compares with other models such as BERT and GPT. XLNet: Generalized Autoregressive Pretraining for Language Understanding — (Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le)", "pdf_url": "https://arxiv.org/pdf/1906.08237", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/xlnet-outperforms-bert-on-several-nlp-tasks-9ec867bb563b.json"}
{"id": "86508243", "bin": "900_1000", "summary_sentences": ["What  They suggest a method to generate (via GANs) high-resolution images that match given segmentation maps.", "They introduce a new loss for GANs.", "They suggest a method to easily change the appearances of object instances in the images.", "How  Architecture  They train two generators (theoretically can be any number of generators).", "The first (small-scale) generator gets a small-scale segmentation map as its input (e.g. a quarter of the intended output size).", "It generates an image matching that segmentation map -- as in any other GAN.", "The small scale generator is then trained for some time.", "Then they add a second generator (large-scale).", "At each execution, first the small-scale generator is executed.", "It receives the small-scale segmentation map as input, transforms that into a feature map and uses that map to generate an output image.", "Then the large-scale generator is executed.", "It first uses convolutions to downscale the large-scale segmentation map.", "Then it adds (in residual fashion) the feature map from the small-scale generator.", "Then it uses these features to generate the large-scale output image.", "Visualization:  Additionally, they train three different discriminators.", "Each discriminator works completely independently and gets a different scale as input (e.g. D1: 1x downscaled, D2: 2x downscaled, D3: 4x downscaled).", "Each image produced by the generator is fed through all three discriminators.", "This allows to do multi-scale discrimination (training the generator at the coarse and fine level).", "Loss  They add a feature matching loss, which incentivizes the generator to produce images that are projected by the discriminator onto features that are similar to real images.", "Equation:  They add a perceptual loss, which works similar to the feature matching loss, but the network to generate the features is VGG16 (instead of the discriminator).", "Equation:  They use LSGAN (least squares GAN) for everything else.", "Instance Segmentation  The generator gets a semantic segmentation map as its input.", "Downside: The map does not indicate where each object instance ends.", "This makes the image generation needlessly more complicated.", "To fix that, they add instance information.", "Just adding the instance segmentation map is difficult, as there can be infinitely many instances, resulting in an input with infinitely many channels.", "Instead, they just use the segmentation map as one channel and add the boundaries of the object instances as a second channel.", "Visualization:  Instance Manipulation  Their aim is to develop a tool in which one can select an object instance and easily change its appearance.", "That appearance change could be created by changing the generator's input noise vector.", "Downside 1: This would also change everything else in the image.", "Downside 2: Potentially many noise vectors might result in practically the same appearance.", "So many vectors would have to be tried by the user.", "They use an instance-wise feature embedding to fix that.", "Step 1:  Use an autoencoder-like network E to transform each input image into a feature map.", "E has an encoder that encodes the image into a small (bottleneck) feature map.", "E has e decoder that decodes the bottleneck into a large feature map (not an image!)", "of the same size as the input image.", "E's output feature map could be added to the generator's input, i.e. no special autoencoder loss is needed.", "Step 2:  E has transformed the input image to a feature map.", "Apply average pooling to that feature map per object instance.", "This effectively creates one feature vector per object instance (upscaled so that it cover's the object instance's area in the original image).", "The feature vector contains information about the object instance's appearance.", "They set each feature vector to have three components (i.e. E's output has three channels).", "Visualization:  Step 3:  Now train the generator as before, but additionally to the semantic segmentation maps it gets the pooled feature maps as inputs.", "As mentioned, E is trained in parallel.", "Step 4:  Generator and E are now trained.", "Convert each image in the dataset to a feature map via E. This results in a large number of instance-specific appearance vectors.", "Run K-Means clustering on these vectors with K=10 clusters per class.", "Result: Per object class 10 different appearance vectors.", "By changing the vector of on object instance to a different one (among these 10), the object's appearance in the image can be changed without significant effects on the remainder of the image.", "Results  Subjectively more realistic looking images than pix2pix and CRF at 2048x1024.", "Tests with Mechanical Turk also show that their images look more realistic to most people.", "Tests with Mechanical Turk indicate that both feature loss and perceptual loss (based on VGG16) improve image quality.", "(Though numbers for perceptual loss aren't that clear.)", "Tests show that the generated images have high matching with the segmentation maps given to the generator.", "Adding instance boundaries as an input channel helps with image quality at object boundaries:  Easy changing of object instance appearances, see video.", "They also test on other datasets (NYU, ADE20K, Helen Faces) and get good results there too."], "summary_text": "What  They suggest a method to generate (via GANs) high-resolution images that match given segmentation maps. They introduce a new loss for GANs. They suggest a method to easily change the appearances of object instances in the images. How  Architecture  They train two generators (theoretically can be any number of generators). The first (small-scale) generator gets a small-scale segmentation map as its input (e.g. a quarter of the intended output size). It generates an image matching that segmentation map -- as in any other GAN. The small scale generator is then trained for some time. Then they add a second generator (large-scale). At each execution, first the small-scale generator is executed. It receives the small-scale segmentation map as input, transforms that into a feature map and uses that map to generate an output image. Then the large-scale generator is executed. It first uses convolutions to downscale the large-scale segmentation map. Then it adds (in residual fashion) the feature map from the small-scale generator. Then it uses these features to generate the large-scale output image. Visualization:  Additionally, they train three different discriminators. Each discriminator works completely independently and gets a different scale as input (e.g. D1: 1x downscaled, D2: 2x downscaled, D3: 4x downscaled). Each image produced by the generator is fed through all three discriminators. This allows to do multi-scale discrimination (training the generator at the coarse and fine level). Loss  They add a feature matching loss, which incentivizes the generator to produce images that are projected by the discriminator onto features that are similar to real images. Equation:  They add a perceptual loss, which works similar to the feature matching loss, but the network to generate the features is VGG16 (instead of the discriminator). Equation:  They use LSGAN (least squares GAN) for everything else. Instance Segmentation  The generator gets a semantic segmentation map as its input. Downside: The map does not indicate where each object instance ends. This makes the image generation needlessly more complicated. To fix that, they add instance information. Just adding the instance segmentation map is difficult, as there can be infinitely many instances, resulting in an input with infinitely many channels. Instead, they just use the segmentation map as one channel and add the boundaries of the object instances as a second channel. Visualization:  Instance Manipulation  Their aim is to develop a tool in which one can select an object instance and easily change its appearance. That appearance change could be created by changing the generator's input noise vector. Downside 1: This would also change everything else in the image. Downside 2: Potentially many noise vectors might result in practically the same appearance. So many vectors would have to be tried by the user. They use an instance-wise feature embedding to fix that. Step 1:  Use an autoencoder-like network E to transform each input image into a feature map. E has an encoder that encodes the image into a small (bottleneck) feature map. E has e decoder that decodes the bottleneck into a large feature map (not an image!) of the same size as the input image. E's output feature map could be added to the generator's input, i.e. no special autoencoder loss is needed. Step 2:  E has transformed the input image to a feature map. Apply average pooling to that feature map per object instance. This effectively creates one feature vector per object instance (upscaled so that it cover's the object instance's area in the original image). The feature vector contains information about the object instance's appearance. They set each feature vector to have three components (i.e. E's output has three channels). Visualization:  Step 3:  Now train the generator as before, but additionally to the semantic segmentation maps it gets the pooled feature maps as inputs. As mentioned, E is trained in parallel. Step 4:  Generator and E are now trained. Convert each image in the dataset to a feature map via E. This results in a large number of instance-specific appearance vectors. Run K-Means clustering on these vectors with K=10 clusters per class. Result: Per object class 10 different appearance vectors. By changing the vector of on object instance to a different one (among these 10), the object's appearance in the image can be changed without significant effects on the remainder of the image. Results  Subjectively more realistic looking images than pix2pix and CRF at 2048x1024. Tests with Mechanical Turk also show that their images look more realistic to most people. Tests with Mechanical Turk indicate that both feature loss and perceptual loss (based on VGG16) improve image quality. (Though numbers for perceptual loss aren't that clear.) Tests show that the generated images have high matching with the segmentation maps given to the generator. Adding instance boundaries as an input channel helps with image quality at object boundaries:  Easy changing of object instance appearances, see video. They also test on other datasets (NYU, ADE20K, Helen Faces) and get good results there too.", "pdf_url": "https://arxiv.org/pdf/1711.11585", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/high_resolution_image_synthesis_with_conditional_gans.json"}
{"id": "41161677", "bin": "900_1000", "summary_sentences": ["Brownout: building more robust cloud applications – Klein et al. 2014  How can we design cloud applications to be resilient in the face of varying resources and user load, and always deliver the best possible user experience?", "That’s a pretty important question these days, and Klein et al. report on a very interesting new development combining control theory and adaptive application behaviour with impressive results.", "Our work borrows from the concept of brownout in electrical grids.", "Brownouts are an intentional voltage drop often used to prevent blackouts through load reduction in case of emergency.", "In such a situation, incandescent light bulbs dim, hence originating the term.", "Applications can saturate – i.e. become unable to serve users in a timely manner.", "Some users may experience high latencies, while others may not receive any service at all.", "The authors argue that it is better to downgrade the user experience and continue serving a larger number of clients with reasonable latency.", "We define a cloud application as brownout compliant if it can gradually downgrade user experience to avoid saturation.", "This is actually very reminiscent of circuit breakers, as described in Nygard’s ‘Release It!’ and popularized by Netflix.", "If you’re already designing with circuit breakers, you’ve probably got all the pieces you need to add brownout support to your application relatively easily.", "To lower the maintenance effort, brownouts should be automatically triggered.", "This enables cloud applications to rapidly and robustly avoid saturation due to unexpected environmental changes, lowering the burden on human operators.", "Of course, the other thing we might do is provide the application with more resources.", "Studies later on in the paper look at what happens when brownout controls are applied as resources are added and removed.", "The results indicate that brownout control should be able to smooth out the application response and maximise user experience during such transitions.", "How does the brownout model work?", "Application designers need to identify the parts of the response that may be considered optional (for example, return product information but not recommendations, or showing a post but not comments), and make it possible to activate the optional computations on a per-request basis.", "The application needs to export a dynamically changeable runtime parameter called the ‘dimmer’.", "The setting of the dimmer controls the probability that the optional computations will be performed when generating a given response.", "A new application component called the controller is added, its goal is to adjust the dimmer as a function of the current performance.", "So whereas a circuit breaker is triggered on failure & timeouts, the dimmer switch acts more like a flow control valve determining how many requests get to execute the optional components.", "we synthesize a control-theoretical solution to automatically decide when to activate those optional features  I’ve long said that adapting an application to changing demands is a control-theory problem (and implementing a RabbitMQ-based autoscaler for a SpringOne keynote a couple of years ago made that abundantly clear) so it’s great to see this approach being used here.", "It’s also why I have a copy of ‘ Feedback Control ‘ on my Kindle waiting to be read.", "…control theory allows us to provide some formal guarantees on the system.", "Our main aim is to close a loop around a cloud application, constraining the application to have a behaviour that is as predictable as possible.", "If your knowledge of control theory is better than mine, you might be able to follow along with the derivation of the controller algorithm!", "The end result (after a bit of time spent decoding on my part) actually seems pretty straightforward.", "It’s a little bit like the wikipedia page on PID Controllers that I had to refer to: scroll past lots of theory and math, till you get to the ‘pseudocode’ section at the bottom and you’ll see what I mean!", "The question everyone wants answered of course is ‘does it work?’ Experiments suggest a very strong yes.", "Tests were performed first with a constant load, and varying resources (e.g. to simulate failure or loss of nodes and subsequent recovery); then with constant resources and varying load (e.g.", "to simulate usage spikes); and finally varying both load and resources.", "The time-series results show that the self-adaptive application behaves as intended.", "The controller adapts the dimmer both to the available capacity and number of users as expected, and keeps the perceived latencies close to the setpoint.", "Moreover, the advantages that the brownout paradigm brings to previously non-adaptive applications can clearly be observed from the results.", "The paper includes a number of charts that show very significant improvements in the ability to continue serving user requests within the desired latency targets when the system is under stress.", "A word of caution though, they’re not the easiest to interpret.", "…self-adaptation through brownout can allow applications to support more users or run on fewer resources than their non-adaptive counterparts.", "Hence our proposition enables cloud applications to more robustly deal with unexpected peaks or unexpected failures, without requiring spare capacity.", "The work in this paper only considers a single server!", "There’s an important extension for multiple servers, some much easier to follow charts, and a discussion on the implications for load balancing that we’ll look at next time…"], "summary_text": "Brownout: building more robust cloud applications – Klein et al. 2014  How can we design cloud applications to be resilient in the face of varying resources and user load, and always deliver the best possible user experience? That’s a pretty important question these days, and Klein et al. report on a very interesting new development combining control theory and adaptive application behaviour with impressive results. Our work borrows from the concept of brownout in electrical grids. Brownouts are an intentional voltage drop often used to prevent blackouts through load reduction in case of emergency. In such a situation, incandescent light bulbs dim, hence originating the term. Applications can saturate – i.e. become unable to serve users in a timely manner. Some users may experience high latencies, while others may not receive any service at all. The authors argue that it is better to downgrade the user experience and continue serving a larger number of clients with reasonable latency. We define a cloud application as brownout compliant if it can gradually downgrade user experience to avoid saturation. This is actually very reminiscent of circuit breakers, as described in Nygard’s ‘Release It!’ and popularized by Netflix. If you’re already designing with circuit breakers, you’ve probably got all the pieces you need to add brownout support to your application relatively easily. To lower the maintenance effort, brownouts should be automatically triggered. This enables cloud applications to rapidly and robustly avoid saturation due to unexpected environmental changes, lowering the burden on human operators. Of course, the other thing we might do is provide the application with more resources. Studies later on in the paper look at what happens when brownout controls are applied as resources are added and removed. The results indicate that brownout control should be able to smooth out the application response and maximise user experience during such transitions. How does the brownout model work? Application designers need to identify the parts of the response that may be considered optional (for example, return product information but not recommendations, or showing a post but not comments), and make it possible to activate the optional computations on a per-request basis. The application needs to export a dynamically changeable runtime parameter called the ‘dimmer’. The setting of the dimmer controls the probability that the optional computations will be performed when generating a given response. A new application component called the controller is added, its goal is to adjust the dimmer as a function of the current performance. So whereas a circuit breaker is triggered on failure & timeouts, the dimmer switch acts more like a flow control valve determining how many requests get to execute the optional components. we synthesize a control-theoretical solution to automatically decide when to activate those optional features  I’ve long said that adapting an application to changing demands is a control-theory problem (and implementing a RabbitMQ-based autoscaler for a SpringOne keynote a couple of years ago made that abundantly clear) so it’s great to see this approach being used here. It’s also why I have a copy of ‘ Feedback Control ‘ on my Kindle waiting to be read. …control theory allows us to provide some formal guarantees on the system. Our main aim is to close a loop around a cloud application, constraining the application to have a behaviour that is as predictable as possible. If your knowledge of control theory is better than mine, you might be able to follow along with the derivation of the controller algorithm! The end result (after a bit of time spent decoding on my part) actually seems pretty straightforward. It’s a little bit like the wikipedia page on PID Controllers that I had to refer to: scroll past lots of theory and math, till you get to the ‘pseudocode’ section at the bottom and you’ll see what I mean! The question everyone wants answered of course is ‘does it work?’ Experiments suggest a very strong yes. Tests were performed first with a constant load, and varying resources (e.g. to simulate failure or loss of nodes and subsequent recovery); then with constant resources and varying load (e.g. to simulate usage spikes); and finally varying both load and resources. The time-series results show that the self-adaptive application behaves as intended. The controller adapts the dimmer both to the available capacity and number of users as expected, and keeps the perceived latencies close to the setpoint. Moreover, the advantages that the brownout paradigm brings to previously non-adaptive applications can clearly be observed from the results. The paper includes a number of charts that show very significant improvements in the ability to continue serving user requests within the desired latency targets when the system is under stress. A word of caution though, they’re not the easiest to interpret. …self-adaptation through brownout can allow applications to support more users or run on fewer resources than their non-adaptive counterparts. Hence our proposition enables cloud applications to more robustly deal with unexpected peaks or unexpected failures, without requiring spare capacity. The work in this paper only considers a single server! There’s an important extension for multiple servers, some much easier to follow charts, and a discussion on the implications for load balancing that we’ll look at next time…", "pdf_url": "http://www.martinamaggio.com/preprints/icse2014-preprint.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/brownout-building-more-robust-cloud-applications.json"}
{"id": "15757806", "bin": "900_1000", "summary_sentences": ["The paper proposes a dataset to diagnose the abstract reasoning capabilities of learning systems.", "The paper shows that a variant of the relational networks, explicitly designed for abstract reasoning, outperforms models like ResNets.", "Idea  Visual reasoning tasks, that are inspired by the human IQ test, are used to evaluate the models in terms of generalization.", "Let’s say that we want to test if the model understands the abstract notion of “increasing”.", "We could train the model on data that captures the notion of “increasing”, in terms of say increasing size (or quantities) of objects and then test it on a dataset where the notion is expressed in terms of increasing intensity of color.", "The dataset is then used to evaluate if the models can find any solution to such abstract reasoning tasks and how well they generalize when the abstract content is specifically controlled.", "Dataset  Raven’s Progressive Matrics (RPMs):  Consists of an incomplete 3x3 matrix of images where the missing image needs to be filled in, typically by choosing from a set of candidate images.", "As such, it is possible to justify multiple answers to be correct though, in practice, the right answer is the one with the simplest explanation.", "Procedurally Generated Matrices (PGMs)  Generating RPM like matrices procedurally by building an abstract structure for matrices.", "The abstract structure S consists of 3 components: (i) Relation types (R), (ii) Object types (O) and (iii) Attribute types (A).", "ie *S = {(r, o, a)  r in R, o in O and a in A}*.", "This can be read as: “Structure S is instantiated on attribute a of object o and exhibits the relation r”.", "For example, S is instantiated on “color” of object “shape” and exhibits the relation “increasing”.", "In general, the structure could be made of more than one such tuple and more are the tuples, harder is the task.", "Given the structure, sample values v for each attribute a while conforming with the relation r. For example, if the attribute is “color” and the relation is “increasing”, the intensity of color must increase.", "The resulting structure is rendered as pixels.", "Test for Generalization  The paper tests for the following generalization scenarios:  Neutral: The structure of the training and test data can contain any tuple.", "Interpolation: The training data contains even-indexed members of the attribute values while the test data contains odd-indexed members of the attribute values.", "Extrapolation: The training data contains first-half of the attribute values while the test data contains the second-half of the attribute values.", "Heldout attribute: Training data contains no tuples with (o = shape, a = color) or (o = line, a = type).", "Heldout triples: Out of 29 possible triples, 7 are held out from training and only used during testing.", "Heldout pair-of-triples: Out of 400 possible sets of pair of triples, 40 were held out and used only during testing.", "Heldout pair-of-triples: Out of 400 possible sets of pair of triples, 40 were held out and used only during testing.", "Heldout attribute pair: Out of 20 (unordered) variable attribute pairs, 4 were held out and used only during testing.", "Models  Input: 8 context panels (from the 3x3) matrix where the last panel needs to be filled.", "CNN-MLP - 4 layer CNN with batchnorm and ReLU.", "ResNet - ResNet-50 (as it perfomed better than ResNet-101 and ResNet-152).", "LSTM  Wild Relation Network (WReN) - A CNN model encodes the 8 panels and the candidate answers and feeds them as input to a relational network.", "Context-blind ResNet - ResNet network without the context (or the 8 input panels).", "Results  WReN model outperforms the other models on the Neutral setup.", "Models have a harder time differentiating between size than quantity.", "WRen is the best performing models in all the setups and rest of the discussion only applies to that model.", "Generalisation is easy in the context of interpolation while worst in case of extrapolation, hinting at the limited generalization capability of the models.", "Auxiliary Training  The model is also trained to predict the relevant relation, object and attribute types using the meta-targets that encode this information.", "The auxiliary training helps in all the cases.", "Further, the model’s accuracy on the main task is where in the cases where it solves the auxiliary tasks well.", "Key Takeaway  For abstract visual reasoning tasks, the choice of models can make a large difference, the case in consideration of ResNets vs Relational Networks.", "Using auxiliary loss that encourages the model to “explain” its reasoning (in this case by predicting the attributes, relations, etc) helps to improve the performance on the main task as well.", "Given that the challenge is motivated by tasks used to measure human IQ, it would have been interesting to get an estimate of human performance on at least a subset of this dataset."], "summary_text": "The paper proposes a dataset to diagnose the abstract reasoning capabilities of learning systems. The paper shows that a variant of the relational networks, explicitly designed for abstract reasoning, outperforms models like ResNets. Idea  Visual reasoning tasks, that are inspired by the human IQ test, are used to evaluate the models in terms of generalization. Let’s say that we want to test if the model understands the abstract notion of “increasing”. We could train the model on data that captures the notion of “increasing”, in terms of say increasing size (or quantities) of objects and then test it on a dataset where the notion is expressed in terms of increasing intensity of color. The dataset is then used to evaluate if the models can find any solution to such abstract reasoning tasks and how well they generalize when the abstract content is specifically controlled. Dataset  Raven’s Progressive Matrics (RPMs):  Consists of an incomplete 3x3 matrix of images where the missing image needs to be filled in, typically by choosing from a set of candidate images. As such, it is possible to justify multiple answers to be correct though, in practice, the right answer is the one with the simplest explanation. Procedurally Generated Matrices (PGMs)  Generating RPM like matrices procedurally by building an abstract structure for matrices. The abstract structure S consists of 3 components: (i) Relation types (R), (ii) Object types (O) and (iii) Attribute types (A). ie *S = {(r, o, a)  r in R, o in O and a in A}*. This can be read as: “Structure S is instantiated on attribute a of object o and exhibits the relation r”. For example, S is instantiated on “color” of object “shape” and exhibits the relation “increasing”. In general, the structure could be made of more than one such tuple and more are the tuples, harder is the task. Given the structure, sample values v for each attribute a while conforming with the relation r. For example, if the attribute is “color” and the relation is “increasing”, the intensity of color must increase. The resulting structure is rendered as pixels. Test for Generalization  The paper tests for the following generalization scenarios:  Neutral: The structure of the training and test data can contain any tuple. Interpolation: The training data contains even-indexed members of the attribute values while the test data contains odd-indexed members of the attribute values. Extrapolation: The training data contains first-half of the attribute values while the test data contains the second-half of the attribute values. Heldout attribute: Training data contains no tuples with (o = shape, a = color) or (o = line, a = type). Heldout triples: Out of 29 possible triples, 7 are held out from training and only used during testing. Heldout pair-of-triples: Out of 400 possible sets of pair of triples, 40 were held out and used only during testing. Heldout pair-of-triples: Out of 400 possible sets of pair of triples, 40 were held out and used only during testing. Heldout attribute pair: Out of 20 (unordered) variable attribute pairs, 4 were held out and used only during testing. Models  Input: 8 context panels (from the 3x3) matrix where the last panel needs to be filled. CNN-MLP - 4 layer CNN with batchnorm and ReLU. ResNet - ResNet-50 (as it perfomed better than ResNet-101 and ResNet-152). LSTM  Wild Relation Network (WReN) - A CNN model encodes the 8 panels and the candidate answers and feeds them as input to a relational network. Context-blind ResNet - ResNet network without the context (or the 8 input panels). Results  WReN model outperforms the other models on the Neutral setup. Models have a harder time differentiating between size than quantity. WRen is the best performing models in all the setups and rest of the discussion only applies to that model. Generalisation is easy in the context of interpolation while worst in case of extrapolation, hinting at the limited generalization capability of the models. Auxiliary Training  The model is also trained to predict the relevant relation, object and attribute types using the meta-targets that encode this information. The auxiliary training helps in all the cases. Further, the model’s accuracy on the main task is where in the cases where it solves the auxiliary tasks well. Key Takeaway  For abstract visual reasoning tasks, the choice of models can make a large difference, the case in consideration of ResNets vs Relational Networks. Using auxiliary loss that encourages the model to “explain” its reasoning (in this case by predicting the attributes, relations, etc) helps to improve the performance on the main task as well. Given that the challenge is motivated by tasks used to measure human IQ, it would have been interesting to get an estimate of human performance on at least a subset of this dataset.", "pdf_url": "http://proceedings.mlr.press/v80/santoro18a/santoro18a.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/measuring-abstract-reasoning-in-neural-networks.json"}
{"id": "98262950", "bin": "900_1000", "summary_sentences": ["What  They propose Group Normalization (GN), a normalization technique similar to Batch Normalization (BN).", "It works examplewise and splits the channels into groups before normalizing similar to BN.", "They argue that this is similar to e.g. HOG or SIFT normalizing over groups.", "As GN works examplewise it is not negatively affected by small batch sizes or batches with non-iid data.", "This allows to train at tiny batch sizes, opening up GPU memory for larger models.", "Their normalization is partially motivated from neuroscience, where some models assume normalization across cell responses.", "How  There are multiple existing normalization techniques similar to GN, which normalize an (N, C, H, W) (examples, channels, height, width) based on z-transformations, i.e. x'_i = (x_i - mean_i)/std_i.", "Each normalization technique works in different ways:  Batch Normalization: Normalizes each channel along the (N, H, W) axes, i.e. computes statistics over all examples in the batch.", "Hence it is dependent on the batch size and each example is influenced by all other examples in the batch.", "BN is the same as Instance Normalization for batch size 1.", "Layer Normalization: Normalizes along the (C, H, W) axes, i.e. computes statistics per example (one mean and one standard deviation value per example).", "Instance Normalization: Normalizes long the (H, W) axes, i.e. computes one mean and standard deviation per example and channel.", "Other examples or channels do not influence the computation.", "Group Normalization (their method): Normalizes roughly along the (C, H, W) axes, similar to Layer Normalization.", "However, the channel axis is subdivided into groups of channels and the normalization happens over all channels within the same group (instead of over all channels in the example as in Layer Normalization).", "GN becomes Instance Normalization when the number of groups is set to the number of channels.", "GN becomes Layer Normalization when the number of groups is set to 1.", "Visualization of the relationship:  Results  They test on ImageNet, training on 8 GPUs.", "They use ResNet-50.", "Their default number of groups in GN is 32.", "At batch size 32: They observe 0.5 percentage points higher validation error with GN as opposed to BN (layer normalization: 1.7 points worse, instance normalization: 4.8 points worse).", "They observe slightly lower training error of GN compared to BN.", "They argue that BN profits from a regularizing effect from the examples in each batch being randomly picked (i.e. mean and variance are a bit stochastic).", "At batch size 2: GN's accuracy remains constant (compared to batch size 32), while BN degrades significantly.", "GN beats BN by 10.6 points.", "They also compare to Batch Renormalization with well chosen hyperparameters and find that it improves BN's performance, but it still performs 2.1 points worse than GN.", "Accuracy vs. batch size:  Channels per group  They found on ImageNet (with ResNet-50) that around 8 or more channels per group performs best.", "They get similar results when training with ResNet-101.", "VGG-16  They trained VGG-16 on ImageNet with no normalization, BN and GN.", "They observed that the feature maps before normalization and ReLU had a significantly wider value range (-80 to +20) when using no normalization as opposed to using normalization (BN: -3 to +3, GN: -4 to +3).", "Object Detection and Segmentation on COCO  They compare BN and GN for object detection and segmentation on COCO using Mask R-CNN with different backbones.", "ResNet-50 backbone  Using GN instead of (frozen) BN improves box AP by 1.1 points and mask AP by 0.8 points.", "Feature Pyramid Network (FPN) backbone  They compare using BN and GN only in the head of Mask R-CNN.", "BN performs significantly worse (9 points) than GN, even though the batch size is at 512 RoIs.", "They argue that this is because the examples are not iid (they all come from the same image).", "GN in the backbone improves AP by 0.5 points.", "Using GN also allows them to train the whole model from scratch (i.e. no pretraining) without training instabilities.", "They reach scores very close to the pretrained versions.", "Video classification on Kinetics  They test on the Kinetics dataset, which requires classifying videos and hence leads to large memory consumption per example.", "They use a ResNet-50 Inflated 3D (I3D) network as baseline.", "Normalization is extended from (H,W) axes to (T,H,W) axes, adding the temporal dimension.", "GN performs overall superior to BN by up to 1.2 points accuracy.", "GN profits from not sub-sampling frames (e.g. using only every second frame as input).", "BN however does not improve its accuracy from deactivating subsampling, because that also required reducing the batch size from 8 to 4 examples.", "The positive effect of deactivating subsampling was masked by the negative effect of reducing the batch size, leading to a false conclusion that subsampling does not degrade accuracy."], "summary_text": "What  They propose Group Normalization (GN), a normalization technique similar to Batch Normalization (BN). It works examplewise and splits the channels into groups before normalizing similar to BN. They argue that this is similar to e.g. HOG or SIFT normalizing over groups. As GN works examplewise it is not negatively affected by small batch sizes or batches with non-iid data. This allows to train at tiny batch sizes, opening up GPU memory for larger models. Their normalization is partially motivated from neuroscience, where some models assume normalization across cell responses. How  There are multiple existing normalization techniques similar to GN, which normalize an (N, C, H, W) (examples, channels, height, width) based on z-transformations, i.e. x'_i = (x_i - mean_i)/std_i. Each normalization technique works in different ways:  Batch Normalization: Normalizes each channel along the (N, H, W) axes, i.e. computes statistics over all examples in the batch. Hence it is dependent on the batch size and each example is influenced by all other examples in the batch. BN is the same as Instance Normalization for batch size 1. Layer Normalization: Normalizes along the (C, H, W) axes, i.e. computes statistics per example (one mean and one standard deviation value per example). Instance Normalization: Normalizes long the (H, W) axes, i.e. computes one mean and standard deviation per example and channel. Other examples or channels do not influence the computation. Group Normalization (their method): Normalizes roughly along the (C, H, W) axes, similar to Layer Normalization. However, the channel axis is subdivided into groups of channels and the normalization happens over all channels within the same group (instead of over all channels in the example as in Layer Normalization). GN becomes Instance Normalization when the number of groups is set to the number of channels. GN becomes Layer Normalization when the number of groups is set to 1. Visualization of the relationship:  Results  They test on ImageNet, training on 8 GPUs. They use ResNet-50. Their default number of groups in GN is 32. At batch size 32: They observe 0.5 percentage points higher validation error with GN as opposed to BN (layer normalization: 1.7 points worse, instance normalization: 4.8 points worse). They observe slightly lower training error of GN compared to BN. They argue that BN profits from a regularizing effect from the examples in each batch being randomly picked (i.e. mean and variance are a bit stochastic). At batch size 2: GN's accuracy remains constant (compared to batch size 32), while BN degrades significantly. GN beats BN by 10.6 points. They also compare to Batch Renormalization with well chosen hyperparameters and find that it improves BN's performance, but it still performs 2.1 points worse than GN. Accuracy vs. batch size:  Channels per group  They found on ImageNet (with ResNet-50) that around 8 or more channels per group performs best. They get similar results when training with ResNet-101. VGG-16  They trained VGG-16 on ImageNet with no normalization, BN and GN. They observed that the feature maps before normalization and ReLU had a significantly wider value range (-80 to +20) when using no normalization as opposed to using normalization (BN: -3 to +3, GN: -4 to +3). Object Detection and Segmentation on COCO  They compare BN and GN for object detection and segmentation on COCO using Mask R-CNN with different backbones. ResNet-50 backbone  Using GN instead of (frozen) BN improves box AP by 1.1 points and mask AP by 0.8 points. Feature Pyramid Network (FPN) backbone  They compare using BN and GN only in the head of Mask R-CNN. BN performs significantly worse (9 points) than GN, even though the batch size is at 512 RoIs. They argue that this is because the examples are not iid (they all come from the same image). GN in the backbone improves AP by 0.5 points. Using GN also allows them to train the whole model from scratch (i.e. no pretraining) without training instabilities. They reach scores very close to the pretrained versions. Video classification on Kinetics  They test on the Kinetics dataset, which requires classifying videos and hence leads to large memory consumption per example. They use a ResNet-50 Inflated 3D (I3D) network as baseline. Normalization is extended from (H,W) axes to (T,H,W) axes, adding the temporal dimension. GN performs overall superior to BN by up to 1.2 points accuracy. GN profits from not sub-sampling frames (e.g. using only every second frame as input). BN however does not improve its accuracy from deactivating subsampling, because that also required reducing the batch size from 8 to 4 examples. The positive effect of deactivating subsampling was masked by the negative effect of reducing the batch size, leading to a false conclusion that subsampling does not degrade accuracy.", "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/group_normalization.json"}
{"id": "26105266", "bin": "900_1000", "summary_sentences": ["What  DCGANs are just a different architecture of GANs.", "In GANs a Generator network (G) generates images.", "A discriminator network (D) learns to differentiate between real images from the training set and images generated by G.  DCGANs basically convert the laplacian pyramid technique (many pairs of G and D to progressively upscale an image) to a single pair of G and D.  How  Their D: Convolutional networks.", "No linear layers.", "No pooling, instead strided layers.", "LeakyReLUs.", "Their G: Starts with 100d noise vector.", "Generates with linear layers 1024x4x4 values.", "Then uses fractionally strided convolutions (move by 0.5 per step) to upscale to 512x8x8.", "This is continued till Cx32x32 or Cx64x64.", "The last layer is a convolution to 3x32x32/3x64x64 (Tanh activation).", "The fractionally strided convolutions do basically the same as the progressive upscaling in the laplacian pyramid.", "So it's basically one laplacian pyramid in a single network and all upscalers are trained jointly leading to higher quality images.", "They use Adam as their optimizer.", "To decrease instability issues they decreased the learning rate to 0.0002 (from 0.001) and the momentum/beta1 to 0.5 (from 0.9).", "Architecture of G using fractionally strided convolutions to progressively upscale the image.", "Results  High quality images.", "Still with distortions and errors, but at first glance they look realistic.", "Smooth interpolations between generated images are possible (by interpolating between the noise vectors and feeding these interpolations into G).", "The features extracted by D seem to have some potential for unsupervised learning.", "There seems to be some potential for vector arithmetics (using the initial noise vectors) similar to the vector arithmetics with wordvectors.", "E.g. to generate mean with sunglasses via vector(men) + vector(sunglasses).", "Generated images, bedrooms.", "Generated images, faces.", "Rough chapter-wise notes  Introduction  For unsupervised learning, they propose to use to train a GAN and then reuse the weights of D.  GANs have traditionally been hard to train.", "Approach and model architecture  They use for D an convnet without linear layers, withput pooling layers (only strides), LeakyReLUs and Batch Normalization.", "They use for G ReLUs (hidden layers) and Tanh (output).", "Details of adversarial training  They trained on LSUN, Imagenet-1k and a custom dataset of faces.", "Minibatch size was 128.", "LeakyReLU alpha 0.2.", "They used Adam with a learning rate of 0.0002 and momentum of 0.5.", "They note that a higher momentum lead to oscillations.", "LSUN  3M images of bedrooms.", "They use an autoencoder based technique to filter out 0.25M near duplicate images.", "Faces  They downloaded 3M images of 10k people.", "They extracted 350k faces with OpenCV.", "Empirical validation of DCGANs capabilities  Classifying CIFAR-10 GANs as a feature extractor  They train a pair of G and D on Imagenet-1k.", "D's top layer has 512*4*4 features.", "They train an SVM on these features to classify the images of CIFAR-10.", "They achieve a score of 82.8%, better than unsupervised K-Means based methods, but worse than Exemplar CNNs.", "Classifying SVHN digits using GANs as a feature extractor  They reuse the same pipeline (D trained on CIFAR-10, SVM) for the StreetView House Numbers dataset.", "They use 1000 SVHN images (with the features from D) to train the SVM.", "They achieve 22.48% test error.", "Investigating and visualizing the internals of the networks  Walking in the latent space  The performs walks in the latent space (= interpolate between input noise vectors and generate several images for the interpolation).", "They argue that this might be a good way to detect overfitting/memorizations as those might lead to very sudden (not smooth) transitions.", "Visualizing the discriminator features  They use guided backpropagation to visualize what the feature maps in D have learned (i.e. to which images they react).", "They can show that their LSUN-bedroom GAN seems to have learned in an unsupervised way what beds and windows look like.", "Forgetting to draw certain objects  They manually annotated the locations of objects in some generated bedroom images.", "Based on these annotations they estimated which feature maps were mostly responsible for generating the objects.", "They deactivated these feature maps and regenerated the images.", "That decreased the appearance of these objects.", "It's however not as easy as one feature map deactivation leading to one object disappearing.", "They deactivated quite a lot of feature maps (200) and they objects were often still quite visible or replaced by artefacts/errors.", "Vector arithmetic on face samples  Wordvectors can be used to perform semantic arithmetic (e.g. king - man + woman = queen).", "The unsupervised representations seem to be useable in a similar fashion.", "E.g. they generated images via G. They then picked several images that showed men with glasses and averaged these image's noise vectors.", "They did with same with men without glasses and women without glasses.", "Then they performed on these vectors men with glasses - mean without glasses + women without glasses to get `womean with glasses"], "summary_text": "What  DCGANs are just a different architecture of GANs. In GANs a Generator network (G) generates images. A discriminator network (D) learns to differentiate between real images from the training set and images generated by G.  DCGANs basically convert the laplacian pyramid technique (many pairs of G and D to progressively upscale an image) to a single pair of G and D.  How  Their D: Convolutional networks. No linear layers. No pooling, instead strided layers. LeakyReLUs. Their G: Starts with 100d noise vector. Generates with linear layers 1024x4x4 values. Then uses fractionally strided convolutions (move by 0.5 per step) to upscale to 512x8x8. This is continued till Cx32x32 or Cx64x64. The last layer is a convolution to 3x32x32/3x64x64 (Tanh activation). The fractionally strided convolutions do basically the same as the progressive upscaling in the laplacian pyramid. So it's basically one laplacian pyramid in a single network and all upscalers are trained jointly leading to higher quality images. They use Adam as their optimizer. To decrease instability issues they decreased the learning rate to 0.0002 (from 0.001) and the momentum/beta1 to 0.5 (from 0.9). Architecture of G using fractionally strided convolutions to progressively upscale the image. Results  High quality images. Still with distortions and errors, but at first glance they look realistic. Smooth interpolations between generated images are possible (by interpolating between the noise vectors and feeding these interpolations into G). The features extracted by D seem to have some potential for unsupervised learning. There seems to be some potential for vector arithmetics (using the initial noise vectors) similar to the vector arithmetics with wordvectors. E.g. to generate mean with sunglasses via vector(men) + vector(sunglasses). Generated images, bedrooms. Generated images, faces. Rough chapter-wise notes  Introduction  For unsupervised learning, they propose to use to train a GAN and then reuse the weights of D.  GANs have traditionally been hard to train. Approach and model architecture  They use for D an convnet without linear layers, withput pooling layers (only strides), LeakyReLUs and Batch Normalization. They use for G ReLUs (hidden layers) and Tanh (output). Details of adversarial training  They trained on LSUN, Imagenet-1k and a custom dataset of faces. Minibatch size was 128. LeakyReLU alpha 0.2. They used Adam with a learning rate of 0.0002 and momentum of 0.5. They note that a higher momentum lead to oscillations. LSUN  3M images of bedrooms. They use an autoencoder based technique to filter out 0.25M near duplicate images. Faces  They downloaded 3M images of 10k people. They extracted 350k faces with OpenCV. Empirical validation of DCGANs capabilities  Classifying CIFAR-10 GANs as a feature extractor  They train a pair of G and D on Imagenet-1k. D's top layer has 512*4*4 features. They train an SVM on these features to classify the images of CIFAR-10. They achieve a score of 82.8%, better than unsupervised K-Means based methods, but worse than Exemplar CNNs. Classifying SVHN digits using GANs as a feature extractor  They reuse the same pipeline (D trained on CIFAR-10, SVM) for the StreetView House Numbers dataset. They use 1000 SVHN images (with the features from D) to train the SVM. They achieve 22.48% test error. Investigating and visualizing the internals of the networks  Walking in the latent space  The performs walks in the latent space (= interpolate between input noise vectors and generate several images for the interpolation). They argue that this might be a good way to detect overfitting/memorizations as those might lead to very sudden (not smooth) transitions. Visualizing the discriminator features  They use guided backpropagation to visualize what the feature maps in D have learned (i.e. to which images they react). They can show that their LSUN-bedroom GAN seems to have learned in an unsupervised way what beds and windows look like. Forgetting to draw certain objects  They manually annotated the locations of objects in some generated bedroom images. Based on these annotations they estimated which feature maps were mostly responsible for generating the objects. They deactivated these feature maps and regenerated the images. That decreased the appearance of these objects. It's however not as easy as one feature map deactivation leading to one object disappearing. They deactivated quite a lot of feature maps (200) and they objects were often still quite visible or replaced by artefacts/errors. Vector arithmetic on face samples  Wordvectors can be used to perform semantic arithmetic (e.g. king - man + woman = queen). The unsupervised representations seem to be useable in a similar fashion. E.g. they generated images via G. They then picked several images that showed men with glasses and averaged these image's noise vectors. They did with same with men without glasses and women without glasses. Then they performed on these vectors men with glasses - mean without glasses + women without glasses to get `womean with glasses", "pdf_url": "http://arxiv.org/pdf/1511.06434", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/unsupervised_representation_learning_with_deep_convolutional_generative_adversarial_networks.json"}
{"id": "97923725", "bin": "900_1000", "summary_sentences": ["Background  Language modeling has been recently addressed using unsupervised training methods such as ELMo and BERT .", "However, it still remains a challenge to properly equip neural networks with long-term dependency.", "Recent models were designed with an attention mechanism to help ease optimization — by dealing with vanishing gradient — and enable the learning of long-term dependency.", "However, the context is of fixed-length in these cases so the model cannot capture longer-term dependency and suffers from a problem known as context fragmentation.", "Context fragmentation refers to when the model lacks the necessary contextual information to predict the first few symbols due to the way the context was selected — usually without respect to a sentence or semantic boundaries.", "Moreover, previous models don’t support information flow across segments during training and employ fixed context length, which means there is no room for the model to capture longer-term dependency.", "In the context of language modeling, hidden states can be reused to allow information flow across segments (a kind of memory).", "This could help to support longer-term dependency and deal with context fragmentation.", "However, for the architecture to support state reuse, temporal coherence must be managed, as we discuss next.", "Transformer-XL  During training, vanilla language models don’t make effective use of context information and segments are treated individually.", "In addition, semantic boundaries during segmentation are usually not respected since most methods employ standard chunked sequences of fixed lengths.", "During the evaluation, fixed-length contexts are used and segments are processed from scratch, which becomes expensive, even though context fragmentation is somewhat addressed.", "This paper aims to focus on the problem of efficiency by better modeling longer-term dependency.", "In language modeling, Transformer networks are limited by a fixed-length context and thus can be improved through learning longer-term dependency.", "The paper proposes a novel method called Transformer-XL (meaning extra long) for language modeling, which enables a Transformer architecture to learn longer-term dependency — via a recurrence mechanism — beyond a fixed length without disrupting temporal coherence.", "The method is different from other previous approaches that focus on other strategies to support long-term dependency such as additional loss signals and augmented memory structure.", "A segment-level recurrent mechanism is introduced which enables the model to reuse previous hidden states at training time, addressing both the issues of fixed-length context and context fragmentation.", "In other words, the historical information can be reused and it can be extended up to as much as GPU memory allows.", "See the training and evaluation phases in the figure below.", "Transformer-XL — training and evaluation phase ( figure source )  To properly reuse hidden states, the authors propose a mechanism called relative positional encodings which helps to avoid temporal confusion.", "Current models can’t distinguish the positional difference between inputs in different segments at different layers.", "Relative position encoding addresses this problem by encoding positional information bias in the hidden states, which differs from other approaches that perform this as the input level.", "Since a Transformer architecture is involved, the process above is achieved by computing the relative distance between each key vector and query vector and injecting it into the attention score.", "With some new parameterization trick of the terms used to derive the attention score between query and vector, the relative position information can be incorporated.", "The recurrence component is now equipped with the proposed relative positional embedding and this whole procedure represents the proposed Transformer-XL architecture.", "Results  Transformer-XL obtains strong results for both word-level and character-level language modeling applied to a variety of datasets such as WikiText-103, text8, and One Billion Word.", "The proposed model is compared with a vanilla model that was recently used for character-level language modeling ( Al-Rfou et al., 2018 ), which also leverages deeper self-attention.", "Note that the vanilla model cannot support dependency lengths larger than the upper bound segment length.", "Transformer-XL reduces previous SoTA perplexity score on several datasets such as text8, enwiki8, One Billion Word, and WikiText-103.", "Besides the SoTA performances, the authors claim that the method is more flexible, faster during evaluation (1874 times speedup), generalizes well on small datasets, and is effective at modeling short and long sequences.", "See a summary of some of the results obtained on the different datasets in the Tables below.", "You can check the rest of the results in the full paper linked below.", "Other Benefits  An ablation study to examine the effects of both the recurrence mechanism and the proposed positional encoding scheme is provided in the paper as well.", "The authors also propose a new metric called Relative Effective Context Length that provides a fair way to compare models that are tested with increased context lengths.", "Further Readings  Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context  The Annotated Transformer by Harvard NLP Group  Attention Guide by Lilian Weng  Attention Is All You Need  Code repository associated with the paper (TensorFlow and PyTorch)  Character-Level Language Modeling with Deeper Self-Attention  If enough interest is expressed, I may feel tempted to prepare a code walkthrough for this work.", "It contains many different components that could be interesting and useful for NLP practitioners and researchers."], "summary_text": "Background  Language modeling has been recently addressed using unsupervised training methods such as ELMo and BERT . However, it still remains a challenge to properly equip neural networks with long-term dependency. Recent models were designed with an attention mechanism to help ease optimization — by dealing with vanishing gradient — and enable the learning of long-term dependency. However, the context is of fixed-length in these cases so the model cannot capture longer-term dependency and suffers from a problem known as context fragmentation. Context fragmentation refers to when the model lacks the necessary contextual information to predict the first few symbols due to the way the context was selected — usually without respect to a sentence or semantic boundaries. Moreover, previous models don’t support information flow across segments during training and employ fixed context length, which means there is no room for the model to capture longer-term dependency. In the context of language modeling, hidden states can be reused to allow information flow across segments (a kind of memory). This could help to support longer-term dependency and deal with context fragmentation. However, for the architecture to support state reuse, temporal coherence must be managed, as we discuss next. Transformer-XL  During training, vanilla language models don’t make effective use of context information and segments are treated individually. In addition, semantic boundaries during segmentation are usually not respected since most methods employ standard chunked sequences of fixed lengths. During the evaluation, fixed-length contexts are used and segments are processed from scratch, which becomes expensive, even though context fragmentation is somewhat addressed. This paper aims to focus on the problem of efficiency by better modeling longer-term dependency. In language modeling, Transformer networks are limited by a fixed-length context and thus can be improved through learning longer-term dependency. The paper proposes a novel method called Transformer-XL (meaning extra long) for language modeling, which enables a Transformer architecture to learn longer-term dependency — via a recurrence mechanism — beyond a fixed length without disrupting temporal coherence. The method is different from other previous approaches that focus on other strategies to support long-term dependency such as additional loss signals and augmented memory structure. A segment-level recurrent mechanism is introduced which enables the model to reuse previous hidden states at training time, addressing both the issues of fixed-length context and context fragmentation. In other words, the historical information can be reused and it can be extended up to as much as GPU memory allows. See the training and evaluation phases in the figure below. Transformer-XL — training and evaluation phase ( figure source )  To properly reuse hidden states, the authors propose a mechanism called relative positional encodings which helps to avoid temporal confusion. Current models can’t distinguish the positional difference between inputs in different segments at different layers. Relative position encoding addresses this problem by encoding positional information bias in the hidden states, which differs from other approaches that perform this as the input level. Since a Transformer architecture is involved, the process above is achieved by computing the relative distance between each key vector and query vector and injecting it into the attention score. With some new parameterization trick of the terms used to derive the attention score between query and vector, the relative position information can be incorporated. The recurrence component is now equipped with the proposed relative positional embedding and this whole procedure represents the proposed Transformer-XL architecture. Results  Transformer-XL obtains strong results for both word-level and character-level language modeling applied to a variety of datasets such as WikiText-103, text8, and One Billion Word. The proposed model is compared with a vanilla model that was recently used for character-level language modeling ( Al-Rfou et al., 2018 ), which also leverages deeper self-attention. Note that the vanilla model cannot support dependency lengths larger than the upper bound segment length. Transformer-XL reduces previous SoTA perplexity score on several datasets such as text8, enwiki8, One Billion Word, and WikiText-103. Besides the SoTA performances, the authors claim that the method is more flexible, faster during evaluation (1874 times speedup), generalizes well on small datasets, and is effective at modeling short and long sequences. See a summary of some of the results obtained on the different datasets in the Tables below. You can check the rest of the results in the full paper linked below. Other Benefits  An ablation study to examine the effects of both the recurrence mechanism and the proposed positional encoding scheme is provided in the paper as well. The authors also propose a new metric called Relative Effective Context Length that provides a fair way to compare models that are tested with increased context lengths. Further Readings  Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context  The Annotated Transformer by Harvard NLP Group  Attention Guide by Lilian Weng  Attention Is All You Need  Code repository associated with the paper (TensorFlow and PyTorch)  Character-Level Language Modeling with Deeper Self-Attention  If enough interest is expressed, I may feel tempted to prepare a code walkthrough for this work. It contains many different components that could be interesting and useful for NLP practitioners and researchers.", "pdf_url": "https://arxiv.org/pdf/1901.02860", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/a-light-introduction-to-transformer-xl-be5737feb13.json"}
{"id": "30621133", "bin": "900_1000", "summary_sentences": ["This paper aims to develop a deep learning method to extract causes behind emotions in a document.", "It’s a relatively new NLP task so the authors mainly aim to show its feasibility using a multi-task learning approach.", "The proposed approach aims to separately extract emotion and causes from text using multi-task learning and then conducts emotion-cause pairing and filtering using an improved version of the multi-task learning model.", "To understand the task, consider the following statement as an example document:  There are five clauses and the fourth clause is considered the emotion clause conveying the emotion of happiness.", "The clauses that contain causes (cause clause) are the second clause and the third clause.", "Previously, the task was known as ECE and involved detecting if a clause was a cause given the annotated emotion label of a document.", "See the example in the previous figure.", "The task is to find the corresponding cause clauses that correspond to the emotion of “happy”.", "The main shortcomings of this task are that emotions of a document must first be annotated before cause extraction, which limits its real-world application and ignores the fact that emotions and causes are mutually indicative.", "In contrast, the proposed ECPE task (shown on the right of the figure) outputs pairs of emotion-cause clauses and doesn’t need to provide annotations in advance.", "As seen in the example, two emotion-cause clauses are extracted without providing the emotion label of “happy”.", "Ideally, identifying causes may be able to improve emotion extraction from text and vice-versa, assuming emotion and cause are not mutually independent.", "The Approach  The framework consists of two parts: 1) extract sets of emotion and cause clauses from each document via two kinds of multi-task learning networks, and 2) conduct emotion-cause pairing and filtering.", "The pairing is done via Cartesian product applied to the set of emotion and cause clauses.", "This yields a set of candidate emotion-cause pairs, on which a filter is applied to remove clause pairs that don’t contain a causal relationship.", "To perform the first part of the framework (i.e. emotion and cause clause extraction), the Bi-LSTM based multi-task framework shown in the figure below is used.", "The model (also referred to as Indep) shows how the prediction for each type of clause is performed via the Bi-LSTM hidden representations which consist of context-aware representations of each clause.", "The assumption is that emotion and cause clauses are not mutually independent, therefore, the authors propose an interactive multi-task learning network to capture the correlation between them.", "To this end, a more interactive Bi-LSTM based multi-task framework (as shown in the figure below) is proposed.", "Compared to the previous multi-task model, the improved model performs a few extra steps in the upper layers which allows the prediction to happen based on the interaction of the two types of representations.", "Note that two separate enhanced models based on this framework are used: one for enhancing emotion extraction based on cause extraction (Inter CE), and one for enhancing cause extraction based on emotion extraction (Inter EC).", "The output of these models is a set of emotion clauses and a set of cause clauses.", "The objective now is to pair the two sets to form emotion-cause pairs that form causal relationships.", "A Cartesian product renders the set of all possible pairs which are represented by a feature vector consisting of three components: emotion clause representation, cause clause representation, and the distance between the two clauses.", "The cause relationship is determined via a logistic regression model which takes as input the emotion-cause candidate pair features, applied a Sigmoid operation, and outputs a 1 or 0 to indicate if the relationship exists for each pair.", "The pairs that output 1 are the final set of emotion-cause pairs and used for evaluation.", "Results  The benchmark emotion-cause dataset by Gui et al., 2016 is used for all experiments.", "Precision, recall, and F1 scores are used as the evaluation metrics.", "The table below shows the experimental results for all the separate tasks using the three proposed models based on multi-task learning.", "Inter-EC produced better results, especially as it relates to the ECPE task.", "One can also observe that both emotion extraction and cause extraction improve each other in individual tasks.", "The results are consistent with the intuition that emotion and cause are mutually indicative.", "Other variations of the models that leverage the dataset available annotations are also tested, which unsurprisingly show improvements since they use the additional information to improve the predictions of the different tasks.", "You can observe the results of the new models (labeled by the Bound tag) below:  The results below show the effectiveness of the pair filtering phase:  Other results using rule-based and machine learning methods are also presented in the paper.", "In summary, the overall results indicate that emotion extraction helps to improve cause extraction and cause extraction also helps to enhance emotion extraction.", "Combined, both sub-tasks are used to improve ECPE.", "The authors also comment that there is room for improvement architecture-wise and also to improve cause extraction which was observed to be the more difficult task.", "Paper: Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts — Rui Xia and Zixiang Ding  Source Code:  [url]"], "summary_text": "This paper aims to develop a deep learning method to extract causes behind emotions in a document. It’s a relatively new NLP task so the authors mainly aim to show its feasibility using a multi-task learning approach. The proposed approach aims to separately extract emotion and causes from text using multi-task learning and then conducts emotion-cause pairing and filtering using an improved version of the multi-task learning model. To understand the task, consider the following statement as an example document:  There are five clauses and the fourth clause is considered the emotion clause conveying the emotion of happiness. The clauses that contain causes (cause clause) are the second clause and the third clause. Previously, the task was known as ECE and involved detecting if a clause was a cause given the annotated emotion label of a document. See the example in the previous figure. The task is to find the corresponding cause clauses that correspond to the emotion of “happy”. The main shortcomings of this task are that emotions of a document must first be annotated before cause extraction, which limits its real-world application and ignores the fact that emotions and causes are mutually indicative. In contrast, the proposed ECPE task (shown on the right of the figure) outputs pairs of emotion-cause clauses and doesn’t need to provide annotations in advance. As seen in the example, two emotion-cause clauses are extracted without providing the emotion label of “happy”. Ideally, identifying causes may be able to improve emotion extraction from text and vice-versa, assuming emotion and cause are not mutually independent. The Approach  The framework consists of two parts: 1) extract sets of emotion and cause clauses from each document via two kinds of multi-task learning networks, and 2) conduct emotion-cause pairing and filtering. The pairing is done via Cartesian product applied to the set of emotion and cause clauses. This yields a set of candidate emotion-cause pairs, on which a filter is applied to remove clause pairs that don’t contain a causal relationship. To perform the first part of the framework (i.e. emotion and cause clause extraction), the Bi-LSTM based multi-task framework shown in the figure below is used. The model (also referred to as Indep) shows how the prediction for each type of clause is performed via the Bi-LSTM hidden representations which consist of context-aware representations of each clause. The assumption is that emotion and cause clauses are not mutually independent, therefore, the authors propose an interactive multi-task learning network to capture the correlation between them. To this end, a more interactive Bi-LSTM based multi-task framework (as shown in the figure below) is proposed. Compared to the previous multi-task model, the improved model performs a few extra steps in the upper layers which allows the prediction to happen based on the interaction of the two types of representations. Note that two separate enhanced models based on this framework are used: one for enhancing emotion extraction based on cause extraction (Inter CE), and one for enhancing cause extraction based on emotion extraction (Inter EC). The output of these models is a set of emotion clauses and a set of cause clauses. The objective now is to pair the two sets to form emotion-cause pairs that form causal relationships. A Cartesian product renders the set of all possible pairs which are represented by a feature vector consisting of three components: emotion clause representation, cause clause representation, and the distance between the two clauses. The cause relationship is determined via a logistic regression model which takes as input the emotion-cause candidate pair features, applied a Sigmoid operation, and outputs a 1 or 0 to indicate if the relationship exists for each pair. The pairs that output 1 are the final set of emotion-cause pairs and used for evaluation. Results  The benchmark emotion-cause dataset by Gui et al., 2016 is used for all experiments. Precision, recall, and F1 scores are used as the evaluation metrics. The table below shows the experimental results for all the separate tasks using the three proposed models based on multi-task learning. Inter-EC produced better results, especially as it relates to the ECPE task. One can also observe that both emotion extraction and cause extraction improve each other in individual tasks. The results are consistent with the intuition that emotion and cause are mutually indicative. Other variations of the models that leverage the dataset available annotations are also tested, which unsurprisingly show improvements since they use the additional information to improve the predictions of the different tasks. You can observe the results of the new models (labeled by the Bound tag) below:  The results below show the effectiveness of the pair filtering phase:  Other results using rule-based and machine learning methods are also presented in the paper. In summary, the overall results indicate that emotion extraction helps to improve cause extraction and cause extraction also helps to enhance emotion extraction. Combined, both sub-tasks are used to improve ECPE. The authors also comment that there is room for improvement architecture-wise and also to improve cause extraction which was observed to be the more difficult task. Paper: Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts — Rui Xia and Zixiang Ding  Source Code:  [url]", "pdf_url": "https://arxiv.org/pdf/1906.01267", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/a-deep-learning-approach-to-improve-emotion-cause-extraction-135bd9ea3899.json"}
{"id": "43650136", "bin": "900_1000", "summary_sentences": ["I discuss two recent related papers in the Deep RL literature in this post.", "The first paper, by Fujimoto et al., introduces techniques for reducing bias and variance in a popular actor-critic method, Deep Deterministic Policy Gradient (DDPG).", "The second paper, by Kostrikov et al., makes a similar contribution by evaluating and addressing bias and variance in inverse RL.", "Both of these papers take widely used Deep RL algorithms, empirically and theoretically demonstrate specific weaknesses, and suggest reasonable improvements.", "These are valuable studies that help develop a better understanding of Deep RL.", "Addressing Function Approx.", "Error in AC Methods  If you are unfamiliar with DDPG, you can check out my blog post on the algorithm.", "The most important thing to know is that the success of the whole algorithm relies on having a critic network that can accurately estimate $Q$-values.", "The only signal the actor network gets in its gradient to help it achieve higher rewards comes from the gradient of the critic wrt the actions selected by the actor.", "If the critic gradient is biased, then the actor will fail to learn anything!", "In Section 4, the authors begin by empirically demonstrating the overestimation bias present in the critic network (action-value estimator) in DDPG.", "They show that the overestimation bias essentially stems from the fact that DPG algorithms have to approximate both the policy and the value functions, and the approximate policy is maximized in the gradient direction provided by the approximate value function (rather than the true value function).", "Then, inspired by Double Q-Learning, they introduce a technique they call “clipped Double Q-Learning in AC” for achieving the same idea.", "Basically, the critic target becomes  This requires introducing a second critic.", "The min makes it so that it is possible to underestimate Q-values, but this is preferable to overestimation.", "Then, to help with variance reduction, they suggest:  Delay updating the actor network until the critic network has almost converged  Add some noise to the actions selected by the actor network when updating the critic to help regularize the critic, reminiscent of Expected SARSA  Their experimental results on MuJoco (they call their algorithm TD3) suggest these improvements are very effective, outperforming PPO, TRPO, ACKTR, and others.", "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning  EDIT: The title of the paper was previously “Addressing Sample Inefficiency and Reward Bias in Inverse RL”  Seemingly inspired by the former, this paper recently came out exploring inverse RL—specifically, generative adversarial imitation learning (GAIL) and behavioral cloning.", "In GAIL, the discriminator learns to distinguish between transitions sampled from an expert and those from a trained policy.", "The policy is rewarded for confusing the discrminiator.", "However, GAIL is typically quite sample inefficient.", "One way the authors suggest to help with the sample inefficiency of GAIL is by using off-policy RL instead of on-policy RL.", "They modify the GAIL objective to be  Basically, $\\pi_E$ is the expert policy, from which trajectories are sampled, and $\\mathscr{R}$ is the replay buffer, from which trajectories are sampled from ~all previous policies.", "They ignore the importance sampling term in practice.", "Since TD3 is technically a deterministic policy gradient algorithm, I’m assuming one way to implement this importance sampling would be to have the actor output the mean of a multivariate Gaussian—this Gaussian could then be used to define the entropy term of the policy and the importance sampling ratio.", "This is fairly common for continuous control tasks like MuJoco…the authors note that the importance sampling wasn’t used in practice, however.", "They further analyzed different reward functions for GAIL, and show that certain GAIL reward functions can actually inhibit learning depending on the particular MDP (e.g., if the environment has a survival bonus or penalty).", "To create a more robust reward function that will learn the expert policies, they suggest explicitly learning rewards for absorbing states of the MDP.", "They implement this by adding an indicator to these particular states so that the GAIL discriminator can identify whether reaching an absorbing state is desirable from the perspective of the expert.", "In the OpenReview thread , one reviewer makes sure to point out that the problems with inverse RL algorithms highlighted in the paper are due to incorrect implementations of the MDP, rather than shortcomings of the algorithms themselves ( see this comment in particular ).", "Very interestingly, they used VR to generate expert trajectories of gripping blocks with a Kuka arm.", "This environment has a per-step penalty, and the normal GAIL reward fails to learn the expert policy due to the the learning inhibition caused by the reward function bias.", "The proposed method learns to imitate the expert quickly due to its added reward for absorbing states.", "It would be great to investigate the effects of using off-policy samples in the objective more carefully (why exactly does importance sampling not matter?", "The absorbing state reward stuff being so useful is surprising, and should be helpful in future applications where GAIL is used for inverse RL."], "summary_text": "I discuss two recent related papers in the Deep RL literature in this post. The first paper, by Fujimoto et al., introduces techniques for reducing bias and variance in a popular actor-critic method, Deep Deterministic Policy Gradient (DDPG). The second paper, by Kostrikov et al., makes a similar contribution by evaluating and addressing bias and variance in inverse RL. Both of these papers take widely used Deep RL algorithms, empirically and theoretically demonstrate specific weaknesses, and suggest reasonable improvements. These are valuable studies that help develop a better understanding of Deep RL. Addressing Function Approx. Error in AC Methods  If you are unfamiliar with DDPG, you can check out my blog post on the algorithm. The most important thing to know is that the success of the whole algorithm relies on having a critic network that can accurately estimate $Q$-values. The only signal the actor network gets in its gradient to help it achieve higher rewards comes from the gradient of the critic wrt the actions selected by the actor. If the critic gradient is biased, then the actor will fail to learn anything! In Section 4, the authors begin by empirically demonstrating the overestimation bias present in the critic network (action-value estimator) in DDPG. They show that the overestimation bias essentially stems from the fact that DPG algorithms have to approximate both the policy and the value functions, and the approximate policy is maximized in the gradient direction provided by the approximate value function (rather than the true value function). Then, inspired by Double Q-Learning, they introduce a technique they call “clipped Double Q-Learning in AC” for achieving the same idea. Basically, the critic target becomes  This requires introducing a second critic. The min makes it so that it is possible to underestimate Q-values, but this is preferable to overestimation. Then, to help with variance reduction, they suggest:  Delay updating the actor network until the critic network has almost converged  Add some noise to the actions selected by the actor network when updating the critic to help regularize the critic, reminiscent of Expected SARSA  Their experimental results on MuJoco (they call their algorithm TD3) suggest these improvements are very effective, outperforming PPO, TRPO, ACKTR, and others. Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning  EDIT: The title of the paper was previously “Addressing Sample Inefficiency and Reward Bias in Inverse RL”  Seemingly inspired by the former, this paper recently came out exploring inverse RL—specifically, generative adversarial imitation learning (GAIL) and behavioral cloning. In GAIL, the discriminator learns to distinguish between transitions sampled from an expert and those from a trained policy. The policy is rewarded for confusing the discrminiator. However, GAIL is typically quite sample inefficient. One way the authors suggest to help with the sample inefficiency of GAIL is by using off-policy RL instead of on-policy RL. They modify the GAIL objective to be  Basically, $\\pi_E$ is the expert policy, from which trajectories are sampled, and $\\mathscr{R}$ is the replay buffer, from which trajectories are sampled from ~all previous policies. They ignore the importance sampling term in practice. Since TD3 is technically a deterministic policy gradient algorithm, I’m assuming one way to implement this importance sampling would be to have the actor output the mean of a multivariate Gaussian—this Gaussian could then be used to define the entropy term of the policy and the importance sampling ratio. This is fairly common for continuous control tasks like MuJoco…the authors note that the importance sampling wasn’t used in practice, however. They further analyzed different reward functions for GAIL, and show that certain GAIL reward functions can actually inhibit learning depending on the particular MDP (e.g., if the environment has a survival bonus or penalty). To create a more robust reward function that will learn the expert policies, they suggest explicitly learning rewards for absorbing states of the MDP. They implement this by adding an indicator to these particular states so that the GAIL discriminator can identify whether reaching an absorbing state is desirable from the perspective of the expert. In the OpenReview thread , one reviewer makes sure to point out that the problems with inverse RL algorithms highlighted in the paper are due to incorrect implementations of the MDP, rather than shortcomings of the algorithms themselves ( see this comment in particular ). Very interestingly, they used VR to generate expert trajectories of gripping blocks with a Kuka arm. This environment has a per-step penalty, and the normal GAIL reward fails to learn the expert policy due to the the learning inhibition caused by the reward function bias. The proposed method learns to imitate the expert quickly due to its added reward for absorbing states. It would be great to investigate the effects of using off-policy samples in the objective more carefully (why exactly does importance sampling not matter? The absorbing state reward stuff being so useful is surprising, and should be helpful in future applications where GAIL is used for inverse RL.", "pdf_url": "https://arxiv.org/pdf/1809.02925v2", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/addressing-challenges-in-deep-rl.json"}
{"id": "16124640", "bin": "900_1000", "summary_sentences": ["This paper tests the following hypothesis, about features learned by a deep network trained on the ImageNet dataset:   *Object features and anticausal features are closely related.", "Context features and causal features are not necessarily related.", "*  First, some definitions.", "Let $X$ be a visual feature (i.e. value of a hidden unit) and $Y$ be information about a label (e.g.", "the log-odds of probability of different object appearing in the image).", "A causal feature would be one for which the causal direction is $X \\rightarrow Y$.", "An anticausal feature would be the opposite case, $X \\leftarrow Y$.", "As for object features, in this paper they are features whose value tends to change a lot when computed on a complete original image versus when computed on an image whose regions *falling inside* object bounding boxes have been blacked out (see Figure 4).", "Contextual features are the opposite, i.e. values change a lot when blacking out the regions *outside* object bounding boxes.", "See section 4.2.1 for how \"object scores\" and \"context scores\" are computed following this description, to quantitatively measure to what extent a feature is an \"object feature\" or a \"context feature\".", "Thus, the paper investigates whether 1) for object features, their relationship with object appearance information is anticausal (i.e. whether the object feature's value seems to be caused by the presence of the object) and whether 2) context features are not clearly causal or anticausal.", "To perform this investigation, the paper first proposes a generic neural network model (dubbed the Neural Causation Coefficient architecture or NCC) to predict a score of whether the relationship between an input variable $X$ and target variable $Y$ is causal.", "This model is trained by taking as input datasets of $X$ and $Y$ pairs synthetically generated in such a way that we know whether $X$ caused $Y$ or the opposite.", "The NCC architecture first embeds each individual $X$,$Y$ instance pair into some hidden representation, performs mean pooling of these representations and then feeds the result to fully connected layers (see Figure 3).", "The paper shows that the proposed NCC model actually achieves SOTA performance on the Tübingen dataset, a collection of real-world cause-effect observational samples.", "Then, the proposed NCC model is used to measure the average object score of features of a deep residual CNN identified as being most causal and most anticausal by NCC.", "The same is done with the context score.", "What is found is that indeed, the object score is always higher for the top anticausal features than for the top causal features.", "However, for the context score, no such clear trend is observed (see Figure 5).", "**My two cents**  I haven't been following the growing literature on machine learning for causal inference, so it was a real pleasure to read this paper and catch up a little bit on that.", "Just for that I would recommend the reading of this paper.", "The paper does a really good job at explaining the notion of *observational causal inference*, which in short builds on the observation that if we assume IID noise on top of a causal (or anticausal) phenomenon, then causation can possibly be inferred by verifying in which direction of causation the IID assumption on the noise seems to hold best (see Figure 2 for a nice illustration, where in (a) the noise is clearly IID, but isn't in (b)).", "Also, irrespective of the study of causal phenomenon in images, the NCC architecture, which achieves SOTA causal prediction performance, is in itself a nice contribution.", "Regarding the application to image features, one thing that is hard to wrap your head around is that, for the $Y$ variable, instead of using the true image label, the log-odds at the output layer are used instead in the study.", "The paper justifies this choice by highlighting that the NCC network was trained on examples where $Y$ is continuous, not discrete.", "On one hand, that justification makes sense.", "On the other, this is odd since the log-odds were in fact computed directly from the visual features, meaning that technically the value of the log-odds are directly caused by all the features (which goes against the hypothesis being tested).", "My best guess is that this isn't an issue only because NCC makes a causal prediction between *a single feature* and $Y$, not *from all features* to $Y$.", "I'd be curious to read the authors' perspective on this.", "Still, this paper at this point is certainly just scratching the surface on this topic.", "For instance, the paper mentions that NCC could be used to encourage the learning of causal or anticausal features, providing a new and intriguing type of regularization.", "This sounds like a very interesting future direction for research, which I'm looking forward to."], "summary_text": "This paper tests the following hypothesis, about features learned by a deep network trained on the ImageNet dataset:   *Object features and anticausal features are closely related. Context features and causal features are not necessarily related. *  First, some definitions. Let $X$ be a visual feature (i.e. value of a hidden unit) and $Y$ be information about a label (e.g. the log-odds of probability of different object appearing in the image). A causal feature would be one for which the causal direction is $X \\rightarrow Y$. An anticausal feature would be the opposite case, $X \\leftarrow Y$. As for object features, in this paper they are features whose value tends to change a lot when computed on a complete original image versus when computed on an image whose regions *falling inside* object bounding boxes have been blacked out (see Figure 4). Contextual features are the opposite, i.e. values change a lot when blacking out the regions *outside* object bounding boxes. See section 4.2.1 for how \"object scores\" and \"context scores\" are computed following this description, to quantitatively measure to what extent a feature is an \"object feature\" or a \"context feature\". Thus, the paper investigates whether 1) for object features, their relationship with object appearance information is anticausal (i.e. whether the object feature's value seems to be caused by the presence of the object) and whether 2) context features are not clearly causal or anticausal. To perform this investigation, the paper first proposes a generic neural network model (dubbed the Neural Causation Coefficient architecture or NCC) to predict a score of whether the relationship between an input variable $X$ and target variable $Y$ is causal. This model is trained by taking as input datasets of $X$ and $Y$ pairs synthetically generated in such a way that we know whether $X$ caused $Y$ or the opposite. The NCC architecture first embeds each individual $X$,$Y$ instance pair into some hidden representation, performs mean pooling of these representations and then feeds the result to fully connected layers (see Figure 3). The paper shows that the proposed NCC model actually achieves SOTA performance on the Tübingen dataset, a collection of real-world cause-effect observational samples. Then, the proposed NCC model is used to measure the average object score of features of a deep residual CNN identified as being most causal and most anticausal by NCC. The same is done with the context score. What is found is that indeed, the object score is always higher for the top anticausal features than for the top causal features. However, for the context score, no such clear trend is observed (see Figure 5). **My two cents**  I haven't been following the growing literature on machine learning for causal inference, so it was a real pleasure to read this paper and catch up a little bit on that. Just for that I would recommend the reading of this paper. The paper does a really good job at explaining the notion of *observational causal inference*, which in short builds on the observation that if we assume IID noise on top of a causal (or anticausal) phenomenon, then causation can possibly be inferred by verifying in which direction of causation the IID assumption on the noise seems to hold best (see Figure 2 for a nice illustration, where in (a) the noise is clearly IID, but isn't in (b)). Also, irrespective of the study of causal phenomenon in images, the NCC architecture, which achieves SOTA causal prediction performance, is in itself a nice contribution. Regarding the application to image features, one thing that is hard to wrap your head around is that, for the $Y$ variable, instead of using the true image label, the log-odds at the output layer are used instead in the study. The paper justifies this choice by highlighting that the NCC network was trained on examples where $Y$ is continuous, not discrete. On one hand, that justification makes sense. On the other, this is odd since the log-odds were in fact computed directly from the visual features, meaning that technically the value of the log-odds are directly caused by all the features (which goes against the hypothesis being tested). My best guess is that this isn't an issue only because NCC makes a causal prediction between *a single feature* and $Y$, not *from all features* to $Y$. I'd be curious to read the authors' perspective on this. Still, this paper at this point is certainly just scratching the surface on this topic. For instance, the paper mentions that NCC could be used to encourage the learning of causal or anticausal features, providing a new and intriguing type of regularization. This sounds like a very interesting future direction for research, which I'm looking forward to.", "pdf_url": "http://arxiv.org/pdf/1605.08179", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/lopez-pazncsb16.json"}
{"id": "91633253", "bin": "900_1000", "summary_sentences": ["Flower Pollination Algorithm for Global Optimization – Xin-She Yang 2013  The last of the optimisation algorithms we’ll look at for now, today’s paper is the most recent (2013) and also by Xin-She Yang.", "Once more, we only get comparisons to genetic algorithms and PSO, and once more the comparison is favourable.", "In many design applications in engineering and industry, we have to try to find the optimal solution to a given problem under highly complex constraints.", "Such constrained optimization problems are often highly nonlinear, to find the optimal solutions is often a very challenging task if it is not impossible.", "Most conventional optimization do not work well for problems with nonlinearity and multimodality.", "(The) current trend is to use nature-inspired metaheuristic algorithms to tackle such difficult problems, and it has been shown that metaheuristics are surprisingly very efficient.", "For this reason, the literature of metaheuristics has expanded tremendously in the last two decades.", "Up to now, researchers have only used a very limited characteristics inspired by nature, and there is room for more algorithm development.", "Flower pollination can be viewed as a ‘survival of the fittest’ optimization process for plant species.", "Pollination can be abiotic or biotic.", "90% of all plants use biotic pollination, in which pollen is transferred by pollinators such as insects and other animals.", "The other 10% use abiotic pollination which uses mechanisms such as wind and water to transfer pollen.", "Flower constancy is the tendency of certain pollinators to exclusively visit certain flowering plant species.", "Such flower constancy may have evolutionary advantages because this will maximize the transfer of flower pollen to the same or conspecific plants, and thus maximizing the reproduction of the same flower species.", "Pollination can be via cross-pollination or self-pollination.", "Cross-pollination is the transfer of pollen from a different plant, whereas self-pollination is the transfer of pollen from a flower of the same plant.", "Biotic cross-pollination (transfer of pollen across different plants by insects and other animals) can occur at a long distance, and thus can be considered as global pollination.", "In addition, bees and birds may behave as Lévy flight behaviour, with jump or fly distance steps obeying a Lévy distribution.", "Furthermore, flower constancy can be used an increment step using the similarity or difference of two flowers.", "The two key steps in the Flower Pollination Algorithm represent global pollination (biotic cross-pollination) and local pollination (abiotic and self-pollination).", "Which of the two pollination processes is used on a particular iteration for a particular flower is controlled by a probability p.  Due to the physical proximity and other factors such as wind, local pollination can have a significant fraction p in the overall pollination activities.", "For simplicity, it is assumed that each plant has a single flower, and each flower produces only one pollen gamete.", "Thus plant, flower, pollen gamete, and problem solution are all identified with each other.", "Global pollination is modelled by the following process:  xit+1 = xit + L(xit – gbest)  xit is the pollen i or solution vector xi at iteration t. gbest is the globally best solution.", "The parameter L is a step size, and is drawn from a  Lévy distribution as we saw with the Cuckoo Search algorithm.", "This models insects moving over a long distance with various distance steps.", "Local pollination is represented by:  xit+1 = xit + ε(xjt – xkt)  where xjt and xkt represent pollens from different flowers of the same plant species.", "This becomes a local random walk if we draw ε from [0,1].", "The whole algorithm comes together as follows:  Initialize a population with random solutions  For each iteration, for each flower in the population:  Draw a random variable rand in [0,1]  If rand < p, then perform global pollination  Else perform local pollination with j and k chosen randomly from among all solutions  Evaluate the new solutions, and if they are better update the population  The winner is the global best solution at the end of the prescribed number of iterations  p = 0.8 was found to work well in simulations.", "Our simulation results have shown that the the proposed flower pollination algorithm is very efficient and can outperform both genetic algorithm and particle swarm optimization.", "The convergence rate is essentially exponential as we have seen from the convergence comparison in the previous section.", "The reasons that FPA is efficient can be twofold: long-distance pollinators and flower consistency.", "Pollinators such as insects can travel long distances, and thus they introduce the ability (into the algorithm) that they can escape any local landscape and subsequently explore a larger search space.", "This acts as exploration moves.", "On the other hand, flower consistency ensures that the same species of the flowers (thus similar solutions) are chosen more frequently and thus guarantee convergence more quickly.", "This step is essentially an exploitation step.", "The interplay and interaction of these key components and the selection of the best solution gbest ensures that the algorithm is very efficient."], "summary_text": "Flower Pollination Algorithm for Global Optimization – Xin-She Yang 2013  The last of the optimisation algorithms we’ll look at for now, today’s paper is the most recent (2013) and also by Xin-She Yang. Once more, we only get comparisons to genetic algorithms and PSO, and once more the comparison is favourable. In many design applications in engineering and industry, we have to try to find the optimal solution to a given problem under highly complex constraints. Such constrained optimization problems are often highly nonlinear, to find the optimal solutions is often a very challenging task if it is not impossible. Most conventional optimization do not work well for problems with nonlinearity and multimodality. (The) current trend is to use nature-inspired metaheuristic algorithms to tackle such difficult problems, and it has been shown that metaheuristics are surprisingly very efficient. For this reason, the literature of metaheuristics has expanded tremendously in the last two decades. Up to now, researchers have only used a very limited characteristics inspired by nature, and there is room for more algorithm development. Flower pollination can be viewed as a ‘survival of the fittest’ optimization process for plant species. Pollination can be abiotic or biotic. 90% of all plants use biotic pollination, in which pollen is transferred by pollinators such as insects and other animals. The other 10% use abiotic pollination which uses mechanisms such as wind and water to transfer pollen. Flower constancy is the tendency of certain pollinators to exclusively visit certain flowering plant species. Such flower constancy may have evolutionary advantages because this will maximize the transfer of flower pollen to the same or conspecific plants, and thus maximizing the reproduction of the same flower species. Pollination can be via cross-pollination or self-pollination. Cross-pollination is the transfer of pollen from a different plant, whereas self-pollination is the transfer of pollen from a flower of the same plant. Biotic cross-pollination (transfer of pollen across different plants by insects and other animals) can occur at a long distance, and thus can be considered as global pollination. In addition, bees and birds may behave as Lévy flight behaviour, with jump or fly distance steps obeying a Lévy distribution. Furthermore, flower constancy can be used an increment step using the similarity or difference of two flowers. The two key steps in the Flower Pollination Algorithm represent global pollination (biotic cross-pollination) and local pollination (abiotic and self-pollination). Which of the two pollination processes is used on a particular iteration for a particular flower is controlled by a probability p.  Due to the physical proximity and other factors such as wind, local pollination can have a significant fraction p in the overall pollination activities. For simplicity, it is assumed that each plant has a single flower, and each flower produces only one pollen gamete. Thus plant, flower, pollen gamete, and problem solution are all identified with each other. Global pollination is modelled by the following process:  xit+1 = xit + L(xit – gbest)  xit is the pollen i or solution vector xi at iteration t. gbest is the globally best solution. The parameter L is a step size, and is drawn from a  Lévy distribution as we saw with the Cuckoo Search algorithm. This models insects moving over a long distance with various distance steps. Local pollination is represented by:  xit+1 = xit + ε(xjt – xkt)  where xjt and xkt represent pollens from different flowers of the same plant species. This becomes a local random walk if we draw ε from [0,1]. The whole algorithm comes together as follows:  Initialize a population with random solutions  For each iteration, for each flower in the population:  Draw a random variable rand in [0,1]  If rand < p, then perform global pollination  Else perform local pollination with j and k chosen randomly from among all solutions  Evaluate the new solutions, and if they are better update the population  The winner is the global best solution at the end of the prescribed number of iterations  p = 0.8 was found to work well in simulations. Our simulation results have shown that the the proposed flower pollination algorithm is very efficient and can outperform both genetic algorithm and particle swarm optimization. The convergence rate is essentially exponential as we have seen from the convergence comparison in the previous section. The reasons that FPA is efficient can be twofold: long-distance pollinators and flower consistency. Pollinators such as insects can travel long distances, and thus they introduce the ability (into the algorithm) that they can escape any local landscape and subsequently explore a larger search space. This acts as exploration moves. On the other hand, flower consistency ensures that the same species of the flowers (thus similar solutions) are chosen more frequently and thus guarantee convergence more quickly. This step is essentially an exploitation step. The interplay and interaction of these key components and the selection of the best solution gbest ensures that the algorithm is very efficient.", "pdf_url": "http://arxiv.org/pdf/1312.5673v1.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/flower-pollination-algorithm-for-global-optimization.json"}
{"id": "63621777", "bin": "900_1000", "summary_sentences": ["What  They describe a CNN architecture that can be used to identify a person given an image of their face.", "How  The expected input is the image of a face (i.e. it does not search for faces in images, the faces already have to be extracted by a different method).", "Face alignment / Frontalization  Target of this step: Get rid of variations within the face images, so that every face seems to look straight into the camera (\"frontalized\").", "2D alignment  They search for landmarks (fiducial points) on the face.", "They use SVRs (features: LBPs) for that.", "After every application of the SVR, the localized landmarks are used to transform/normalize the face.", "Then the SVR is applied again.", "By doing this, the locations of the landmarks are gradually refined.", "They use the detected landmarks to normalize the face images (via scaling, rotation and translation).", "3D alignment  The 2D alignment allows to normalize variations within the 2D-plane, not out-of-plane variations (e.g. seeing that face from its left/right side).", "To normalize out-of-plane variations they need a 3D transformation.", "They detect an additional 67 landmarks on the faces (again via SVRs).", "They construct a human face mesh from a dataset (USF Human-ID).", "They map the 67 landmarks to that mesh.", "They then use some more complicated steps to recover the frontalized face image.", "CNN architecture  The CNN receives the frontalized face images (152x152, RGB).", "It then applies the following steps:  Convolution, 32 filters, 11x11, ReLU (-> 32x142x142, CxHxW)  Max pooling over 3x3, stride 2 (-> 32x71x71)  Convolution, 16 filters, 9x9, ReLU (-> 16x63x63)  Local Convolution, 16 filters, 9x9, ReLU (-> 16x55x55)  Local Convolution, 16 filters, 7x7, ReLU (-> 16x25x25)  Local Convolution, 16 filters, 5x5, ReLU (-> 16x21x21)  Fully Connected, 4096, ReLU  Fully Connected, 4030, Softmax  Local Convolutions use a different set of learned weights at every \"pixel\" (while a normal convolution uses the same set of weights at all locations).", "They can afford to use local convolutions because of their frontalization, which roughly forces specific landmarks to be at specific locations.", "They use dropout (apparently only after the first fully connected layer).", "They normalize \"the features\" (probably the 4096 fully connected layer).", "Each component is divided by its maximum value across a training set.", "Additionally, the whole vector is L2-normalized.", "The goal of this step is to make the network less sensitive to illumination changes.", "The whole network has about 120 million parameters.", "Visualization of the architecture:  Training  The network receives images, each showing a face, and is trained to classify the identity of the face (e.g. gets image of Obama, has to return \"that's Obama\").", "They use cross-entropy as their loss.", "Face verification  In order to tell whether two images of faces show the same person they try three different methods.", "Each of these relies on the vector extracted by the first fully connected layer in the network (4096d).", "Let these vectors be f1 (image 1) and f2 (image 2).", "The methods are then:  Inner product between f1 and f2.", "The classification (same person/not same person) is then done by a simple threshold.", "Weighted X^2 (chi-squared) distance.", "Equation, per vector component i: weight_i (f1[i] - f2[i])^2 / (f1[i] + f2[i]).", "The vector is then fed into an SVM.", "Siamese network.", "Means here simply that the absolute distance between f1 and f2 is calculated (|f1-f2|), each component is weighted by a learned weight and then the sum of the components is calculated.", "If the result is above a threshold, the faces are considered to show the same person.", "Results  They train their network on the Social Face Classification (SFC) dataset.", "That seems to be a Facebook-internal dataset (i.e. not public) with 4.4 million faces of 4k people.", "When applied to the LFW dataset:  Face recognition (\"which person is shown in the image\") (apparently they retrained the whole model on LFW for this task?", "):  Simple SVM with LBP (i.e. not their network): 91.4% mean accuracy.", "Their model, with frontalization, with 2d alignment: ???", "no value.", "Their model, no frontalization (only 2d alignment): 94.3% mean accuracy.", "Their model, no frontalization, no 2d alignment: 87.9% mean accuracy.", "Face verification (two images -> same/not same person) (apparently also trained on LFW?", "unclear):  Method 1 (inner product + threshold): 95.92% mean accuracy.", "Method 2 (X^2 vector + SVM): 97.00% mean accurracy.", "Method 3 (siamese): Apparently 96.17% accuracy alone, and 97.25% when used in an ensemble with other methods (under special training schedule using SFC dataset).", "When applied to the YTF dataset (YouTube video frames):  92.5% accuracy via X^2-method."], "summary_text": "What  They describe a CNN architecture that can be used to identify a person given an image of their face. How  The expected input is the image of a face (i.e. it does not search for faces in images, the faces already have to be extracted by a different method). Face alignment / Frontalization  Target of this step: Get rid of variations within the face images, so that every face seems to look straight into the camera (\"frontalized\"). 2D alignment  They search for landmarks (fiducial points) on the face. They use SVRs (features: LBPs) for that. After every application of the SVR, the localized landmarks are used to transform/normalize the face. Then the SVR is applied again. By doing this, the locations of the landmarks are gradually refined. They use the detected landmarks to normalize the face images (via scaling, rotation and translation). 3D alignment  The 2D alignment allows to normalize variations within the 2D-plane, not out-of-plane variations (e.g. seeing that face from its left/right side). To normalize out-of-plane variations they need a 3D transformation. They detect an additional 67 landmarks on the faces (again via SVRs). They construct a human face mesh from a dataset (USF Human-ID). They map the 67 landmarks to that mesh. They then use some more complicated steps to recover the frontalized face image. CNN architecture  The CNN receives the frontalized face images (152x152, RGB). It then applies the following steps:  Convolution, 32 filters, 11x11, ReLU (-> 32x142x142, CxHxW)  Max pooling over 3x3, stride 2 (-> 32x71x71)  Convolution, 16 filters, 9x9, ReLU (-> 16x63x63)  Local Convolution, 16 filters, 9x9, ReLU (-> 16x55x55)  Local Convolution, 16 filters, 7x7, ReLU (-> 16x25x25)  Local Convolution, 16 filters, 5x5, ReLU (-> 16x21x21)  Fully Connected, 4096, ReLU  Fully Connected, 4030, Softmax  Local Convolutions use a different set of learned weights at every \"pixel\" (while a normal convolution uses the same set of weights at all locations). They can afford to use local convolutions because of their frontalization, which roughly forces specific landmarks to be at specific locations. They use dropout (apparently only after the first fully connected layer). They normalize \"the features\" (probably the 4096 fully connected layer). Each component is divided by its maximum value across a training set. Additionally, the whole vector is L2-normalized. The goal of this step is to make the network less sensitive to illumination changes. The whole network has about 120 million parameters. Visualization of the architecture:  Training  The network receives images, each showing a face, and is trained to classify the identity of the face (e.g. gets image of Obama, has to return \"that's Obama\"). They use cross-entropy as their loss. Face verification  In order to tell whether two images of faces show the same person they try three different methods. Each of these relies on the vector extracted by the first fully connected layer in the network (4096d). Let these vectors be f1 (image 1) and f2 (image 2). The methods are then:  Inner product between f1 and f2. The classification (same person/not same person) is then done by a simple threshold. Weighted X^2 (chi-squared) distance. Equation, per vector component i: weight_i (f1[i] - f2[i])^2 / (f1[i] + f2[i]). The vector is then fed into an SVM. Siamese network. Means here simply that the absolute distance between f1 and f2 is calculated (|f1-f2|), each component is weighted by a learned weight and then the sum of the components is calculated. If the result is above a threshold, the faces are considered to show the same person. Results  They train their network on the Social Face Classification (SFC) dataset. That seems to be a Facebook-internal dataset (i.e. not public) with 4.4 million faces of 4k people. When applied to the LFW dataset:  Face recognition (\"which person is shown in the image\") (apparently they retrained the whole model on LFW for this task? ):  Simple SVM with LBP (i.e. not their network): 91.4% mean accuracy. Their model, with frontalization, with 2d alignment: ??? no value. Their model, no frontalization (only 2d alignment): 94.3% mean accuracy. Their model, no frontalization, no 2d alignment: 87.9% mean accuracy. Face verification (two images -> same/not same person) (apparently also trained on LFW? unclear):  Method 1 (inner product + threshold): 95.92% mean accuracy. Method 2 (X^2 vector + SVM): 97.00% mean accurracy. Method 3 (siamese): Apparently 96.17% accuracy alone, and 97.25% when used in an ensemble with other methods (under special training schedule using SFC dataset). When applied to the YTF dataset (YouTube video frames):  92.5% accuracy via X^2-method.", "pdf_url": "https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/deepface.json"}
{"id": "17350626", "bin": "900_1000", "summary_sentences": ["Tensorflow implementation  What  They suggest a GAN algorithm that is based on an autoencoder with Wasserstein distance.", "Their method generates highly realistic human faces.", "Their method has a convergence measure, which reflects the quality of the generates images.", "Their method has a diversity hyperparameter, which can be used to set the tradeoff between image diversity and image quality.", "How  Like other GANs, their method uses a generator G and a discriminator D.  Generator  The generator is fairly standard.", "It gets a noise vector z as input and uses upsampling+convolutions to generate images.", "It uses ELUs and no BN.", "Discriminator  The discriminator is a full autoencoder (i.e. it converts input images to 8x8x3 tensors, then reconstructs them back to images).", "It has skip-connections from the 8x8x3 layer to each upsampling layer.", "It also uses ELUs and no BN.", "Their method now has the following steps:  Collect real images x_real.", "Generate fake images x_fake = G(z).", "Reconstruct the real images r_real = D(x_real).", "Reconstruct the fake images r_fake = D(x_fake).", "Using an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of real images d_real = Lp(x_real, r_real).", "Using an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of fake images d_fake = Lp(x_fake, r_fake).", "The loss of D is now L_D = d_real - d_fake.", "The loss of G is now L_G = -L_D.", "About the loss  r_real and r_fake are really losses (e.g. L1-loss or L2-loss).", "In the paper they use L(...) for that.", "Here they are referenced as d_* in order to avoid confusion.", "The loss L_D is based on the Wasserstein distance, as in WGAN.", "L_D assumes, that the losses d_real and d_fake are normally distributed and tries to move their mean values.", "Ideally, the discriminator produces very different means for real/fake images, while the generator leads to very similar means.", "Their formulation of the Wasserstein distance does not require K-Lipschitz functions, which is why they don't have the weight clipping from WGAN.", "Equilibrium  The generator and discriminator are at equilibrium, if E[r_fake] = E[r_real].", "(That's undesirable, because it means that D can't differentiate between fake and real images, i.e. G doesn't get a proper gradient any more.)", "Let g = E[r_fake] / E[r_real], then:  Low g means that E[r_fake] is low and/or E[r_real] is high, which means that real images are not as well reconstructed as fake images.", "This means, that the discriminator will be more heavily trained towards reconstructing real images correctly (as that is the main source of error).", "High g conversely means that real images are well reconstructed (compared to fake ones) and that the discriminator will be trained more towards fake ones.", "g gives information about how much G and D should be trained each (so that none of the two overwhelms the other).", "They introduce a hyperparameter gamma (from interval [0,1]), which reflects the target value of the balance g.  Using gamma, they change their losses L_D and L_G slightly:  L_D = d_real - k_t d_fake  L_G = r_fake  k_t+1 = k_t + lambda_k (gamma d_real - d_fake).", "k_t is a control term that controls how much D is supposed to focus on the fake images.", "It changes with every batch.", "k_t is clipped to [0,1] and initialized at 0 (max focus on reconstructing real images).", "lambda_k is like the learning rate of the control term, set to 0.001.", "Note that gamma d_real - d_fake = 0 <=> gamma d_real = d_fake <=> gamma = d_fake / d_real.", "Convergence measure  They measure the convergence of their model using M:  M = d_real + |gamma d_real - d_fake|  M goes down, if d_real goes down (D becomes better at autoencoding real images).", "M goes down, if the difference in reconstruction error between real and fake images goes down, i.e. if G becomes better at generating fake images.", "Other  They use Adam with learning rate 0.0001.", "They decrease it by a factor of 2 whenever M stalls.", "Higher initial learning rate could lead to model collapse or visual artifacs.", "They generate images of max size 128x128.", "They don't use more than 128 filters per conv layer.", "Results  NOTES:  Below example images are NOT from generators trained on CelebA.", "They used a custom dataset of celebrity images.", "They don't show any example images from the dataset.", "The generated images look like there is less background around the faces, making the task easier.", "Few example images.", "Unclear how much cherry picking was involved.", "Though the results from the tensorflow example (see like at top) make it look like the examples are representative (aside from speckle-artifacts).", "No LSUN Bedrooms examples.", "Human faces are comparatively easy to generate.", "Example images at 128x128:  Effect of changing the target balance gamma:  High gamma leads to more diversity at lower quality.", "Interpolations:  Convergence measure M and associated image quality during the training:"], "summary_text": "Tensorflow implementation  What  They suggest a GAN algorithm that is based on an autoencoder with Wasserstein distance. Their method generates highly realistic human faces. Their method has a convergence measure, which reflects the quality of the generates images. Their method has a diversity hyperparameter, which can be used to set the tradeoff between image diversity and image quality. How  Like other GANs, their method uses a generator G and a discriminator D.  Generator  The generator is fairly standard. It gets a noise vector z as input and uses upsampling+convolutions to generate images. It uses ELUs and no BN. Discriminator  The discriminator is a full autoencoder (i.e. it converts input images to 8x8x3 tensors, then reconstructs them back to images). It has skip-connections from the 8x8x3 layer to each upsampling layer. It also uses ELUs and no BN. Their method now has the following steps:  Collect real images x_real. Generate fake images x_fake = G(z). Reconstruct the real images r_real = D(x_real). Reconstruct the fake images r_fake = D(x_fake). Using an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of real images d_real = Lp(x_real, r_real). Using an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of fake images d_fake = Lp(x_fake, r_fake). The loss of D is now L_D = d_real - d_fake. The loss of G is now L_G = -L_D. About the loss  r_real and r_fake are really losses (e.g. L1-loss or L2-loss). In the paper they use L(...) for that. Here they are referenced as d_* in order to avoid confusion. The loss L_D is based on the Wasserstein distance, as in WGAN. L_D assumes, that the losses d_real and d_fake are normally distributed and tries to move their mean values. Ideally, the discriminator produces very different means for real/fake images, while the generator leads to very similar means. Their formulation of the Wasserstein distance does not require K-Lipschitz functions, which is why they don't have the weight clipping from WGAN. Equilibrium  The generator and discriminator are at equilibrium, if E[r_fake] = E[r_real]. (That's undesirable, because it means that D can't differentiate between fake and real images, i.e. G doesn't get a proper gradient any more.) Let g = E[r_fake] / E[r_real], then:  Low g means that E[r_fake] is low and/or E[r_real] is high, which means that real images are not as well reconstructed as fake images. This means, that the discriminator will be more heavily trained towards reconstructing real images correctly (as that is the main source of error). High g conversely means that real images are well reconstructed (compared to fake ones) and that the discriminator will be trained more towards fake ones. g gives information about how much G and D should be trained each (so that none of the two overwhelms the other). They introduce a hyperparameter gamma (from interval [0,1]), which reflects the target value of the balance g.  Using gamma, they change their losses L_D and L_G slightly:  L_D = d_real - k_t d_fake  L_G = r_fake  k_t+1 = k_t + lambda_k (gamma d_real - d_fake). k_t is a control term that controls how much D is supposed to focus on the fake images. It changes with every batch. k_t is clipped to [0,1] and initialized at 0 (max focus on reconstructing real images). lambda_k is like the learning rate of the control term, set to 0.001. Note that gamma d_real - d_fake = 0 <=> gamma d_real = d_fake <=> gamma = d_fake / d_real. Convergence measure  They measure the convergence of their model using M:  M = d_real + |gamma d_real - d_fake|  M goes down, if d_real goes down (D becomes better at autoencoding real images). M goes down, if the difference in reconstruction error between real and fake images goes down, i.e. if G becomes better at generating fake images. Other  They use Adam with learning rate 0.0001. They decrease it by a factor of 2 whenever M stalls. Higher initial learning rate could lead to model collapse or visual artifacs. They generate images of max size 128x128. They don't use more than 128 filters per conv layer. Results  NOTES:  Below example images are NOT from generators trained on CelebA. They used a custom dataset of celebrity images. They don't show any example images from the dataset. The generated images look like there is less background around the faces, making the task easier. Few example images. Unclear how much cherry picking was involved. Though the results from the tensorflow example (see like at top) make it look like the examples are representative (aside from speckle-artifacts). No LSUN Bedrooms examples. Human faces are comparatively easy to generate. Example images at 128x128:  Effect of changing the target balance gamma:  High gamma leads to more diversity at lower quality. Interpolations:  Convergence measure M and associated image quality during the training:", "pdf_url": "https://arxiv.org/pdf/1703.10717", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/began.json"}
{"id": "87885377", "bin": "900_1000", "summary_sentences": ["Credit: Arxiv  Recent progress in NLP has mostly been due to the effective pretraining of large neural networks such as BERT and XLNet .", "While the performances of these models — in a transfer learning setting — are impressive, there is very little understanding of why and what aspects of language they learn.", "This paper aims to investigate what aspects of language (i.e. linguistic features) BERT learns from unlabeled data.", "The main approach is to observe the attention component of the pre-trained model and obtain interesting insights about what type syntactical information the attention heads capture.", "The proposed analysis method probes the attention heads at a broader level (analysis of surface-level patterns) and at the individual level (analysis of specific linguistic phenomena).", "Broad Attention  For the surface-level pattern analysis, the attention maps of a BERT-based model are extracted when applied to 1000 random Wikipedia segments.", "Note that no masking was used as in the original BERT training.", "The model has 12 layers containing 12 attention heads each.", "This broader analysis shows that BERT’s attention heads pay little attention to the current token but rather specialize to attend heavily on the next or previous token, especially in the earlier layers.", "A substantial amount of BERT’ attention focuses on a few special tokens such as the deliminator token [SEP] which means that such tokens play a vital role in BERT’s performance.", "The figure below shows the average attention behavior in each layer for some special tokens such as [CLS] and [SEP].", "At first, the authors suspected that the [SEP] token is used as an indicator to collect valuable segment-level information which is passed on to other attention heads.", "However, the attention heads processing [SEP] almost entirely keep fixated on themselves and the other [SEP] tokens.", "Further theoretical analysis shows that attention over [SEP] tokens could serve as a way for the BERT model to control when an attention head’s function is applicable or not (also referred to as a no-op).", "By measuring the entropy of each head’s attention distribution it is possible to observe that some of these heads, particularly in the lower layers, display very broad attention.", "The authors state that the output of these heads translates to a bag-of-vectors representation of the sentences.", "(See figure below)  Interestingly, entropies for all attention heads on the [CLS] token also have similar behavior with the last layer having huge entropy.", "This translates to a representation that attends broadly, aggregating a representation used as input for the next sentence prediction.", "Individual Attention Heads  The previous analysis focused on broad attention, so this part focuses on probing the individual attention heads to find out what aspects of language they have learned.", "Two tasks were used in this analysis: dependency parsing and coreference resolution.", "The first analysis aims to evaluate the attention heads on dependency relation prediction.", "The authors found that no single attention head does well at syntax overall.", "The main finding in this analysis was that in almost all the relations, the dependent attends to the head word, which makes sense because dependents only have one head.", "The figure below shows a relation example where the attention head 8–11 perform well.", "You can see that the determiners expectedly attend to their corresponding noun.", "(See more interesting examples in the paper)  The second analysis aims to evaluate the attention heads on the semantic task of coreference resolution.", "Keep in mind that coreference is more difficult as links are usually longer than in syntactic dependencies.", "The evaluation is done via asking the following: “what percent of the time does the head word of a coreferent mention most attend to the head of one of that mention’s antecedents.”  The figure below shows an example where the attention head 5–4 achieves satisfactory accuracy on the antecedent selection task.", "With a few more creative tests (see paper for full details), the authors found that BERT’s attention maps have a fairly thorough representation of English syntax.", "In addition, it was found that BERT’s vector representations do not capture more syntactic information as compared to its attention maps.", "Via the proposed tests above, the authors argue that BERT learns some aspects of syntax purely as a by-product of self-supervised training.", "Upon further investigation of the individual attention heads behavior for a given layer, the authors found that some heads behave similarly, possible due to some attention weights being zeroed-out via dropout.", "A surprising result, given that other researchers found that encouraging different behavior in attention heads improves a Transformer’s performance.", "There is more opportunity to conduct extended analysis to help further understand these behaviors in the attention layer.", "Overall, probing attention maps offer a feasible technique to understand what neural networks learn about language.", "Paper: What Does BERT Look At?", "An Analysis of BERT’s Attention — Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning  Further Readings:  Attention mechanisms  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  XLNet outperforms BERT on several NLP Tasks  An Overview of Modern NLP"], "summary_text": "Credit: Arxiv  Recent progress in NLP has mostly been due to the effective pretraining of large neural networks such as BERT and XLNet . While the performances of these models — in a transfer learning setting — are impressive, there is very little understanding of why and what aspects of language they learn. This paper aims to investigate what aspects of language (i.e. linguistic features) BERT learns from unlabeled data. The main approach is to observe the attention component of the pre-trained model and obtain interesting insights about what type syntactical information the attention heads capture. The proposed analysis method probes the attention heads at a broader level (analysis of surface-level patterns) and at the individual level (analysis of specific linguistic phenomena). Broad Attention  For the surface-level pattern analysis, the attention maps of a BERT-based model are extracted when applied to 1000 random Wikipedia segments. Note that no masking was used as in the original BERT training. The model has 12 layers containing 12 attention heads each. This broader analysis shows that BERT’s attention heads pay little attention to the current token but rather specialize to attend heavily on the next or previous token, especially in the earlier layers. A substantial amount of BERT’ attention focuses on a few special tokens such as the deliminator token [SEP] which means that such tokens play a vital role in BERT’s performance. The figure below shows the average attention behavior in each layer for some special tokens such as [CLS] and [SEP]. At first, the authors suspected that the [SEP] token is used as an indicator to collect valuable segment-level information which is passed on to other attention heads. However, the attention heads processing [SEP] almost entirely keep fixated on themselves and the other [SEP] tokens. Further theoretical analysis shows that attention over [SEP] tokens could serve as a way for the BERT model to control when an attention head’s function is applicable or not (also referred to as a no-op). By measuring the entropy of each head’s attention distribution it is possible to observe that some of these heads, particularly in the lower layers, display very broad attention. The authors state that the output of these heads translates to a bag-of-vectors representation of the sentences. (See figure below)  Interestingly, entropies for all attention heads on the [CLS] token also have similar behavior with the last layer having huge entropy. This translates to a representation that attends broadly, aggregating a representation used as input for the next sentence prediction. Individual Attention Heads  The previous analysis focused on broad attention, so this part focuses on probing the individual attention heads to find out what aspects of language they have learned. Two tasks were used in this analysis: dependency parsing and coreference resolution. The first analysis aims to evaluate the attention heads on dependency relation prediction. The authors found that no single attention head does well at syntax overall. The main finding in this analysis was that in almost all the relations, the dependent attends to the head word, which makes sense because dependents only have one head. The figure below shows a relation example where the attention head 8–11 perform well. You can see that the determiners expectedly attend to their corresponding noun. (See more interesting examples in the paper)  The second analysis aims to evaluate the attention heads on the semantic task of coreference resolution. Keep in mind that coreference is more difficult as links are usually longer than in syntactic dependencies. The evaluation is done via asking the following: “what percent of the time does the head word of a coreferent mention most attend to the head of one of that mention’s antecedents.”  The figure below shows an example where the attention head 5–4 achieves satisfactory accuracy on the antecedent selection task. With a few more creative tests (see paper for full details), the authors found that BERT’s attention maps have a fairly thorough representation of English syntax. In addition, it was found that BERT’s vector representations do not capture more syntactic information as compared to its attention maps. Via the proposed tests above, the authors argue that BERT learns some aspects of syntax purely as a by-product of self-supervised training. Upon further investigation of the individual attention heads behavior for a given layer, the authors found that some heads behave similarly, possible due to some attention weights being zeroed-out via dropout. A surprising result, given that other researchers found that encouraging different behavior in attention heads improves a Transformer’s performance. There is more opportunity to conduct extended analysis to help further understand these behaviors in the attention layer. Overall, probing attention maps offer a feasible technique to understand what neural networks learn about language. Paper: What Does BERT Look At? An Analysis of BERT’s Attention — Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning  Further Readings:  Attention mechanisms  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  XLNet outperforms BERT on several NLP Tasks  An Overview of Modern NLP", "pdf_url": "https://arxiv.org/pdf/1906.04341v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/aspects-of-language-captured-by-bert-32bc3c54016f.json"}
{"id": "82220044", "bin": "900_1000", "summary_sentences": ["Optimizing Search Engines using Clickthrough Data – Joachims, 2002  Today’s choice is another KDD ‘test-of-time’ winner .", "The paper introduced the problem of ranking documents w.r.t.", "a query using not explicit user feedback but implicit user feedback in the form of clickthrough data.", "The author presented the Ranking SVM Algorithm to solve the proposed ranking problem.", "The paper has stimulated much follow-up research, and the Ranking SVM Algorithm has been widely used for many applications, as evidenced by the very large number of citations.", "The work has also been included in various textbooks.", "It’s hard to believe that it was as recently as 2002 that the idea of using clickthrough data to improve search engine rankings was ground-breaking research!", "It seems so obvious to us now, but it must have been a very exciting breakthrough at the time.", "(And of course, there’s the tricky question of exactly how to do it, once you’ve had the insight).", "Which WWW page(s) does a user actually want to retrieve when he types some keywords into a search engine?", "… Unfortunately, experience shows that users are only rarely willing to give explicit feedback.", "However, this paper argues that sufficient information is already hidden in the logfiles of WWW search engines.", "Since major search engines receive millions of queries per day, such data is available in abundance.", "Compared to explicit feedback data, which is typically elicited in laborious user studies, any information that can be extracted from logfiles is virtually free and substantially more timely.", "We can model clickthrough data as a triplet (q,r,c) : query, ranked list of results, clicks.", "The query q and the returned ranking r can easily be recorded whenever the resulting ranking is displayed to the user.", "For recording the clicks, a simple proxy system can keep a logfile.", "Each query is assigned a unique ID which is stored in the query-log along with the query words and the presented ranking.", "The links on the results-page presented to the user do not lead directly to the suggested document, but point to a proxy server.", "These links encode the query-ID and the URL of the suggested document.", "When the user clicks on the link, the proxy-server records the URL and the query-ID in the click-log.", "The proxy then uses the HTTP Location command to forward the user to the target URL.", "This process can be made transparent to the user and does not influence system performance.", "We still need to be a little bit careful interpreting (q,r,c) triplets – users are naturally biased to click on links towards the top of the search rankings so we can’t treat a click as an absolute indication of search result relevance.", "But we can interpret it as a relative ranking of the results.", "Suppose a search returns a set of links link1, link2, … linkn.", "A user that clicks on link3 but not link1 or link2 must have scanned past  links 1 and 2 before clicking on 3.", "This strategy is captured by the following algorithm:  For a ranking (link1,link2,link3, …) and a set C containing the ranks of the clicked-on links, extract a preference example linki <r* linkj, for all pairs 1 &leq; j < i, with i ∈ C and j ∉ C.  “Unfortunately, this type of feedback is not suitable for standard machine learning algorithms.”  Consider a search engine with an operational retrieval function f, and let rf(q) be the ranking returned by f for the query q.", "We want to evaluate how closely its ranking rf(q) approximates the optimal ordering r*.", "Kendall’s τ is used for this, being the most frequently used measure in statistics for comparing the ordinal correlation of two random variables.", "Take two finite strict orderings over some set of documents D, ra and rb.", "Examine all pairs of documents d1 and d2 in D – if ra and rb agree on the relative ranking call (d1,d2) a concordant pair, otherwise if they disagree they are a discordant pair.", "Let P be the number of concordant pairs, and Q be the number of discordant pairs.", "Kendall’s τ is simply the fraction of the total number of pairs that are concordant:  τ(ra,rb) = (P – Q) / (P + Q)  We are now in a position to define the problem of learning a ranking function.", "For a fixed but unknown distribution Pr(q, r∗) of queries and target rankings on a document collection D with m documents, the goal is to learn a retrieval function f(q) for which the expected Kendall’s τ is maximal.", "This turns out to be equivalent to minimizing the number of discordant pairs (Q).", "Unfortunately… this problem is NP-hard, but it is possible to approximate the solution.", "Joachims shows how to do this with a Support Vector Machine.", "My SVM-fu isn’t up to giving any meaningful analysis of the way this is done, so I’ll just show you where Joachims ends up:  εi,j,k are the introduced approximation variables that makes the problem practical, and C is a parameter that enables the trading off of margin size against training error."], "summary_text": "Optimizing Search Engines using Clickthrough Data – Joachims, 2002  Today’s choice is another KDD ‘test-of-time’ winner . The paper introduced the problem of ranking documents w.r.t. a query using not explicit user feedback but implicit user feedback in the form of clickthrough data. The author presented the Ranking SVM Algorithm to solve the proposed ranking problem. The paper has stimulated much follow-up research, and the Ranking SVM Algorithm has been widely used for many applications, as evidenced by the very large number of citations. The work has also been included in various textbooks. It’s hard to believe that it was as recently as 2002 that the idea of using clickthrough data to improve search engine rankings was ground-breaking research! It seems so obvious to us now, but it must have been a very exciting breakthrough at the time. (And of course, there’s the tricky question of exactly how to do it, once you’ve had the insight). Which WWW page(s) does a user actually want to retrieve when he types some keywords into a search engine? … Unfortunately, experience shows that users are only rarely willing to give explicit feedback. However, this paper argues that sufficient information is already hidden in the logfiles of WWW search engines. Since major search engines receive millions of queries per day, such data is available in abundance. Compared to explicit feedback data, which is typically elicited in laborious user studies, any information that can be extracted from logfiles is virtually free and substantially more timely. We can model clickthrough data as a triplet (q,r,c) : query, ranked list of results, clicks. The query q and the returned ranking r can easily be recorded whenever the resulting ranking is displayed to the user. For recording the clicks, a simple proxy system can keep a logfile. Each query is assigned a unique ID which is stored in the query-log along with the query words and the presented ranking. The links on the results-page presented to the user do not lead directly to the suggested document, but point to a proxy server. These links encode the query-ID and the URL of the suggested document. When the user clicks on the link, the proxy-server records the URL and the query-ID in the click-log. The proxy then uses the HTTP Location command to forward the user to the target URL. This process can be made transparent to the user and does not influence system performance. We still need to be a little bit careful interpreting (q,r,c) triplets – users are naturally biased to click on links towards the top of the search rankings so we can’t treat a click as an absolute indication of search result relevance. But we can interpret it as a relative ranking of the results. Suppose a search returns a set of links link1, link2, … linkn. A user that clicks on link3 but not link1 or link2 must have scanned past  links 1 and 2 before clicking on 3. This strategy is captured by the following algorithm:  For a ranking (link1,link2,link3, …) and a set C containing the ranks of the clicked-on links, extract a preference example linki <r* linkj, for all pairs 1 &leq; j < i, with i ∈ C and j ∉ C.  “Unfortunately, this type of feedback is not suitable for standard machine learning algorithms.”  Consider a search engine with an operational retrieval function f, and let rf(q) be the ranking returned by f for the query q. We want to evaluate how closely its ranking rf(q) approximates the optimal ordering r*. Kendall’s τ is used for this, being the most frequently used measure in statistics for comparing the ordinal correlation of two random variables. Take two finite strict orderings over some set of documents D, ra and rb. Examine all pairs of documents d1 and d2 in D – if ra and rb agree on the relative ranking call (d1,d2) a concordant pair, otherwise if they disagree they are a discordant pair. Let P be the number of concordant pairs, and Q be the number of discordant pairs. Kendall’s τ is simply the fraction of the total number of pairs that are concordant:  τ(ra,rb) = (P – Q) / (P + Q)  We are now in a position to define the problem of learning a ranking function. For a fixed but unknown distribution Pr(q, r∗) of queries and target rankings on a document collection D with m documents, the goal is to learn a retrieval function f(q) for which the expected Kendall’s τ is maximal. This turns out to be equivalent to minimizing the number of discordant pairs (Q). Unfortunately… this problem is NP-hard, but it is possible to approximate the solution. Joachims shows how to do this with a Support Vector Machine. My SVM-fu isn’t up to giving any meaningful analysis of the way this is done, so I’ll just show you where Joachims ends up:  εi,j,k are the introduced approximation variables that makes the problem practical, and C is a parameter that enables the trading off of margin size against training error.", "pdf_url": "http://www.cs.cornell.edu/People/tj/publications/joachims_02c.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/optimizing-search-engines-using-clickthrough-data.json"}
{"id": "39155988", "bin": "900_1000", "summary_sentences": ["Predicate Logic as Programming Language – Kowalski 1974  The purpose of programming languages is to enable the communication from man to machine of problems and their general means of solution.", "Kowalski shows us that predicate logic can be used as the basis of a “useful and practical, high-level, non-deterministic programming language with sound theoretical foundations.” This is the foundation for Prolog and Datalog.", "As a programming language, predicate logic is the only language which is entirely user-oriented.", "It differs from existing high-level languages in that it possesses no features which are meaningful only in machine terms.", "Sentences are expressed in clausal form with a very simple syntax.", "B1,...,Bm <- A1,...,An  which can be interpreted as meaning B1 or … or Bm is implied by A1 and … and An.", "It is our thesis, that clausal form defines a natural and useful language in its own right, that thoughts can be conveniently expressed directly in clausal form, and that literal translation from another language, such as full predicate logic, often distorts the original thought.", "The Horn clause subset of predicated logic is that where clauses contain at most one disjunction (B) term.", "This is the form used for computation in the paper.", "A simple factorial example is given.", "Let Fact(x,y) denote the fact that the factorial of x is y, and let s(x) be the successor of x in the natural numbers.", "Finally, let Times(x,y,z) denote that x * y is z.", "Then:  -- the factorial of 0 is s(0), i.e. 1  Fact(0,s(0)) <-    -- if v is the factorial of x, and s(x) * v is u, then -- the factorial of s(x) is u Fact(s(x),u) <- Fact(x,v), Times(s(x),v,u)  To read these programs, I mentally insert an ‘if’ ; we can deduce the terms on the left if the terms on the right are true.", "Contrast this to a functional declaration where we mentally insert an ‘is’:  factorial 0     = 1 factorial (n+1) = (n+1) * factorial n  Non-determinism and the potential for parallelism  Predicate logic is an essentially non-deterministic programming language.", "Non-determinism is due to the fact that a given program and activating goal statement may admit more than a single legitimate computation.", "This is nicely illustrated with a program for sorting lists.", "Let Sort(x,y) denote that y is a sorted version of x; Perm(x,y) that y is a permutation of x; Ord(y) that y is ordered; Del(x,y,z) that deleting x from y results in z; and LE(x,y) that x is less than or equal to y; then:  Sort(x,y) <- Perm(x,y), Ord(y)      Perm(nil,nil) <- Perm(z,cons(x,y)) <- Perm(z',y), Del(x,z,z')      Del(x,cons(x,y),y) <- Del(x,cons(y,z),cons(y,z')) <- Del(x,z,z')      Ord(nil) <- Ord(cons(x,nil)) <- Ord(cons(x,cons(y,z))) <- LE(x,y), Ord(cons(y,z))  Consider three difference approaches to computing the sorted version of a list based on these declarations:  Generate a permutation y of x, and then test to see if y is ordered, or  Generate an ordered list y, and then test to see if it is a permutation of x, or  Grow increasingly longer ordered subsets of x, adding one element from x at each stage.", "Clearly the difference in efficiency can be enormous, but the meaning, as determined by the input-output relation Sort(x,y), computed by the program, is the same.", "It is in this sense that the sequencing of procedure calls can be said to have no semantics.", "This is fundamentally what enables highly-parallel executions in languages such as Dedalus , a Datalog derivative:  The use of parallel processes and co-routines is a particular way of sequencing procedure calls.", "The possibility of independent parallel processing arises when, for example, different procedure calls in the same body share no variables.", "In such a case, the independent procedure calls can be activated simultaneously and, given a single processor, their execution sequences can be interleaved arbitrarily.", "40 years later, and we’re seeing a mini-revival in interest in Datalog and its derivatives."], "summary_text": "Predicate Logic as Programming Language – Kowalski 1974  The purpose of programming languages is to enable the communication from man to machine of problems and their general means of solution. Kowalski shows us that predicate logic can be used as the basis of a “useful and practical, high-level, non-deterministic programming language with sound theoretical foundations.” This is the foundation for Prolog and Datalog. As a programming language, predicate logic is the only language which is entirely user-oriented. It differs from existing high-level languages in that it possesses no features which are meaningful only in machine terms. Sentences are expressed in clausal form with a very simple syntax. B1,...,Bm <- A1,...,An  which can be interpreted as meaning B1 or … or Bm is implied by A1 and … and An. It is our thesis, that clausal form defines a natural and useful language in its own right, that thoughts can be conveniently expressed directly in clausal form, and that literal translation from another language, such as full predicate logic, often distorts the original thought. The Horn clause subset of predicated logic is that where clauses contain at most one disjunction (B) term. This is the form used for computation in the paper. A simple factorial example is given. Let Fact(x,y) denote the fact that the factorial of x is y, and let s(x) be the successor of x in the natural numbers. Finally, let Times(x,y,z) denote that x * y is z. Then:  -- the factorial of 0 is s(0), i.e. 1  Fact(0,s(0)) <-    -- if v is the factorial of x, and s(x) * v is u, then -- the factorial of s(x) is u Fact(s(x),u) <- Fact(x,v), Times(s(x),v,u)  To read these programs, I mentally insert an ‘if’ ; we can deduce the terms on the left if the terms on the right are true. Contrast this to a functional declaration where we mentally insert an ‘is’:  factorial 0     = 1 factorial (n+1) = (n+1) * factorial n  Non-determinism and the potential for parallelism  Predicate logic is an essentially non-deterministic programming language. Non-determinism is due to the fact that a given program and activating goal statement may admit more than a single legitimate computation. This is nicely illustrated with a program for sorting lists. Let Sort(x,y) denote that y is a sorted version of x; Perm(x,y) that y is a permutation of x; Ord(y) that y is ordered; Del(x,y,z) that deleting x from y results in z; and LE(x,y) that x is less than or equal to y; then:  Sort(x,y) <- Perm(x,y), Ord(y)      Perm(nil,nil) <- Perm(z,cons(x,y)) <- Perm(z',y), Del(x,z,z')      Del(x,cons(x,y),y) <- Del(x,cons(y,z),cons(y,z')) <- Del(x,z,z')      Ord(nil) <- Ord(cons(x,nil)) <- Ord(cons(x,cons(y,z))) <- LE(x,y), Ord(cons(y,z))  Consider three difference approaches to computing the sorted version of a list based on these declarations:  Generate a permutation y of x, and then test to see if y is ordered, or  Generate an ordered list y, and then test to see if it is a permutation of x, or  Grow increasingly longer ordered subsets of x, adding one element from x at each stage. Clearly the difference in efficiency can be enormous, but the meaning, as determined by the input-output relation Sort(x,y), computed by the program, is the same. It is in this sense that the sequencing of procedure calls can be said to have no semantics. This is fundamentally what enables highly-parallel executions in languages such as Dedalus , a Datalog derivative:  The use of parallel processes and co-routines is a particular way of sequencing procedure calls. The possibility of independent parallel processing arises when, for example, different procedure calls in the same body share no variables. In such a case, the independent procedure calls can be activated simultaneously and, given a single processor, their execution sequences can be interleaved arbitrarily. 40 years later, and we’re seeing a mini-revival in interest in Datalog and its derivatives.", "pdf_url": "http://www-public.it-sudparis.eu/~gibson/Teaching/CSC4504/ReadingMaterial/Kowalski74.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/predicate-logic-as-programming-language.json"}
{"id": "11542688", "bin": "900_1000", "summary_sentences": ["The Scalable Commutativity Rule: Designing Scalable Software for Multicore Processors – Clements et al. 2013  The way you design your interface (API) has a significant impact on the scalability you can achieve with any implementation.", "Clements et al. define the Scalable Commutativity Rule – which will look familiar to those who study distributed systems – and a tool called COMMUTER, which together can guide your API design.", "Using these techniques they were able to redesign 18 file-system related POSIX calls and greatly increase scalability.", "Two aspects I really like in this work are (a) the focus on the impact of your API design on ultimate scalability (vs. implementation optimisations and techniques), and (b) the notion that while not all operations may commute all of the time, there can be significant regions in the execution history where they can.", "This is useful information we can exploit…  If all this sounds interesting, I highly recommend reading the full paper.", "My copy is extensively highlighted and I can only give you a flavour here.", "At the core of our approach is this scalable commutativity rule: In any situation where several operations commute—meaning there’s no way to distinguish their execution order using the interface—they have an implementation whose memory accesses are conflict-free during those operations.", "Or, more concisely, whenever interface operations commute, they can be implemented in a way that scales.", "Clements et al. study SIM commutativity: State dependent, Interface-based, and monotonic.", "When operations commute in the context of a specific system state, specific operation arguments, and specific concurrent operations, we show that an implementation exists that is conflict-free for that state and those arguments and concurrent operations.", "This exposes many more opportunities to apply the rule to real interfaces—and thus discover scalable implementations—than a more conventional notion of commutativity would.", "Consider Unix system calls.", "Very few unconditionally commute in every state and history – getpid() is one such example.", "But many more can conditionally commute based on state and arguments etc.", "For example, calls to open(\"a\", O_CREATE|O_EXCL) when called from processes with different working directories.", "The COMMUTER tool takes an interface model, and determines the precise conditions under which sets of operations commute.", "“The tool can be integrated into the development process to drive initial design and implementation, to incrementally improve existing implementations, or to help developers understand the commutativity of an interface.”  To reason about implementation scalability, we need to model implementations in enough detail to tell whether different threads’ “memory accesses” are conflict-free…conflict freedom is our proxy for scalability.", "This requires just enough modelling of the state behind the interface to check for access conflicts.", "An important result of the formally defined scalable commutativity rule (see section 3.4) is that in a region of history which SIM commutes, there exists a correct implementation in that region whose steps are conflict free.", "You just have to find that implementation!", "The authors show one way of constructing it as a proof that it is always possible, but this is not recommended as an actual implementation technique.", "We believe it is easier to create practical scalable implementations for operations that commute in more situations.", "The arguments and system states for which a set of operations commutes often collapse into fairly well-defined classes (e.g., file creation might commute whenever the containing directories are different).", "In practice, implementations scale for whole classes of states and arguments, not just for specific histories… In the end, a system designer must decide which situations involving commutative operations are most important, and find practical implementation strategies that scale in those situations.", "COMMUTER  The user expresses a symbolic model of the interface in Python, and the ANALYZER component of COMMUTER generates expressions in terms of arguments and state that indicate exactly when sets of operations commute.", "These expressions can be directly inspected, and also passed to TESTGEN which can convert these conditions into real test cases.", "To capture different conflict conditions as well as path conditions, we introduce a new notion called conflict coverage.", "Conflict coverage exercises all possible access patterns on shared data structures: looking up two distinct items from different operations, looking up the same item, etc.", "TESTGEN approximates conflict coverage…  For a model of 18 POSIX system calls, TESTGEN produces a total of 13,664 test cases, and these find scalability issues in the Linux ramfs file system and virtual memory system.", "MTRACE uses a modified version of qemu and guest Linux kernel to run the test cases generated by TESTGEN against a real implementation, and checks that the implementation is conflict-free in each case.", "Applying COMMUTER in the design of a file system interface  The authors used COMMUTER to guide the design of a new file system called ScaleFS:  Given that Linux does not scale in many cases, how hard is it to implement scalable file systems and virtual memory systems?", "To answer this question, we designed and implemented a ramfs-like in-memory file system called ScaleFS and a virtual memory system called RadixVM for sv6, our research kernel based on xv6…  Figure 6 below shows the scalability of system call pairs as achieved by ramfs (left) and ScaleFS (right).", "A big reduction in the number of conflicting cases.", "COMMUTER, sv6, and a browser for the data in this  paper are available at  [url]"], "summary_text": "The Scalable Commutativity Rule: Designing Scalable Software for Multicore Processors – Clements et al. 2013  The way you design your interface (API) has a significant impact on the scalability you can achieve with any implementation. Clements et al. define the Scalable Commutativity Rule – which will look familiar to those who study distributed systems – and a tool called COMMUTER, which together can guide your API design. Using these techniques they were able to redesign 18 file-system related POSIX calls and greatly increase scalability. Two aspects I really like in this work are (a) the focus on the impact of your API design on ultimate scalability (vs. implementation optimisations and techniques), and (b) the notion that while not all operations may commute all of the time, there can be significant regions in the execution history where they can. This is useful information we can exploit…  If all this sounds interesting, I highly recommend reading the full paper. My copy is extensively highlighted and I can only give you a flavour here. At the core of our approach is this scalable commutativity rule: In any situation where several operations commute—meaning there’s no way to distinguish their execution order using the interface—they have an implementation whose memory accesses are conflict-free during those operations. Or, more concisely, whenever interface operations commute, they can be implemented in a way that scales. Clements et al. study SIM commutativity: State dependent, Interface-based, and monotonic. When operations commute in the context of a specific system state, specific operation arguments, and specific concurrent operations, we show that an implementation exists that is conflict-free for that state and those arguments and concurrent operations. This exposes many more opportunities to apply the rule to real interfaces—and thus discover scalable implementations—than a more conventional notion of commutativity would. Consider Unix system calls. Very few unconditionally commute in every state and history – getpid() is one such example. But many more can conditionally commute based on state and arguments etc. For example, calls to open(\"a\", O_CREATE|O_EXCL) when called from processes with different working directories. The COMMUTER tool takes an interface model, and determines the precise conditions under which sets of operations commute. “The tool can be integrated into the development process to drive initial design and implementation, to incrementally improve existing implementations, or to help developers understand the commutativity of an interface.”  To reason about implementation scalability, we need to model implementations in enough detail to tell whether different threads’ “memory accesses” are conflict-free…conflict freedom is our proxy for scalability. This requires just enough modelling of the state behind the interface to check for access conflicts. An important result of the formally defined scalable commutativity rule (see section 3.4) is that in a region of history which SIM commutes, there exists a correct implementation in that region whose steps are conflict free. You just have to find that implementation! The authors show one way of constructing it as a proof that it is always possible, but this is not recommended as an actual implementation technique. We believe it is easier to create practical scalable implementations for operations that commute in more situations. The arguments and system states for which a set of operations commutes often collapse into fairly well-defined classes (e.g., file creation might commute whenever the containing directories are different). In practice, implementations scale for whole classes of states and arguments, not just for specific histories… In the end, a system designer must decide which situations involving commutative operations are most important, and find practical implementation strategies that scale in those situations. COMMUTER  The user expresses a symbolic model of the interface in Python, and the ANALYZER component of COMMUTER generates expressions in terms of arguments and state that indicate exactly when sets of operations commute. These expressions can be directly inspected, and also passed to TESTGEN which can convert these conditions into real test cases. To capture different conflict conditions as well as path conditions, we introduce a new notion called conflict coverage. Conflict coverage exercises all possible access patterns on shared data structures: looking up two distinct items from different operations, looking up the same item, etc. TESTGEN approximates conflict coverage…  For a model of 18 POSIX system calls, TESTGEN produces a total of 13,664 test cases, and these find scalability issues in the Linux ramfs file system and virtual memory system. MTRACE uses a modified version of qemu and guest Linux kernel to run the test cases generated by TESTGEN against a real implementation, and checks that the implementation is conflict-free in each case. Applying COMMUTER in the design of a file system interface  The authors used COMMUTER to guide the design of a new file system called ScaleFS:  Given that Linux does not scale in many cases, how hard is it to implement scalable file systems and virtual memory systems? To answer this question, we designed and implemented a ramfs-like in-memory file system called ScaleFS and a virtual memory system called RadixVM for sv6, our research kernel based on xv6…  Figure 6 below shows the scalability of system call pairs as achieved by ramfs (left) and ScaleFS (right). A big reduction in the number of conflicting cases. COMMUTER, sv6, and a browser for the data in this  paper are available at  [url]", "pdf_url": "https://people.csail.mit.edu/nickolai/papers/clements-sc.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/the-scalable-commutativity-rule-designing-scalable-software-for-multicore-processors.json"}
{"id": "89853012", "bin": "900_1000", "summary_sentences": ["The paper introduces an architecture for end-to-end Reinforcement Learning (RL) optimization for task-oriented dialogue systems and its application to a multimodal task - grounding the dialogue into a visual context.", "Encoder Decoder Models vs RL Models  Encoder Decoder models do not account for the planning problems (which are inherent in the dialogue systems) and do not integrate seamlessly with external contexts or knowledge bases.", "RL models can handle the planning problem but require online learning and a predefined structure of the task.", "Dataset  Corpus of data collected for GuessWhat?!", "game  The goal of the game is to locate an unknown object in a rich image scene by asking a series of questions.", "The authors use their previous work in GuessWhat?!", "paper to build a supervised agent and a neural training environment.", "The agent and the environment are used to train a Deep RL agent online which can solve the task.", "Link to summary of GuessWhat?!", "paper  Training Environment  Oracle  Given an image and a question about the image, the oracle can reply in \"yes\" or \"no\" or \"not applicable\".", "Questioner  Given an image, and a list of previous question&answers (if applicable), the questioner generates a question with the aim of locating the object in the image.", "Guesser  Once the questioner is confident of having identified the image, the oracle presents a list of objects to the guesser to choose from.", "These components are based on the previous work by the authors where they develop nueral baselines for these components.", "GuessWhat!?", "as a Markov Decision Process  The state of the system is defined as the set of:  list of words/tokens generated so far in the current question  list of questions and answers so far  input image  The action space comprises of set of all the words in the vocabulary (5K in this context).", "Transition Graph  if <stop> word is choosen as the action, the full dialogue is terminated.", "if <?> word is choosen as the action, the current question is considered completed and the answer is sampled from the oracle.", "if any other word is chosen as the action, it is used to update the state of the systema and the process continues.", "Reward is defined for every state-action pair.", "The dialogue is automatically terminated after Jmax questions while the questions are automatically terminated after Imax words.", "The game can be modelled as an episodic RL scenario where generator can be trained using Policy Gradient methods.", "The paper defines the reward as 1 if the guesser could identify the correct object and 0 otherwise.", "Even though MDP assumption of \"reward being a function of state and action\" does not hold, policy gradient method employed by the paper (called as REINFORCE) is still applicable if the MDP is partially observable.", "Training Process  Train the oracle, the questioner and the guesser independently.", "Finetune the trained questioner using the proposed RL framework.", "Results  On test set, the baseline approach obtained 45% accuracy while the paper reports 53% accuracy.", "Beam search baseline (case of supervised encoder-decoder model) tends to repeat questions - an indication of poor generalization.", "Beam search basline also tends to generate longer sentences and incoherent sequence of questions (indicating towards the lack of planning component).", "RL trained model favours enumerating object categories and spatial information.", "RL trained model tends to generate shorter dialogues and uses a smaller set of vocabulary.", "One player, called as the oracle, is randomly assigned an object in the given image.", "The second player, called as the questioner, tries to locate the object, given just the image.", "The questioner can ask a series of questions about the object and the oracle can reply in \"yes\" or \"no\" or \"not applicable\".", "Once the questioner is confident of having identified the image, the oracle presents a list of objects to the questioner to choose from.", "A small penalty is added, every time a question is asked, so as to encourage informative questions only.", "GuessWhat?!", "Game  One player, called as the oracle, is randomly assigned an object in the given image.", "The second player, called as the questioner, tries to locate the object, given just the image.", "The questioner can ask a series of questions about the object and the oracle can reply in \"yes\" or \"no\" or \"not applicable\".", "Once the questioner is confident of having identified the image, the oracle presents a list of objects to the questioner to choose from.", "A small penalty is added, every time a question is asked, so as to encourage informative questions only.", "Dataset  A filtered subset of images from MSCOCO is used as the image set.", "Two separate tasks create on Amazon Mechanical Turk (AMT) - for the role of oracle and questioner.", "Data was post processed -- both manually and using AMT -- to account for things like spelling mistakes and validation.", "Final dataset comprises of 150K thousand human game iterations with 800K question-answer pairs on 60K images.", "Dataset is available at  [url]"], "summary_text": "The paper introduces an architecture for end-to-end Reinforcement Learning (RL) optimization for task-oriented dialogue systems and its application to a multimodal task - grounding the dialogue into a visual context. Encoder Decoder Models vs RL Models  Encoder Decoder models do not account for the planning problems (which are inherent in the dialogue systems) and do not integrate seamlessly with external contexts or knowledge bases. RL models can handle the planning problem but require online learning and a predefined structure of the task. Dataset  Corpus of data collected for GuessWhat?! game  The goal of the game is to locate an unknown object in a rich image scene by asking a series of questions. The authors use their previous work in GuessWhat?! paper to build a supervised agent and a neural training environment. The agent and the environment are used to train a Deep RL agent online which can solve the task. Link to summary of GuessWhat?! paper  Training Environment  Oracle  Given an image and a question about the image, the oracle can reply in \"yes\" or \"no\" or \"not applicable\". Questioner  Given an image, and a list of previous question&answers (if applicable), the questioner generates a question with the aim of locating the object in the image. Guesser  Once the questioner is confident of having identified the image, the oracle presents a list of objects to the guesser to choose from. These components are based on the previous work by the authors where they develop nueral baselines for these components. GuessWhat!? as a Markov Decision Process  The state of the system is defined as the set of:  list of words/tokens generated so far in the current question  list of questions and answers so far  input image  The action space comprises of set of all the words in the vocabulary (5K in this context). Transition Graph  if <stop> word is choosen as the action, the full dialogue is terminated. if <?> word is choosen as the action, the current question is considered completed and the answer is sampled from the oracle. if any other word is chosen as the action, it is used to update the state of the systema and the process continues. Reward is defined for every state-action pair. The dialogue is automatically terminated after Jmax questions while the questions are automatically terminated after Imax words. The game can be modelled as an episodic RL scenario where generator can be trained using Policy Gradient methods. The paper defines the reward as 1 if the guesser could identify the correct object and 0 otherwise. Even though MDP assumption of \"reward being a function of state and action\" does not hold, policy gradient method employed by the paper (called as REINFORCE) is still applicable if the MDP is partially observable. Training Process  Train the oracle, the questioner and the guesser independently. Finetune the trained questioner using the proposed RL framework. Results  On test set, the baseline approach obtained 45% accuracy while the paper reports 53% accuracy. Beam search baseline (case of supervised encoder-decoder model) tends to repeat questions - an indication of poor generalization. Beam search basline also tends to generate longer sentences and incoherent sequence of questions (indicating towards the lack of planning component). RL trained model favours enumerating object categories and spatial information. RL trained model tends to generate shorter dialogues and uses a smaller set of vocabulary. One player, called as the oracle, is randomly assigned an object in the given image. The second player, called as the questioner, tries to locate the object, given just the image. The questioner can ask a series of questions about the object and the oracle can reply in \"yes\" or \"no\" or \"not applicable\". Once the questioner is confident of having identified the image, the oracle presents a list of objects to the questioner to choose from. A small penalty is added, every time a question is asked, so as to encourage informative questions only. GuessWhat?! Game  One player, called as the oracle, is randomly assigned an object in the given image. The second player, called as the questioner, tries to locate the object, given just the image. The questioner can ask a series of questions about the object and the oracle can reply in \"yes\" or \"no\" or \"not applicable\". Once the questioner is confident of having identified the image, the oracle presents a list of objects to the questioner to choose from. A small penalty is added, every time a question is asked, so as to encourage informative questions only. Dataset  A filtered subset of images from MSCOCO is used as the image set. Two separate tasks create on Amazon Mechanical Turk (AMT) - for the role of oracle and questioner. Data was post processed -- both manually and using AMT -- to account for things like spelling mistakes and validation. Final dataset comprises of 150K thousand human game iterations with 800K question-answer pairs on 60K images. Dataset is available at  [url]", "pdf_url": "https://www.robots.ox.ac.uk/~vgg/publications/2014/Pickup14/pickup14.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/bc739e6815ab6217e0cf0a8f706786.json"}
{"id": "82387857", "bin": "900_1000", "summary_sentences": ["Making smart contracts smarter Luu et al., CCS 2016  This is the fourth in a series of papers from the ACM Queue Research for Practice ‘ Cryptocurrencies, Blockchains and Smart Contracts ‘ selections, in which Luu at al.", "look at smart contracts in Ethereum.", "Smart contracts are a really intriguing idea and have generated a lot of interest/excitement, but they also have a number of properties which make them both likely targets for attackers and also hard to get right.", "Regular readers of The Morning Paper will not be surprised to see our old friend error and exception handling popping up as one of the chief causes of problems again!", "After scanning 19,366 Ethereum contracts using the OYENTE tool described in the paper, the authors found vulnerabilities in 8,833 of them.", "Here’s the plan: after a brief introduction to smart contracts, we’ll discuss what it is that makes them especially attractive targets, followed by a look at typical vulnerabilities.", "We’ll then finish up by seeing what we can do about the situation to make contracts more secure in the future.", "What exactly is a smart contract?", "A smart contract is identified by an address (a 160-bit identifier) and its code resides on the blockchain.", "Users invoke a smart contract in present cryptocurrencies by sending transactions to the contract address.", "Specifically, if a new transaction is accepted by the blockchain and has a contract address as the recipient, then all participants on the mining network execute the contract code with the current state of the blockchain and the transaction payloads as inputs.", "The network then agrees on the output and the next state of the contract by participating in a consensus protocol.", "In Ethereum, contracts are introduce to the blockchain via special creation transactions.", "Contracts are essentially functions whose Ethereum Virtual Machine (EVM) bytecode is incorporated in the blockchain as part of the creation transaction.", "The contracts themselves can be written in higher-level languages and compiled to EVM bytecode.", "Contract functions are stateful: they have private storage on the blockchain, and can also hold some amount of virtual Ether coins.", "The private storage is allocated and initialized by running a constructor, subsequent transactions sent to the contract address invoke the anonymous function.", "Here’s an example Puzzle contract:  Note the contract state declared on lines 2-6, constructor on lines 8-13, and anonymous transaction function on lines 15-29.", "A default input variable msg holds the sender, amount of Ether sent to the contract, and any included data as part of the invocation.", "In this particular contract, if the owner initiates the transaction (line 16) they can extract the current reward value and replace it with some other amount (lines 17-21).", "Anyone else invoking the transaction can submit a potential solution, and will receive the reward if the solution is accepted (lines 23-29).", "All miners execute the transaction, which will incur some computation cost:  Ethereum pays miners some fees proportional to the required computation.", "Specifically, each instruction in the Ethereum bytecode has a pre-specified amount of gas.", "When a user sends a transaction to invoke a contract, she has to specify how much gas she is willing to provide for the execution (called gasLimit) as well as the price for each gas unit (called gasPrice).", "A miner who includes the transaction in his proposed block subsequently receives the transaction fee corresponding to the amount of gas the execution actually burns multiplied by gasPrice.", "If the execution costs more than the gasLimit then execution is terminated and the state is restored to the initial state at the start of the function execution.", "The miner still receives gasLimit compensation though.", "Why are smart contracts attractive targets?", "Smart contracts have associated value – potentially handling large numbers of coins worth hundreds of dollars apiece.", "The 8,833 contracts in the first 1,460,000 blocks in the Ethereum network had a total balance of over 3 million Ether (about $30M USD) at the time the paper was written.", "The infamous attack on ‘TheDAO’ caused a loss of about $60M to TheDAO’s investors.", "Smart contract vulnerabilities  So we know that smart contracts have value as attack targets.", "They also have a combination of features that should make any experienced software developer raise an eyebrow:  They execute in permissionless networks which arbitrary participants can join (i.e., under byzantine conditions)  Miners and/or callers have meaningful control over the environment in which the transactions execute (which transactions to accept, transaction ordering, setting of block timestamp, manipulation of call stack)  All of the above must be reasoned about in an environment which punishes anyone who doesn’t get it right first time – there is no patching mechanism:  There is no way to patch a buggy smart contract, regardless of its popularity or how much money it has, without reversing the blockchain (a formidable task).", "Therefore, reasoning about the correctness of smart contracts before deployment is critical, as is designing a safe smart contract system.", "Note: you can explicitly design versioning/upgrade capabilities into your smart contract code, since contracts can call each other.", "See e.g.,  [url]"], "summary_text": "Making smart contracts smarter Luu et al., CCS 2016  This is the fourth in a series of papers from the ACM Queue Research for Practice ‘ Cryptocurrencies, Blockchains and Smart Contracts ‘ selections, in which Luu at al. look at smart contracts in Ethereum. Smart contracts are a really intriguing idea and have generated a lot of interest/excitement, but they also have a number of properties which make them both likely targets for attackers and also hard to get right. Regular readers of The Morning Paper will not be surprised to see our old friend error and exception handling popping up as one of the chief causes of problems again! After scanning 19,366 Ethereum contracts using the OYENTE tool described in the paper, the authors found vulnerabilities in 8,833 of them. Here’s the plan: after a brief introduction to smart contracts, we’ll discuss what it is that makes them especially attractive targets, followed by a look at typical vulnerabilities. We’ll then finish up by seeing what we can do about the situation to make contracts more secure in the future. What exactly is a smart contract? A smart contract is identified by an address (a 160-bit identifier) and its code resides on the blockchain. Users invoke a smart contract in present cryptocurrencies by sending transactions to the contract address. Specifically, if a new transaction is accepted by the blockchain and has a contract address as the recipient, then all participants on the mining network execute the contract code with the current state of the blockchain and the transaction payloads as inputs. The network then agrees on the output and the next state of the contract by participating in a consensus protocol. In Ethereum, contracts are introduce to the blockchain via special creation transactions. Contracts are essentially functions whose Ethereum Virtual Machine (EVM) bytecode is incorporated in the blockchain as part of the creation transaction. The contracts themselves can be written in higher-level languages and compiled to EVM bytecode. Contract functions are stateful: they have private storage on the blockchain, and can also hold some amount of virtual Ether coins. The private storage is allocated and initialized by running a constructor, subsequent transactions sent to the contract address invoke the anonymous function. Here’s an example Puzzle contract:  Note the contract state declared on lines 2-6, constructor on lines 8-13, and anonymous transaction function on lines 15-29. A default input variable msg holds the sender, amount of Ether sent to the contract, and any included data as part of the invocation. In this particular contract, if the owner initiates the transaction (line 16) they can extract the current reward value and replace it with some other amount (lines 17-21). Anyone else invoking the transaction can submit a potential solution, and will receive the reward if the solution is accepted (lines 23-29). All miners execute the transaction, which will incur some computation cost:  Ethereum pays miners some fees proportional to the required computation. Specifically, each instruction in the Ethereum bytecode has a pre-specified amount of gas. When a user sends a transaction to invoke a contract, she has to specify how much gas she is willing to provide for the execution (called gasLimit) as well as the price for each gas unit (called gasPrice). A miner who includes the transaction in his proposed block subsequently receives the transaction fee corresponding to the amount of gas the execution actually burns multiplied by gasPrice. If the execution costs more than the gasLimit then execution is terminated and the state is restored to the initial state at the start of the function execution. The miner still receives gasLimit compensation though. Why are smart contracts attractive targets? Smart contracts have associated value – potentially handling large numbers of coins worth hundreds of dollars apiece. The 8,833 contracts in the first 1,460,000 blocks in the Ethereum network had a total balance of over 3 million Ether (about $30M USD) at the time the paper was written. The infamous attack on ‘TheDAO’ caused a loss of about $60M to TheDAO’s investors. Smart contract vulnerabilities  So we know that smart contracts have value as attack targets. They also have a combination of features that should make any experienced software developer raise an eyebrow:  They execute in permissionless networks which arbitrary participants can join (i.e., under byzantine conditions)  Miners and/or callers have meaningful control over the environment in which the transactions execute (which transactions to accept, transaction ordering, setting of block timestamp, manipulation of call stack)  All of the above must be reasoned about in an environment which punishes anyone who doesn’t get it right first time – there is no patching mechanism:  There is no way to patch a buggy smart contract, regardless of its popularity or how much money it has, without reversing the blockchain (a formidable task). Therefore, reasoning about the correctness of smart contracts before deployment is critical, as is designing a safe smart contract system. Note: you can explicitly design versioning/upgrade capabilities into your smart contract code, since contracts can call each other. See e.g.,  [url]", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2976749.2978309?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/making-smart-contracts-smarter.json"}
