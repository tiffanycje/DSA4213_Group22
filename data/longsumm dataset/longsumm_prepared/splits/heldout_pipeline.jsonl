{"id": "76854692", "bin": "0_100", "summary_sentences": ["!", "[]( [url]"], "summary_text": "! []( [url]", "pdf_url": "http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Mathe_Reinforcement_Learning_for_CVPR_2016_paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/mathe_2016_cvpr.json"}
{"id": "60641106", "bin": "0_100", "summary_sentences": ["This combines the ideas of recurrent attention to perform object detection in an image  [ref]  for multiple objects  [ref]  with semantic segmentation  [ref] .", "Segmenting subregions is to avoid a global resolution bias (the object would take up the majority of pixels) and to allow multiple scales of objects to be segmented.", "Here is a video that demos the method described in the paper:   [url]"], "summary_text": "This combines the ideas of recurrent attention to perform object detection in an image  [ref]  for multiple objects  [ref]  with semantic segmentation  [ref] . Segmenting subregions is to avoid a global resolution bias (the object would take up the majority of pixels) and to allow multiple scales of objects to be segmented. Here is a video that demos the method described in the paper:   [url]", "pdf_url": "http://arxiv.org/pdf/1605.09410v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/1605.09410.json"}
{"id": "28171009", "bin": "0_100", "summary_sentences": ["This paper proposes a method to send messages between cell phones over Bluetooth by using the device name field.", "This allows devices to communicate directly with each other without pairing."], "summary_text": "This paper proposes a method to send messages between cell phones over Bluetooth by using the device name field. This allows devices to communicate directly with each other without pairing.", "pdf_url": "http://arxiv.org/pdf/1307.7814", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/cohen13a.json"}
{"id": "8363020", "bin": "1000_1100", "summary_sentences": ["Image captioning involves the task of generating natural language descriptions of visual content with the use of datasets comprising of image-caption pairs.", "The figure below visually demonstrates the task and the datasets used in this type of work along with some examples.", "For instance, the second image on the left shows a child sitting on a couch, which can also be inferred from the accompanying image-caption shown in the example.", "On the left-hand side, we have image-caption examples obtained from COCO , which is a very popular object-captioning dataset.", "nocaps (shown on the right) is the benchmark dataset proposed in this paper and includes three different settings: in-domain (only COCO classes), near-domain (COCO and novel classes), and out-of-domain (only novel classes).", "These settings are explained later, for now, you only need to be concerned with the fact that the proposed dataset, nocaps, aims to complement current image captioning datasets, such as COCO, as opposed to completely replacing them.", "The challenge with current image captioning models is that they generalize poorly to images in the wild.", "This happens because most models are trained to capture a tiny amount of visual concepts as compared to what a human may encounter in everyday life.", "Take the COCO dataset, for example, models trained on it can only describe images containing dogs and umbrellas, but not dolphins.", "In order to build more robust real-world applications, such as an assistant for people with impaired vision, the above limitations need to be addressed.", "Specifically, large-scale classes of objects need to be supported to generalize better on an image captioning task.", "The proposed work supports 500+ novel classes, a huge improvement compared to the 80 classes found in COCO.", "This paper aims to develop image captioning models that learn visual concepts from alternative data sources such as object detection datasets.", "One of those large-scale object detection datasets is Open Images V4 .", "The training dataset for the benchmark consists of a combination of COCO and Open Images V4 training sets.", "Keep in mind that no extra image-caption pairs are provided besides those found in COCO since the Open Images V4 training portion only consists of images annotated with bounding boxes.", "The validation and test set are comprised of images from the Open Images object detection dataset.", "Overall, the authors propose a benchmark with 10 reference captions per image and many more visual concepts as contained in COCO.", "In addition, 600 classes are incorporated via the object detection dataset, which is significantly larger than COCO which contains only 80 object classes.", "Each selected image was captioned by 11 AMT workers via caption collection interfaces as shown in the Figure below.", "Note that priming refers to the technique where workers are given a small guide (in this case labels) to help with annotating rare images.", "In summary, as compared to COCO captions, the proposed benchmark, nocaps, have greater visual diversity, more object classes per image, and longer and more diverse captions (with large vocabulary).", "See paper for more information on how both the dataset and benchmark are prepared.", "The benchmark system utilizes COCO paired image-caption data to learn to generate syntactically correct captions while leveraging Open Images object detection dataset to learn more visual concepts.", "In essence, the COCO dataset is the only image-caption information considered for training, while captions from nocaps validation set are used for validation and testing datasets.", "One of the aims of the nocaps benchmark is to increase the difficulty of the image captioning task by increasing diversity of captions and images.", "However, the authors note that the performance, obtained with automatic evaluation metrics, is weaker than the human baseline.", "But the hope is to improve interpretation of results and obtain more insights.", "The authors propose to investigate two popular methods for object captioning on their benchmark: Neural Baby Talk (NBT) and Up-Down, with and without constrained beam search (CBS).", "A Faster R-CNN model is trained on image feature representations extracted from both the Visual Genome and Open Image datasets.", "As a reminder, with COCO, it is very common to use object detection features trained on Visual Genome since the images are sourced from COCO.", "Specifically, VG features refer to the use of Visual Genome alone and VGOI refers to the combination of Visual Genome and Open Images datasets.", "(Learn more about the experimental setup from the paper).", "The experimental results are reported in the table above.", "We can observe that the Up-Down model, with VG features alone (row 1), perform better than when using VGOI, perhaps indicating that classes in Open Images may be a lot more sparse, increasing the complexity of the task.", "Results from the Neural Baby Talk (NBT) model can also be observed to be lower than the Up-Down model.", "However, both methods are outperformed by the “Human” model, particularly for the nocaps validation dataset.", "You can find a more detailed discussion of the results in the paper.", "Finally, below we can observe a few image examples from nocaps with the generated captions produced by each model type.", "The in-domain model (trained only on COCO) fails to identify novel objects such as gun/rifle and insect/centipede due to the shortage of visual concepts as explained earlier.", "Near-domain means that both object classes from COCO and Open Images were used.", "Out-of-domain means that no COCO classes were used.", "For both near-domain and out-of-domain images, the captions are somewhat better but still need improvement.", "Overall, the performance of the benchmark models, which use the nocap dataset, improve marginally over a strong baseline but fall short when compared to the human baseline, which means there is still room for improvement in image-captioning tasks.", "Reference  nocaps: novel object captioning at scale — Harsh Agrawal, Karan Desai, Xinlei Chen, Rishabh Jain, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson"], "summary_text": "Image captioning involves the task of generating natural language descriptions of visual content with the use of datasets comprising of image-caption pairs. The figure below visually demonstrates the task and the datasets used in this type of work along with some examples. For instance, the second image on the left shows a child sitting on a couch, which can also be inferred from the accompanying image-caption shown in the example. On the left-hand side, we have image-caption examples obtained from COCO , which is a very popular object-captioning dataset. nocaps (shown on the right) is the benchmark dataset proposed in this paper and includes three different settings: in-domain (only COCO classes), near-domain (COCO and novel classes), and out-of-domain (only novel classes). These settings are explained later, for now, you only need to be concerned with the fact that the proposed dataset, nocaps, aims to complement current image captioning datasets, such as COCO, as opposed to completely replacing them. The challenge with current image captioning models is that they generalize poorly to images in the wild. This happens because most models are trained to capture a tiny amount of visual concepts as compared to what a human may encounter in everyday life. Take the COCO dataset, for example, models trained on it can only describe images containing dogs and umbrellas, but not dolphins. In order to build more robust real-world applications, such as an assistant for people with impaired vision, the above limitations need to be addressed. Specifically, large-scale classes of objects need to be supported to generalize better on an image captioning task. The proposed work supports 500+ novel classes, a huge improvement compared to the 80 classes found in COCO. This paper aims to develop image captioning models that learn visual concepts from alternative data sources such as object detection datasets. One of those large-scale object detection datasets is Open Images V4 . The training dataset for the benchmark consists of a combination of COCO and Open Images V4 training sets. Keep in mind that no extra image-caption pairs are provided besides those found in COCO since the Open Images V4 training portion only consists of images annotated with bounding boxes. The validation and test set are comprised of images from the Open Images object detection dataset. Overall, the authors propose a benchmark with 10 reference captions per image and many more visual concepts as contained in COCO. In addition, 600 classes are incorporated via the object detection dataset, which is significantly larger than COCO which contains only 80 object classes. Each selected image was captioned by 11 AMT workers via caption collection interfaces as shown in the Figure below. Note that priming refers to the technique where workers are given a small guide (in this case labels) to help with annotating rare images. In summary, as compared to COCO captions, the proposed benchmark, nocaps, have greater visual diversity, more object classes per image, and longer and more diverse captions (with large vocabulary). See paper for more information on how both the dataset and benchmark are prepared. The benchmark system utilizes COCO paired image-caption data to learn to generate syntactically correct captions while leveraging Open Images object detection dataset to learn more visual concepts. In essence, the COCO dataset is the only image-caption information considered for training, while captions from nocaps validation set are used for validation and testing datasets. One of the aims of the nocaps benchmark is to increase the difficulty of the image captioning task by increasing diversity of captions and images. However, the authors note that the performance, obtained with automatic evaluation metrics, is weaker than the human baseline. But the hope is to improve interpretation of results and obtain more insights. The authors propose to investigate two popular methods for object captioning on their benchmark: Neural Baby Talk (NBT) and Up-Down, with and without constrained beam search (CBS). A Faster R-CNN model is trained on image feature representations extracted from both the Visual Genome and Open Image datasets. As a reminder, with COCO, it is very common to use object detection features trained on Visual Genome since the images are sourced from COCO. Specifically, VG features refer to the use of Visual Genome alone and VGOI refers to the combination of Visual Genome and Open Images datasets. (Learn more about the experimental setup from the paper). The experimental results are reported in the table above. We can observe that the Up-Down model, with VG features alone (row 1), perform better than when using VGOI, perhaps indicating that classes in Open Images may be a lot more sparse, increasing the complexity of the task. Results from the Neural Baby Talk (NBT) model can also be observed to be lower than the Up-Down model. However, both methods are outperformed by the “Human” model, particularly for the nocaps validation dataset. You can find a more detailed discussion of the results in the paper. Finally, below we can observe a few image examples from nocaps with the generated captions produced by each model type. The in-domain model (trained only on COCO) fails to identify novel objects such as gun/rifle and insect/centipede due to the shortage of visual concepts as explained earlier. Near-domain means that both object classes from COCO and Open Images were used. Out-of-domain means that no COCO classes were used. For both near-domain and out-of-domain images, the captions are somewhat better but still need improvement. Overall, the performance of the benchmark models, which use the nocap dataset, improve marginally over a strong baseline but fall short when compared to the human baseline, which means there is still room for improvement in image-captioning tasks. Reference  nocaps: novel object captioning at scale — Harsh Agrawal, Karan Desai, Xinlei Chen, Rishabh Jain, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson", "pdf_url": "https://arxiv.org/pdf/1812.08658", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/large-scale-image-captioning-a2c0191ffd3c.json"}
{"id": "67744436", "bin": "1000_1100", "summary_sentences": ["What  Usually GANs transform a noise vector z into images.", "z might be sampled from a normal or uniform distribution.", "The effect of this is, that the components in z are deeply entangled.", "Changing single components has hardly any influence on the generated images.", "One has to change multiple components to affect the image.", "The components end up not being interpretable.", "Ideally one would like to have meaningful components, e.g. for human faces one that controls the hair length and a categorical one that controls the eye color.", "They suggest a change to GANs based on Mutual Information, which leads to interpretable components.", "E.g. for MNIST a component that controls the stroke thickness and a categorical component that controls the digit identity (1, 2, 3, ...).", "These components are learned in a (mostly) unsupervised fashion.", "How  The latent code c  \"Normal\" GANs parameterize the generator as G(z), i.e. G receives a noise vector and transforms it into an image.", "This is changed to G(z, c), i.e. G now receives a noise vector z and a latent code c and transforms both into an image.", "c can contain multiple variables following different distributions, e.g. in MNIST a categorical variable for the digit identity and a gaussian one for the stroke thickness.", "Mutual Information  If using a latent code via G(z, c), nothing forces the generator to actually use c. It can easily ignore it and just deteriorate to G(z).", "To prevent that, they force G to generate images x in a way that c must be recoverable.", "So, if you have an image x you must be able to reliable tell which latent code c it has, which means that G must use c in a meaningful way.", "This relationship can be expressed with mutual information, i.e. the mutual information between x and c must be high.", "The mutual information between two variables X and Y is defined as I(X; Y) = entropy(X) - entropy(X|Y) = entropy(Y) - entropy(Y|X).", "If the mutual information between X and Y is high, then knowing Y helps you to decently predict the value of X (and the other way round).", "If the mutual information between X and Y is low, then knowing Y doesn't tell you much about the value of X (and the other way round).", "The new GAN loss becomes old loss - lambda * I(G(z, c); c), i.e. the higher the mutual information, the lower the result of the loss function.", "Variational Mutual Information Maximization  In order to minimize I(G(z, c); c), one has to know the distribution P(c|x) (from image to latent code), which however is unknown.", "So instead they create Q(c|x), which is an approximation of P(c|x).", "I(G(z, c); c) is then computed using a lower bound maximization, similar to the one in variational autoencoders (called \"Variational Information Maximization\", hence the name \"InfoGAN\").", "Basic equation: LowerBoundOfMutualInformation(G, Q) = E[log Q(c|x)] + H(c) <= I(G(z, c); c)  c is the latent code.", "x is the generated image.", "H(c) is the entropy of the latent codes (constant throughout the optimization).", "Optimization w.r.t.", "Q is done directly.", "Optimization w.r.t.", "G is done via the reparameterization trick.", "If Q(c|x) approximates P(c|x) perfectly, the lower bound becomes the mutual information (\"the lower bound becomes tight\").", "In practice, Q(c|x) is implemented as a neural network.", "Both Q and D have to process the generated images, which means that they can share many convolutional layers, significantly reducing the extra cost of training Q.", "Results  MNIST  They use for c one categorical variable (10 values) and two continuous ones (uniform between -1 and +1).", "InfoGAN learns to associate the categorical one with the digit identity and the continuous ones with rotation and width.", "Applying Q(c|x) to an image and then classifying only on the categorical variable (i.e. fully unsupervised) yields 95% accuracy.", "Sampling new images with exaggerated continuous variables in the range [-2,+2] yields sound images (i.e. the network generalizes well).", "3D face images  InfoGAN learns to represent the faces via pose, elevation, lighting.", "They used five uniform variables for c. (So two of them apparently weren't associated with anything sensible?", "They are not mentioned.)", "3D chair images  InfoGAN learns to represent the chairs via identity (categorical) and rotation or width (apparently they did two experiments).", "They used one categorical variable (four values) and one continuous variable (uniform [-1, +1]).", "SVHN  InfoGAN learns to represent lighting and to spot the center digit.", "They used four categorical variables (10 values each) and two continuous variables (uniform [-1, +1]).", "(Again, a few variables were apparently not associated with anything sensible?)", "CelebA  InfoGAN learns to represent pose, presence of sunglasses (not perfectly), hair style and emotion (in the sense of \"smiling or not smiling\").", "They used 10 categorical variables (10 values each).", "(Again, a few variables were apparently not associated with anything sensible?)"], "summary_text": "What  Usually GANs transform a noise vector z into images. z might be sampled from a normal or uniform distribution. The effect of this is, that the components in z are deeply entangled. Changing single components has hardly any influence on the generated images. One has to change multiple components to affect the image. The components end up not being interpretable. Ideally one would like to have meaningful components, e.g. for human faces one that controls the hair length and a categorical one that controls the eye color. They suggest a change to GANs based on Mutual Information, which leads to interpretable components. E.g. for MNIST a component that controls the stroke thickness and a categorical component that controls the digit identity (1, 2, 3, ...). These components are learned in a (mostly) unsupervised fashion. How  The latent code c  \"Normal\" GANs parameterize the generator as G(z), i.e. G receives a noise vector and transforms it into an image. This is changed to G(z, c), i.e. G now receives a noise vector z and a latent code c and transforms both into an image. c can contain multiple variables following different distributions, e.g. in MNIST a categorical variable for the digit identity and a gaussian one for the stroke thickness. Mutual Information  If using a latent code via G(z, c), nothing forces the generator to actually use c. It can easily ignore it and just deteriorate to G(z). To prevent that, they force G to generate images x in a way that c must be recoverable. So, if you have an image x you must be able to reliable tell which latent code c it has, which means that G must use c in a meaningful way. This relationship can be expressed with mutual information, i.e. the mutual information between x and c must be high. The mutual information between two variables X and Y is defined as I(X; Y) = entropy(X) - entropy(X|Y) = entropy(Y) - entropy(Y|X). If the mutual information between X and Y is high, then knowing Y helps you to decently predict the value of X (and the other way round). If the mutual information between X and Y is low, then knowing Y doesn't tell you much about the value of X (and the other way round). The new GAN loss becomes old loss - lambda * I(G(z, c); c), i.e. the higher the mutual information, the lower the result of the loss function. Variational Mutual Information Maximization  In order to minimize I(G(z, c); c), one has to know the distribution P(c|x) (from image to latent code), which however is unknown. So instead they create Q(c|x), which is an approximation of P(c|x). I(G(z, c); c) is then computed using a lower bound maximization, similar to the one in variational autoencoders (called \"Variational Information Maximization\", hence the name \"InfoGAN\"). Basic equation: LowerBoundOfMutualInformation(G, Q) = E[log Q(c|x)] + H(c) <= I(G(z, c); c)  c is the latent code. x is the generated image. H(c) is the entropy of the latent codes (constant throughout the optimization). Optimization w.r.t. Q is done directly. Optimization w.r.t. G is done via the reparameterization trick. If Q(c|x) approximates P(c|x) perfectly, the lower bound becomes the mutual information (\"the lower bound becomes tight\"). In practice, Q(c|x) is implemented as a neural network. Both Q and D have to process the generated images, which means that they can share many convolutional layers, significantly reducing the extra cost of training Q. Results  MNIST  They use for c one categorical variable (10 values) and two continuous ones (uniform between -1 and +1). InfoGAN learns to associate the categorical one with the digit identity and the continuous ones with rotation and width. Applying Q(c|x) to an image and then classifying only on the categorical variable (i.e. fully unsupervised) yields 95% accuracy. Sampling new images with exaggerated continuous variables in the range [-2,+2] yields sound images (i.e. the network generalizes well). 3D face images  InfoGAN learns to represent the faces via pose, elevation, lighting. They used five uniform variables for c. (So two of them apparently weren't associated with anything sensible? They are not mentioned.) 3D chair images  InfoGAN learns to represent the chairs via identity (categorical) and rotation or width (apparently they did two experiments). They used one categorical variable (four values) and one continuous variable (uniform [-1, +1]). SVHN  InfoGAN learns to represent lighting and to spot the center digit. They used four categorical variables (10 values each) and two continuous variables (uniform [-1, +1]). (Again, a few variables were apparently not associated with anything sensible?) CelebA  InfoGAN learns to represent pose, presence of sunglasses (not perfectly), hair style and emotion (in the sense of \"smiling or not smiling\"). They used 10 categorical variables (10 values each). (Again, a few variables were apparently not associated with anything sensible?)", "pdf_url": "https://arxiv.org/pdf/1606.03657", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/infogan.json"}
{"id": "31543932", "bin": "100_200", "summary_sentences": ["In terms of model based RL, learning dynamics models is imperfect, which often leads to the learned policy overfitting to the learned dynamics model, doing well in the learned simulator but not in the real world.", "Key solution idea: No need to try to learn one accurate simulator.", "We can learn an ensemble of models that together will sufficiently represent the space.", "If we learn an ensemble of models (to be used as many learned simulators) we can denoise estimates of performance.", "In a meta-learning sense these simulations become the tasks.", "The real world is then just yet another task, to which the policy could adapt quickly.", "One experimental observation is that at the start of training there is a lot of variation between learned simulators, and then the simulations come together over training, which might also point to this approach providing improved exploration.", "This summary was written with the help of Pieter Abbeel."], "summary_text": "In terms of model based RL, learning dynamics models is imperfect, which often leads to the learned policy overfitting to the learned dynamics model, doing well in the learned simulator but not in the real world. Key solution idea: No need to try to learn one accurate simulator. We can learn an ensemble of models that together will sufficiently represent the space. If we learn an ensemble of models (to be used as many learned simulators) we can denoise estimates of performance. In a meta-learning sense these simulations become the tasks. The real world is then just yet another task, to which the policy could adapt quickly. One experimental observation is that at the start of training there is a lot of variation between learned simulators, and then the simulations come together over training, which might also point to this approach providing improved exploration. This summary was written with the help of Pieter Abbeel.", "pdf_url": "http://proceedings.mlr.press/v87/clavera18a/clavera18a.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/claverars0aa18.json"}
{"id": "963444", "bin": "100_200", "summary_sentences": ["The goal is to compress a neural network based on figuring out the most significant neurons.", "They sample from Determinantal Point Process (DPP) in order to find set of neurons that have the most dissimilar activations and then project remaining neurons to them in order to reduce number of neurons overall.", "DPPs compute the probability of volume of dissimilarity over volume of all neurons:  $$P(\\text{subset } Y) = \\frac{det(L_Y)}{det(L+I)}$$   More dissimilarity means higher probability.", "A simple sample of the neurons outputs are taken given the training set."], "summary_text": "The goal is to compress a neural network based on figuring out the most significant neurons. They sample from Determinantal Point Process (DPP) in order to find set of neurons that have the most dissimilar activations and then project remaining neurons to them in order to reduce number of neurons overall. DPPs compute the probability of volume of dissimilarity over volume of all neurons:  $$P(\\text{subset } Y) = \\frac{det(L_Y)}{det(L+I)}$$   More dissimilarity means higher probability. A simple sample of the neurons outputs are taken given the training set.", "pdf_url": "http://arxiv.org/pdf/1511.05077", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/mariets15a.json"}
{"id": "61603670", "bin": "1100_1200", "summary_sentences": ["The Design and Implementation of the Wave Transactional Filesystem – Escriva & Sirer 2015  Since we’ve been looking at various combinations of storage and transactions, it seemed appropriate to start this week with the Wave Transactional Filesystem.", "Throughout the paper you’ll find this abbreviated as WTF, but my brain can’t read that without supplying the alternate meaning so I shall refer to it as Wave FS during this post except when quoting directly from the paper.", "Distributed filesystems are a cornerstone of modern data processing applications… Yet current distributed filesystems exhibit a tension between retaining the familiar semantics of local filesystems and achieving high performance in the distributed setting.", "Often designs will compromise consistency for performance, require special hardware, or artificially restrict the filesystem interface.", "The Wave Transactional Filesystem (Wave FS) is a distributed filesystem that provides a transactional model.", "A transaction can span multiple files and contain any combination of reads, writes, and seeks.", "The key to this is a file-slicing abstraction, on top of which both a traditional POSIX interface and a file-slice aware interface are provided.", "A broad evaluation shows that WTF achieves throughput and latency similar to industry-standard HDFS, while simultaneously offering stronger guarantees and a richer API.", "A sample application built with file slicing outperforms traditional approaches by a factor of four by reducing the overall I/O cost.", "The ability to make transactional changes to multiple files at scale is novel in the distributed systems space, and the file slicing APIs enable a new class of applications that are difficult to implement efficiently with current APIs.", "Together, these features are a potent combination that enables a new class of high performance applications.", "The basic idea is very easy to understand: file data is kept in immutable, arbitrarily-sized, byte-addressable, sequences of bytes called slices.", "Think of this like an old fashioned film reel containing a sequence of video frames.", "If a portion of the file is to be overwritten, the new bytes are written to a new slice (like recording a new take of a scene).", "We then go the the cutting room floor to splice the new slice into the original at the desired point.", "This is where the analogy breaks down a little, because Wave FS never changes the original bytes on disk.", "Instead information about the slices and splice-points is kept in metadata separate to the slices themselves.", "By reading the metadata and following the instructions there it is possible to reconstitute the current state of the file.", "A file in Wave FS is a sequence of slices and their associated offsets.", "A worked example should help to make this clear.", "Let’s consider the history of a file foo, which we happen to write/update in 1MB chunks.", "The first write creates a 2MB slice, ‘A’.", "Then we append 2MB to the end of the file by writing slice ‘B’ and updating the metadata.", "Another process overwrites 2MB in the center of the file:  The next update overwrites the third MB:  And so does the final update:  Compaction compresses the metadata, and garbage collection can later kick-in and delete slice D.  Thus we have immutable slices, and mutable metadata.", "This representation has some inherent advantages over block-based designs.", "Specifically, the abstraction provides a separation between metadata and data that enables filesystem-level transactions to be implemented using, solely, transactions over the metadata.", "Data is stored in the slices, while the metadata is a sequence of slices.", "WTF can transactionally change these sequences to change the files they represent, without having to rewrite the data.", "Custom storage servers hold filesystem data and handle the bulk of the I/O requests.", "They know nothing about the structure of the filesystem and treat all data as opaque slices.", "References to the slices and the metadata that describes how to reconstitute them into files is kept in HyperDex.", "The procedures for reading and writing follow directly from the abstraction.", "A writer creates one or more slices on the storage servers, and overlays them at the appropriate positions within the file by appending their slice pointers to the metadata list.", "Readers retrieve the metadata list, compact it, and determine which slices must be retrieved from the storage servers to fulfill the read.", "The correctness of this design relies upon the metadata storage providing primitives to atomically read and append to the list.", "HyperDex natively supports both of these operations.", "Because each writer writes slices before appending to the metadata list, it is guaranteed that any transaction that can see these immutable slices is serialized after the writing transaction commits.", "It can then retrieve the slices directly.", "The transactional guarantees of WTF extend directly from this design as well: a WTF transaction will execute a single HyperDex transaction consisting of multiple append and retrieve operations.", "To support arbitrarily large files and efficient operations on the list of pointers, partitions a file into fixed-size regions, each with its own list.", "Wave FS also implements transaction retry in its client library on top of HyperDex.", "This allows for example, an append operation to succeed even if the underlying file length has changed (the semantics of append depend on adding the bytes at the end, not at a certain index position).", "The slice-aware alternative API provides yank, paste and append calls that are analogous to read, write, and append but operate on slices instead of bytes.", "There is also a punch verb that zeros out bytes and frees the underlying storage, as well as concat and copy functions built on these primitives.", "Wave FS employs a locality-aware slice placement algorithm to improve disk locality for nearby file ranges, and uses replication to add a configurable degree of fault-tolerance.", "To accomplish this, it augments the metadata list such that each entry references multiple slice pointers that are replicas of the data.", "On the write path, writers create multiple replica slices and append their pointers atomically.", "Readers may read from any of the replicas, as they hold identical data.", "The metadata storage derives its fault tolerance from the strong guarantees offered by HyperDex."], "summary_text": "The Design and Implementation of the Wave Transactional Filesystem – Escriva & Sirer 2015  Since we’ve been looking at various combinations of storage and transactions, it seemed appropriate to start this week with the Wave Transactional Filesystem. Throughout the paper you’ll find this abbreviated as WTF, but my brain can’t read that without supplying the alternate meaning so I shall refer to it as Wave FS during this post except when quoting directly from the paper. Distributed filesystems are a cornerstone of modern data processing applications… Yet current distributed filesystems exhibit a tension between retaining the familiar semantics of local filesystems and achieving high performance in the distributed setting. Often designs will compromise consistency for performance, require special hardware, or artificially restrict the filesystem interface. The Wave Transactional Filesystem (Wave FS) is a distributed filesystem that provides a transactional model. A transaction can span multiple files and contain any combination of reads, writes, and seeks. The key to this is a file-slicing abstraction, on top of which both a traditional POSIX interface and a file-slice aware interface are provided. A broad evaluation shows that WTF achieves throughput and latency similar to industry-standard HDFS, while simultaneously offering stronger guarantees and a richer API. A sample application built with file slicing outperforms traditional approaches by a factor of four by reducing the overall I/O cost. The ability to make transactional changes to multiple files at scale is novel in the distributed systems space, and the file slicing APIs enable a new class of applications that are difficult to implement efficiently with current APIs. Together, these features are a potent combination that enables a new class of high performance applications. The basic idea is very easy to understand: file data is kept in immutable, arbitrarily-sized, byte-addressable, sequences of bytes called slices. Think of this like an old fashioned film reel containing a sequence of video frames. If a portion of the file is to be overwritten, the new bytes are written to a new slice (like recording a new take of a scene). We then go the the cutting room floor to splice the new slice into the original at the desired point. This is where the analogy breaks down a little, because Wave FS never changes the original bytes on disk. Instead information about the slices and splice-points is kept in metadata separate to the slices themselves. By reading the metadata and following the instructions there it is possible to reconstitute the current state of the file. A file in Wave FS is a sequence of slices and their associated offsets. A worked example should help to make this clear. Let’s consider the history of a file foo, which we happen to write/update in 1MB chunks. The first write creates a 2MB slice, ‘A’. Then we append 2MB to the end of the file by writing slice ‘B’ and updating the metadata. Another process overwrites 2MB in the center of the file:  The next update overwrites the third MB:  And so does the final update:  Compaction compresses the metadata, and garbage collection can later kick-in and delete slice D.  Thus we have immutable slices, and mutable metadata. This representation has some inherent advantages over block-based designs. Specifically, the abstraction provides a separation between metadata and data that enables filesystem-level transactions to be implemented using, solely, transactions over the metadata. Data is stored in the slices, while the metadata is a sequence of slices. WTF can transactionally change these sequences to change the files they represent, without having to rewrite the data. Custom storage servers hold filesystem data and handle the bulk of the I/O requests. They know nothing about the structure of the filesystem and treat all data as opaque slices. References to the slices and the metadata that describes how to reconstitute them into files is kept in HyperDex. The procedures for reading and writing follow directly from the abstraction. A writer creates one or more slices on the storage servers, and overlays them at the appropriate positions within the file by appending their slice pointers to the metadata list. Readers retrieve the metadata list, compact it, and determine which slices must be retrieved from the storage servers to fulfill the read. The correctness of this design relies upon the metadata storage providing primitives to atomically read and append to the list. HyperDex natively supports both of these operations. Because each writer writes slices before appending to the metadata list, it is guaranteed that any transaction that can see these immutable slices is serialized after the writing transaction commits. It can then retrieve the slices directly. The transactional guarantees of WTF extend directly from this design as well: a WTF transaction will execute a single HyperDex transaction consisting of multiple append and retrieve operations. To support arbitrarily large files and efficient operations on the list of pointers, partitions a file into fixed-size regions, each with its own list. Wave FS also implements transaction retry in its client library on top of HyperDex. This allows for example, an append operation to succeed even if the underlying file length has changed (the semantics of append depend on adding the bytes at the end, not at a certain index position). The slice-aware alternative API provides yank, paste and append calls that are analogous to read, write, and append but operate on slices instead of bytes. There is also a punch verb that zeros out bytes and frees the underlying storage, as well as concat and copy functions built on these primitives. Wave FS employs a locality-aware slice placement algorithm to improve disk locality for nearby file ranges, and uses replication to add a configurable degree of fault-tolerance. To accomplish this, it augments the metadata list such that each entry references multiple slice pointers that are replicas of the data. On the write path, writers create multiple replica slices and append their pointers atomically. Readers may read from any of the replicas, as they hold identical data. The metadata storage derives its fault tolerance from the strong guarantees offered by HyperDex.", "pdf_url": "http://arxiv.org/pdf/1509.07821v1.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/wavefs.json"}
{"id": "45612165", "bin": "1100_1200", "summary_sentences": ["This paper proposes a variant of Neural Turing Machine (NTM) for meta-learning or \"learning to learn\", in the specific context of few-shot learning (i.e. learning from few examples).", "Specifically, the proposed model is trained to ingest as input a training set of examples and improve its output predictions as examples are processed, in a purely feed-forward way.", "This is a form of meta-learning because the model is trained so that its forward pass effectively executes a form of \"learning\" from the examples it is fed as input.", "During training, the model is fed multiples sequences (referred to as episodes) of labeled examples $({\\bf x}_1, {\\rm null}), ({\\bf x}_2, y_1), \\dots, ({\\bf x}_T, y_{T-1})$, where $T$ is the size of the episode.", "For instance, if the model is trained to learn how to do 5-class classification from 10 examples per class, $T$ would be $5 \\times 10 = 50$.", "Mainly, the paper presents experiments on the Omniglot dataset, which has 1623 classes.", "In these experiments, classes are separated into 1200 \"training classes\" and 423 \"test classes\", and each episode is generated by randomly selecting 5 classes (each assigned some arbitrary vector representation, e.g. a  one-hot vector that is consistent within the episode, but not across episodes) and constructing a randomly ordered sequence of 50 examples from within the chosen 5 classes.", "Moreover, the correct label $y_t$ of a given input ${\\bf x}_t$ is always provided only at the next time step, but the model is trained to be good at its prediction of the label of ${\\bf x}_t$ at the current time step.", "This is akin to the scenario of online learning on a stream of examples, where the label of an example is revealed only once the model has made a prediction.", "The proposed NTM is different from the original NTM of Alex Graves, mostly in how it writes into its memory.", "The authors propose to focus writing to either the least recently used memory location or the most recently used memory location.", "Moreover, the least recently used memory location is reset to zero before every write (an operation that seems to be ignored when backpropagating gradients).", "Intuitively, the proposed NTM should learn a strategy by which, given a new input, it looks into its memory for information from other examples earlier in the episode (perhaps similarly to what a nearest neighbor classifier would do) to predict the class of the new input.", "The paper presents experiments in learning to do multiclass classification on the Omniglot dataset and regression based on functions synthetically generated by a GP.", "The highlights are that:  1.", "The proposed model performs much better than an LSTM and better than an NTM with the original write mechanism of Alex Graves (for classification).", "2.", "The proposed model even performs better than a 1st nearest neighbor classifier.", "3.", "The proposed model is even shown to outperform human performance, for the 5-class scenario.", "4.", "The proposed model has decent performance on the regression task, compared to GP predictions using the groundtruth kernel.", "**My two cents**  This is probably one of my favorite ICML 2016 papers.", "I really think meta-learning is a problem that deserves more attention, and this paper presents both an interesting proposal for how to do it and an interesting empirical investigation of it.", "Much like previous work [\\[1\\]][1] [\\[2\\]][2], learning is based on automatically generating a meta-learning training set.", "This is clever I think, since a very large number of such \"meta-learning\" examples (the episodes) can be constructed, thus transforming what is normally a \"small data problem\" (few shot learning) into a \"big data problem\", for which deep learning is more effective.", "I'm particularly impressed by how the proposed model outperforms a 1-nearest neighbor classifier.", "That said, the proposed NTM actually performs 4 reads at each time step, which suggests that a fairer comparison might be with a 4-nearest neighbor classifier.", "I do wonder how this baseline would compare.", "I'm also impressed with the observation that the proposed model surpassed humans.", "The paper also proposes to use 5-letter words to describe classes, instead of one-hot vectors.", "The motivation is that this should make it easier for the model to scale to much more than 5 classes.", "However, I don't entirely follow the logic as to why one-hot vectors are problematic.", "In fact, I would think that arbitrarily assigning 5-letter words to classes would instead imply some similarity between classes that share letters that is arbitrary and doesn't reflect true class similarity.", "Also, while I find it encouraging that the performance for regression of the proposed model is decent, I'm curious about how it would compare with a GP approach that incrementally learns the kernel's hyper-parameter (instead of using the groundtruth values, which makes this baseline unrealistically strong).", "Finally, I'm still not 100% sure how exactly the NTM is able to implement the type of feed-forward inference I'd expect to be required.", "I would expect it to learn a memory representation of examples that combines information from the input vector ${\\bf x}_t$ *and* its label $y_t$.", "However, since the label of an input is presented at the following time step in an episode, it is not intuitive to me then how the read/write mechanisms are able to deal with this misalignment.", "My only guess is that since the controller is an LSTM, then it can somehow remember ${\\bf x}_t$ until it gets $y_t$ and appropriately include the combined information into the memory.", "This could be supported by the fact that using a non-recurrent feed-forward controller is much worse than using an LSTM controller.", "But I'm not 100% sure of this either.", "All the above being said, this is still a really great paper, which I hope will help stimulate more research on meta-learning.", "Hopefully code for this paper can eventually be released, which would help in popularizing the topic.", "[1]:  [url]"], "summary_text": "This paper proposes a variant of Neural Turing Machine (NTM) for meta-learning or \"learning to learn\", in the specific context of few-shot learning (i.e. learning from few examples). Specifically, the proposed model is trained to ingest as input a training set of examples and improve its output predictions as examples are processed, in a purely feed-forward way. This is a form of meta-learning because the model is trained so that its forward pass effectively executes a form of \"learning\" from the examples it is fed as input. During training, the model is fed multiples sequences (referred to as episodes) of labeled examples $({\\bf x}_1, {\\rm null}), ({\\bf x}_2, y_1), \\dots, ({\\bf x}_T, y_{T-1})$, where $T$ is the size of the episode. For instance, if the model is trained to learn how to do 5-class classification from 10 examples per class, $T$ would be $5 \\times 10 = 50$. Mainly, the paper presents experiments on the Omniglot dataset, which has 1623 classes. In these experiments, classes are separated into 1200 \"training classes\" and 423 \"test classes\", and each episode is generated by randomly selecting 5 classes (each assigned some arbitrary vector representation, e.g. a  one-hot vector that is consistent within the episode, but not across episodes) and constructing a randomly ordered sequence of 50 examples from within the chosen 5 classes. Moreover, the correct label $y_t$ of a given input ${\\bf x}_t$ is always provided only at the next time step, but the model is trained to be good at its prediction of the label of ${\\bf x}_t$ at the current time step. This is akin to the scenario of online learning on a stream of examples, where the label of an example is revealed only once the model has made a prediction. The proposed NTM is different from the original NTM of Alex Graves, mostly in how it writes into its memory. The authors propose to focus writing to either the least recently used memory location or the most recently used memory location. Moreover, the least recently used memory location is reset to zero before every write (an operation that seems to be ignored when backpropagating gradients). Intuitively, the proposed NTM should learn a strategy by which, given a new input, it looks into its memory for information from other examples earlier in the episode (perhaps similarly to what a nearest neighbor classifier would do) to predict the class of the new input. The paper presents experiments in learning to do multiclass classification on the Omniglot dataset and regression based on functions synthetically generated by a GP. The highlights are that:  1. The proposed model performs much better than an LSTM and better than an NTM with the original write mechanism of Alex Graves (for classification). 2. The proposed model even performs better than a 1st nearest neighbor classifier. 3. The proposed model is even shown to outperform human performance, for the 5-class scenario. 4. The proposed model has decent performance on the regression task, compared to GP predictions using the groundtruth kernel. **My two cents**  This is probably one of my favorite ICML 2016 papers. I really think meta-learning is a problem that deserves more attention, and this paper presents both an interesting proposal for how to do it and an interesting empirical investigation of it. Much like previous work [\\[1\\]][1] [\\[2\\]][2], learning is based on automatically generating a meta-learning training set. This is clever I think, since a very large number of such \"meta-learning\" examples (the episodes) can be constructed, thus transforming what is normally a \"small data problem\" (few shot learning) into a \"big data problem\", for which deep learning is more effective. I'm particularly impressed by how the proposed model outperforms a 1-nearest neighbor classifier. That said, the proposed NTM actually performs 4 reads at each time step, which suggests that a fairer comparison might be with a 4-nearest neighbor classifier. I do wonder how this baseline would compare. I'm also impressed with the observation that the proposed model surpassed humans. The paper also proposes to use 5-letter words to describe classes, instead of one-hot vectors. The motivation is that this should make it easier for the model to scale to much more than 5 classes. However, I don't entirely follow the logic as to why one-hot vectors are problematic. In fact, I would think that arbitrarily assigning 5-letter words to classes would instead imply some similarity between classes that share letters that is arbitrary and doesn't reflect true class similarity. Also, while I find it encouraging that the performance for regression of the proposed model is decent, I'm curious about how it would compare with a GP approach that incrementally learns the kernel's hyper-parameter (instead of using the groundtruth values, which makes this baseline unrealistically strong). Finally, I'm still not 100% sure how exactly the NTM is able to implement the type of feed-forward inference I'd expect to be required. I would expect it to learn a memory representation of examples that combines information from the input vector ${\\bf x}_t$ *and* its label $y_t$. However, since the label of an input is presented at the following time step in an episode, it is not intuitive to me then how the read/write mechanisms are able to deal with this misalignment. My only guess is that since the controller is an LSTM, then it can somehow remember ${\\bf x}_t$ until it gets $y_t$ and appropriately include the combined information into the memory. This could be supported by the fact that using a non-recurrent feed-forward controller is much worse than using an LSTM controller. But I'm not 100% sure of this either. All the above being said, this is still a really great paper, which I hope will help stimulate more research on meta-learning. Hopefully code for this paper can eventually be released, which would help in popularizing the topic. [1]:  [url]", "pdf_url": "http://arxiv.org/pdf/1605.06065v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/1605.06065.json"}
{"id": "73180675", "bin": "1200_1300", "summary_sentences": ["What  The paper describes a method to separate content and style from each other in an image.", "The style can then be transfered to a new image.", "Examples:  Let a photograph look like a painting of van Gogh.", "Improve a dark beach photo by taking the style from a sunny beach photo.", "How  They use the pretrained 19-layer VGG net as their base network.", "They assume that two images are provided: One with the content, one with the desired style.", "They feed the content image through the VGG net and extract the activations of the last convolutional layer.", "These activations are called the content representation.", "They feed the style image through the VGG net and extract the activations of all convolutional layers.", "They transform each layer to a Gram Matrix representation.", "These Gram Matrices are called the style representation.", "How to calculate a Gram Matrix:  Take the activations of a layer.", "That layer will contain some convolution filters (e.g. 128), each one having its own activations.", "Convert each filter's activations to a (1-dimensional) vector.", "Pick all pairs of filters.", "Calculate the scalar product of both filter's vectors.", "Add the scalar product result as an entry to a matrix of size #filters x #filters (e.g. 128x128).", "Repeat that for every pair to get the Gram Matrix.", "The Gram Matrix roughly represents the texture of the image.", "Now you have the content representation (activations of a layer) and the style representation (Gram Matrices).", "Create a new image of the size of the content image.", "Fill it with random white noise.", "Feed that image through VGG to get its content representation and style representation.", "(This step will be repeated many times during the image creation.)", "Make changes to the new image using gradient descent to optimize a loss function.", "The loss function has two components:  The mean squared error between the new image's content representation and the previously extracted content representation.", "The mean squared error between the new image's style representation and the previously extracted style representation.", "Add up both components to get the total loss.", "Give both components a weight to alter for more/less style matching (at the expense of content matching).", "One example input image with different styles added to it.", "Rough chapter-wise notes  Page 1  A painted image can be decomposed in its content and its artistic style.", "Here they use a neural network to separate content and style from each other (and to apply that style to an existing image).", "Page 2  Representations get more abstract as you go deeper in networks, hence they should more resemble the actual content (as opposed to the artistic style).", "They call the feature responses in higher layers content representation.", "To capture style information, they use a method that was originally designed to capture texture information.", "They somehow build a feature space on top of the existing one, that is somehow dependent on correlations of features.", "That leads to a \"stationary\" (?)", "and multi-scale representation of the style.", "Page 3  They use VGG as their base CNN.", "Page 4  Based on the extracted style features, they can generate a new image, which has equal activations in these style features.", "The new image should match the style (texture, color, localized structures) of the artistic image.", "The style features become more and more abtstract with higher layers.", "They call that multi-scale the style representation.", "The key contribution of the paper is a method to separate style and content representation from each other.", "These representations can then be used to change the style of an existing image (by changing it so that its content representation stays the same, but its style representation matches the artwork).", "Page 6  The generated images look most appealing if all features from the style representation are used.", "(The lower layers tend to reflect small features, the higher layers tend to reflect larger features.)", "Content and style can't be separated perfectly.", "Their loss function has two terms, one for content matching and one for style matching.", "The terms can be increased/decreased to match content or style more.", "Page 8  Previous techniques work only on limited or simple domains or used non-parametric approaches (see non-photorealistic rendering).", "Previously neural networks have been used to classify the time period of paintings (based on their style).", "They argue that separating content from style might be useful and many other domains (other than transfering style of paintings to images).", "Page 9  The style representation is gathered by measuring correlations between activations of neurons.", "They argue that this is somehow similar to what \"complex cells\" in the primary visual system (V1) do.", "They note that deep convnets seem to automatically learn to separate content from style, probably because it is helpful for style-invariant classification.", "Page 9, Methods  They use the 19 layer VGG net as their basis.", "They use only its convolutional layers, not the linear ones.", "They use average pooling instead of max pooling, as that produced slightly better results.", "Page 10, Methods  The information about the image that is contained in layers can be visualized.", "To do that, extract the features of a layer as the labels, then start with a white noise image and change it via gradient descent until the generated features have minimal distance (MSE) to the extracted features.", "The build a style representation by calculating Gram Matrices for each layer.", "Page 11, Methods  The Gram Matrix is generated in the following way:  Convert each filter of a convolutional layer to a 1-dimensional vector.", "For a pair of filters i, j calculate the value in the Gram Matrix by calculating the scalar product of the two vectors of the filters.", "Do that for every pair of filters, generating a matrix of size #filters x #filters.", "That is the Gram Matrix.", "Again, a white noise image can be changed with gradient descent to match the style of a given image (i.e. minimize MSE between two Gram Matrices).", "That can be extended to match the style of several layers by measuring the MSE of the Gram Matrices of each layer and giving each layer a weighting.", "Page 12, Methods  To transfer the style of a painting to an existing image, proceed as follows:  Start with a white noise image.", "Optimize that image with gradient descent so that it minimizes both the content loss (relative to the image) and the style loss (relative to the painting).", "Each distance (content, style) can be weighted to have more or less influence on the loss function."], "summary_text": "What  The paper describes a method to separate content and style from each other in an image. The style can then be transfered to a new image. Examples:  Let a photograph look like a painting of van Gogh. Improve a dark beach photo by taking the style from a sunny beach photo. How  They use the pretrained 19-layer VGG net as their base network. They assume that two images are provided: One with the content, one with the desired style. They feed the content image through the VGG net and extract the activations of the last convolutional layer. These activations are called the content representation. They feed the style image through the VGG net and extract the activations of all convolutional layers. They transform each layer to a Gram Matrix representation. These Gram Matrices are called the style representation. How to calculate a Gram Matrix:  Take the activations of a layer. That layer will contain some convolution filters (e.g. 128), each one having its own activations. Convert each filter's activations to a (1-dimensional) vector. Pick all pairs of filters. Calculate the scalar product of both filter's vectors. Add the scalar product result as an entry to a matrix of size #filters x #filters (e.g. 128x128). Repeat that for every pair to get the Gram Matrix. The Gram Matrix roughly represents the texture of the image. Now you have the content representation (activations of a layer) and the style representation (Gram Matrices). Create a new image of the size of the content image. Fill it with random white noise. Feed that image through VGG to get its content representation and style representation. (This step will be repeated many times during the image creation.) Make changes to the new image using gradient descent to optimize a loss function. The loss function has two components:  The mean squared error between the new image's content representation and the previously extracted content representation. The mean squared error between the new image's style representation and the previously extracted style representation. Add up both components to get the total loss. Give both components a weight to alter for more/less style matching (at the expense of content matching). One example input image with different styles added to it. Rough chapter-wise notes  Page 1  A painted image can be decomposed in its content and its artistic style. Here they use a neural network to separate content and style from each other (and to apply that style to an existing image). Page 2  Representations get more abstract as you go deeper in networks, hence they should more resemble the actual content (as opposed to the artistic style). They call the feature responses in higher layers content representation. To capture style information, they use a method that was originally designed to capture texture information. They somehow build a feature space on top of the existing one, that is somehow dependent on correlations of features. That leads to a \"stationary\" (?) and multi-scale representation of the style. Page 3  They use VGG as their base CNN. Page 4  Based on the extracted style features, they can generate a new image, which has equal activations in these style features. The new image should match the style (texture, color, localized structures) of the artistic image. The style features become more and more abtstract with higher layers. They call that multi-scale the style representation. The key contribution of the paper is a method to separate style and content representation from each other. These representations can then be used to change the style of an existing image (by changing it so that its content representation stays the same, but its style representation matches the artwork). Page 6  The generated images look most appealing if all features from the style representation are used. (The lower layers tend to reflect small features, the higher layers tend to reflect larger features.) Content and style can't be separated perfectly. Their loss function has two terms, one for content matching and one for style matching. The terms can be increased/decreased to match content or style more. Page 8  Previous techniques work only on limited or simple domains or used non-parametric approaches (see non-photorealistic rendering). Previously neural networks have been used to classify the time period of paintings (based on their style). They argue that separating content from style might be useful and many other domains (other than transfering style of paintings to images). Page 9  The style representation is gathered by measuring correlations between activations of neurons. They argue that this is somehow similar to what \"complex cells\" in the primary visual system (V1) do. They note that deep convnets seem to automatically learn to separate content from style, probably because it is helpful for style-invariant classification. Page 9, Methods  They use the 19 layer VGG net as their basis. They use only its convolutional layers, not the linear ones. They use average pooling instead of max pooling, as that produced slightly better results. Page 10, Methods  The information about the image that is contained in layers can be visualized. To do that, extract the features of a layer as the labels, then start with a white noise image and change it via gradient descent until the generated features have minimal distance (MSE) to the extracted features. The build a style representation by calculating Gram Matrices for each layer. Page 11, Methods  The Gram Matrix is generated in the following way:  Convert each filter of a convolutional layer to a 1-dimensional vector. For a pair of filters i, j calculate the value in the Gram Matrix by calculating the scalar product of the two vectors of the filters. Do that for every pair of filters, generating a matrix of size #filters x #filters. That is the Gram Matrix. Again, a white noise image can be changed with gradient descent to match the style of a given image (i.e. minimize MSE between two Gram Matrices). That can be extended to match the style of several layers by measuring the MSE of the Gram Matrices of each layer and giving each layer a weighting. Page 12, Methods  To transfer the style of a painting to an existing image, proceed as follows:  Start with a white noise image. Optimize that image with gradient descent so that it minimizes both the content loss (relative to the image) and the style loss (relative to the painting). Each distance (content, style) can be weighted to have more or less influence on the loss function.", "pdf_url": "http://arxiv.org/pdf/1508.06576", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/a_neural_algorithm_for_artistic_style.json"}
{"id": "19173630", "bin": "1200_1300", "summary_sentences": ["What  GANs are based on adversarial training.", "Adversarial training is a basic technique to train generative models (so here primarily models that create new images).", "In an adversarial training one model (G, Generator) generates things (e.g. images).", "Another model (D, discriminator) sees real things (e.g. real images) as well as fake things (e.g.", "images from G) and has to learn how to differentiate the two.", "Neural Networks are models that can be trained in an adversarial way (and are the only models discussed here).", "How  G is a simple neural net (e.g. just one fully connected hidden layer).", "It takes a vector as input (e.g. 100 dimensions) and produces an image as output.", "D is a simple neural net (e.g. just one fully connected hidden layer).", "It takes an image as input and produces a quality rating as output (0-1, so sigmoid).", "You need a training set of things to be generated, e.g. images of human faces.", "Let the batch size be B.  G is trained the following way:  Create B vectors of 100 random values each, e.g. sampled uniformly from [-1, +1].", "(Number of values per components depends on the chosen input size of G.)  Feed forward the vectors through G to create new images.", "Feed forward the images through D to create ratings.", "Use a cross entropy loss on these ratings.", "All of these (fake) images should be viewed as label=0 by D. If D gives them label=1, the error will be low (G did a good job).", "Perform a backward pass of the errors through D (without training D).", "That generates gradients/errors per image and pixel.", "Perform a backward pass of these errors through G to train G.  D is trained the following way:  Create B/2 images using G (again, B/2 random vectors, feed forward through G).", "Chose B/2 images from the training set.", "Real images get label=1.", "Merge the fake and real images to one batch.", "Fake images get label=0.", "Feed forward the batch through D.  Measure the error using cross entropy.", "Perform a backward pass with the error through D.  Train G for one batch, then D for one (or more) batches.", "Sometimes D can be too slow to catch up with D, then you need more iterations of D per batch of G.  Results  Good looking images MNIST-numbers and human faces.", "(Grayscale, rather homogeneous datasets.)", "Not so good looking images of CIFAR-10.", "(Color, rather heterogeneous datasets.)", "Faces generated by MLP GANs.", "(Rightmost column shows examples from the training set.)", "Rough chapter-wise notes  Introduction  Discriminative models performed well so far, generative models not so much.", "Their suggested new architecture involves a generator and a discriminator.", "The generator learns to create content (e.g. images), the discriminator learns to differentiate between real content and generated content.", "Analogy: Generator produces counterfeit art, discriminator's job is to judge whether a piece of art is a counterfeit.", "This principle could be used with many techniques, but they use neural nets (MLPs) for both the generator as well as the discriminator.", "Adversarial Nets  They have a Generator G (simple neural net)  G takes a random vector as input (e.g. vector of 100 random values between -1 and +1).", "G creates an image as output.", "They have a Discriminator D (simple neural net)  D takes an image as input (can be real or generated by G).", "D creates a rating as output (quality, i.e. a value between 0 and 1, where 0 means \"probably fake\").", "Outputs from G are fed into D. The result can then be backpropagated through D and then G. G is trained to maximize log(D(image)), so to create a high value of D(image).", "D is trained to produce only 1s for images from G.  Both are trained simultaneously, i.e. one batch for G, then one batch for D, then one batch for G...  D can also be trained multiple times in a row.", "That allows it to catch up with G.  Theoretical Results  Let  pd(x): Probability that image x appears in the training set.", "pg(x): Probability that image x appears in the images generated by G.  If G is now fixed then the best possible D classifies according to: D(x) = pd(x) / (pd(x) + pg(x))  It is proofable that there is only one global optimum for GANs, which is reached when G perfectly replicates the training set probability distribution.", "(Assuming unlimited capacity of the models and unlimited training time.)", "It is proofable that G and D will converge to the global optimum, so long as D gets enough steps per training iteration to model the distribution generated by G. (Again, assuming unlimited capacity/time.)", "Note that these things are proofed for the general principle for GANs.", "Implementing GANs with neural nets can then introduce problems typical for neural nets (e.g. getting stuck in saddle points).", "Experiments  They tested on MNIST, Toronto Face Database (TFD) and CIFAR-10.", "They used MLPs for G and D.  G contained ReLUs and Sigmoids.", "D contained Maxouts.", "D had Dropout, G didn't.", "They use a Parzen Window Estimate aka KDE (sigma obtained via cross validation) to estimate the quality of their images.", "They note that KDE is not really a great technique for such high dimensional spaces, but its the only one known.", "Results on MNIST and TDF are great.", "(Note: both grayscale)  CIFAR-10 seems to match more the texture but not really the structure.", "Noise is noticeable in CIFAR-10 (a bit in TFD too).", "Comes from MLPs (no convolutions).", "Their KDE score for MNIST and TFD is competitive or better than other approaches.", "Advantages and Disadvantages  Advantages  No Markov Chains, only backprob  Inference-free training  Wide variety of functions can be incorporated into the model (?)", "Generator never sees any real example.", "It only gets gradients.", "(Prevents overfitting?)", "Can represent a wide variety of distributions, including sharp ones (Markov chains only work with blurry images).", "Disadvantages  No explicit representation of the distribution modeled by G (?)", "D and G must be well synchronized during training  If G is trained to much (i.e. D can't catch up), it can collapse many components of the random input vectors to the same output (\"Helvetica\")"], "summary_text": "What  GANs are based on adversarial training. Adversarial training is a basic technique to train generative models (so here primarily models that create new images). In an adversarial training one model (G, Generator) generates things (e.g. images). Another model (D, discriminator) sees real things (e.g. real images) as well as fake things (e.g. images from G) and has to learn how to differentiate the two. Neural Networks are models that can be trained in an adversarial way (and are the only models discussed here). How  G is a simple neural net (e.g. just one fully connected hidden layer). It takes a vector as input (e.g. 100 dimensions) and produces an image as output. D is a simple neural net (e.g. just one fully connected hidden layer). It takes an image as input and produces a quality rating as output (0-1, so sigmoid). You need a training set of things to be generated, e.g. images of human faces. Let the batch size be B.  G is trained the following way:  Create B vectors of 100 random values each, e.g. sampled uniformly from [-1, +1]. (Number of values per components depends on the chosen input size of G.)  Feed forward the vectors through G to create new images. Feed forward the images through D to create ratings. Use a cross entropy loss on these ratings. All of these (fake) images should be viewed as label=0 by D. If D gives them label=1, the error will be low (G did a good job). Perform a backward pass of the errors through D (without training D). That generates gradients/errors per image and pixel. Perform a backward pass of these errors through G to train G.  D is trained the following way:  Create B/2 images using G (again, B/2 random vectors, feed forward through G). Chose B/2 images from the training set. Real images get label=1. Merge the fake and real images to one batch. Fake images get label=0. Feed forward the batch through D.  Measure the error using cross entropy. Perform a backward pass with the error through D.  Train G for one batch, then D for one (or more) batches. Sometimes D can be too slow to catch up with D, then you need more iterations of D per batch of G.  Results  Good looking images MNIST-numbers and human faces. (Grayscale, rather homogeneous datasets.) Not so good looking images of CIFAR-10. (Color, rather heterogeneous datasets.) Faces generated by MLP GANs. (Rightmost column shows examples from the training set.) Rough chapter-wise notes  Introduction  Discriminative models performed well so far, generative models not so much. Their suggested new architecture involves a generator and a discriminator. The generator learns to create content (e.g. images), the discriminator learns to differentiate between real content and generated content. Analogy: Generator produces counterfeit art, discriminator's job is to judge whether a piece of art is a counterfeit. This principle could be used with many techniques, but they use neural nets (MLPs) for both the generator as well as the discriminator. Adversarial Nets  They have a Generator G (simple neural net)  G takes a random vector as input (e.g. vector of 100 random values between -1 and +1). G creates an image as output. They have a Discriminator D (simple neural net)  D takes an image as input (can be real or generated by G). D creates a rating as output (quality, i.e. a value between 0 and 1, where 0 means \"probably fake\"). Outputs from G are fed into D. The result can then be backpropagated through D and then G. G is trained to maximize log(D(image)), so to create a high value of D(image). D is trained to produce only 1s for images from G.  Both are trained simultaneously, i.e. one batch for G, then one batch for D, then one batch for G...  D can also be trained multiple times in a row. That allows it to catch up with G.  Theoretical Results  Let  pd(x): Probability that image x appears in the training set. pg(x): Probability that image x appears in the images generated by G.  If G is now fixed then the best possible D classifies according to: D(x) = pd(x) / (pd(x) + pg(x))  It is proofable that there is only one global optimum for GANs, which is reached when G perfectly replicates the training set probability distribution. (Assuming unlimited capacity of the models and unlimited training time.) It is proofable that G and D will converge to the global optimum, so long as D gets enough steps per training iteration to model the distribution generated by G. (Again, assuming unlimited capacity/time.) Note that these things are proofed for the general principle for GANs. Implementing GANs with neural nets can then introduce problems typical for neural nets (e.g. getting stuck in saddle points). Experiments  They tested on MNIST, Toronto Face Database (TFD) and CIFAR-10. They used MLPs for G and D.  G contained ReLUs and Sigmoids. D contained Maxouts. D had Dropout, G didn't. They use a Parzen Window Estimate aka KDE (sigma obtained via cross validation) to estimate the quality of their images. They note that KDE is not really a great technique for such high dimensional spaces, but its the only one known. Results on MNIST and TDF are great. (Note: both grayscale)  CIFAR-10 seems to match more the texture but not really the structure. Noise is noticeable in CIFAR-10 (a bit in TFD too). Comes from MLPs (no convolutions). Their KDE score for MNIST and TFD is competitive or better than other approaches. Advantages and Disadvantages  Advantages  No Markov Chains, only backprob  Inference-free training  Wide variety of functions can be incorporated into the model (?) Generator never sees any real example. It only gets gradients. (Prevents overfitting?) Can represent a wide variety of distributions, including sharp ones (Markov chains only work with blurry images). Disadvantages  No explicit representation of the distribution modeled by G (?) D and G must be well synchronized during training  If G is trained to much (i.e. D can't catch up), it can collapse many components of the random input vectors to the same output (\"Helvetica\")", "pdf_url": "http://arxiv.org/pdf/1406.2661", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/generative_adversarial_networks.json"}
{"id": "34958521", "bin": "1300_1400", "summary_sentences": ["On the information bottleneck theory of deep learning Anonymous et al., ICLR’18 submission  Last week we looked at the Information bottleneck theory of deep learning paper from Schwartz-Viz & Tishby ( Part I , Part II ).", "I really enjoyed that paper and the different light it shed on what’s happening inside deep neural networks.", "Sathiya Keerthi got in touch with me to share today’s paper, a blind submission to ICLR’18, in which the authors conduct a critical analysis of some of the information bottleneck theory findings.", "It’s an important update pointing out some of the limitations of the approach.", "Sathiya gave a recent talk summarising results on understanding optimisation and generalisation, ‘ Interplay between Optimization and Generalization in DNNs ,’ which is well worth checking out if this topic interests you.", "Definitely some more papers there that are going on my backlog to help increase my own understanding!", "Let’s get back to today’s paper!", "The authors start out by reproducing the information plane dynamics from the Schwartz-Viz & Tishby paper, and then go on to conduct further experiments: replacing the tanh activation with ReLU to see what impact that has; exploring the link between generalisation and compression; investigating whether the randomness is important to compression during training; and studying the extent to which task-irrelevant information is also compressed.", "The short version of their findings is that the results reported by Schwartz-Viz and Tishby don’t seem to generalise well to other network architectures: the two phases seen during training depend on the choice of activation function; there is no evidence of a causal connection between compression and generalisation; and that when compression does occur, it is not necessarily dependent on randomness from SGD.", "Our results highlight the importance of noise assumptions in applying information theoretic analyses to deep learning systems, and complicate the IB theory of deep learning by demonstrating instances where representation compression and generalization performance can diverge.", "The quest for deeper understanding continues!", "The impact of activation function choice  The starting point for our analysis is the observation that changing the activation function can markedly change the trajectory of a network in the information plane.", "The authors used code supplied by Schwartz-Vis and Tishby to first replicate the results that we saw last week (Fig 1A below), and then changed the network to use ReLU instead — rectified linear activation functions  .", "The resulting information plane dynamics are show in Fig 1B.", "The phase shift that we saw with the original tanh activation functions disappears!", "The mutual information with the input monotonically increases in all ReLu layers, with no apparent compression phase.", "Thus, the choice of nonlinearity substantively affects the dynamics in the information plane.", "Using a very simple three neuron network, the authors explore this phenomenon further.", "A scalar Gaussian input distribution  is fed through a scalar first layer weight  and passed through a neural nonlinearity  to yield hidden unit activity  .", "In order to calculate the mutual information, the hidden unit activity  is binned into 30 uniform bins, to yield the discrete variable  .", "With the tanh nonlinearity, mutual information first increases and then decreases.", "With the ReLU nonlinearity it always increases.", "What’s happening is that with large weights, the tanh function saturates, falling back to providing mutual information with the input of approximately 1 bit (i.e, the discrete variable concentrates in just two bins around 1 and -1).", "With the ReLU though, half of the inputs are negative and land in the bin around 0, but the other half are Gaussian distributed and have entropy that increases with the size of weight.", "So it turns out that this double saturating nature of tanh is central to the original results.", "… double-saturating nonlinearities can lead to compression of information about the input, as hidden units enter their saturation regime, due to the binning procedure used to calculate mutual information.", "We note that this binning procedure can be viewed as implicitly adding noise to the hidden layer activity: a range of X values map to a single bin, such that the mapping between X and T is no longer perfectly invertible.", "The binning procedure is crucial for the information theoretic analysis, “however, this noise is not added in practice either during training or testing in these neural networks.”  The saturation of tanh explains the presence of the compression period where mutual information decreases, and also explains why training slows down as tanh networks enter their compression phase: some fraction of inputs have saturated the nonlinearities, reducing backpropagated error gradients.", "Generalisation independent of compression  Next the authors use the information plane lens to further study the relationship between compression and generalisation.", "… we exploit recent results on the generalization dynamics in simple linear networks trained in an student-teacher setup (Seung et al., 1992; Advani & Saxe, 2017).", "This setting allows exact calculation of the generalization performance of the network, exact calculation of the mutual information of the representation (without any binning procedure), and, though we do not do so here, direct comparison to the IB bound which is already known for linear Gaussian problems.", "No compression is observed in the information plane (panel D in the figure above), although the network does learn a map that generalise well on the task and shows minimal overtraining.", "Experimentation to force varying degrees of overfitting shows networks with similar behaviour in the information plane can nevertheless have differing generalisation performance.", "This establishes a dissociation between behavior in the information plane and generalization dynamics: networks that compress may or may not generalize well, and that networks that do not compress may or may not generalize well.", "Does randomness help compression?", "Next the authors investigate what contributes to compression in the first place, looking at the differences in the information plane between stochastic gradient descent and batch gradient descent.", "Whereas SGD takes a sample from the dataset and calculates the error gradient with respect to it, batch gradient descent uses the total error across all examples — “and crucially, therefore has no randomness or diffusion-like behaviour in its updates.”  Both tanh and linear networks are trained with both SGD and BGD, and the resulting information plane dynamics look like this:  We find largely consistent information dynamics in both instances, with robust compression in tanh networks for both methods.", "Thus randomness in the training process does not appear to contributed substantially to compression of information about the input.", "This finding is consistent with the view presented in §2 that compression arises predominantly from the double saturating nonlinearity.", "(Which seems to pretty much rule out the hope from the Schwartz-Viz & Tishby paper that we would find alternatives to SGD that support better diffusion and faster training).", "The compression of task-irrelevant information  A final experiment partitions the input X into a set of task-relevant inputs and a known to be task_irrelevant_ inputs.", "The former contribute signal therefore, while the latter only contribute noise.", "Thus good generalisation would seem to require ignoring the noise.", "The authors found that information for the task-irrelevant subspace does compress, at the same time as fitting is occurring for the task-relevant information, even though overall there is no observable compression phase.", "The bottom line  Our results suggest that compression dynamics in the information plane are not a general feature of deep networks, but are critically influenced by the nonlinearities employed by the network… information compression may parallel the situation with sharp minima; although empirical evidence has shown a correlation with generalization error in certain settings and architectures; further theoretical analysis has shown that sharp minima can in fact generalize well."], "summary_text": "On the information bottleneck theory of deep learning Anonymous et al., ICLR’18 submission  Last week we looked at the Information bottleneck theory of deep learning paper from Schwartz-Viz & Tishby ( Part I , Part II ). I really enjoyed that paper and the different light it shed on what’s happening inside deep neural networks. Sathiya Keerthi got in touch with me to share today’s paper, a blind submission to ICLR’18, in which the authors conduct a critical analysis of some of the information bottleneck theory findings. It’s an important update pointing out some of the limitations of the approach. Sathiya gave a recent talk summarising results on understanding optimisation and generalisation, ‘ Interplay between Optimization and Generalization in DNNs ,’ which is well worth checking out if this topic interests you. Definitely some more papers there that are going on my backlog to help increase my own understanding! Let’s get back to today’s paper! The authors start out by reproducing the information plane dynamics from the Schwartz-Viz & Tishby paper, and then go on to conduct further experiments: replacing the tanh activation with ReLU to see what impact that has; exploring the link between generalisation and compression; investigating whether the randomness is important to compression during training; and studying the extent to which task-irrelevant information is also compressed. The short version of their findings is that the results reported by Schwartz-Viz and Tishby don’t seem to generalise well to other network architectures: the two phases seen during training depend on the choice of activation function; there is no evidence of a causal connection between compression and generalisation; and that when compression does occur, it is not necessarily dependent on randomness from SGD. Our results highlight the importance of noise assumptions in applying information theoretic analyses to deep learning systems, and complicate the IB theory of deep learning by demonstrating instances where representation compression and generalization performance can diverge. The quest for deeper understanding continues! The impact of activation function choice  The starting point for our analysis is the observation that changing the activation function can markedly change the trajectory of a network in the information plane. The authors used code supplied by Schwartz-Vis and Tishby to first replicate the results that we saw last week (Fig 1A below), and then changed the network to use ReLU instead — rectified linear activation functions  . The resulting information plane dynamics are show in Fig 1B. The phase shift that we saw with the original tanh activation functions disappears! The mutual information with the input monotonically increases in all ReLu layers, with no apparent compression phase. Thus, the choice of nonlinearity substantively affects the dynamics in the information plane. Using a very simple three neuron network, the authors explore this phenomenon further. A scalar Gaussian input distribution  is fed through a scalar first layer weight  and passed through a neural nonlinearity  to yield hidden unit activity  . In order to calculate the mutual information, the hidden unit activity  is binned into 30 uniform bins, to yield the discrete variable  . With the tanh nonlinearity, mutual information first increases and then decreases. With the ReLU nonlinearity it always increases. What’s happening is that with large weights, the tanh function saturates, falling back to providing mutual information with the input of approximately 1 bit (i.e, the discrete variable concentrates in just two bins around 1 and -1). With the ReLU though, half of the inputs are negative and land in the bin around 0, but the other half are Gaussian distributed and have entropy that increases with the size of weight. So it turns out that this double saturating nature of tanh is central to the original results. … double-saturating nonlinearities can lead to compression of information about the input, as hidden units enter their saturation regime, due to the binning procedure used to calculate mutual information. We note that this binning procedure can be viewed as implicitly adding noise to the hidden layer activity: a range of X values map to a single bin, such that the mapping between X and T is no longer perfectly invertible. The binning procedure is crucial for the information theoretic analysis, “however, this noise is not added in practice either during training or testing in these neural networks.”  The saturation of tanh explains the presence of the compression period where mutual information decreases, and also explains why training slows down as tanh networks enter their compression phase: some fraction of inputs have saturated the nonlinearities, reducing backpropagated error gradients. Generalisation independent of compression  Next the authors use the information plane lens to further study the relationship between compression and generalisation. … we exploit recent results on the generalization dynamics in simple linear networks trained in an student-teacher setup (Seung et al., 1992; Advani & Saxe, 2017). This setting allows exact calculation of the generalization performance of the network, exact calculation of the mutual information of the representation (without any binning procedure), and, though we do not do so here, direct comparison to the IB bound which is already known for linear Gaussian problems. No compression is observed in the information plane (panel D in the figure above), although the network does learn a map that generalise well on the task and shows minimal overtraining. Experimentation to force varying degrees of overfitting shows networks with similar behaviour in the information plane can nevertheless have differing generalisation performance. This establishes a dissociation between behavior in the information plane and generalization dynamics: networks that compress may or may not generalize well, and that networks that do not compress may or may not generalize well. Does randomness help compression? Next the authors investigate what contributes to compression in the first place, looking at the differences in the information plane between stochastic gradient descent and batch gradient descent. Whereas SGD takes a sample from the dataset and calculates the error gradient with respect to it, batch gradient descent uses the total error across all examples — “and crucially, therefore has no randomness or diffusion-like behaviour in its updates.”  Both tanh and linear networks are trained with both SGD and BGD, and the resulting information plane dynamics look like this:  We find largely consistent information dynamics in both instances, with robust compression in tanh networks for both methods. Thus randomness in the training process does not appear to contributed substantially to compression of information about the input. This finding is consistent with the view presented in §2 that compression arises predominantly from the double saturating nonlinearity. (Which seems to pretty much rule out the hope from the Schwartz-Viz & Tishby paper that we would find alternatives to SGD that support better diffusion and faster training). The compression of task-irrelevant information  A final experiment partitions the input X into a set of task-relevant inputs and a known to be task_irrelevant_ inputs. The former contribute signal therefore, while the latter only contribute noise. Thus good generalisation would seem to require ignoring the noise. The authors found that information for the task-irrelevant subspace does compress, at the same time as fitting is occurring for the task-relevant information, even though overall there is no observable compression phase. The bottom line  Our results suggest that compression dynamics in the information plane are not a general feature of deep networks, but are critically influenced by the nonlinearities employed by the network… information compression may parallel the situation with sharp minima; although empirical evidence has shown a correlation with generalization error in certain settings and architectures; further theoretical analysis has shown that sharp minima can in fact generalize well.", "pdf_url": "https://openreview.net/pdf?id=ry_WPG-A-", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/on-the-information-bottleneck-theory-of-deep-learning.json"}
{"id": "65595064", "bin": "1300_1400", "summary_sentences": ["CheriABI: enforcing valid pointer provenance and minimizing pointer privilege in the POSIX C run-time environment Davis et al., ASPLOS’19  Last week we saw the benefits of rethinking memory and pointer models at the hardware level when it came to object storage and compression ( Zippads ).", "CHERI also rethinks the way that pointers and memory work, but the goal here is memory protection.", "The scope of the work stands out as particularly impressive:  We have adapted a complete C, C++, and assembly-language software stack, including the open source FreeBSD OS (nearly 800 UNIX programs and more than 200 libraries including OpenSSH, OpenSSL, and bsnmpd) and PostgreSQL database, to employ ubiquitous capability-based pointer and virtual-address protection.", "The protections are hardware implemented and cannot be forged in software.", "The process model, user-kernel interactions, dynamic linking, and memory management concerns are all in scope, and the protection spans the OS/DBMS boundary.", "The basic question here is whether it is practical to support a large-scale C-language software stack with strong pointer-based protection… with only modest changes to existing C code-bases and with reasonable performance cost.", "We answer this question affirmatively.", "That ‘reasonable’ performance cost is a 6.8% slowdown, significantly better than e.g. the 50% overheads of Address Sanitizer .", "Conceptual model  CHERI is guided by two underlying principles:  The well-known principle of least privilege: running software should have the minimum privileges possible to do what it needs to do, and  A new principle identified in this work, the principle of intentional use: where a set of privileges is available to a piece of software, an invoked privilege should be selected explicitly rather than implicitly.The conceptual model that ensues has the following properties:  memory accesses are based not just on arbitrary integers (checked against only the process address space), but also on abstract capabilities that confer an appropriate set of memory permissions.", "abstract capabilities are constructed only through legitimate provenance chains of operations, successively reducing permissions from initial maximally permissive capabilities provided at machine reset  code is not given access to excessive capabilities  And this all has to work for whole-system executions, not just the C-language portion of user processes.", "The goal of all this of course is to prevent attackers injecting, manipulating or abusing pointers in the runtime environment.", "CHERI implementation  CHERI adds a new hardware data type for strongly protected C-language pointers, the CHERI capability (the evaluation uses an FPGA-based implementation).", "A capability combines a good old-fashioned address pointer with bounds constraining the range of addresses and permissions limiting its use.", "The resulting pointers are 128-bits wide, together with one out-of-band tag bit.", "Provenance validation ensures that only capabilities derived via valid transformations of valid capabilities using capability instructions can be used  Capability integrity prevents direct in-memory manipulation of architectural capability encodings.", "If any violation is detected the tag bit is cleared, and the data can no longer be interpreted as a capability.", "Monotonicity prevents the permissions or bounds associated with a capability from being increased.", "The instruction set contains explicit instructions for working with capabilities, and legacy instructions addressing memory via vitual addresses are indirected through a ‘default data capability’ (DDC) register.", "The core of CHERI has been covered in earlier papers, what’s new in this paper is the extension of capability support to the full userspace process environment including interactions with system calls, signals, dynamic linking, and process debugging.", "This enables all legacy loads and stores via the DDC to be eliminated.", "The abstract capability model is implemented with a subtle combination of architectural capabilities (as provided by the hardware) and the critical systems code involved in managing paging, context switching, linking, memory allocation, and suchlike.", "The work includes changes to the CHERI ISA, the C compiler, the C language runtime, the virtual memory APIs, and the CheriBSD kernel.", "At hardware reset the boot code is granted maximally permissive architectural capabilities.", "The kernel then narrows these to ones separately covering userspace, kernel code, and kernel data.", "When a process address space is then replaced by execve, the kernel establishes new memory mappings for the contents of the address space, subdividing the previously created userspace capability.", "On a context switch the kernel saves and restores user-thread register capability state, and updates virtual-physical mappings.", "Similar housekeeping needs to be done when swapping and on signal delivery.", "All standard methods of accessing process memory have been altered to use an explicit capability, so the kernel can only access the memory specified and authorized by the user process, as shown below.", "Evaluation  The CheriABI implementation is used to compile FreeBSD and PostgreSQL, then all of the respective test suites are run.", "In the table below, the numbers in each column represent the number of test programs, not the number of individual tests.", "The MIPS rows show the test suite results on a standard mips64 system.", "Digging into the PostgreSQL test failures, just over half are due to test assumptions about output order or pointer size, the remaining half-dozen or so still need further investigation.", "Most programs (almost 800 C programs in the FreeBSD source tree) require no modifications.", "The following table breaks down the types of changes required for those that do:  The biggest cause of change (42 cases) is calling conventions (CC) in BSD libraries when using variadic arguments.", "With capabilities these require correct function prototypes, and when programs declare their own callbacks fixing each one is the only solution.", "Microbenchmarks (MiBench) and the FreeBSD system call timing benchmarks show modest performance impact in some cases, and performance improvements in others (3.4% slower to 9.8% faster for system calls).", "For a macro-benchmark PostgreSQL’s initdb tool was used.", "PostgreSQL runs 6.8% slower as a CheriABI binary.", "The memory safety benefits are evaluated using the BOdiagsuite of 291 programs.", "Each program has three memory-safety violating variants: min is typically an off-by-one error, med is an off-by-8-bytes error, and large is an off-by 4096 bytes error.", "CheriABI is compared against vanilla mips64 and Address Sanitizer.", "It shows a very high success rate in detecting these safety violations.", "Note that Address Sanitizer has high overheads (3x stack memory, 12.5% total memory, and around 50% performance).", "In addition to finding test-suite issues, we have found and fixed dozens of bugs including buffer bounds violations and variadic argument misuse in FreeBSD programs, libraries, and tests.", "The last word  We have demonstrated a complete memory-safe UNIX system that is practical for general use… our implementation of CheriABI shows the existence of a path forward from our current run-time foundations set on the shifting sands of integer pointers, to a future where strong referential integrity enforces the principles of least privilege and intentionality even on lowest-level software.", "And an afterword!", "It strikes me that you can think of a foreign-key reference in a database a bit like a memory address pointer (the virtual address space it indexes into consists of the rows of the foreign table).", "Today those foreign keys are just like unprotected integer pointers in the world of virtual memory – they have no associated capabilities or protections, can be directly manipulated, etc.. What if queries returned ‘capabilities’ instead of raw keys??", "That might be an interesting model for thinking about the four Ps: provenance, purpose, permissions and privacy."], "summary_text": "CheriABI: enforcing valid pointer provenance and minimizing pointer privilege in the POSIX C run-time environment Davis et al., ASPLOS’19  Last week we saw the benefits of rethinking memory and pointer models at the hardware level when it came to object storage and compression ( Zippads ). CHERI also rethinks the way that pointers and memory work, but the goal here is memory protection. The scope of the work stands out as particularly impressive:  We have adapted a complete C, C++, and assembly-language software stack, including the open source FreeBSD OS (nearly 800 UNIX programs and more than 200 libraries including OpenSSH, OpenSSL, and bsnmpd) and PostgreSQL database, to employ ubiquitous capability-based pointer and virtual-address protection. The protections are hardware implemented and cannot be forged in software. The process model, user-kernel interactions, dynamic linking, and memory management concerns are all in scope, and the protection spans the OS/DBMS boundary. The basic question here is whether it is practical to support a large-scale C-language software stack with strong pointer-based protection… with only modest changes to existing C code-bases and with reasonable performance cost. We answer this question affirmatively. That ‘reasonable’ performance cost is a 6.8% slowdown, significantly better than e.g. the 50% overheads of Address Sanitizer . Conceptual model  CHERI is guided by two underlying principles:  The well-known principle of least privilege: running software should have the minimum privileges possible to do what it needs to do, and  A new principle identified in this work, the principle of intentional use: where a set of privileges is available to a piece of software, an invoked privilege should be selected explicitly rather than implicitly.The conceptual model that ensues has the following properties:  memory accesses are based not just on arbitrary integers (checked against only the process address space), but also on abstract capabilities that confer an appropriate set of memory permissions. abstract capabilities are constructed only through legitimate provenance chains of operations, successively reducing permissions from initial maximally permissive capabilities provided at machine reset  code is not given access to excessive capabilities  And this all has to work for whole-system executions, not just the C-language portion of user processes. The goal of all this of course is to prevent attackers injecting, manipulating or abusing pointers in the runtime environment. CHERI implementation  CHERI adds a new hardware data type for strongly protected C-language pointers, the CHERI capability (the evaluation uses an FPGA-based implementation). A capability combines a good old-fashioned address pointer with bounds constraining the range of addresses and permissions limiting its use. The resulting pointers are 128-bits wide, together with one out-of-band tag bit. Provenance validation ensures that only capabilities derived via valid transformations of valid capabilities using capability instructions can be used  Capability integrity prevents direct in-memory manipulation of architectural capability encodings. If any violation is detected the tag bit is cleared, and the data can no longer be interpreted as a capability. Monotonicity prevents the permissions or bounds associated with a capability from being increased. The instruction set contains explicit instructions for working with capabilities, and legacy instructions addressing memory via vitual addresses are indirected through a ‘default data capability’ (DDC) register. The core of CHERI has been covered in earlier papers, what’s new in this paper is the extension of capability support to the full userspace process environment including interactions with system calls, signals, dynamic linking, and process debugging. This enables all legacy loads and stores via the DDC to be eliminated. The abstract capability model is implemented with a subtle combination of architectural capabilities (as provided by the hardware) and the critical systems code involved in managing paging, context switching, linking, memory allocation, and suchlike. The work includes changes to the CHERI ISA, the C compiler, the C language runtime, the virtual memory APIs, and the CheriBSD kernel. At hardware reset the boot code is granted maximally permissive architectural capabilities. The kernel then narrows these to ones separately covering userspace, kernel code, and kernel data. When a process address space is then replaced by execve, the kernel establishes new memory mappings for the contents of the address space, subdividing the previously created userspace capability. On a context switch the kernel saves and restores user-thread register capability state, and updates virtual-physical mappings. Similar housekeeping needs to be done when swapping and on signal delivery. All standard methods of accessing process memory have been altered to use an explicit capability, so the kernel can only access the memory specified and authorized by the user process, as shown below. Evaluation  The CheriABI implementation is used to compile FreeBSD and PostgreSQL, then all of the respective test suites are run. In the table below, the numbers in each column represent the number of test programs, not the number of individual tests. The MIPS rows show the test suite results on a standard mips64 system. Digging into the PostgreSQL test failures, just over half are due to test assumptions about output order or pointer size, the remaining half-dozen or so still need further investigation. Most programs (almost 800 C programs in the FreeBSD source tree) require no modifications. The following table breaks down the types of changes required for those that do:  The biggest cause of change (42 cases) is calling conventions (CC) in BSD libraries when using variadic arguments. With capabilities these require correct function prototypes, and when programs declare their own callbacks fixing each one is the only solution. Microbenchmarks (MiBench) and the FreeBSD system call timing benchmarks show modest performance impact in some cases, and performance improvements in others (3.4% slower to 9.8% faster for system calls). For a macro-benchmark PostgreSQL’s initdb tool was used. PostgreSQL runs 6.8% slower as a CheriABI binary. The memory safety benefits are evaluated using the BOdiagsuite of 291 programs. Each program has three memory-safety violating variants: min is typically an off-by-one error, med is an off-by-8-bytes error, and large is an off-by 4096 bytes error. CheriABI is compared against vanilla mips64 and Address Sanitizer. It shows a very high success rate in detecting these safety violations. Note that Address Sanitizer has high overheads (3x stack memory, 12.5% total memory, and around 50% performance). In addition to finding test-suite issues, we have found and fixed dozens of bugs including buffer bounds violations and variadic argument misuse in FreeBSD programs, libraries, and tests. The last word  We have demonstrated a complete memory-safe UNIX system that is practical for general use… our implementation of CheriABI shows the existence of a path forward from our current run-time foundations set on the shifting sands of integer pointers, to a future where strong referential integrity enforces the principles of least privilege and intentionality even on lowest-level software. And an afterword! It strikes me that you can think of a foreign-key reference in a database a bit like a memory address pointer (the virtual address space it indexes into consists of the rows of the foreign table). Today those foreign keys are just like unprotected integer pointers in the world of virtual memory – they have no associated capabilities or protections, can be directly manipulated, etc.. What if queries returned ‘capabilities’ instead of raw keys?? That might be an interesting model for thinking about the four Ps: provenance, purpose, permissions and privacy.", "pdf_url": "https://www.cl.cam.ac.uk/research/security/ctsrd/pdfs/201904-asplos-cheriabi.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/cheri-abi.json"}
{"id": "51254358", "bin": "1300_1400", "summary_sentences": ["Deep code search Gu et al., ICSE’18  The problem with searching for code is that the query, e.g. “read an object from xml,” doesn’t look very much like the source code snippets that are the intended results, e.g.", ":  *  That’s why we have Stack Overflow!", "Stack Overflow can help with ‘how to’ style queries, but it can’t help with searches inside codebases you care about.", "For example, “where in this codebase are events queued on a thread?”  …an effective code search engine should be able to understand the semantic meanings of natural language queries and source code in order to improve the accuracy of code search.", "DeepCS is just such a search engine for code, based on the CODEnn (Code-Description Embedding Neural Network) network model.", "During training, it takes code snippets (methods) and corresponding natural language descriptions (from the method comments) and learns a joint-embedding.", "I.e., it learns embeddings such that a method description and its corresponding code snippet are both mapped to a similar point in the same shared embedding space.", "Then given a natural language query, it can embed the query in vector space and look for nearby code snippets.", "Compared to Lucene powered code search tools, and the recently proposed state-of-the-art CodeHow search tool, CODEnn gives excellent results.", "Embeddings  One of the most popular posts over time on this blog has been ‘ The amazing power of word vectors ,’ which describes the process of turning words into vectors.", "To turn a sequence of words into a vector, one common approach is to use an RNN (e.g., LSTM).", "For example, given a sentence such as ‘parse xml file,’ the RNN reads the first word, ‘parse,’ maps it into a vector  , and then computes the RNN hidden state  using  .", "Then it reads the second word, ‘xml,’ maps that into word vector  and updates the hidden state  to  .", "We k keep going in this manner until we reach the end of the sentence, and then use the final hidden state (  in this example) as the sentence embedding.", "We don’t want just any old embedding though.", "We want to learn embeddings such that code snippets (think of them like ‘code sentences’ for now, we’ll get to the details shortly) and their corresponding descriptions have nearby embeddings.", "Joint Embedding, also known as multi-model embedding, is a technique to jointly embed/correlate heterogeneous data into a unified vector space so that semantically similar concepts across the two modalities occupy nearby regions of the space.", "When we’re training, we use a similarity measure (e.g. cosine) in the loss function to encourage the joint mapping.", "CODEnn high level architecture  CODEnn has three main components: a code embedding network to embed source code snippets in vectors; a description embedding network to embed natural language descriptions into vectors; and a similarity module (cosine) to measure the degree of similarity between code and descriptions.", "At a high level it looks like this:  If we zoom in one level, we can start to see the details of the network constructions:  The code embedding network  A code snippet (a method) is turned into a tuple (M,A,T) where M is a sequence of camelCase split tokens in the method name, A is an API sequence (the method invocations made by the method body), and T is the set of tokens in the snippet.", "The method name and API invocation sequences are both embedded (separately) into vectors using RNNs with maxpooling.", "The tokens have no strict order, so they are embedded using a conventional multilayer perceptron.", "The three resulting vectors are then fused into one vector through a fully connected layer.", "The description embedding network  The description embedding network uses an RNN with maxpooling to encode the natural language description from the (first sentence of) the method comment.", "Training  During training, training instances are given as triples (C, D+, D-), where C is a code snipped, D+ is the correct (actual) description of C, and D- is an incorrect description, chosen randomly from the pool of all descriptions.", "The loss function seeks to maximise the cosine similarity between C and D+, and make the distance between C and D- as large as possible.", "The training corpus is based on open-source Java projects on GitHub, resulting in a body of just over 18M commented Java methods.", "For each Java method, we use the method declaration as the code element and the first sentence of its documentation comment as its natural language description.", "According to the Javadoc guidance, the first sentence is usually a summary of a method.", "The method bodies are parsed using the Eclipse JDT compiler.", "(Note, some of the tools from Source{d} may also be useful here).", "All the RNNs are embodied as LSTMs with 200 hidden units in each direction, and word embeddings have 100 dimensions.", "Indexing and querying  To index a codebase, DeepCS embeds all code snippets in the codebase into vectors using offline processing.", "Then during online searching DeepCS embeds the natural language user query, and estimates the cosine similarities between the query embedding and pre-computed code snippet embeddings.", "The top K closest code snippets are returned as the query results.", "Evaluation  Evaluation is done using a search codebase constructed from ~10 thousand Java projects on GitHub (different to the training set), with at least 20 stars each.", "All code is indexed (including methods without any comments), resulting in just over 16M indexed methods.", "We build a benchmark of queries from the top 50 voted Java programming questions in Stack Overflow.", "To achieve so, we browse the list of Java-tagged questions in Stack Overflow and sort them according to the votes that each one receives.", "To qualify, the questions must be about a concrete Java programming task and include a code snippet in the accepted answer.", "The 50 such resulting questions are shown in the following table.", "In the right-hand column we can see the FRank (rank of the first hit result in the result list) for DeepCS, CodeHow, and a conventional Lucene-based search tool.", "Here are some representative query results.", "“queue an event to be run on the thread” vs “run an event on a thread queue” (both have similar words in the query, but are quite different).", "“get the content of an input stream as a string using a specified character encoding”  “read an object from an xml file” (note that the words xml, object, and read don’t appear in the result, but DeepCS still finds it)  “play a song” (another example of associative search, where DeepCS can recommend results with semantically related words such as audio).", "DeepCS isn’t perfect of course, and sometimes ranks partially relevant results higher than exact matching ones.", "This is because DeepCS ranks results by just considering their semantic vectors.", "In future work, more code features (such as programming context) could be considered in our model to further adjust the results."], "summary_text": "Deep code search Gu et al., ICSE’18  The problem with searching for code is that the query, e.g. “read an object from xml,” doesn’t look very much like the source code snippets that are the intended results, e.g. :  *  That’s why we have Stack Overflow! Stack Overflow can help with ‘how to’ style queries, but it can’t help with searches inside codebases you care about. For example, “where in this codebase are events queued on a thread?”  …an effective code search engine should be able to understand the semantic meanings of natural language queries and source code in order to improve the accuracy of code search. DeepCS is just such a search engine for code, based on the CODEnn (Code-Description Embedding Neural Network) network model. During training, it takes code snippets (methods) and corresponding natural language descriptions (from the method comments) and learns a joint-embedding. I.e., it learns embeddings such that a method description and its corresponding code snippet are both mapped to a similar point in the same shared embedding space. Then given a natural language query, it can embed the query in vector space and look for nearby code snippets. Compared to Lucene powered code search tools, and the recently proposed state-of-the-art CodeHow search tool, CODEnn gives excellent results. Embeddings  One of the most popular posts over time on this blog has been ‘ The amazing power of word vectors ,’ which describes the process of turning words into vectors. To turn a sequence of words into a vector, one common approach is to use an RNN (e.g., LSTM). For example, given a sentence such as ‘parse xml file,’ the RNN reads the first word, ‘parse,’ maps it into a vector  , and then computes the RNN hidden state  using  . Then it reads the second word, ‘xml,’ maps that into word vector  and updates the hidden state  to  . We k keep going in this manner until we reach the end of the sentence, and then use the final hidden state (  in this example) as the sentence embedding. We don’t want just any old embedding though. We want to learn embeddings such that code snippets (think of them like ‘code sentences’ for now, we’ll get to the details shortly) and their corresponding descriptions have nearby embeddings. Joint Embedding, also known as multi-model embedding, is a technique to jointly embed/correlate heterogeneous data into a unified vector space so that semantically similar concepts across the two modalities occupy nearby regions of the space. When we’re training, we use a similarity measure (e.g. cosine) in the loss function to encourage the joint mapping. CODEnn high level architecture  CODEnn has three main components: a code embedding network to embed source code snippets in vectors; a description embedding network to embed natural language descriptions into vectors; and a similarity module (cosine) to measure the degree of similarity between code and descriptions. At a high level it looks like this:  If we zoom in one level, we can start to see the details of the network constructions:  The code embedding network  A code snippet (a method) is turned into a tuple (M,A,T) where M is a sequence of camelCase split tokens in the method name, A is an API sequence (the method invocations made by the method body), and T is the set of tokens in the snippet. The method name and API invocation sequences are both embedded (separately) into vectors using RNNs with maxpooling. The tokens have no strict order, so they are embedded using a conventional multilayer perceptron. The three resulting vectors are then fused into one vector through a fully connected layer. The description embedding network  The description embedding network uses an RNN with maxpooling to encode the natural language description from the (first sentence of) the method comment. Training  During training, training instances are given as triples (C, D+, D-), where C is a code snipped, D+ is the correct (actual) description of C, and D- is an incorrect description, chosen randomly from the pool of all descriptions. The loss function seeks to maximise the cosine similarity between C and D+, and make the distance between C and D- as large as possible. The training corpus is based on open-source Java projects on GitHub, resulting in a body of just over 18M commented Java methods. For each Java method, we use the method declaration as the code element and the first sentence of its documentation comment as its natural language description. According to the Javadoc guidance, the first sentence is usually a summary of a method. The method bodies are parsed using the Eclipse JDT compiler. (Note, some of the tools from Source{d} may also be useful here). All the RNNs are embodied as LSTMs with 200 hidden units in each direction, and word embeddings have 100 dimensions. Indexing and querying  To index a codebase, DeepCS embeds all code snippets in the codebase into vectors using offline processing. Then during online searching DeepCS embeds the natural language user query, and estimates the cosine similarities between the query embedding and pre-computed code snippet embeddings. The top K closest code snippets are returned as the query results. Evaluation  Evaluation is done using a search codebase constructed from ~10 thousand Java projects on GitHub (different to the training set), with at least 20 stars each. All code is indexed (including methods without any comments), resulting in just over 16M indexed methods. We build a benchmark of queries from the top 50 voted Java programming questions in Stack Overflow. To achieve so, we browse the list of Java-tagged questions in Stack Overflow and sort them according to the votes that each one receives. To qualify, the questions must be about a concrete Java programming task and include a code snippet in the accepted answer. The 50 such resulting questions are shown in the following table. In the right-hand column we can see the FRank (rank of the first hit result in the result list) for DeepCS, CodeHow, and a conventional Lucene-based search tool. Here are some representative query results. “queue an event to be run on the thread” vs “run an event on a thread queue” (both have similar words in the query, but are quite different). “get the content of an input stream as a string using a specified character encoding”  “read an object from an xml file” (note that the words xml, object, and read don’t appear in the result, but DeepCS still finds it)  “play a song” (another example of associative search, where DeepCS can recommend results with semantically related words such as audio). DeepCS isn’t perfect of course, and sometimes ranks partially relevant results higher than exact matching ones. This is because DeepCS ranks results by just considering their semantic vectors. In future work, more code features (such as programming context) could be considered in our model to further adjust the results.", "pdf_url": "https://guxd.github.io/papers/deepcs.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/deep-code-search.json"}
{"id": "72529720", "bin": "1400_1500", "summary_sentences": ["What  R-CNN and its successor Fast R-CNN both rely on a \"classical\" method to find region proposals in images (i.e. \"Which regions of the image look like they might be objects?\").", "That classical method is selective search.", "Selective search is quite slow (about two seconds per image) and hence the bottleneck in Fast R-CNN.", "They replace it with a neural network (region proposal network, aka RPN).", "The RPN reuses the same features used for the remainder of the Fast R-CNN network, making the region proposal step almost free (about 10ms).", "How  They now have three components in their network:  A model for feature extraction, called the \"feature extraction network\" (FEN).", "Initialized with the weights of a pretrained network (e.g. VGG16).", "A model to use these features and generate region proposals, called the \"Region Proposal Network\" (RPN).", "A model to use these features and region proposals to classify each regions proposal's object and readjust the bounding box, called the \"classification network\" (CN).", "Initialized with the weights of a pretrained network (e.g. VGG16).", "Usually, FEN will contain the convolutional layers of the pretrained model (e.g. VGG16), while CN will contain the fully connected layers.", "(Note: Only \"RPN\" really pops up in the paper, the other two remain more or less unnamed.", "I added the two names to simplify the description.)", "Rough architecture outline:  The basic method at test is as follows:  Use FEN to convert the image to features.", "Apply RPN to the features to generate region proposals.", "Use Region of Interest Pooling (RoI-Pooling) to convert the features of each region proposal to a fixed sized vector.", "Apply CN to the RoI-vectors to a) predict the class of each object (out of K object classes and 1 background class) and b) readjust the bounding box dimensions (top left coordinate, height, width).", "RPN  Basic idea:  Place anchor points on the image, all with the same distance to each other (regular grid).", "Around each anchor point, extract rectangular image areas in various shapes and sizes (\"anchor boxes\"), e.g. thin/square/wide and small/medium/large rectangles.", "(More precisely: The features of these areas are extracted.)", "Visualization:  Feed the features of these areas through a classifier and let it rate/predict the \"regionness\" of the rectangle in a range between 0 and 1.", "Values greater than 0.5 mean that the classifier thinks the rectangle might be a bounding box.", "(CN has to analyze that further.)", "Feed the features of these areas through a regressor and let it optimize the region size (top left coordinate, height, width).", "That way you get all kinds of possible bounding box shapes, even though you only use a few base shapes.", "Implementation:  The regular grid of anchor points naturally arises due to the downscaling of the FEN, it doesn't have to be implemented explicitly.", "The extraction of anchor boxes and classification + regression can be efficiently implemented using convolutions.", "They first apply a 3x3 convolution on the feature maps.", "Note that the convolution covers a large image area due to the downscaling.", "Not so clear, but sounds like they use 256 filters/kernels for that convolution.", "Then they apply some 1x1 convolutions for the classification and regression.", "They use 2*k 1x1 convolutions for classification and 4*k 1x1 convolutions for regression, where k is the number of different shapes of anchor boxes.", "They use k=9 anchor box types: Three sizes (small, medium, large), each in three shapes (thin, square, wide).", "The way they build training examples (below) forces some 1x1 convolutions to react only to some anchor box types.", "Training:  Positive examples are anchor boxes that have an IoU with a ground truth bounding box of 0.7 or more.", "If no anchor point has such an IoU with a specific box, the one with the highest IoU is used instead.", "Negative examples are all anchor boxes that have IoU that do not exceed 0.3 for any bounding box.", "Any anchor point that falls in neither of these groups does not contribute to the loss.", "Anchor boxes that would violate image boundaries are not used as examples.", "The loss is similar to the one in Fast R-CNN: A sum consisting of log loss for the classifier and smooth L1 loss (=smoother absolute distance) for regression.", "Per batch they only sample examples from one image (for efficiency).", "They use 128 positive examples and 128 negative ones.", "If they can't come up with 128 positive examples, they add more negative ones.", "Test:  They use non-maximum suppression (NMS) to remove too identical region proposals, i.e. among all region proposals that have an IoU overlap of 0.7 or more, they pick the one that has highest score.", "They use the 300 proposals with highest score after NMS (or less if there aren't that many).", "Feature sharing  They want to share the features of the FEN between the RPN and the CN.", "So they need a special training method that fine-tunes all three components while keeping the features extracted by FEN useful for both RPN and CN at the same time (not only for one of them).", "Their training methods are:  Alternating traing: One batch for FEN+RPN, one batch for FEN+CN, then again one batch for FEN+RPN and so on.", "Approximate joint training: Train one network of FEN+RPN+CN.", "Merge the gradients of RPN and CN that arrive at FEN via simple summation.", "This method does not compute a gradient from CN through the RPN's regression task, as that is non-trivial.", "(This runs 25-50% faster than alternating training, accuracy is mostly the same.)", "Non-approximate joint training: This would compute the above mentioned missing gradient, but isn't implemented.", "4-step alternating training:  Clone FEN to FEN1 and FEN2.", "Train the pair FEN1 + RPN.", "Train the pair FEN2 + CN using the region proposals from the trained RPN.", "Fine-tune the pair FEN2 + RPN.", "FEN2 is fixed, RPN takes the weights from step 2.", "Fine-tune the pair FEN2 + CN.", "FEN2 is fixed, CN takes the weights from step 3, region proposals come from RPN from step 4.", "Results  Example images:  Pascal VOC (with VGG16 as FEN)  Using an RPN instead of SS (selective search) slightly improved mAP from 66.9% to 69.9%.", "Training RPN and CN on the same FEN (sharing FEN's weights) does not worsen the mAP, but instead improves it slightly from 68.5% to 69.9%.", "Using the RPN instead of SS significantly speeds up the network, from 1830ms/image (less than 0.5fps) to 198ms/image (5fps).", "(Both stats with VGG16.", "They also use ZF as the FEN, which puts them at 17fps, but mAP is lower.)", "Using per anchor point more scales and shapes (ratios) for the anchor boxes improves results.", "1 scale, 1 ratio: 65.8% mAP (scale 128*128, ratio 1:1) or 66.7% mAP (scale 256*256, same ratio).", "3 scales, 3 ratios: 69.9% mAP (scales 128*128, 256*256, 512*512; ratios 1:1, 1:2, 2:1).", "Two-staged vs one-staged  Instead of the two-stage system (first, generate proposals via RPN, then classify them via CN), they try a one-staged system.", "In the one-staged system they move a sliding window over the computed feature maps and regress at every location the bounding box sizes and classify the box.", "When doing this, their performance drops from 58.7% to about 54%."], "summary_text": "What  R-CNN and its successor Fast R-CNN both rely on a \"classical\" method to find region proposals in images (i.e. \"Which regions of the image look like they might be objects?\"). That classical method is selective search. Selective search is quite slow (about two seconds per image) and hence the bottleneck in Fast R-CNN. They replace it with a neural network (region proposal network, aka RPN). The RPN reuses the same features used for the remainder of the Fast R-CNN network, making the region proposal step almost free (about 10ms). How  They now have three components in their network:  A model for feature extraction, called the \"feature extraction network\" (FEN). Initialized with the weights of a pretrained network (e.g. VGG16). A model to use these features and generate region proposals, called the \"Region Proposal Network\" (RPN). A model to use these features and region proposals to classify each regions proposal's object and readjust the bounding box, called the \"classification network\" (CN). Initialized with the weights of a pretrained network (e.g. VGG16). Usually, FEN will contain the convolutional layers of the pretrained model (e.g. VGG16), while CN will contain the fully connected layers. (Note: Only \"RPN\" really pops up in the paper, the other two remain more or less unnamed. I added the two names to simplify the description.) Rough architecture outline:  The basic method at test is as follows:  Use FEN to convert the image to features. Apply RPN to the features to generate region proposals. Use Region of Interest Pooling (RoI-Pooling) to convert the features of each region proposal to a fixed sized vector. Apply CN to the RoI-vectors to a) predict the class of each object (out of K object classes and 1 background class) and b) readjust the bounding box dimensions (top left coordinate, height, width). RPN  Basic idea:  Place anchor points on the image, all with the same distance to each other (regular grid). Around each anchor point, extract rectangular image areas in various shapes and sizes (\"anchor boxes\"), e.g. thin/square/wide and small/medium/large rectangles. (More precisely: The features of these areas are extracted.) Visualization:  Feed the features of these areas through a classifier and let it rate/predict the \"regionness\" of the rectangle in a range between 0 and 1. Values greater than 0.5 mean that the classifier thinks the rectangle might be a bounding box. (CN has to analyze that further.) Feed the features of these areas through a regressor and let it optimize the region size (top left coordinate, height, width). That way you get all kinds of possible bounding box shapes, even though you only use a few base shapes. Implementation:  The regular grid of anchor points naturally arises due to the downscaling of the FEN, it doesn't have to be implemented explicitly. The extraction of anchor boxes and classification + regression can be efficiently implemented using convolutions. They first apply a 3x3 convolution on the feature maps. Note that the convolution covers a large image area due to the downscaling. Not so clear, but sounds like they use 256 filters/kernels for that convolution. Then they apply some 1x1 convolutions for the classification and regression. They use 2*k 1x1 convolutions for classification and 4*k 1x1 convolutions for regression, where k is the number of different shapes of anchor boxes. They use k=9 anchor box types: Three sizes (small, medium, large), each in three shapes (thin, square, wide). The way they build training examples (below) forces some 1x1 convolutions to react only to some anchor box types. Training:  Positive examples are anchor boxes that have an IoU with a ground truth bounding box of 0.7 or more. If no anchor point has such an IoU with a specific box, the one with the highest IoU is used instead. Negative examples are all anchor boxes that have IoU that do not exceed 0.3 for any bounding box. Any anchor point that falls in neither of these groups does not contribute to the loss. Anchor boxes that would violate image boundaries are not used as examples. The loss is similar to the one in Fast R-CNN: A sum consisting of log loss for the classifier and smooth L1 loss (=smoother absolute distance) for regression. Per batch they only sample examples from one image (for efficiency). They use 128 positive examples and 128 negative ones. If they can't come up with 128 positive examples, they add more negative ones. Test:  They use non-maximum suppression (NMS) to remove too identical region proposals, i.e. among all region proposals that have an IoU overlap of 0.7 or more, they pick the one that has highest score. They use the 300 proposals with highest score after NMS (or less if there aren't that many). Feature sharing  They want to share the features of the FEN between the RPN and the CN. So they need a special training method that fine-tunes all three components while keeping the features extracted by FEN useful for both RPN and CN at the same time (not only for one of them). Their training methods are:  Alternating traing: One batch for FEN+RPN, one batch for FEN+CN, then again one batch for FEN+RPN and so on. Approximate joint training: Train one network of FEN+RPN+CN. Merge the gradients of RPN and CN that arrive at FEN via simple summation. This method does not compute a gradient from CN through the RPN's regression task, as that is non-trivial. (This runs 25-50% faster than alternating training, accuracy is mostly the same.) Non-approximate joint training: This would compute the above mentioned missing gradient, but isn't implemented. 4-step alternating training:  Clone FEN to FEN1 and FEN2. Train the pair FEN1 + RPN. Train the pair FEN2 + CN using the region proposals from the trained RPN. Fine-tune the pair FEN2 + RPN. FEN2 is fixed, RPN takes the weights from step 2. Fine-tune the pair FEN2 + CN. FEN2 is fixed, CN takes the weights from step 3, region proposals come from RPN from step 4. Results  Example images:  Pascal VOC (with VGG16 as FEN)  Using an RPN instead of SS (selective search) slightly improved mAP from 66.9% to 69.9%. Training RPN and CN on the same FEN (sharing FEN's weights) does not worsen the mAP, but instead improves it slightly from 68.5% to 69.9%. Using the RPN instead of SS significantly speeds up the network, from 1830ms/image (less than 0.5fps) to 198ms/image (5fps). (Both stats with VGG16. They also use ZF as the FEN, which puts them at 17fps, but mAP is lower.) Using per anchor point more scales and shapes (ratios) for the anchor boxes improves results. 1 scale, 1 ratio: 65.8% mAP (scale 128*128, ratio 1:1) or 66.7% mAP (scale 256*256, same ratio). 3 scales, 3 ratios: 69.9% mAP (scales 128*128, 256*256, 512*512; ratios 1:1, 1:2, 2:1). Two-staged vs one-staged  Instead of the two-stage system (first, generate proposals via RPN, then classify them via CN), they try a one-staged system. In the one-staged system they move a sliding window over the computed feature maps and regress at every location the bounding box sizes and classify the box. When doing this, their performance drops from 58.7% to about 54%.", "pdf_url": "https://arxiv.org/pdf/1506.01497", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/faster_r-cnn.json"}
{"id": "24169489", "bin": "1400_1500", "summary_sentences": ["The Art of Testing Less Without Sacrificing Quality – Herzig et al. 2015  Why on earth would anyone want to test less?", "Maybe if you could guarantee the same eventually quality, and save a couple of million dollars along the way…  By  nature, system and compliance tests are complex and time-consuming although they rarely find a defect.", "Large complex  software products tend to run on millions of configuration in the  field and emulating these configurations requires multiple test  infrastructures and procedures that are expensive to run in terms  of cost and time.", "Making tests faster is desirable but usually  requires enormous development efforts.", "Simply removing tests  increases the risk of expensive bugs being shipped as part of the final product… At the same time, long running  test processes increasingly conflict with the need to deliver  software products in shorter periods of time while maintaining  or increasing product quality and reliability.", "Increasing  productivity through running less tests is desirable but threatens  product quality, as code defects may remain undetected.", "So long as you run every test eventually you’re guaranteed to catch all defects that would have been caught.", "That means there’s a trade-off you can make between the costs of executing a test now, and the (increased) costs of possibly finding a defect later in the cycle.", "Herzig et al. present THEO, which evaluates these costs and chooses the path with the lower expected cost.", "The evaluation is done in the context of Microsoft’s Office, Windows, and Dynamics products,  which means we also get some really interesting data on these costs at Microsoft.", "The results (in simulation) cover more than 26 months of industrial product execution and more than 37 million test executions, and the savings are impressive:  THEO would have reduced the number of test executions by up  to 50% cutting down test time by up to 47%.", "At the same time,  product quality was not sacrificed as the process ensures that all  tests are ran at least once on all code changes.", "Removing tests  would result in between 0.2% and 13% of defects being caught  later in the development process, thus increasing the cost of  fixing those defects.", "Nevertheless simulation shows that THEO  produced an overall cost reduction of up to $2 million per development year, per product.", "Through reducing the overall  test time, THEO would also have other impacts on the product  development process, such as increasing code velocity and  productivity.", "These improvements are hard to quantify but are likely to increase the cost savings estimated in this paper.", "The technique and results described in this paper have  convinced an increasing number of product teams, within  Microsoft, to provide dedicate resources to explore ways to  integrate THEO into their actual live production test  environments.", "Let’s take a look at the model THEO uses.", "THEO does not require any additional test case instrumentation, and uses only the test name, time taken for the test to run, and test result as input.", "This data is already collected by test runners.", "The test result could be a pass or a fail, and if it’s a fail it could be because of  a genuine defect or because of a problem with the test case itself (false positive).", "By tying failed tests back to raised bug reports, THEO is able to distinguish between these two scenarios .", "If the bug report results in a (non test-)code fix the test is presumed to have found  a code defect, otherwise it is assumed to have been a false alarm.", "If the failure was not investigated the outcome is ‘unknown.’ Tests run in different execution contexts (and may have different probabilities of finding bugs in different contexts).", "An execution context is a collection of properties – for the study BuildType, Architecture, Language, and Branch were used.", "This is a crucial point as a test may show different  execution behaviors for different execution contexts.", "For  example, a test might find more issues on code of one branch  than another depending on the type of changes performed on that  branch.", "For example, tests cases testing core functionality might  find more defects on a branch containing kernel changes than on  a branch managing media changes.", "Thus, our approach will not  only differentiate between test cases, but also bind historic defect  detection capabilities of a test to its execution context.", "At the core of the model are estimates of the probability that a given test execution will find a genuine defect (true positive), and that it will raise a false alarm (false positive).", "Pdefect(test,context) = #detectedDefects(test,context) / #executions(test,context)  PfalsePositive(test,context) = #falseAlarms(test,context) / #executions(test,context)  All pretty straightforward…  Both probability measurements  consider the entire history from the beginning of monitoring  until the moment the test is about to be executed.", "Consequently,  probability measures get more stable and more reliable the more  historic information we gathered for the corresponding test.", "Given these probabilities, all that remains is to estimate the associated costs.", "If the estimated cost of skipping a test, Costskip, is less than the estimated cost of executing a test, Costexec then THEO will skip the test (with the proviso that every test eventually gets executed).", "The cost of executing a test is captured as a function of the machine time spent on test execution and time wasted on investigating false positives:  Costexec = Costmachine + (PfalsePositive * Costinspect)  For the Microsoft development environment, Costmachine was estimated at $0.03/hour, corresponding roughly to the cost of a memory intense Azure image including power and hardware consumption as well as maintenance.", "Costinspect at Microsoft is $9.60.", "“It considers the size of the test  inspection teams, the number of inspections performed and the  average salary of engineers on the team.”  The cost of skipping a test is captured as a function of the probability that the test would have found a genuine defect, the (increased) cost of fixing a defect found later in the cycle, the delay before the defect is found, and the number of engineers affected.", "Costskip = Pdefect * Costescaped * Timedelay*#Engineers  The constant Costescaped represents the average cost of an escaped defect.", "This cost depends on the number of people that  will be affected by the escaped defect and the time duration the  defect remains undetected.", "We used a value of $4.20 per  developer and hour of delay for Costescaped.", "This value  represents the average cost of a bug elapsing within Microsoft.", "Depending on the time the defect remains undetected and the  number of additional engineers affected, elapsing a defect from  a development branch into the main trunk branch in Windows  can cost tens of thousands of dollars.", "The #Engineers is determined by counting the number of engineers whose code changes pass the code branch.", "Timedelay is the average timespan required to fix historic defects on the corresponding branch.", "The final piece of the puzzle is ensuring that every test gets executed eventually:  To ensure this happens we use two separate criteria,  depending on the development process:  Option 1: For single branch development processes, e.g. Microsoft Office, we enforce each test to execute at least  every third day.", "Since all code changes are applied to the  same branch, re-execution of each test for each execution  context periodically ensures that each code change has to go  through the same verification procedures as performed  originally.", "Option 2: For multi-branch development processes, e.g. Microsoft Windows, we enforce to execute a combination of  test and execution context on the branch closest to trunk on which the test had been executed originally."], "summary_text": "The Art of Testing Less Without Sacrificing Quality – Herzig et al. 2015  Why on earth would anyone want to test less? Maybe if you could guarantee the same eventually quality, and save a couple of million dollars along the way…  By  nature, system and compliance tests are complex and time-consuming although they rarely find a defect. Large complex  software products tend to run on millions of configuration in the  field and emulating these configurations requires multiple test  infrastructures and procedures that are expensive to run in terms  of cost and time. Making tests faster is desirable but usually  requires enormous development efforts. Simply removing tests  increases the risk of expensive bugs being shipped as part of the final product… At the same time, long running  test processes increasingly conflict with the need to deliver  software products in shorter periods of time while maintaining  or increasing product quality and reliability. Increasing  productivity through running less tests is desirable but threatens  product quality, as code defects may remain undetected. So long as you run every test eventually you’re guaranteed to catch all defects that would have been caught. That means there’s a trade-off you can make between the costs of executing a test now, and the (increased) costs of possibly finding a defect later in the cycle. Herzig et al. present THEO, which evaluates these costs and chooses the path with the lower expected cost. The evaluation is done in the context of Microsoft’s Office, Windows, and Dynamics products,  which means we also get some really interesting data on these costs at Microsoft. The results (in simulation) cover more than 26 months of industrial product execution and more than 37 million test executions, and the savings are impressive:  THEO would have reduced the number of test executions by up  to 50% cutting down test time by up to 47%. At the same time,  product quality was not sacrificed as the process ensures that all  tests are ran at least once on all code changes. Removing tests  would result in between 0.2% and 13% of defects being caught  later in the development process, thus increasing the cost of  fixing those defects. Nevertheless simulation shows that THEO  produced an overall cost reduction of up to $2 million per development year, per product. Through reducing the overall  test time, THEO would also have other impacts on the product  development process, such as increasing code velocity and  productivity. These improvements are hard to quantify but are likely to increase the cost savings estimated in this paper. The technique and results described in this paper have  convinced an increasing number of product teams, within  Microsoft, to provide dedicate resources to explore ways to  integrate THEO into their actual live production test  environments. Let’s take a look at the model THEO uses. THEO does not require any additional test case instrumentation, and uses only the test name, time taken for the test to run, and test result as input. This data is already collected by test runners. The test result could be a pass or a fail, and if it’s a fail it could be because of  a genuine defect or because of a problem with the test case itself (false positive). By tying failed tests back to raised bug reports, THEO is able to distinguish between these two scenarios . If the bug report results in a (non test-)code fix the test is presumed to have found  a code defect, otherwise it is assumed to have been a false alarm. If the failure was not investigated the outcome is ‘unknown.’ Tests run in different execution contexts (and may have different probabilities of finding bugs in different contexts). An execution context is a collection of properties – for the study BuildType, Architecture, Language, and Branch were used. This is a crucial point as a test may show different  execution behaviors for different execution contexts. For  example, a test might find more issues on code of one branch  than another depending on the type of changes performed on that  branch. For example, tests cases testing core functionality might  find more defects on a branch containing kernel changes than on  a branch managing media changes. Thus, our approach will not  only differentiate between test cases, but also bind historic defect  detection capabilities of a test to its execution context. At the core of the model are estimates of the probability that a given test execution will find a genuine defect (true positive), and that it will raise a false alarm (false positive). Pdefect(test,context) = #detectedDefects(test,context) / #executions(test,context)  PfalsePositive(test,context) = #falseAlarms(test,context) / #executions(test,context)  All pretty straightforward…  Both probability measurements  consider the entire history from the beginning of monitoring  until the moment the test is about to be executed. Consequently,  probability measures get more stable and more reliable the more  historic information we gathered for the corresponding test. Given these probabilities, all that remains is to estimate the associated costs. If the estimated cost of skipping a test, Costskip, is less than the estimated cost of executing a test, Costexec then THEO will skip the test (with the proviso that every test eventually gets executed). The cost of executing a test is captured as a function of the machine time spent on test execution and time wasted on investigating false positives:  Costexec = Costmachine + (PfalsePositive * Costinspect)  For the Microsoft development environment, Costmachine was estimated at $0.03/hour, corresponding roughly to the cost of a memory intense Azure image including power and hardware consumption as well as maintenance. Costinspect at Microsoft is $9.60. “It considers the size of the test  inspection teams, the number of inspections performed and the  average salary of engineers on the team.”  The cost of skipping a test is captured as a function of the probability that the test would have found a genuine defect, the (increased) cost of fixing a defect found later in the cycle, the delay before the defect is found, and the number of engineers affected. Costskip = Pdefect * Costescaped * Timedelay*#Engineers  The constant Costescaped represents the average cost of an escaped defect. This cost depends on the number of people that  will be affected by the escaped defect and the time duration the  defect remains undetected. We used a value of $4.20 per  developer and hour of delay for Costescaped. This value  represents the average cost of a bug elapsing within Microsoft. Depending on the time the defect remains undetected and the  number of additional engineers affected, elapsing a defect from  a development branch into the main trunk branch in Windows  can cost tens of thousands of dollars. The #Engineers is determined by counting the number of engineers whose code changes pass the code branch. Timedelay is the average timespan required to fix historic defects on the corresponding branch. The final piece of the puzzle is ensuring that every test gets executed eventually:  To ensure this happens we use two separate criteria,  depending on the development process:  Option 1: For single branch development processes, e.g. Microsoft Office, we enforce each test to execute at least  every third day. Since all code changes are applied to the  same branch, re-execution of each test for each execution  context periodically ensures that each code change has to go  through the same verification procedures as performed  originally. Option 2: For multi-branch development processes, e.g. Microsoft Windows, we enforce to execute a combination of  test and execution context on the branch closest to trunk on which the test had been executed originally.", "pdf_url": "http://research.microsoft.com/pubs/238350/The%20Art%20of%20Testing%20Less%20without%20Sacrificing%20Quality.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/the-art-of-testing-less-without-sacrificing-quality.json"}
{"id": "24824894", "bin": "1400_1500", "summary_sentences": ["Can you trust the trend?", "Discovering Simpson’s paradoxes in social data Alipourfard et al., WSDM’18  In ‘ Same stats, different graphs ,’ we saw some compelling examples of how summary statistics can hide important underlying patterns in data.", "Today’s paper choice shows how you can detect instances of Simpson’s paradox, thus revealing the presence of interesting subgroups, and hopefully avoid drawing the wrong conclusions.", "For the evaluation part of the work, the authors look at question-answering on Stack Exchange (Stack Overflow, for many readers of this blog I suspect).", "We investigate how Simpson’s paradox affects analysis of trends in social data.", "According to the paradox, the trends observed in data that has been aggregated over an entire population may be different from, and even opposite to, those of the underlying subgroups.", "Let’s jump straight to an example.", "In Stack Exchange someone posts a question and other users of the system post answers (ignoring the part about the question first being deemed worthy of the forum by the powers-that-be).", "Users can vote for answers that they find helpful, and the original poster of the question can accept one of the answers as the best one.", "What factors influence whether or not a particular answer is accepted as the best one?", "One of the variables that has been studied here is when in a user’s session an answer is posted.", "Suppose I’m feeling particularly helpful today, and I log onto to Stack Overflow and start answering questions to the best of my ability.", "I answer one question, then another, then another.", "Is my answer to the first question more or less likely to be accepted as a best answer than my answer to the third question?", "If we look at the data from 9.6M Stack Exchange questions, we see the following trend:  It seems that the more questions I’ve previously answered in a given session, the greater the probability my next answer will be accepted as the best one!", "That seems a bit odd.", "Do question answerers get into the flow?", "Do they start picking ‘easier’ questions (for them) as the session goes on?", "Here’s another plot of exactly the same dataset, but with the data disaggregated by session length.", "Each different coloured line represents a session of a different length (i.e., sessions where the user answered only one question, sessions where the user answered two questions, and so on).", "When we compare sessions of the same length, we clearly see exactly the opposite trend: answers later in a session tend to fare worse than earlier ones!", "The truth is that “each successive answer posted during a session by a user on Stack Exchange is shorter, less well documented with external links and code, and less likely to be accepted by the asker as the best answer.”  When measuring how an outcome changes as a function of an independent variable, the characteristics of the population over which the trend is measured may change as a function of the independent variable due to survivor bias.", "To illustrate, here’s an example of that Stack Exchange data broken down by session length.", "Note the massive class imbalances (many more sessions of length one than length eight for example).", "When calculating acceptance probability for the aggregate data, consider the case of computing the probability that the third answer in a session is accepted as the best one.", "Of the 12.8M total data points, 9.6M of them aren’t eligible to contribute to this analysis (sessions of length one or two).", "Thus there is a survivorship bias – when we get to the third answer, the first two have already failed to be accepted, indicating that perhaps the new answer is facing weaker competition.", "This increases the probability that the third answer will be accepted as the best.", "(And so on, as session lengths get longer and longer).", "… despite accumulating evidence that Simpson’s paradox affects inference of trends in social and behavioral data, researchers do not routinely test for it in their studies.", "Identifying Simpson’s paradoxes  We’d like to know if a Simpson’s paradox exists so that we don’t draw the wrong conclusions, and also because it normally suggests something interesting happening in the data: subgroups of the population which differ in their behaviour in ways which are significant enough to affect aggregate trends.", "We propose a method to systematically uncover Simpson’s paradox for trends in data.", "Let Y be the outcome being measured (e.g., the probability than an answer is accepted as the best one), and  be the set of m independent variables or features (e.g, the reputation of the answering user, the number of answers so far, and so on).", "The method finds pairs of variables  such that a trend in Y as a function of  disappears or reverses when the data is disaggregated by conditioning on  .", "If  is categorical, then we can simply group data by the unique values.", "For continuous various (or discrete variables with a very large range), you can bin the elements.", "The experiments in the paper used bins of fixed size, but other binning strategies are available.", "A trend in Y as a function of  can be expressed as  And the reverse trend when conditioned on  can be expressed as:  We’re looking for pairs where both equation (1) and (2) are true simultaneously.", "The process starts out by fitting linear models.", "Let the relationship between Y and  be modelled by  (Here  is just the intercept of the regression function, and the trend parameter  quantifies the effect of  on Y).", "We can use a similar linear model (with different values of alpha and beta) for the conditioned expectation:  When fitting linear models  we have not only fitted a trend parameter  but also a p-value which gives the probability of finding an intercept  [  ??? ]", "at least as extreme as the fitted value under the null hypothesis  .", "From this, we have three possibilities:  is not statistically different from zero  is statistically different from zero and positive  is statistically different from zero and negative  By comparing the sign of  from the aggregated fit with the signs of the  s from the disaggregated fits we can test for Simpson’s paradox.", "Although [our equations] state that the signs from the disaggregated curves should all be different from the aggregrated curve, in practice this is too strict, especially as human behavioral data is noisy.", "Thus, we compare the sign of the fit to aggregated data to the simple average of the signs of fits to disaggregated data.", "Here’s the algorithm pseudocode:  Using the Stack Exchange data, the authors used this algorithm to find several instances of Simpson’s paradox:  We looked at one of these earlier.", "Here’s a breakdown of the paradox regarding acceptance probability versus the total number of answers posted by a user in their account lifetime.", "An analysis of the mathematical formulation of Simpson’s paradox presented above also reveals two necessary conditions for a paradox to arise:  The distribution of the conditioning variable  must be dependent on  (i.e., as  changes, so does the distribution of values of  ).", "The expectation of  , conditioned on  , must not be independent of  .", "(I.e., for a given value of  , as  changes, so does the expected value of Y).", "The last word  Since social data is often generated by a mixture of subgroups, existence of Simpson’s paradox suggests that these subgroups differ systematically and significantly in their behavior.", "By isolating important subgroups in social data, our method can yield insights into their behaviors."], "summary_text": "Can you trust the trend? Discovering Simpson’s paradoxes in social data Alipourfard et al., WSDM’18  In ‘ Same stats, different graphs ,’ we saw some compelling examples of how summary statistics can hide important underlying patterns in data. Today’s paper choice shows how you can detect instances of Simpson’s paradox, thus revealing the presence of interesting subgroups, and hopefully avoid drawing the wrong conclusions. For the evaluation part of the work, the authors look at question-answering on Stack Exchange (Stack Overflow, for many readers of this blog I suspect). We investigate how Simpson’s paradox affects analysis of trends in social data. According to the paradox, the trends observed in data that has been aggregated over an entire population may be different from, and even opposite to, those of the underlying subgroups. Let’s jump straight to an example. In Stack Exchange someone posts a question and other users of the system post answers (ignoring the part about the question first being deemed worthy of the forum by the powers-that-be). Users can vote for answers that they find helpful, and the original poster of the question can accept one of the answers as the best one. What factors influence whether or not a particular answer is accepted as the best one? One of the variables that has been studied here is when in a user’s session an answer is posted. Suppose I’m feeling particularly helpful today, and I log onto to Stack Overflow and start answering questions to the best of my ability. I answer one question, then another, then another. Is my answer to the first question more or less likely to be accepted as a best answer than my answer to the third question? If we look at the data from 9.6M Stack Exchange questions, we see the following trend:  It seems that the more questions I’ve previously answered in a given session, the greater the probability my next answer will be accepted as the best one! That seems a bit odd. Do question answerers get into the flow? Do they start picking ‘easier’ questions (for them) as the session goes on? Here’s another plot of exactly the same dataset, but with the data disaggregated by session length. Each different coloured line represents a session of a different length (i.e., sessions where the user answered only one question, sessions where the user answered two questions, and so on). When we compare sessions of the same length, we clearly see exactly the opposite trend: answers later in a session tend to fare worse than earlier ones! The truth is that “each successive answer posted during a session by a user on Stack Exchange is shorter, less well documented with external links and code, and less likely to be accepted by the asker as the best answer.”  When measuring how an outcome changes as a function of an independent variable, the characteristics of the population over which the trend is measured may change as a function of the independent variable due to survivor bias. To illustrate, here’s an example of that Stack Exchange data broken down by session length. Note the massive class imbalances (many more sessions of length one than length eight for example). When calculating acceptance probability for the aggregate data, consider the case of computing the probability that the third answer in a session is accepted as the best one. Of the 12.8M total data points, 9.6M of them aren’t eligible to contribute to this analysis (sessions of length one or two). Thus there is a survivorship bias – when we get to the third answer, the first two have already failed to be accepted, indicating that perhaps the new answer is facing weaker competition. This increases the probability that the third answer will be accepted as the best. (And so on, as session lengths get longer and longer). … despite accumulating evidence that Simpson’s paradox affects inference of trends in social and behavioral data, researchers do not routinely test for it in their studies. Identifying Simpson’s paradoxes  We’d like to know if a Simpson’s paradox exists so that we don’t draw the wrong conclusions, and also because it normally suggests something interesting happening in the data: subgroups of the population which differ in their behaviour in ways which are significant enough to affect aggregate trends. We propose a method to systematically uncover Simpson’s paradox for trends in data. Let Y be the outcome being measured (e.g., the probability than an answer is accepted as the best one), and  be the set of m independent variables or features (e.g, the reputation of the answering user, the number of answers so far, and so on). The method finds pairs of variables  such that a trend in Y as a function of  disappears or reverses when the data is disaggregated by conditioning on  . If  is categorical, then we can simply group data by the unique values. For continuous various (or discrete variables with a very large range), you can bin the elements. The experiments in the paper used bins of fixed size, but other binning strategies are available. A trend in Y as a function of  can be expressed as  And the reverse trend when conditioned on  can be expressed as:  We’re looking for pairs where both equation (1) and (2) are true simultaneously. The process starts out by fitting linear models. Let the relationship between Y and  be modelled by  (Here  is just the intercept of the regression function, and the trend parameter  quantifies the effect of  on Y). We can use a similar linear model (with different values of alpha and beta) for the conditioned expectation:  When fitting linear models  we have not only fitted a trend parameter  but also a p-value which gives the probability of finding an intercept  [  ??? ] at least as extreme as the fitted value under the null hypothesis  . From this, we have three possibilities:  is not statistically different from zero  is statistically different from zero and positive  is statistically different from zero and negative  By comparing the sign of  from the aggregated fit with the signs of the  s from the disaggregated fits we can test for Simpson’s paradox. Although [our equations] state that the signs from the disaggregated curves should all be different from the aggregrated curve, in practice this is too strict, especially as human behavioral data is noisy. Thus, we compare the sign of the fit to aggregated data to the simple average of the signs of fits to disaggregated data. Here’s the algorithm pseudocode:  Using the Stack Exchange data, the authors used this algorithm to find several instances of Simpson’s paradox:  We looked at one of these earlier. Here’s a breakdown of the paradox regarding acceptance probability versus the total number of answers posted by a user in their account lifetime. An analysis of the mathematical formulation of Simpson’s paradox presented above also reveals two necessary conditions for a paradox to arise:  The distribution of the conditioning variable  must be dependent on  (i.e., as  changes, so does the distribution of values of  ). The expectation of  , conditioned on  , must not be independent of  . (I.e., for a given value of  , as  changes, so does the expected value of Y). The last word  Since social data is often generated by a mixture of subgroups, existence of Simpson’s paradox suggests that these subgroups differ systematically and significantly in their behavior. By isolating important subgroups in social data, our method can yield insights into their behaviors.", "pdf_url": "https://arxiv.org/pdf/1801.04385", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/can-you-trust-the-trend-discovering-simpsons-paradoxes-in-social-data.json"}
{"id": "50524744", "bin": "1400_1500", "summary_sentences": ["Information-Flow Analysis of Android Applications in DroidSafe – Gordon et al. 2015  This is the first of three papers we’ll be looking at this week from the NDSS’15 conference that took place earlier this month.", "DroidSafe is a tool that looks for potential leaks of sensitive information in Android applications.", "And it works incredibly well!", "DroidSafe detects all malicious information flow leaks inserted into 24 real-world Android applications by three independent, hostile Red-Team organizations.", "The previous state-of-the art analysis, in contrast, detects less than 10% of these malicious flows.", "The definition of sensitive data includes the unique device ID, sensor data (location, acceleration etc.", "), file data, image data and meta-data, email and SMS messages, passwords, network traffic and screen-shots.", "DroidSafe is a static analysis framework that analyzes the application before it executes.", "Given the size, richness, and complexity of the Android API and runtime (about 1.3 million lines of code) this is a significant challenge.", "It’s important to detect leaks, and to be practical it’s also important not to generate too many false positives.", "The Android API version 4.4.3 includes over 3,500 classes visible to an application developer.", "Analyzing the complete source code for the API is exceedingly difficult because it is implemented over multiple languages and some of the implementation is device-specific.", "Thus, static analysis frameworks rely on modelling the Android API semantics.", "Beyond the sheer scale of the challenge, event dispatching, callbacks, and inter-component communication all add to the difficulty.", "Event dispatching can lead to many different orderings of events, and event handlers are not called directly in application code.", "Callback handlers can include arguments passed by the runtime to the application for processing – which could include data from the application (tainted data), depending on the execution sequence prior to the event.", "Inter-component communication (ICC) is via Intent objects – and the resolution of an Intent destination is complex and may be dynamically determined.", "Starting with the Android Open Source Project (AOSP) source code, “it quickly became apparent that the size and complexity of the Android environment made it necessary to develop the model and the analysis together as an integrated whole, with the design decisions in the model and the analysis working together synergistically to enable an effective solution to the Android static information-flow analysis problem.” Stubs were developed to cover code outside of the AOSP Java codebase:  Examples of semantics missing in the AOSP and added via accurate analysis stubs include native methods; event callback initiation with accurate context; component life-cycle events; and hidden state maintained by the Android runtime and accessible to the application only via the Android API.", "For the information flow analysis, 4051 sensitive source methods and 2116 sensitive sink methods were manually identified and classified.", "In addition the implementation of 117 classes in the Java standard library and Android library were carefully simplified to increase precision and decrease analysis time (on the order of 5 to 10 minutes).", "At the core of the model are 550 Android classes that account for over 98.1% of the total calls made by over 95K applications downloaded from the Google Play Store.", "These were manually reviewed to confirm that the implementation fully covered semantics for data flow, object instantiation and aliasing, and that the event callbacks defined are called explicitly by the model with the proper context.", "At the core of the approach is an analysis method called ‘Points-to’ analysis:  Points-to analysis (PTA) is a foundational static program analysis that computes a static abstraction of all the heap locations that a pointer (reference) variable may point to during program execution.", "In addition to the points-to relation, points-to analysis also constructs a call graph as modern languages re-quire points-to results to calculate targets for dynamic dispatch and functional lambda calculations.", "The PTA implementation implements object sensitivity (identifying flows that originate in different object instances).", "“Object sensitivity is notoriously difficult to understand and implement.” It also requires large amounts of memory.", "Prior to optimising, several applications could not be analysed even with 64GB of heap memory.", "With optimisations, all tested applications now fit in under 34GB.", "The optimisation involved analysing a suite of Android applications to determine an appropriate context-depth (from 0 to 4) for each API class.", "(Traditional approaches use a fixed context depth).", "For inter-component communication, string analysis is performed to work out possible runtime targets using the JSA String Analyzer.", "This creates a regular expression representing the possible values of the string value.", "After JSA is run, we replace resolved string values in the application code with constants representing their computed regular expression, and perform a pass of our points-to analysis such that these values can be propagated globally.", "Additional transformations are made for ICC initiation calls and Android Service components to improve model precision.", "The resulting information flow analysis is built on top of the Soot Java Analysis framework and comprises approximately 70Kloc of Java code.", "Our information-flow analysis computes an over-approximation of all the memory states that occur during the execution of a program.", "The analysis is designed as a forward data-flow analysis.", "For each type of statement, we define a transfer function in terms of how it changes the state of memory.", "We divide memory into four separate areas that store local variables, instance fields, static fields, and arrays, reflecting the semantics of the Java programming language.", "Each of the memory areas is modelled as a function whose codomain consists of a set of information values.", "An information value is a tuple of the type of information and the source code location where the information was first injected.", "Our analysis can identify not only the kind of information being exfiltrated but the code location of the source.", "Results  We evaluate DroidSafe on 24 complete real-world Android applications that, as part of the DARPA Automated Program Analysis for Cybersecurity (APAC) program, have been augmented with malicious information flow leaks by three hostile Red Team organizations.", "The goal of these organizations was to develop information leaks that would either evade detection by static analysis tools or overwhelm static analysis tools into producing unacceptable results (by, for example, manipulating the tool into reporting an overwhelming number of false positive flows).", "DroidSafe accurately detects all of the 69 malicious flows in these applications (while reporting a manageable total number of flows).", "A current state-of-the-art Android information-flow analysis system, Flow-Droid + IccTA, in contrast, detects only 6 of the 69 malicious flows, and has a larger ratio of total flows reported to true malicious flows reported.", "DroidSafe was also evaluated against the DroidBench suite and gave the highest reported accuracy and highest precision for the suite to date at 94.3% and 87.6% respectively.", "Unsurprisingly, DroidSafe gets 100% accuracy and precision on its own test suite – but the next best tool could only achieve 34.9% accuracy and 79.9% precision.", "As these results illustrate, DroidSafe implements an analysis of unprecedented accuracy and precision.", "To the best of our knowledge, DroidSafe provides the first usable information-flow analysis for Android applications.", "The Secret Sauce  Our experience developing DroidSafe shows that 1) there is no substitute for an accurate and precise model of the application environment, and 2) using the model to drive the design decisions behind the analysis and supporting techniques (such as accurate analysis stubs) is one effective but (inevitably) labor-intensive way to obtain an acceptably precise and accurate analysis.", "As long as there are complex application frameworks, we anticipate that making an appropriate set of design decisions (such as the use of a scalable flow insensitive analysis) to successfully navigate the trade-off space that the application framework implicitly presents will be a necessary prerequisite for obtaining acceptable accuracy and precision."], "summary_text": "Information-Flow Analysis of Android Applications in DroidSafe – Gordon et al. 2015  This is the first of three papers we’ll be looking at this week from the NDSS’15 conference that took place earlier this month. DroidSafe is a tool that looks for potential leaks of sensitive information in Android applications. And it works incredibly well! DroidSafe detects all malicious information flow leaks inserted into 24 real-world Android applications by three independent, hostile Red-Team organizations. The previous state-of-the art analysis, in contrast, detects less than 10% of these malicious flows. The definition of sensitive data includes the unique device ID, sensor data (location, acceleration etc. ), file data, image data and meta-data, email and SMS messages, passwords, network traffic and screen-shots. DroidSafe is a static analysis framework that analyzes the application before it executes. Given the size, richness, and complexity of the Android API and runtime (about 1.3 million lines of code) this is a significant challenge. It’s important to detect leaks, and to be practical it’s also important not to generate too many false positives. The Android API version 4.4.3 includes over 3,500 classes visible to an application developer. Analyzing the complete source code for the API is exceedingly difficult because it is implemented over multiple languages and some of the implementation is device-specific. Thus, static analysis frameworks rely on modelling the Android API semantics. Beyond the sheer scale of the challenge, event dispatching, callbacks, and inter-component communication all add to the difficulty. Event dispatching can lead to many different orderings of events, and event handlers are not called directly in application code. Callback handlers can include arguments passed by the runtime to the application for processing – which could include data from the application (tainted data), depending on the execution sequence prior to the event. Inter-component communication (ICC) is via Intent objects – and the resolution of an Intent destination is complex and may be dynamically determined. Starting with the Android Open Source Project (AOSP) source code, “it quickly became apparent that the size and complexity of the Android environment made it necessary to develop the model and the analysis together as an integrated whole, with the design decisions in the model and the analysis working together synergistically to enable an effective solution to the Android static information-flow analysis problem.” Stubs were developed to cover code outside of the AOSP Java codebase:  Examples of semantics missing in the AOSP and added via accurate analysis stubs include native methods; event callback initiation with accurate context; component life-cycle events; and hidden state maintained by the Android runtime and accessible to the application only via the Android API. For the information flow analysis, 4051 sensitive source methods and 2116 sensitive sink methods were manually identified and classified. In addition the implementation of 117 classes in the Java standard library and Android library were carefully simplified to increase precision and decrease analysis time (on the order of 5 to 10 minutes). At the core of the model are 550 Android classes that account for over 98.1% of the total calls made by over 95K applications downloaded from the Google Play Store. These were manually reviewed to confirm that the implementation fully covered semantics for data flow, object instantiation and aliasing, and that the event callbacks defined are called explicitly by the model with the proper context. At the core of the approach is an analysis method called ‘Points-to’ analysis:  Points-to analysis (PTA) is a foundational static program analysis that computes a static abstraction of all the heap locations that a pointer (reference) variable may point to during program execution. In addition to the points-to relation, points-to analysis also constructs a call graph as modern languages re-quire points-to results to calculate targets for dynamic dispatch and functional lambda calculations. The PTA implementation implements object sensitivity (identifying flows that originate in different object instances). “Object sensitivity is notoriously difficult to understand and implement.” It also requires large amounts of memory. Prior to optimising, several applications could not be analysed even with 64GB of heap memory. With optimisations, all tested applications now fit in under 34GB. The optimisation involved analysing a suite of Android applications to determine an appropriate context-depth (from 0 to 4) for each API class. (Traditional approaches use a fixed context depth). For inter-component communication, string analysis is performed to work out possible runtime targets using the JSA String Analyzer. This creates a regular expression representing the possible values of the string value. After JSA is run, we replace resolved string values in the application code with constants representing their computed regular expression, and perform a pass of our points-to analysis such that these values can be propagated globally. Additional transformations are made for ICC initiation calls and Android Service components to improve model precision. The resulting information flow analysis is built on top of the Soot Java Analysis framework and comprises approximately 70Kloc of Java code. Our information-flow analysis computes an over-approximation of all the memory states that occur during the execution of a program. The analysis is designed as a forward data-flow analysis. For each type of statement, we define a transfer function in terms of how it changes the state of memory. We divide memory into four separate areas that store local variables, instance fields, static fields, and arrays, reflecting the semantics of the Java programming language. Each of the memory areas is modelled as a function whose codomain consists of a set of information values. An information value is a tuple of the type of information and the source code location where the information was first injected. Our analysis can identify not only the kind of information being exfiltrated but the code location of the source. Results  We evaluate DroidSafe on 24 complete real-world Android applications that, as part of the DARPA Automated Program Analysis for Cybersecurity (APAC) program, have been augmented with malicious information flow leaks by three hostile Red Team organizations. The goal of these organizations was to develop information leaks that would either evade detection by static analysis tools or overwhelm static analysis tools into producing unacceptable results (by, for example, manipulating the tool into reporting an overwhelming number of false positive flows). DroidSafe accurately detects all of the 69 malicious flows in these applications (while reporting a manageable total number of flows). A current state-of-the-art Android information-flow analysis system, Flow-Droid + IccTA, in contrast, detects only 6 of the 69 malicious flows, and has a larger ratio of total flows reported to true malicious flows reported. DroidSafe was also evaluated against the DroidBench suite and gave the highest reported accuracy and highest precision for the suite to date at 94.3% and 87.6% respectively. Unsurprisingly, DroidSafe gets 100% accuracy and precision on its own test suite – but the next best tool could only achieve 34.9% accuracy and 79.9% precision. As these results illustrate, DroidSafe implements an analysis of unprecedented accuracy and precision. To the best of our knowledge, DroidSafe provides the first usable information-flow analysis for Android applications. The Secret Sauce  Our experience developing DroidSafe shows that 1) there is no substitute for an accurate and precise model of the application environment, and 2) using the model to drive the design decisions behind the analysis and supporting techniques (such as accurate analysis stubs) is one effective but (inevitably) labor-intensive way to obtain an acceptably precise and accurate analysis. As long as there are complex application frameworks, we anticipate that making an appropriate set of design decisions (such as the use of a scalable flow insensitive analysis) to successfully navigate the trade-off space that the application framework implicitly presents will be a necessary prerequisite for obtaining acceptable accuracy and precision.", "pdf_url": "http://www.internetsociety.org/sites/default/files/02_1_2.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/information-flow-analysis-of-android-applications-in-droidsafe.json"}
{"id": "11313245", "bin": "1500_1600", "summary_sentences": ["Fast In-memory Transaction Processing using RDMA and HTM – Wei et al. 2015  This paper tries to answer a natural question: with advanced processor features and fast interconnects, can we build a transaction processing system that is at least one order of magnitude faster than the state-of-the-art systems without using such features?", "The authors build a distributed transaction system, DrTM, that exploits Hardware Transactional Memory (HTM) and Remote Direct Memory Access (RDMA):  Hardware transactional memory (HTM) has recently come to the mass market in the form of Intel’s restricted transactional memory (RTM).", "The features like atomicity, consistency and isolation (ACI) make it very promising for database transactions.", "Meanwhile, RDMA, which provides direct memory access (DMA) to the memory of a remote machine, has recently gained considerable interests in the systems community.", "With a 6-node cluster (20 cores/server), DrTM achieves 5.52M transactions per second on TPC-C.  It’s interesting to compare that number to RIFL that we looked at last week – from Figure 14 in the RIFL paper we can see that RAMCloud with its kernel bypass transport (fastest configuration) does about 1250 txns per minute (about 21 tps) on TPC-C with a 6-node cluster (only 4 cores/server in this setup).", "RAMCloud is tuned for low latency of course, and has an average latency of 1ms.", "Using I-Confluence analysis, and extrapolating from the figures in Coordination Avoidance in Database Systems , Bailis et al. achieve about 480K transactions per second on TPC-C with a 6-node cluster (32 vcpus/server).", "That system scaled linearly up to 12.7M tps (with a 200 node cluster).", "We’d have to debate whether the hardware was comparable, etc.", "(well, clearly it is not because one system is using direct hardware support, though fewer cores per server) – but an order of magnitude difference is significant.", "How fast could we go if we implemented an I-Confluent system on top of HTM and RDMA???", "To get this level of performance, DrTM depends on moving as much concurrency control as possible into HTM.", "One key challenge is the limited working set of HTM, and DrTM using transaction chopping to keep large transactions within it.", "A second key challenge is that RDMA cannot be used within an HTM region:  DrTM addresses this with a concurrency control protocol that combines HTM and two-phase locking (2PL)  to preserve serializability.", "Speciﬁcally, DrTM uses RDMA-based compare-and-swap (CAS) to lock and fetch the corresponding database records from remote machines before starting an HTM transaction.", "Thanks to the strong consistency of RDMA and the strong atomicity of HTM, any concurrent conﬂicting transactions on a remote machine will be aborted.", "DrTM leverages this property to preserve serializability among distributed transactions.", "To guarantee forward progress, DrTM further provides contention management by leveraging the fallback handler of HTM to prevent possible deadlock and livelock.", "HTM (RTM)  Intel’s Restricted Transactional Memory (RTM) provides strong atomicity within a single machine, where a non-transactional code will unconditionally abort a transaction when their accesses conﬂict.", "RTM uses the ﬁrst-level cache to track the write set and an implementation-speciﬁc structure to track the read set, and relies on the cache coherence protocol to detect conﬂicts.", "Upon a conﬂict, at least one transaction will be aborted.", "RTM provides a set of interfaces including XBEGIN, XEND and XABORT, which will begin, end and abort a transaction accordingly.", "The read/write set of an RTM transaction is limited in size due to the private cache and buffers on the CPU that are used to support it.", "Abort rates increase significantly as working set sizes increase, and a transaction that exceeds the hardware capacity will always be aborted.", "Any use of network I/O will also cause the transaction to be aborted.", "RDMA  Remote Direct Memory Access (RDMA) is a networking feature to provide cross-machine accesses with high speed, low latency and low CPU overhead.", "Much prior work has demonstrated the beneﬁt of using RDMA for in-memory stores and computing platforms.", "RDMA has three communication options.", "In order of increasing performance these are IP-emulation to enable socket-based code to be used unmodified, an MPI with SEND/RECV verbs, and ‘one-sided RDMA’ which provides one-way direct access to the memory of another machine bypassing the CPU.", "One-sided RDMA provides only read, write, and two atomic operations fetch_and_add, and compare _and_swap.", "DrTM  DrTM partitions data into shards spread across many machines connected by RDMA.", "It uses one worker-thread per core, each thread executes and commits a single transaction at a time.", "DrTM exposes a partitioned global address space, though a process still needs to distinguish between local and remote accesses.", "Remote access is primarily via one-sided RDMA operations for efficiency.", "The memory store layer of DrTM provides a general key-value store interface to the upper layers:  To optimize for different access patterns, DrTM provides both an ordered store in the form of a B+ tree and an unordered store in the form of a hash table.", "For the ordered store, we use the B+ tree in DBX, which uses HTM to protect the major B+ tree operations and was shown to have comparable performance with state-of-the-art concurrent B+ tree.", "For the unordered store, we further design and implement a highly optimized hash table based on RDMA and HTM.", "There is prior work on RDMA-optimised hash-tables, but nothing that exploits the combination of HTM and RDMA.", "DrTM leverages the strong atomicity of HTM and strong consistency of RDMA to design an HTM/RDMA- friendly hash table.", "First, DrTM decouples the race detection from the hash table by leveraging the strong atomicity of HTM, where all local operations (e.g., READ/WRITE/ INSERT/DELETE) on key-value pairs are protected by HTM transactions and thus any conﬂicting accesses will abort the HTM transaction.", "This signiﬁcantly simpliﬁes the data structures and operations for race detection.", "Second, DrTM uses one-sided RDMA operations to perform both READ and WRITE to remote key-value pairs without involv- ing the host machine.", "Finally, DrTM separates keys and values as well as its metadata into decoupled memory region, resulting in two-level lookups like Pilaf.", "This makes it efﬁcient to leverage one-sided RDMA READ for lookups, as one RDMA READ can fetch a cluster of keys.", "Further, the separated key-value pair makes it possible to implement RDMA-friendly, location-based and host-transparent caching.", "DrTM uses cluster chaining for hashing.", "When caching lookup results, DrTM chooses to cache the key’s location rather than value.", "This minimizes the lookup cost while still retaining strongly consistent reads and writes.", "To support distributed transactions, DrTM combines HTM with a higher-level two-phase locking (2PL) protocol:  …to preserve serializability among conﬂicting transactions on multiple nodes, we design a 2PL-like protocol to coordinate accesses to the same database records from local and remote worker threads.", "To bridge HTM(which essentially uses OCC) and 2PL, DrTM implements the exclusive and shared locks using one-sided RDMA operations, which are cache- coherent with local accesses and thus provide strong consistency with HTM.", "The challenge is that any RDMA operation inside an HTM transaction will automatically cause it to abort.", "To this end, DrTM uses 2PL to safely accumulate all remote records into a local cache prior to the actual execution in an HTM transaction, and write back the committed updates to other machines until the local commit of the HTM transaction or discard temporal updates after an HTM abort.", "DrTM therefore requires prior knowledge of the read/write sets of transactions for locking and prefetching in the ‘start’ phase.", "For typical OLTP transactions such as TPC-C this is not normally a problem.", "On each individual machine DrTM uses HTM to provide transaction suppport.", "To mitigate the working set size limitations, DrTM uses transaction  chopping ‘with optimisations’ to decompose larger transactions into smaller pieces when needed.", "For read-only transactions with very large read sets DrTM provides a separate scheme to execute read-only transactions without HTM.", "DrTM transactions support strict serializability.", "Currently DrTM provides durability, but not high availability:  DrTM currently preserves durability rather than availability in case of machine failures, as done in recent in-memory databases.", "How to provide availability, e.g., through efﬁciently replicated logging, will be our future work.", "The source code of DrTM will soon be available at  [url]"], "summary_text": "Fast In-memory Transaction Processing using RDMA and HTM – Wei et al. 2015  This paper tries to answer a natural question: with advanced processor features and fast interconnects, can we build a transaction processing system that is at least one order of magnitude faster than the state-of-the-art systems without using such features? The authors build a distributed transaction system, DrTM, that exploits Hardware Transactional Memory (HTM) and Remote Direct Memory Access (RDMA):  Hardware transactional memory (HTM) has recently come to the mass market in the form of Intel’s restricted transactional memory (RTM). The features like atomicity, consistency and isolation (ACI) make it very promising for database transactions. Meanwhile, RDMA, which provides direct memory access (DMA) to the memory of a remote machine, has recently gained considerable interests in the systems community. With a 6-node cluster (20 cores/server), DrTM achieves 5.52M transactions per second on TPC-C.  It’s interesting to compare that number to RIFL that we looked at last week – from Figure 14 in the RIFL paper we can see that RAMCloud with its kernel bypass transport (fastest configuration) does about 1250 txns per minute (about 21 tps) on TPC-C with a 6-node cluster (only 4 cores/server in this setup). RAMCloud is tuned for low latency of course, and has an average latency of 1ms. Using I-Confluence analysis, and extrapolating from the figures in Coordination Avoidance in Database Systems , Bailis et al. achieve about 480K transactions per second on TPC-C with a 6-node cluster (32 vcpus/server). That system scaled linearly up to 12.7M tps (with a 200 node cluster). We’d have to debate whether the hardware was comparable, etc. (well, clearly it is not because one system is using direct hardware support, though fewer cores per server) – but an order of magnitude difference is significant. How fast could we go if we implemented an I-Confluent system on top of HTM and RDMA??? To get this level of performance, DrTM depends on moving as much concurrency control as possible into HTM. One key challenge is the limited working set of HTM, and DrTM using transaction chopping to keep large transactions within it. A second key challenge is that RDMA cannot be used within an HTM region:  DrTM addresses this with a concurrency control protocol that combines HTM and two-phase locking (2PL)  to preserve serializability. Speciﬁcally, DrTM uses RDMA-based compare-and-swap (CAS) to lock and fetch the corresponding database records from remote machines before starting an HTM transaction. Thanks to the strong consistency of RDMA and the strong atomicity of HTM, any concurrent conﬂicting transactions on a remote machine will be aborted. DrTM leverages this property to preserve serializability among distributed transactions. To guarantee forward progress, DrTM further provides contention management by leveraging the fallback handler of HTM to prevent possible deadlock and livelock. HTM (RTM)  Intel’s Restricted Transactional Memory (RTM) provides strong atomicity within a single machine, where a non-transactional code will unconditionally abort a transaction when their accesses conﬂict. RTM uses the ﬁrst-level cache to track the write set and an implementation-speciﬁc structure to track the read set, and relies on the cache coherence protocol to detect conﬂicts. Upon a conﬂict, at least one transaction will be aborted. RTM provides a set of interfaces including XBEGIN, XEND and XABORT, which will begin, end and abort a transaction accordingly. The read/write set of an RTM transaction is limited in size due to the private cache and buffers on the CPU that are used to support it. Abort rates increase significantly as working set sizes increase, and a transaction that exceeds the hardware capacity will always be aborted. Any use of network I/O will also cause the transaction to be aborted. RDMA  Remote Direct Memory Access (RDMA) is a networking feature to provide cross-machine accesses with high speed, low latency and low CPU overhead. Much prior work has demonstrated the beneﬁt of using RDMA for in-memory stores and computing platforms. RDMA has three communication options. In order of increasing performance these are IP-emulation to enable socket-based code to be used unmodified, an MPI with SEND/RECV verbs, and ‘one-sided RDMA’ which provides one-way direct access to the memory of another machine bypassing the CPU. One-sided RDMA provides only read, write, and two atomic operations fetch_and_add, and compare _and_swap. DrTM  DrTM partitions data into shards spread across many machines connected by RDMA. It uses one worker-thread per core, each thread executes and commits a single transaction at a time. DrTM exposes a partitioned global address space, though a process still needs to distinguish between local and remote accesses. Remote access is primarily via one-sided RDMA operations for efficiency. The memory store layer of DrTM provides a general key-value store interface to the upper layers:  To optimize for different access patterns, DrTM provides both an ordered store in the form of a B+ tree and an unordered store in the form of a hash table. For the ordered store, we use the B+ tree in DBX, which uses HTM to protect the major B+ tree operations and was shown to have comparable performance with state-of-the-art concurrent B+ tree. For the unordered store, we further design and implement a highly optimized hash table based on RDMA and HTM. There is prior work on RDMA-optimised hash-tables, but nothing that exploits the combination of HTM and RDMA. DrTM leverages the strong atomicity of HTM and strong consistency of RDMA to design an HTM/RDMA- friendly hash table. First, DrTM decouples the race detection from the hash table by leveraging the strong atomicity of HTM, where all local operations (e.g., READ/WRITE/ INSERT/DELETE) on key-value pairs are protected by HTM transactions and thus any conﬂicting accesses will abort the HTM transaction. This signiﬁcantly simpliﬁes the data structures and operations for race detection. Second, DrTM uses one-sided RDMA operations to perform both READ and WRITE to remote key-value pairs without involv- ing the host machine. Finally, DrTM separates keys and values as well as its metadata into decoupled memory region, resulting in two-level lookups like Pilaf. This makes it efﬁcient to leverage one-sided RDMA READ for lookups, as one RDMA READ can fetch a cluster of keys. Further, the separated key-value pair makes it possible to implement RDMA-friendly, location-based and host-transparent caching. DrTM uses cluster chaining for hashing. When caching lookup results, DrTM chooses to cache the key’s location rather than value. This minimizes the lookup cost while still retaining strongly consistent reads and writes. To support distributed transactions, DrTM combines HTM with a higher-level two-phase locking (2PL) protocol:  …to preserve serializability among conﬂicting transactions on multiple nodes, we design a 2PL-like protocol to coordinate accesses to the same database records from local and remote worker threads. To bridge HTM(which essentially uses OCC) and 2PL, DrTM implements the exclusive and shared locks using one-sided RDMA operations, which are cache- coherent with local accesses and thus provide strong consistency with HTM. The challenge is that any RDMA operation inside an HTM transaction will automatically cause it to abort. To this end, DrTM uses 2PL to safely accumulate all remote records into a local cache prior to the actual execution in an HTM transaction, and write back the committed updates to other machines until the local commit of the HTM transaction or discard temporal updates after an HTM abort. DrTM therefore requires prior knowledge of the read/write sets of transactions for locking and prefetching in the ‘start’ phase. For typical OLTP transactions such as TPC-C this is not normally a problem. On each individual machine DrTM uses HTM to provide transaction suppport. To mitigate the working set size limitations, DrTM uses transaction  chopping ‘with optimisations’ to decompose larger transactions into smaller pieces when needed. For read-only transactions with very large read sets DrTM provides a separate scheme to execute read-only transactions without HTM. DrTM transactions support strict serializability. Currently DrTM provides durability, but not high availability:  DrTM currently preserves durability rather than availability in case of machine failures, as done in recent in-memory databases. How to provide availability, e.g., through efﬁciently replicated logging, will be our future work. The source code of DrTM will soon be available at  [url]", "pdf_url": "http://sigops.org/sosp/sosp15/current/2015-Monterey/printable/158-wei.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/fast-in-memory-transaction-processing-using-rdma-and-rtm.json"}
{"id": "88655260", "bin": "1500_1600", "summary_sentences": ["Software-defined far memory in warehouse-scale computers Lagar-Cavilla et al., ASPLOS’19  Memory (DRAM) remains comparatively expensive, while in-memory computing demands are growing rapidly.", "This makes memory a critical factor in the total cost of ownership (TCO) of large compute clusters, or as Google like to call them “Warehouse-scale computers (WSCs).”  This paper describes a “far memory” system that has been in production deployment at Google since 2016.", "Far memory sits in-between DRAM and flash and colder in-memory data can be migrated to it:  Our software-defined far memory is significantly cheaper (67% or higher memory cost reduction) at relatively good access speeds (6µs) and allows us to store a significant fraction of infrequently accessed data (on average, 20%), translating to significant TCO savings at warehouse scale.", "With a far memory tier in place operators can choose between packing more jobs onto each machine, or reducing the DRAM capacity, both of which lead to TCO reductions.", "Google were able to bring about a 4-5% reduction in memory TCO (worth millions of dollars!)", "while having negligible impact on applications.", "In introducing far memory Google faced a number of challenges: workloads are very diverse and change all the time, both in job mixes and in utilisation (including diurnal patterns), and there is near zero tolerance for application slowdown.", "If extra provisioned capacity was needed to offset a slowdown for example then this could easily offset all potential TCO savings.", "This boils down to a single digit µs latency toleration in the tail for far memory, and in addition to security and privacy concerns, rules out remote memory solutions.", "Google’s “far” memory it turns out is exactly the same memory, but with compressed data stored in it!", "The opportunity  One of the very earliest question to be addressed is how to define ‘cold’ memory, and given that, how much opportunity there is for moving cold memory into a far memory store.", "We focus on a definition that draws from the following two principles: (1) the value of temporal locality, by classifying as cold a memory page that has not been accessed beyond a threshold of T seconds; (2) a proxy for the application effect of far memory, by measuring the rate of accesses to cold memory pages, called promotion rate.", "With T set at 120 seconds, 32% of the memory usage in a Google WSC is cold on average.", "At this threshold applications access 15% of their total cold memory on average every minute.", "Across individual machines in a cluster, the percentage of cold memory varies for 1% to 52%.", "It also varies across jobs:  …storing cold memory to cheaper but slower far memory has great potential of saving TCO in WSCs.", "But for this to be realized in a practical manner, the system has to (1) be able to accurately control its aggressiveness to minimize the impact on application performance, and (2) be resilient to the variation of cold memory behavior across different machines,clusters, and jobs.", "Enter zswap!", "Google use zswap to implement their far memory tier.", "Zswap is readily available and runs as a swap device in the Linux kernel.", "Memory pages moved to zswap are compressed (but the compressed pages stay in memory).", "Thus we’re fundamentally trading (de)-compression latency at access time for the ability to pack more data in memory.", "Using zswap means that no new hardware solutions are required, enabling rapid deployment across clusters.", "…quick deployment of a readily available technology and harvesting its benefits for a longer period of time is more economical than waiting for a few years to deploy newer platforms promising potentially bigger TCO savings.", "zswap’s default control plane did not meet Googles strict performance slowdown and CPU overhead budgets though, so they built a new one to identify cold pages and proactively migrate them to far memory while treating performance as a first-class constraint.", "Cold memory pages are identified in the background and proactively compressed.", "Once accessed, a decompressed page stays in that state until it becomes cold again.", "The key to an efficient system is the identification of cold pages: the cold age threshold determines how many seconds we can go without a page being accessed before it is declared cold.", "The objective is to find the lowest cold age threshold that still allows the system to satisfy its performance constraints.", "A good proxy metric for the overhead introduce by the system is the promotion rate: the rate of swapping pages from far memory to near memory.", "For a given promotion rate, large jobs with more total memory are likely to see less of a slowdown than smaller jobs…  … therefore we design our system to keep the promotion rate below P% of the application’s working set size per minute, which serves as a Service Level Objective for far memory performance.", "From extensive A/B testing, P was empirically determined to be 0.2%/minute.", "At this level the compression/decompression overhead does not interfere with other colocated jobs on the same machine.", "What cold age threshold results in at 0.2%/min promotion rate though?", "Google maintain a promotion histogram for each job in the kernel, which records the total promotion rate of pages colder than the threshold T. This gives an indication of past performance, but we also want to be responsive to spikes.", "So the overall threshold is managed as follows:  The best cold age threshold is tracked for each 1 minute period, and the K-th percentile is used as the threshold for the next one (so we’ll violate approximately 100-K% of the times under steady state conditions)  If jobs access more cold memory during the minute than the chosen K-th percentile then the best cold age threshold from the previous minute is used instead  Zswap is disabled for the first S seconds of job execution to avoid making decisions based on insufficient information.", "The system also collects per-job cold-page histograms for a given set of predefined cold age thresholds.", "These are used to perform offline analysis for potential memory savings under different cold-age thresholds.", "ML-based auto-tuning  To find optimal values for K and S, Google built a model for offline what-if explorations based on collected far-memory traces, that can model one week of an entire WSCs far memory behaviour in less than an hour.", "This model is used by a Gaussian Process (GP) Bandit machine learning model to guide the parameter search towards an optimal point with a minimal number of trials.", "The best parameter configuration found by this process is periodically deployed to the WSC with a carefully monitored phased rollout.", "The big advantage of the ML based approach is that it can continuously adapt to changes in the workload and WSC configuration without needing constant manual tuning.", "To the best of our knowledge, this is the first use of a GP Bandit for optimizing a WSC.", "Evaluation  The far memory system has been deployed in production since 2016.", "The following chart shows the change in cold memory coverage over that time, including the introduction of the autotuner which gave an additional 20% boost.", "Cold memory coverage varies over machine and time, but at the cluster level it remains stable.", "This enabled Google to convert zswap’s cold memory coverage into lower memory provisioning, achieving a 4-5% reduction in DRAM CTO.", "“These savings are realized with no difference in performance SLIs.”  There are very low promotion rates in practice, both before and after deployment of the autotuner.", "CPU overhead for compression and decompression is very low as well (0.001% and 0.005% respectively).", "One of the biggest consumers of DRAM is Bigtable, storing petabytes of data in memory and serving millions of operations per second.", "The following chart shows an A/B test result for Bigtable with and without zswap enabled.", "During this period site engineers monitored application-level performance metrics and observed no SLO violations.", "For Bigtable, zswap achieves 5-15% cold memory coverage.", "Our system has been in deployment in Google’s WSC for several years and our results show that this far memory tier is very effective in saving memory CapEx costs without negatively impacting application performance… Ultimately an exciting end state would be one where the system uses both hardware and software approaches and multiple tiers of far memory (sub-µs tier-1 and single µs tier-2), all managed intelligently with machine learning and working harmoniously to address the DRAM scaling challenge."], "summary_text": "Software-defined far memory in warehouse-scale computers Lagar-Cavilla et al., ASPLOS’19  Memory (DRAM) remains comparatively expensive, while in-memory computing demands are growing rapidly. This makes memory a critical factor in the total cost of ownership (TCO) of large compute clusters, or as Google like to call them “Warehouse-scale computers (WSCs).”  This paper describes a “far memory” system that has been in production deployment at Google since 2016. Far memory sits in-between DRAM and flash and colder in-memory data can be migrated to it:  Our software-defined far memory is significantly cheaper (67% or higher memory cost reduction) at relatively good access speeds (6µs) and allows us to store a significant fraction of infrequently accessed data (on average, 20%), translating to significant TCO savings at warehouse scale. With a far memory tier in place operators can choose between packing more jobs onto each machine, or reducing the DRAM capacity, both of which lead to TCO reductions. Google were able to bring about a 4-5% reduction in memory TCO (worth millions of dollars!) while having negligible impact on applications. In introducing far memory Google faced a number of challenges: workloads are very diverse and change all the time, both in job mixes and in utilisation (including diurnal patterns), and there is near zero tolerance for application slowdown. If extra provisioned capacity was needed to offset a slowdown for example then this could easily offset all potential TCO savings. This boils down to a single digit µs latency toleration in the tail for far memory, and in addition to security and privacy concerns, rules out remote memory solutions. Google’s “far” memory it turns out is exactly the same memory, but with compressed data stored in it! The opportunity  One of the very earliest question to be addressed is how to define ‘cold’ memory, and given that, how much opportunity there is for moving cold memory into a far memory store. We focus on a definition that draws from the following two principles: (1) the value of temporal locality, by classifying as cold a memory page that has not been accessed beyond a threshold of T seconds; (2) a proxy for the application effect of far memory, by measuring the rate of accesses to cold memory pages, called promotion rate. With T set at 120 seconds, 32% of the memory usage in a Google WSC is cold on average. At this threshold applications access 15% of their total cold memory on average every minute. Across individual machines in a cluster, the percentage of cold memory varies for 1% to 52%. It also varies across jobs:  …storing cold memory to cheaper but slower far memory has great potential of saving TCO in WSCs. But for this to be realized in a practical manner, the system has to (1) be able to accurately control its aggressiveness to minimize the impact on application performance, and (2) be resilient to the variation of cold memory behavior across different machines,clusters, and jobs. Enter zswap! Google use zswap to implement their far memory tier. Zswap is readily available and runs as a swap device in the Linux kernel. Memory pages moved to zswap are compressed (but the compressed pages stay in memory). Thus we’re fundamentally trading (de)-compression latency at access time for the ability to pack more data in memory. Using zswap means that no new hardware solutions are required, enabling rapid deployment across clusters. …quick deployment of a readily available technology and harvesting its benefits for a longer period of time is more economical than waiting for a few years to deploy newer platforms promising potentially bigger TCO savings. zswap’s default control plane did not meet Googles strict performance slowdown and CPU overhead budgets though, so they built a new one to identify cold pages and proactively migrate them to far memory while treating performance as a first-class constraint. Cold memory pages are identified in the background and proactively compressed. Once accessed, a decompressed page stays in that state until it becomes cold again. The key to an efficient system is the identification of cold pages: the cold age threshold determines how many seconds we can go without a page being accessed before it is declared cold. The objective is to find the lowest cold age threshold that still allows the system to satisfy its performance constraints. A good proxy metric for the overhead introduce by the system is the promotion rate: the rate of swapping pages from far memory to near memory. For a given promotion rate, large jobs with more total memory are likely to see less of a slowdown than smaller jobs…  … therefore we design our system to keep the promotion rate below P% of the application’s working set size per minute, which serves as a Service Level Objective for far memory performance. From extensive A/B testing, P was empirically determined to be 0.2%/minute. At this level the compression/decompression overhead does not interfere with other colocated jobs on the same machine. What cold age threshold results in at 0.2%/min promotion rate though? Google maintain a promotion histogram for each job in the kernel, which records the total promotion rate of pages colder than the threshold T. This gives an indication of past performance, but we also want to be responsive to spikes. So the overall threshold is managed as follows:  The best cold age threshold is tracked for each 1 minute period, and the K-th percentile is used as the threshold for the next one (so we’ll violate approximately 100-K% of the times under steady state conditions)  If jobs access more cold memory during the minute than the chosen K-th percentile then the best cold age threshold from the previous minute is used instead  Zswap is disabled for the first S seconds of job execution to avoid making decisions based on insufficient information. The system also collects per-job cold-page histograms for a given set of predefined cold age thresholds. These are used to perform offline analysis for potential memory savings under different cold-age thresholds. ML-based auto-tuning  To find optimal values for K and S, Google built a model for offline what-if explorations based on collected far-memory traces, that can model one week of an entire WSCs far memory behaviour in less than an hour. This model is used by a Gaussian Process (GP) Bandit machine learning model to guide the parameter search towards an optimal point with a minimal number of trials. The best parameter configuration found by this process is periodically deployed to the WSC with a carefully monitored phased rollout. The big advantage of the ML based approach is that it can continuously adapt to changes in the workload and WSC configuration without needing constant manual tuning. To the best of our knowledge, this is the first use of a GP Bandit for optimizing a WSC. Evaluation  The far memory system has been deployed in production since 2016. The following chart shows the change in cold memory coverage over that time, including the introduction of the autotuner which gave an additional 20% boost. Cold memory coverage varies over machine and time, but at the cluster level it remains stable. This enabled Google to convert zswap’s cold memory coverage into lower memory provisioning, achieving a 4-5% reduction in DRAM CTO. “These savings are realized with no difference in performance SLIs.”  There are very low promotion rates in practice, both before and after deployment of the autotuner. CPU overhead for compression and decompression is very low as well (0.001% and 0.005% respectively). One of the biggest consumers of DRAM is Bigtable, storing petabytes of data in memory and serving millions of operations per second. The following chart shows an A/B test result for Bigtable with and without zswap enabled. During this period site engineers monitored application-level performance metrics and observed no SLO violations. For Bigtable, zswap achieves 5-15% cold memory coverage. Our system has been in deployment in Google’s WSC for several years and our results show that this far memory tier is very effective in saving memory CapEx costs without negatively impacting application performance… Ultimately an exciting end state would be one where the system uses both hardware and software approaches and multiple tiers of far memory (sub-µs tier-1 and single µs tier-2), all managed intelligently with machine learning and working harmoniously to address the DRAM scaling challenge.", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3297858.3304053?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/sw-far-memory.json"}
{"id": "48141212", "bin": "1500_1600", "summary_sentences": ["Eraser: A dynamic data race detector for multi-threaded programs – Savage et al. 1997  Debugging a multithreaded program can be difficult.", "Simple errors in synchronization can produce timing-dependent data races that can take weeks or months to track down.", "Eraser dynamically detects data races in multi-threaded programs.", "There are two basic approaches to doing this, one based on ‘happens-before’ relationships, and one based on ensuring consistent locking discipline.", "Eraser uses the latter strategy.", "We have aimed Eraser specifically at the lock-based synchronization used in modern multithreaded programs.", "Eraser simply checks that all shared-memory accesses follow a consistent locking discipline.", "A locking discipline is a programming policy that ensures the absence of data races.", "For example, a simple locking discipline is to require that every variable shared between threads is protected by a mutual exclusion lock.", "We will argue that for many programs Eraser’s approach of enforcing a locking discipline is simpler, more efficient, and more thorough at catching races than the approach based on happens-before.", "Let’s get some definitions out of the way, briefly look at happens-before (the prior art), and then take a look at the core lockset algorithm that Eraser uses.", "Definitions  Lock:  A lock is a simple synchronization object used for mutual exclusion; it is either available, or owned by a thread.", "The operations on a lock mu are lock(mu) and unlock(mu).", "Thus it is essentially a binary semaphore used for mutual exclusion, but differs from a semaphore in that only the ownerof a lock is allowed to release it.", "Data Race:  A data race occurs when two concurrent threads access a shared variable and when at least one access is a write and the threads use no explicit mechanism to prevent the accesses from being simultaneous.", "Happens-before  Happens-before is a partial order of all events of all threads in a concurrent execution.", "For any single thread, events are ordered in the order in which they occur.", "Between threads, events are ordered according to the properties of the synchronization objects they access.", "If one thread accesses a synchronization object, and the next access to the object is by a different thread, then the first access is defined to happen before the second if the semantics of the synchronization object forbid a schedule in which these two interactions are exchanged in time.", "For example,  if one thread must release a lock before another thread can acquire it then we have a happens-before relationship.", "Without the presence of some sychnronization object, then we are at the mercy of data races.", "If two threads both access a shared variable, and the accesses are not ordered by the happens-before relation, then in another execution of the program in which the slower thread ran faster and/or the faster thread ran slower, the two accesses could have happened simultaneously; that is, adata race could have occurred, whether or not it actually did occur.", "All previous dynamic race detection tools that we know of are based on this observation.", "The authors of the paper point out that detectors based on happens-before are at the mercy of the interleaving produced by the scheduler for whether they can detect a race or not (see the easy to follow example in the paper).", "The Lockset algorithm used by Eraser does not have this property.", "The Lockset Algorithm  The basic version of the algorithm seeks to ensure that every access to a shared variable is protected by some lock.", "Whatever that lock is (since we don’t know this up front), we want the lock to be held by any thread that accesses the variable.", "Eraser infers the protection relationship between lock and variable during program execution.", "For each shared variable v, Eraser maintains the set C(v) of candidate locks for v. This set contains those locks that have protected v for the computation so far.", "That is, a lock l is in C(v) if, in the computation up to that point, every thread that has accessed v was holding l at the moment of the access.", "When a new variable v is initialized, its candidate set C(v) is considered to hold all possible locks.", "When the variable is accessed, Eraser updates C(v) with the intersection of C(v) and the set of locks held by the current thread.", "This process, called lockset refinement, ensures that any lock that consistently protects v is contained in C(v).", "If some lock lconsistently protects v, it will remain in C(v) as C(v) is refined.", "If C(v) becomes empty this indicates that there is no lock that consistently protects v.  If locks_held(t) is the set of locks held by thread t,  then the algorithm can be succintly expressed as follows;  For each v, initialize C(v) to the set of all locks.", "On each access to v by thread t, set C(v) :=  the intersection of C(v) and locks_held(t); if C(v) = { }, then issue a warning.", "Which is pretty neat for something so simple!", "There are some refinements necessary to handle the case of initialization (which may often occur without locking),  variables that are read-only after initialization (final variables) and can be safely read without locks, and read-write locks that allow multiple simultaneous readers but  write exclusivity.", "These are all fairly easily dealt with:  For initialization, reporting of warnings is deferred until after initialization.", "The end of initialization is taken as the point at which a second thread first accesses the variable.", "To support final variables, races are only reported after a variable has become write-shared by more than one thread.", "To deal with multiple reader, single writer variables, locks held purely in read mode are removed from the candidate set when a write occurs, as these locks do not protect against a race between the writer and some other reader thread:  Many programs use single-writer, multiple-reader locks as well as simple locks.", "To accommodate this style we introduce our last refinement of the locking discipline: we require that for each variable v, some lock m protects v, meaning m is held in write mode for every write of v, and m is held in some mode (read or write) for every read of v.  One issue with Eraser is that it can create false alarms.", "These fall into three broad catagories:  Memory reuse:  if a program allocates its own memory blocks  and later on privately recycles them, Eraser has no way of knowing that the ‘new’ memory is now protected by a new set of locks.", "Private locks: using a locking mechanism other than pthreads, which the Eraser implementation described in the paper would then be unable to detect.", "Benign race: genuine races, but which do not affect the result of the program (some of these may be intentional)  Eraser introduced an annotation model to suppress unwanted warnings.", "Not implemented in Eraser, but a useful extension, is the ability to detect deadlocks.", "The paper gives us a sketch of how this can work:  A simple discipline that avoids deadlock is to choose a partial order among all locks and to program each thread so that whenever it holds more than one lock, it acquires them in ascending order.", "This discipline is similar to the locking discipline for avoiding data races: it is suitable for checking by dynamic monitoring, and it is easier to produce a test case that exposes a breach of the discipline than it is to produce a test case that actually causes a deadlock.", "A results of checking several programs with Eraser are reported – and as expected, it does a good job of flushing out bugs!", "A note on Valgrind  Today the most commonly used tool is arguably Valgrind, which implements data-race detection with DRD .", "This is what the Valgrind manual has to say on the topic:  There exist two different approaches for verifying the correctness of multithreaded programs at runtime.", "The approach of the so-called Eraser algorithm is to verify whether all shared memory accesses follow a consistent locking strategy.", "And the happens-before data race detectors verify directly whether all interthread memory accesses are ordered by synchronization operations.", "While the last approach is more complex to implement, and while it is more sensitive to OS scheduling, it is a general approach that works for all classes of multithreaded programs.", "An important advantage of happens-before data race detectors is that these do not report any false positives."], "summary_text": "Eraser: A dynamic data race detector for multi-threaded programs – Savage et al. 1997  Debugging a multithreaded program can be difficult. Simple errors in synchronization can produce timing-dependent data races that can take weeks or months to track down. Eraser dynamically detects data races in multi-threaded programs. There are two basic approaches to doing this, one based on ‘happens-before’ relationships, and one based on ensuring consistent locking discipline. Eraser uses the latter strategy. We have aimed Eraser specifically at the lock-based synchronization used in modern multithreaded programs. Eraser simply checks that all shared-memory accesses follow a consistent locking discipline. A locking discipline is a programming policy that ensures the absence of data races. For example, a simple locking discipline is to require that every variable shared between threads is protected by a mutual exclusion lock. We will argue that for many programs Eraser’s approach of enforcing a locking discipline is simpler, more efficient, and more thorough at catching races than the approach based on happens-before. Let’s get some definitions out of the way, briefly look at happens-before (the prior art), and then take a look at the core lockset algorithm that Eraser uses. Definitions  Lock:  A lock is a simple synchronization object used for mutual exclusion; it is either available, or owned by a thread. The operations on a lock mu are lock(mu) and unlock(mu). Thus it is essentially a binary semaphore used for mutual exclusion, but differs from a semaphore in that only the ownerof a lock is allowed to release it. Data Race:  A data race occurs when two concurrent threads access a shared variable and when at least one access is a write and the threads use no explicit mechanism to prevent the accesses from being simultaneous. Happens-before  Happens-before is a partial order of all events of all threads in a concurrent execution. For any single thread, events are ordered in the order in which they occur. Between threads, events are ordered according to the properties of the synchronization objects they access. If one thread accesses a synchronization object, and the next access to the object is by a different thread, then the first access is defined to happen before the second if the semantics of the synchronization object forbid a schedule in which these two interactions are exchanged in time. For example,  if one thread must release a lock before another thread can acquire it then we have a happens-before relationship. Without the presence of some sychnronization object, then we are at the mercy of data races. If two threads both access a shared variable, and the accesses are not ordered by the happens-before relation, then in another execution of the program in which the slower thread ran faster and/or the faster thread ran slower, the two accesses could have happened simultaneously; that is, adata race could have occurred, whether or not it actually did occur. All previous dynamic race detection tools that we know of are based on this observation. The authors of the paper point out that detectors based on happens-before are at the mercy of the interleaving produced by the scheduler for whether they can detect a race or not (see the easy to follow example in the paper). The Lockset algorithm used by Eraser does not have this property. The Lockset Algorithm  The basic version of the algorithm seeks to ensure that every access to a shared variable is protected by some lock. Whatever that lock is (since we don’t know this up front), we want the lock to be held by any thread that accesses the variable. Eraser infers the protection relationship between lock and variable during program execution. For each shared variable v, Eraser maintains the set C(v) of candidate locks for v. This set contains those locks that have protected v for the computation so far. That is, a lock l is in C(v) if, in the computation up to that point, every thread that has accessed v was holding l at the moment of the access. When a new variable v is initialized, its candidate set C(v) is considered to hold all possible locks. When the variable is accessed, Eraser updates C(v) with the intersection of C(v) and the set of locks held by the current thread. This process, called lockset refinement, ensures that any lock that consistently protects v is contained in C(v). If some lock lconsistently protects v, it will remain in C(v) as C(v) is refined. If C(v) becomes empty this indicates that there is no lock that consistently protects v.  If locks_held(t) is the set of locks held by thread t,  then the algorithm can be succintly expressed as follows;  For each v, initialize C(v) to the set of all locks. On each access to v by thread t, set C(v) :=  the intersection of C(v) and locks_held(t); if C(v) = { }, then issue a warning. Which is pretty neat for something so simple! There are some refinements necessary to handle the case of initialization (which may often occur without locking),  variables that are read-only after initialization (final variables) and can be safely read without locks, and read-write locks that allow multiple simultaneous readers but  write exclusivity. These are all fairly easily dealt with:  For initialization, reporting of warnings is deferred until after initialization. The end of initialization is taken as the point at which a second thread first accesses the variable. To support final variables, races are only reported after a variable has become write-shared by more than one thread. To deal with multiple reader, single writer variables, locks held purely in read mode are removed from the candidate set when a write occurs, as these locks do not protect against a race between the writer and some other reader thread:  Many programs use single-writer, multiple-reader locks as well as simple locks. To accommodate this style we introduce our last refinement of the locking discipline: we require that for each variable v, some lock m protects v, meaning m is held in write mode for every write of v, and m is held in some mode (read or write) for every read of v.  One issue with Eraser is that it can create false alarms. These fall into three broad catagories:  Memory reuse:  if a program allocates its own memory blocks  and later on privately recycles them, Eraser has no way of knowing that the ‘new’ memory is now protected by a new set of locks. Private locks: using a locking mechanism other than pthreads, which the Eraser implementation described in the paper would then be unable to detect. Benign race: genuine races, but which do not affect the result of the program (some of these may be intentional)  Eraser introduced an annotation model to suppress unwanted warnings. Not implemented in Eraser, but a useful extension, is the ability to detect deadlocks. The paper gives us a sketch of how this can work:  A simple discipline that avoids deadlock is to choose a partial order among all locks and to program each thread so that whenever it holds more than one lock, it acquires them in ascending order. This discipline is similar to the locking discipline for avoiding data races: it is suitable for checking by dynamic monitoring, and it is easier to produce a test case that exposes a breach of the discipline than it is to produce a test case that actually causes a deadlock. A results of checking several programs with Eraser are reported – and as expected, it does a good job of flushing out bugs! A note on Valgrind  Today the most commonly used tool is arguably Valgrind, which implements data-race detection with DRD . This is what the Valgrind manual has to say on the topic:  There exist two different approaches for verifying the correctness of multithreaded programs at runtime. The approach of the so-called Eraser algorithm is to verify whether all shared memory accesses follow a consistent locking strategy. And the happens-before data race detectors verify directly whether all interthread memory accesses are ordered by synchronization operations. While the last approach is more complex to implement, and while it is more sensitive to OS scheduling, it is a general approach that works for all classes of multithreaded programs. An important advantage of happens-before data race detectors is that these do not report any false positives.", "pdf_url": "http://www.cs.duke.edu/courses/cps210/spring06/papers/eraser.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/eraser-a-dynamic-data-race-detector-for-multi-threaded-programs.json"}
{"id": "76874430", "bin": "1500_1600", "summary_sentences": ["Let’s talk about storage and recovery methods for non-volatile memory database systems Arulraj et al., SIGMOD 2015  Update: fixed a bunch of broken links.", "I can’t believe I only just found out about this paper!", "It’s exactly what I’ve been looking for in terms of an analysis of the impacts of NVM on data storage systems, and the potential benefits of adapting storage algorithms to better exploit the technology.", "See my “ All change please ” post from earlier this year for a summary of NVM itself.", "Today’s paper also contains some background on that, but I’m going to focus on the heart of the material – in which the authors implement three of the common DBMS storage engine strategies and run them on an NVM system to see how they fare, before seeing what improvements can be made when the algorithms are then adapted to be optimised for NVM.", "Is NVM going to make a big difference, and will we need to rearchitect to get the most out of it?", "Yes (although it’s not quite an order-of-magnitude difference):  We then present NVM-aware variants of these architectures that leverage the persistence and byte-addressability properties of NVM in their storage and recovery methods.", "Our experimental evaluation on an NVM hardware emulator shows that these engines achieve up to 5.5x higher throughput than their traditional counterparts while reducing the amount of wear due to write operations by up to 2x.", "A very short primer on NVM  NVM provides low latency reads and writes on the same order of magnitude as DRAM, coupled with persistent writes and a storage capacity similar to SSDs.", "NVM can scale beyond DRAM, and uses less power (DRAM consumes about 40% of the overall power consumed by a server).", "Flash-based SSDs use less power but are slower and only support block-based access.", "Although the advantages of NVM are obvious, making full use of them in an OLTP DBMS is non-trivial…  Traditional DBMS engines are designed to cope with orders-of-magnitude differences in latency between volatile and non-volatile storage, and to optimise for block-level granularity with non-volatile storage.", "Many of the coping strategies employed are unnecessary in an NVM-only system.", "The authors perform their tests using an Intel Labs hardware emulator which is able to simulate varying hardware profiles expected from different NVM devices.", "The DBMS testbed  The authors study systems that are NVM-only (not two-level or greater storage hierarchies).", "They developed a single lightweight DBMS testbed into which multiple storage engines can be plugged.", "Using this consistent harness for all tests means that differences in performance will be due solely to the storage engines themselves.", "They develop traditional implementations of three storage engines, each using different approaches for supporting durable updates: (i) an in-place updates engine, (ii) a copy-on-write updates engine, and (iii) a log-structured updates engine.", "After taking a performance baseline for these engines using both YCSB and TPC-C workloads, they then develop NVM-aware derivatives of each of the engines and evaluate those for comparison.", "We get to see what changes are made, and the difference that they make.", "In-Place updates engine  The in-place updates engine keeps only a single version of each tuple at at all times.", "New values are written directly on top of old ones.", "The design of the in-place engine used in the study was based on VoltDB.", "A Write-Ahead Log (WAL) is used to assist in recovery from crashes and power failures, using a variant of ARIES adapted for main-memory DBMSs with byte-addressable storage engines.", "(See also MARS ).", "The standard in-place update engine has a high rate of data duplication – recording updates both in the WAL and in the table storage area.", "The logging infrastructure is designed on the assumption that the durable storage is much slower than memory, and thus batches updates (which increases response latency).", "Given this, we designed the NVM-InP engine to avoid these issues.", "Now when a transaction inserts a tuple, rather than copying the tuple to the WAL, the NVM-InP engine only records a non-volatile pointer to the tuple in the WAL.", "This is sufficient because both the pointer and the tuple referred to by the pointer are stored on NVM.", "Thus, the engine can use the pointer to access the tuple after the system restarts without needing to re-apply changes in the WAL.", "It also stores indexes as non-volatile B+trees that can be accessed immediately when the system restarts without rebuilding.", "There is no need to replay the log during recovery as committed transactions are durable immediately a transaction commits.", "The effects of uncommitted transactions that may be present in the database do need to be undone though.", "Copy-on-write updates engine  Instead of modifying the original tuple, a CoW engine creates a copy and then modifies that.", "It uses different look-up directories for accessing versions of tuples (aka shadow paging).", "There is no need for a WAL for recovery under this scheme.", "When a transaction commits, the engine updates the master record atomically to point to the new version of a tuple.", "In the study, the CoW engine uses LMDB’s copy-on-write B-trees, storing directories on the filesystem with tuples in an HDD/SDD optimized format with all fields inlined.", "The CoW engine incurs a high overhead in propagating changes to the dirty directory – even if a transaction only modifies one tuple a whole block is copied to the filesystem.", "The NVM-CoW engine employs three optimizations to reduce these overheads.", "First, it uses a non-volatile copy-on-write B+tree that it maintains using the allocator interface.", "Second, the NVM-CoW engine directly persists the tuple copies and only records non-volatile tuple pointers in the dirty directory.", "Lastly, it uses the lightweight durability mechanism of the allocator interface to persist changes in the copy-on-write B+tree.", "It thus avoids the transformation and copying costs incurred by the original engine.", "Log-structured updates engine  The log-structured engine employs Log-structured merge (LSM) trees.", "Each tree consists of a collection of runs of data, each run is an ordered set of entries recording changes made to tuples.", "Runs reside either in volatile memory or stable storage with changes batched in memory and periodically cascaded out to stable storage.", "The contents of the memory table are lost on restart, so the engine maintains a WAL.", "The NVM version using a WAL stored on NVM.", "It avoids data duplication in the memory table and WAL by recording only non-volatile pointers to tuple modifications in the WAL.", "Instead of periodically flushing memory tables to stable storage tables (in stable storage optimised format), memory tables are simply marked as immutable .", "A few performance comparisons  Throughput comparisons at different latencies across the six engines for YCSB:  (Click for larger view)  On YCSB read-only workloads, NVM-InP is no faster than In-P, but NVM-CoW is 1.9-2.1x faster than straight CoW, and NVM-Log is 2.8x faster than Log.", "Under balanced and write-heavy workloads the NVM variants do much better, the NVM-CoW engine being 4.3-5.5x faster than straight CoW.", "Under TPC-C the NVM engines are 1.8-2.1x faster than the traditional engines.", "And for TPC-C:  Example of the difference in number of reads and writes (TPC-C):  On YCSB the NVM-aware engines perform up to 53% fewer loads, and 17-48% fewer stores on write-heavy workloads.", "For TPC-C, the NVM-aware engines perform 31-42% fewer writes.", "Total storage footprints:  NVM-InP and NVM-Log use 17-21% less storage on YCSB, and NVM-CoW uses 25% less.", "The NVM engines use 31-38% less storage on TPC-C – the space savings are more significant due to the write-intensive workload with long-running transactions.", "The takeaway  NVM access latency is the most significant factor in determining the runtime performance of the engines.", "The NVM aware variants achieve better absolute throughput (up to 5.5x), and perform fewer store operations (less than half for write-intensive workloads) which helps to extend device lifetime.", "They also use less storage (17-38% depending on workload) overall, which is important because early NVM products are expected to be relatively expensive.", "Overall, we find that the NVM-aware In-place engine performs the best across a wide set of workload mixtures and skew settings for all NVM latency configurations….", "It achieved the best throughput among all the engines with the least amount of wear on the NVM device."], "summary_text": "Let’s talk about storage and recovery methods for non-volatile memory database systems Arulraj et al., SIGMOD 2015  Update: fixed a bunch of broken links. I can’t believe I only just found out about this paper! It’s exactly what I’ve been looking for in terms of an analysis of the impacts of NVM on data storage systems, and the potential benefits of adapting storage algorithms to better exploit the technology. See my “ All change please ” post from earlier this year for a summary of NVM itself. Today’s paper also contains some background on that, but I’m going to focus on the heart of the material – in which the authors implement three of the common DBMS storage engine strategies and run them on an NVM system to see how they fare, before seeing what improvements can be made when the algorithms are then adapted to be optimised for NVM. Is NVM going to make a big difference, and will we need to rearchitect to get the most out of it? Yes (although it’s not quite an order-of-magnitude difference):  We then present NVM-aware variants of these architectures that leverage the persistence and byte-addressability properties of NVM in their storage and recovery methods. Our experimental evaluation on an NVM hardware emulator shows that these engines achieve up to 5.5x higher throughput than their traditional counterparts while reducing the amount of wear due to write operations by up to 2x. A very short primer on NVM  NVM provides low latency reads and writes on the same order of magnitude as DRAM, coupled with persistent writes and a storage capacity similar to SSDs. NVM can scale beyond DRAM, and uses less power (DRAM consumes about 40% of the overall power consumed by a server). Flash-based SSDs use less power but are slower and only support block-based access. Although the advantages of NVM are obvious, making full use of them in an OLTP DBMS is non-trivial…  Traditional DBMS engines are designed to cope with orders-of-magnitude differences in latency between volatile and non-volatile storage, and to optimise for block-level granularity with non-volatile storage. Many of the coping strategies employed are unnecessary in an NVM-only system. The authors perform their tests using an Intel Labs hardware emulator which is able to simulate varying hardware profiles expected from different NVM devices. The DBMS testbed  The authors study systems that are NVM-only (not two-level or greater storage hierarchies). They developed a single lightweight DBMS testbed into which multiple storage engines can be plugged. Using this consistent harness for all tests means that differences in performance will be due solely to the storage engines themselves. They develop traditional implementations of three storage engines, each using different approaches for supporting durable updates: (i) an in-place updates engine, (ii) a copy-on-write updates engine, and (iii) a log-structured updates engine. After taking a performance baseline for these engines using both YCSB and TPC-C workloads, they then develop NVM-aware derivatives of each of the engines and evaluate those for comparison. We get to see what changes are made, and the difference that they make. In-Place updates engine  The in-place updates engine keeps only a single version of each tuple at at all times. New values are written directly on top of old ones. The design of the in-place engine used in the study was based on VoltDB. A Write-Ahead Log (WAL) is used to assist in recovery from crashes and power failures, using a variant of ARIES adapted for main-memory DBMSs with byte-addressable storage engines. (See also MARS ). The standard in-place update engine has a high rate of data duplication – recording updates both in the WAL and in the table storage area. The logging infrastructure is designed on the assumption that the durable storage is much slower than memory, and thus batches updates (which increases response latency). Given this, we designed the NVM-InP engine to avoid these issues. Now when a transaction inserts a tuple, rather than copying the tuple to the WAL, the NVM-InP engine only records a non-volatile pointer to the tuple in the WAL. This is sufficient because both the pointer and the tuple referred to by the pointer are stored on NVM. Thus, the engine can use the pointer to access the tuple after the system restarts without needing to re-apply changes in the WAL. It also stores indexes as non-volatile B+trees that can be accessed immediately when the system restarts without rebuilding. There is no need to replay the log during recovery as committed transactions are durable immediately a transaction commits. The effects of uncommitted transactions that may be present in the database do need to be undone though. Copy-on-write updates engine  Instead of modifying the original tuple, a CoW engine creates a copy and then modifies that. It uses different look-up directories for accessing versions of tuples (aka shadow paging). There is no need for a WAL for recovery under this scheme. When a transaction commits, the engine updates the master record atomically to point to the new version of a tuple. In the study, the CoW engine uses LMDB’s copy-on-write B-trees, storing directories on the filesystem with tuples in an HDD/SDD optimized format with all fields inlined. The CoW engine incurs a high overhead in propagating changes to the dirty directory – even if a transaction only modifies one tuple a whole block is copied to the filesystem. The NVM-CoW engine employs three optimizations to reduce these overheads. First, it uses a non-volatile copy-on-write B+tree that it maintains using the allocator interface. Second, the NVM-CoW engine directly persists the tuple copies and only records non-volatile tuple pointers in the dirty directory. Lastly, it uses the lightweight durability mechanism of the allocator interface to persist changes in the copy-on-write B+tree. It thus avoids the transformation and copying costs incurred by the original engine. Log-structured updates engine  The log-structured engine employs Log-structured merge (LSM) trees. Each tree consists of a collection of runs of data, each run is an ordered set of entries recording changes made to tuples. Runs reside either in volatile memory or stable storage with changes batched in memory and periodically cascaded out to stable storage. The contents of the memory table are lost on restart, so the engine maintains a WAL. The NVM version using a WAL stored on NVM. It avoids data duplication in the memory table and WAL by recording only non-volatile pointers to tuple modifications in the WAL. Instead of periodically flushing memory tables to stable storage tables (in stable storage optimised format), memory tables are simply marked as immutable . A few performance comparisons  Throughput comparisons at different latencies across the six engines for YCSB:  (Click for larger view)  On YCSB read-only workloads, NVM-InP is no faster than In-P, but NVM-CoW is 1.9-2.1x faster than straight CoW, and NVM-Log is 2.8x faster than Log. Under balanced and write-heavy workloads the NVM variants do much better, the NVM-CoW engine being 4.3-5.5x faster than straight CoW. Under TPC-C the NVM engines are 1.8-2.1x faster than the traditional engines. And for TPC-C:  Example of the difference in number of reads and writes (TPC-C):  On YCSB the NVM-aware engines perform up to 53% fewer loads, and 17-48% fewer stores on write-heavy workloads. For TPC-C, the NVM-aware engines perform 31-42% fewer writes. Total storage footprints:  NVM-InP and NVM-Log use 17-21% less storage on YCSB, and NVM-CoW uses 25% less. The NVM engines use 31-38% less storage on TPC-C – the space savings are more significant due to the write-intensive workload with long-running transactions. The takeaway  NVM access latency is the most significant factor in determining the runtime performance of the engines. The NVM aware variants achieve better absolute throughput (up to 5.5x), and perform fewer store operations (less than half for write-intensive workloads) which helps to extend device lifetime. They also use less storage (17-38% depending on workload) overall, which is important because early NVM products are expected to be relatively expensive. Overall, we find that the NVM-aware In-place engine performs the best across a wide set of workload mixtures and skew settings for all NVM latency configurations…. It achieved the best throughput among all the engines with the least amount of wear on the NVM device.", "pdf_url": "http://www.pdl.cmu.edu/PDL-FTP/NVM/storage.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/lets-talk-about-storage-and-recovery-methods-for-non-volatile-memory-database-systems.json"}
{"id": "53935450", "bin": "1500_1600", "summary_sentences": ["Towards a hands-free query optimizer through deep learning Marcus & Papaemmanouil, CIDR’19  Where the SageDB paper stopped— at the exploration of learned models to assist in query optimisation— today’s paper choice picks up, looking exclusively at the potential to apply learning (in this case deep reinforcement learning) to build a better optimiser.", "Why reinforcement learning?", "Query optimisers are traditionally composed of carefully tuned and complex heuristics based on years of experience.", "Feedback from the actual execution of query plans can be used to update cardinality estimates.", "Database cracking, adaptive indexing , and adaptive query processing all incorporate elements of feedback as well.", "In this vision paper, we argue that recent advances in deep reinforcement learning (DRL) can be applied to query optimization, resulting in a “hands-free” optimizer that (1) can tune itself for a particular database automatically without requiring intervention from expert DBAs, and (2) tightly incorporates feedback from past query optimizations and executions in order to improve the performance of query execution plans generated in the future.", "If we view query optimisation as a DRL problem, then in reinforcement learning terminology the optimiser is the agent, the current query plan is the state, and each available action represents an individual change to the query plan.", "The agent learns a policy which informs the actions it chooses under differing circumstances.", "Once the agent decides to take no further actions the episode is complete and the agent’s reward is (ideally) a measure of how well the generated plan actually performed.", "There are a number of challenges, explored in this paper, with making this conceptual mapping work well in practice.", "Not least of which is that evaluating the reward function (executing a query plan to see how well it performs) is very expensive compared to e.g. computing the score in an Atari game.", "The ReJOIN join order enumerator  ReJOIN explores some of these ideas on a subset of the overall query optimisation problem: learning a join order enumerator.", "Each query sent to ReJOIN is an episode, the state represents subtrees of a binary join tree together with information about the query join and selection predicates.", "Actions combine two subtrees into a single tree.", "Once all input relations are joined the episode ends, and ReJOIN assigns a reward based on the optimiser’s cost model.", "It’s policy network is updated on the basis of this score.", "The final join ordering is passed to the optimiser to complete the physical plan.", "Using the optimiser’s cost model as a proxy for the ultimate performance of a generated query plan enables join orderings to be evaluated much more quickly.", "The following chart shows how ReJOIN learns to produce good join orders during training.", "It takes nearly 9000 episodes (queries) to become competitive with PostgreSQL.", "Once ReJOIN has caught up with PostgreSQL, it goes on to surpass it, producing orderings with lower cost.", "Also of note here is that after training, ReJOIN produces its query plans faster than PostgreSQL’s built-in join enumerator in many cases.", "The bottom-up nature of ReJOIN’s algorithm is  , whereas PostgreSQL’s greedy bottom-up algorithm is  .", "In addition to being limited to just join ordering, ReJOIN’s use of the query optimiser’s cost model to generate reward signals means that it is still dependent on a well-tuned cost model, which is a big part of the problem we wanted to solve in the first place.", "Ideally we’d like to extend the approach to handle full physical plan generation, and also remove the dependency on having an existing cost model.", "Challenges in extending the approach  Once we go from just join ordering to the full search space including operator and access path selection etc., the approach from ReJOIN is unable to learn effective polities in reasonable time.", "An initial model failed to out-perform random choice even after 72 hours of training.", "If we use actual query execution time to generate the reward, then initial policies which will often generate very inefficient plans will take a long time to obtain a reward signal.", "Thus we learn the slowest at exactly the point when we’d like to learn the fastest and the system takes a prohibitive amount of time to converge to good results.", "(An experiment with ReJOIN using real query latency instead of optimiser cost confirmed this).", "Finally, query latency as a reward signal doesn’t meet the expectations of many DRL algorithms that the reward signal is dense and linear.", "A dense reward signal is one that provides incremental feedback with every action the agent takes (such as the score updating in an Atari game), not just at the end of the episode.", "The linear assumption means that an algorithm may attempt to maximise the sum of many small rewards within an episode.", "We have identified how the large search space, delayed reward signal, and costly performance indicators provide substantial hurdles to naive applications of DRL to query optimization.", "Should we just give up on the idea then?", "All is not lost yet!", "The last section of the paper details a number of alternative approaches that could overcome some of these hurdles.", "Research directions  Three techniques that may help to make DRL-based query optimisation practical again are learning from demonstration, cost-model bootstrapping, and incremental learning.", "We’ll look at each of these briefly in turn next (there are no results from applying or evaluating these ideas as yet).", "Learning from demonstration  In learning from demonstration, a model is first trained to mimic the behaviour of an existing expert, and then goes on to learn directly from its actions.", "In the context of query optimisation, we would first train a model to mimic the actions taken by an existing optimiser (indexes, join orderings, pruning of bad plans etc.", "), and then switch to optimising queries directly bypassing the ‘mentor’ optimiser.", "In this second phase the agent fine-tunes its own policy.", "The advantage of this strategy is that mimicking the existing optimiser in the early stages helps the optimiser agent to avoid the ‘obviously bad’ parts of the search space.", "Since the behavior of the model in the second phase should not initially stray too far from the behavior of the expert system, we do not have to worry about executing any exceptionally poor query plans.", "Additionally, since the second training phase only needs to fine-tune an already-performant model, the delayed reward signal is of far less consequence.", "Cost model bootstrapping  Cost model bootstrapping uses a very similar in spirit two-phase approach.", "In the first phase, instead of learning to mimic the actions of an existing expert optimiser, the judgements of the existing expert optimiser (i.e., it’s cost model) are used to bring the agent to an initial level of competence.", "The optimiser’s cost model is used as the reward signal during initial training, and once the agent has learned to produce good policies according to the cost model, it is then switched to learning from a reward based on actual query latency.", "One complication is doing this is that the reward units (scale) need to be consistent across the costs produced by the query optimiser cost model, and the latency measurements of actual query executions.", "We could apply some kind of normalisation across the two, or alternatively transfer the weights from the first network to a new network (transfer learning).", "Incremental learning  Incremental learning attempts to mitigate the issues stemming from poor query plans early in the learning cycle by beginning learning on simpler problems:  … incrementally learning query optimization by first training a model to handle simple cases and slowly introducing more complexity.", "This approach makes the extremely large search space more manageable by dividing it into smaller pieces.", "In the context of query optimisation, we can make problems easier by reducing the number of relations, or by reducing the number of dimensions to be considered.", "That leads to a problem space that looks like this:  We could therefore try starting with a small number of pipeline phases and gradually introducing more, as shown in the following figure:  Or we could try starting with small examples and gradually focus on larger and larger queries.", "Maybe a hybrid strategy will be best, starting with join order and small queries, and gradually increasing sophistication in both dimensions."], "summary_text": "Towards a hands-free query optimizer through deep learning Marcus & Papaemmanouil, CIDR’19  Where the SageDB paper stopped— at the exploration of learned models to assist in query optimisation— today’s paper choice picks up, looking exclusively at the potential to apply learning (in this case deep reinforcement learning) to build a better optimiser. Why reinforcement learning? Query optimisers are traditionally composed of carefully tuned and complex heuristics based on years of experience. Feedback from the actual execution of query plans can be used to update cardinality estimates. Database cracking, adaptive indexing , and adaptive query processing all incorporate elements of feedback as well. In this vision paper, we argue that recent advances in deep reinforcement learning (DRL) can be applied to query optimization, resulting in a “hands-free” optimizer that (1) can tune itself for a particular database automatically without requiring intervention from expert DBAs, and (2) tightly incorporates feedback from past query optimizations and executions in order to improve the performance of query execution plans generated in the future. If we view query optimisation as a DRL problem, then in reinforcement learning terminology the optimiser is the agent, the current query plan is the state, and each available action represents an individual change to the query plan. The agent learns a policy which informs the actions it chooses under differing circumstances. Once the agent decides to take no further actions the episode is complete and the agent’s reward is (ideally) a measure of how well the generated plan actually performed. There are a number of challenges, explored in this paper, with making this conceptual mapping work well in practice. Not least of which is that evaluating the reward function (executing a query plan to see how well it performs) is very expensive compared to e.g. computing the score in an Atari game. The ReJOIN join order enumerator  ReJOIN explores some of these ideas on a subset of the overall query optimisation problem: learning a join order enumerator. Each query sent to ReJOIN is an episode, the state represents subtrees of a binary join tree together with information about the query join and selection predicates. Actions combine two subtrees into a single tree. Once all input relations are joined the episode ends, and ReJOIN assigns a reward based on the optimiser’s cost model. It’s policy network is updated on the basis of this score. The final join ordering is passed to the optimiser to complete the physical plan. Using the optimiser’s cost model as a proxy for the ultimate performance of a generated query plan enables join orderings to be evaluated much more quickly. The following chart shows how ReJOIN learns to produce good join orders during training. It takes nearly 9000 episodes (queries) to become competitive with PostgreSQL. Once ReJOIN has caught up with PostgreSQL, it goes on to surpass it, producing orderings with lower cost. Also of note here is that after training, ReJOIN produces its query plans faster than PostgreSQL’s built-in join enumerator in many cases. The bottom-up nature of ReJOIN’s algorithm is  , whereas PostgreSQL’s greedy bottom-up algorithm is  . In addition to being limited to just join ordering, ReJOIN’s use of the query optimiser’s cost model to generate reward signals means that it is still dependent on a well-tuned cost model, which is a big part of the problem we wanted to solve in the first place. Ideally we’d like to extend the approach to handle full physical plan generation, and also remove the dependency on having an existing cost model. Challenges in extending the approach  Once we go from just join ordering to the full search space including operator and access path selection etc., the approach from ReJOIN is unable to learn effective polities in reasonable time. An initial model failed to out-perform random choice even after 72 hours of training. If we use actual query execution time to generate the reward, then initial policies which will often generate very inefficient plans will take a long time to obtain a reward signal. Thus we learn the slowest at exactly the point when we’d like to learn the fastest and the system takes a prohibitive amount of time to converge to good results. (An experiment with ReJOIN using real query latency instead of optimiser cost confirmed this). Finally, query latency as a reward signal doesn’t meet the expectations of many DRL algorithms that the reward signal is dense and linear. A dense reward signal is one that provides incremental feedback with every action the agent takes (such as the score updating in an Atari game), not just at the end of the episode. The linear assumption means that an algorithm may attempt to maximise the sum of many small rewards within an episode. We have identified how the large search space, delayed reward signal, and costly performance indicators provide substantial hurdles to naive applications of DRL to query optimization. Should we just give up on the idea then? All is not lost yet! The last section of the paper details a number of alternative approaches that could overcome some of these hurdles. Research directions  Three techniques that may help to make DRL-based query optimisation practical again are learning from demonstration, cost-model bootstrapping, and incremental learning. We’ll look at each of these briefly in turn next (there are no results from applying or evaluating these ideas as yet). Learning from demonstration  In learning from demonstration, a model is first trained to mimic the behaviour of an existing expert, and then goes on to learn directly from its actions. In the context of query optimisation, we would first train a model to mimic the actions taken by an existing optimiser (indexes, join orderings, pruning of bad plans etc. ), and then switch to optimising queries directly bypassing the ‘mentor’ optimiser. In this second phase the agent fine-tunes its own policy. The advantage of this strategy is that mimicking the existing optimiser in the early stages helps the optimiser agent to avoid the ‘obviously bad’ parts of the search space. Since the behavior of the model in the second phase should not initially stray too far from the behavior of the expert system, we do not have to worry about executing any exceptionally poor query plans. Additionally, since the second training phase only needs to fine-tune an already-performant model, the delayed reward signal is of far less consequence. Cost model bootstrapping  Cost model bootstrapping uses a very similar in spirit two-phase approach. In the first phase, instead of learning to mimic the actions of an existing expert optimiser, the judgements of the existing expert optimiser (i.e., it’s cost model) are used to bring the agent to an initial level of competence. The optimiser’s cost model is used as the reward signal during initial training, and once the agent has learned to produce good policies according to the cost model, it is then switched to learning from a reward based on actual query latency. One complication is doing this is that the reward units (scale) need to be consistent across the costs produced by the query optimiser cost model, and the latency measurements of actual query executions. We could apply some kind of normalisation across the two, or alternatively transfer the weights from the first network to a new network (transfer learning). Incremental learning  Incremental learning attempts to mitigate the issues stemming from poor query plans early in the learning cycle by beginning learning on simpler problems:  … incrementally learning query optimization by first training a model to handle simple cases and slowly introducing more complexity. This approach makes the extremely large search space more manageable by dividing it into smaller pieces. In the context of query optimisation, we can make problems easier by reducing the number of relations, or by reducing the number of dimensions to be considered. That leads to a problem space that looks like this:  We could therefore try starting with a small number of pipeline phases and gradually introducing more, as shown in the following figure:  Or we could try starting with small examples and gradually focus on larger and larger queries. Maybe a hybrid strategy will be best, starting with join order and small queries, and gradually increasing sophistication in both dimensions.", "pdf_url": "http://cidrdb.org/cidr2019/papers/p96-marcus-cidr19.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/towards-a-hands-free-query-optimizer-through-deep-learning.json"}
{"id": "77534612", "bin": "200_300", "summary_sentences": ["R-NET is an end-to-end trained neural network model for machine comprehension.", "It starts by matching the question and the given passage (using gated attention based RNN) to obtain question-aware passage representation.", "Next, it uses a self-matching attention mechanism to refine the passage representation by matching the passage against itself.", "Lastly, it uses pointer networks to determine the position of the answer in the passage.", "Datasets  SQuAD  MS-MARCO  Architecture  Question / Passage Encoder  Concatenate the word level and character level embeddings for each word and feed into a bidirectional GRU to obtain question and passage representation.", "Gated Attention based RNN  Given question and passage representation, sentence pair representation is generated via soft-alignment of the words in the question and in the passage.", "The newly added gate captures the relation between the question and the current passage word as only some parts of the passage are relevant for answering the given question.", "Self Matching Attention  The passage representation obtained so far would not capture most of the context.", "So the current representation is matched against itself so as to collect evidence from the entire passage and encode the evidence relevant to the current passage word and question.", "Output Layer  Use pointer network (initialized using attention pooling over answer representation) to predict the position of the answer.", "Loss function is the sum of negative log probabilities of start and end positions.", "Results  R-NET is ranked second on SQuAD Leaderboard as of 7th August, 2017 and achieves best-published results on MS-MARCO dataset.", "Using ideas like sentence ranking, using syntax information performing multihop inference and augmenting question dataset (using seqToseq network) do not help in improving the performance."], "summary_text": "R-NET is an end-to-end trained neural network model for machine comprehension. It starts by matching the question and the given passage (using gated attention based RNN) to obtain question-aware passage representation. Next, it uses a self-matching attention mechanism to refine the passage representation by matching the passage against itself. Lastly, it uses pointer networks to determine the position of the answer in the passage. Datasets  SQuAD  MS-MARCO  Architecture  Question / Passage Encoder  Concatenate the word level and character level embeddings for each word and feed into a bidirectional GRU to obtain question and passage representation. Gated Attention based RNN  Given question and passage representation, sentence pair representation is generated via soft-alignment of the words in the question and in the passage. The newly added gate captures the relation between the question and the current passage word as only some parts of the passage are relevant for answering the given question. Self Matching Attention  The passage representation obtained so far would not capture most of the context. So the current representation is matched against itself so as to collect evidence from the entire passage and encode the evidence relevant to the current passage word and question. Output Layer  Use pointer network (initialized using attention pooling over answer representation) to predict the position of the answer. Loss function is the sum of negative log probabilities of start and end positions. Results  R-NET is ranked second on SQuAD Leaderboard as of 7th August, 2017 and achieves best-published results on MS-MARCO dataset. Using ideas like sentence ranking, using syntax information performing multihop inference and augmenting question dataset (using seqToseq network) do not help in improving the performance.", "pdf_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/r-net-machine-reading-comprehension-with-self-matching-networks.json"}
{"id": "2774914", "bin": "200_300", "summary_sentences": ["HARP is an architecture to learn low-dimensional node embeddings by compressing the input graph into smaller graphs.", ".", "Given a graph G = (V, E), compute a series of successively smaller (coarse) graphs G0, …, GL.", "Learn the node representations in GL and successively refine the embeddings for larger graphs in the series.", "The architecture is independent of the algorithms used to embed the nodes or to refine the node representations.", "Graph coarsening technique that preserves global structure  Collapse edges and stars to preserve first and second order proximity.", "Edge collapsing - select the subset of E such that no two edges are incident on the same vertex and merge their nodes into a single node and merge their edges as well.", "Star collapsing - given star structure, collapse the pairs of neighboring nodes (of the central node).", "In practice, first apply star collapsing, followed by edge collapsing.", "Extending node representation from coarse graph to finer graph  Lets say node1 and node2 were merged into node12 during coarsening.", "First copy the representation of node12 into node1, node2.", "Additionally, if hierarchical softmax was used, extend the B-tree such that node12 is replaced by 2 child nodes node1 and node2.", "Time complexity for HARP + DeepWalk is O(number of walks * |V|) while for HARP + LINE is O(number of iterations * |E|).", "The asymptotic complexity remains the same as the HARP-less version for the two cases.", "Multilabel classification task shows that HAR improves all the node embedding technique with gains up to 14%."], "summary_text": "HARP is an architecture to learn low-dimensional node embeddings by compressing the input graph into smaller graphs. . Given a graph G = (V, E), compute a series of successively smaller (coarse) graphs G0, …, GL. Learn the node representations in GL and successively refine the embeddings for larger graphs in the series. The architecture is independent of the algorithms used to embed the nodes or to refine the node representations. Graph coarsening technique that preserves global structure  Collapse edges and stars to preserve first and second order proximity. Edge collapsing - select the subset of E such that no two edges are incident on the same vertex and merge their nodes into a single node and merge their edges as well. Star collapsing - given star structure, collapse the pairs of neighboring nodes (of the central node). In practice, first apply star collapsing, followed by edge collapsing. Extending node representation from coarse graph to finer graph  Lets say node1 and node2 were merged into node12 during coarsening. First copy the representation of node12 into node1, node2. Additionally, if hierarchical softmax was used, extend the B-tree such that node12 is replaced by 2 child nodes node1 and node2. Time complexity for HARP + DeepWalk is O(number of walks * |V|) while for HARP + LINE is O(number of iterations * |E|). The asymptotic complexity remains the same as the HARP-less version for the two cases. Multilabel classification task shows that HAR improves all the node embedding technique with gains up to 14%.", "pdf_url": "https://arxiv.org/pdf/1706.07845", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/harp-hierarchical-representation-learning-for-networks.json"}
{"id": "99440206", "bin": "200_300", "summary_sentences": ["The paper presents the concept of “network motifs” to understand the structural design of a network or a graph.", "Idea  A network motif is defined as “a pattern of inter-connections occurring in complex networks in numbers that are significantly higher than those in randomized networks”.", "In the practical setting, given an input network, we first create randomized networks which have same single node characteristics (like a number of incoming and outgoing edges) as the input network.", "The patterns that occur at a much higher frequency in the input graph (than the randomized graphs) are reported as motifs.", "More specifically, the patterns for which the probability of appearing in a randomized network an equal or more number of times than in the real network is lower than a cutoff value (say 0.01).", "Motivation  Real-life networks exhibit properties like “small world” property ( the majority of nodes are within a distance of fewer than 7 hops from each other) and “scale-free” property (fraction of nodes having k edges decays as a power-law).", "Motifs are one such structural property that is exhibited by networks in biochemistry, neurobiology, ecology, and engineering.", "Further, motifs shared by graphs of different domains are different which hints at the usefulness of motifs as a fundamental structural property of the graph and relates to the process of evolution of the graph."], "summary_text": "The paper presents the concept of “network motifs” to understand the structural design of a network or a graph. Idea  A network motif is defined as “a pattern of inter-connections occurring in complex networks in numbers that are significantly higher than those in randomized networks”. In the practical setting, given an input network, we first create randomized networks which have same single node characteristics (like a number of incoming and outgoing edges) as the input network. The patterns that occur at a much higher frequency in the input graph (than the randomized graphs) are reported as motifs. More specifically, the patterns for which the probability of appearing in a randomized network an equal or more number of times than in the real network is lower than a cutoff value (say 0.01). Motivation  Real-life networks exhibit properties like “small world” property ( the majority of nodes are within a distance of fewer than 7 hops from each other) and “scale-free” property (fraction of nodes having k edges decays as a power-law). Motifs are one such structural property that is exhibited by networks in biochemistry, neurobiology, ecology, and engineering. Further, motifs shared by graphs of different domains are different which hints at the usefulness of motifs as a fundamental structural property of the graph and relates to the process of evolution of the graph.", "pdf_url": "https://science.sciencemag.org/content/298/5594/824/tab-pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/network-motifs-simple-building-blocks-of-complex-networks.json"}
{"id": "7570292", "bin": "300_400", "summary_sentences": ["Hindsight Experience Replay(HER) is a sample efficient technique to learn from sparse rewards.", "Idea  Assume a footballer misses the goal narrowly.", "Even though the player does not get any “reward”(in terms of goal), the player realizes that had the goal post been shifted a bit, it would have resulted in a goal(reward).", "The same intuition is applied for the RL agent - let us say that the true goal state was g while the agent ends up in the state s.  While the action sequence is not useful for reaching the goal state g, it is indeed useful for reaching state s. Hence the trajectory could be replayed with the goal as s(and not g).", "Technical Details  Multi-goal policy trained using Universal Value Function Approximation (UVFA).", "Every episode starts by sampling a start state and a goal state.", "Each goal has a different reward function.", "Policy uses both the current state and the current goal state and leads to a state transition sequence s1, s2,…, sn.", "Each of these transitions si -> si+1 are stored in a buffer with both the original goal and a subset of the other goals.", "For the goal selection, following strategies are tried:  Future - goal state is the state k steps after observing the state transition.", "Final - goal state is the final state of the current episode.", "Episode - k random states are selected from the current episode.", "Randon - k states are selected randomly.", "Any off-policy algorithm can be used.", "Specifically, DDPG is used.", "Experiments  Robotic arm simulated using MuJoCo for push, slide and pick and place tasks.", "DDPG with and without HER evaluated on the 3 tasks.", "DDPG with the HER variant significantly outperforms the baseline in all the cases."], "summary_text": "Hindsight Experience Replay(HER) is a sample efficient technique to learn from sparse rewards. Idea  Assume a footballer misses the goal narrowly. Even though the player does not get any “reward”(in terms of goal), the player realizes that had the goal post been shifted a bit, it would have resulted in a goal(reward). The same intuition is applied for the RL agent - let us say that the true goal state was g while the agent ends up in the state s.  While the action sequence is not useful for reaching the goal state g, it is indeed useful for reaching state s. Hence the trajectory could be replayed with the goal as s(and not g). Technical Details  Multi-goal policy trained using Universal Value Function Approximation (UVFA). Every episode starts by sampling a start state and a goal state. Each goal has a different reward function. Policy uses both the current state and the current goal state and leads to a state transition sequence s1, s2,…, sn. Each of these transitions si -> si+1 are stored in a buffer with both the original goal and a subset of the other goals. For the goal selection, following strategies are tried:  Future - goal state is the state k steps after observing the state transition. Final - goal state is the final state of the current episode. Episode - k random states are selected from the current episode. Randon - k states are selected randomly. Any off-policy algorithm can be used. Specifically, DDPG is used. Experiments  Robotic arm simulated using MuJoCo for push, slide and pick and place tasks. DDPG with and without HER evaluated on the 3 tasks. DDPG with the HER variant significantly outperforms the baseline in all the cases.", "pdf_url": "https://arxiv.org/pdf/1707.01495", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/hindsight-experience-replay.json"}
{"id": "63511892", "bin": "300_400", "summary_sentences": ["What:  They introduced a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network.", "How:  Spatial Transformer allows the spatial manipulation of the data (any feature map or particularly input image).", "This differentiable module can be inserted into any CNN, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself.", "The action of the spatial transformer is conditioned on individual data samples, with the appropriate behavior learned during training for the task in question.", "No additional supervision or modification of the optimization process is required.", "Spatial manipulation consists of cropping, translation, rotation, scale, and skew.", "STN structure:  Localization net: predicts parameters of the transform theta.", "For 2d case, it's 2 x 3 matrix.", "For 3d case, it's 3 x 4 matrix.", "Grid generator: Uses predictions of Localization net to create a sampling grid, which is a set of points where the input map should be sampled to produce the transformed output.", "Sampler: Produces the output map sampled from the input feature map at the predicted grid points.", "Notes:  Localization net can predict several transformations(thetas) for subsequent transformation applied to the input image(feature map).", "The final regression layer should be initialized to regress the identity transform (zero weights, identity transform bias).", "Grid generator and Transforms:  The transformation can have any parameterized form, provided that it is differentiable with respect to the parameters  The most popular is just a 2d affine transform:  or particularly an attention mechanism:  The source/target transformation and sampling is equivalent to the standard texture mapping and coordinates used in graphics.", "Sampler:  The key why STN works.", "They introduced a (sub-)differentiable sampling mechanism that allows loss gradients to flow back not only to the \"input\" feature map, but also to the sampling grid coordinates, and therefore back to the transformation parameters θ and Localisation Net.", "Results:  Street View House Numbers multi-digit recognition:  Distored MNIST:  CUB-200-2011 birds dataset:  MNIST addition:"], "summary_text": "What:  They introduced a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. How:  Spatial Transformer allows the spatial manipulation of the data (any feature map or particularly input image). This differentiable module can be inserted into any CNN, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself. The action of the spatial transformer is conditioned on individual data samples, with the appropriate behavior learned during training for the task in question. No additional supervision or modification of the optimization process is required. Spatial manipulation consists of cropping, translation, rotation, scale, and skew. STN structure:  Localization net: predicts parameters of the transform theta. For 2d case, it's 2 x 3 matrix. For 3d case, it's 3 x 4 matrix. Grid generator: Uses predictions of Localization net to create a sampling grid, which is a set of points where the input map should be sampled to produce the transformed output. Sampler: Produces the output map sampled from the input feature map at the predicted grid points. Notes:  Localization net can predict several transformations(thetas) for subsequent transformation applied to the input image(feature map). The final regression layer should be initialized to regress the identity transform (zero weights, identity transform bias). Grid generator and Transforms:  The transformation can have any parameterized form, provided that it is differentiable with respect to the parameters  The most popular is just a 2d affine transform:  or particularly an attention mechanism:  The source/target transformation and sampling is equivalent to the standard texture mapping and coordinates used in graphics. Sampler:  The key why STN works. They introduced a (sub-)differentiable sampling mechanism that allows loss gradients to flow back not only to the \"input\" feature map, but also to the sampling grid coordinates, and therefore back to the transformation parameters θ and Localisation Net. Results:  Street View House Numbers multi-digit recognition:  Distored MNIST:  CUB-200-2011 birds dataset:  MNIST addition:", "pdf_url": "https://arxiv.org/pdf/1506.02025", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/stn.json"}
{"id": "12711637", "bin": "300_400", "summary_sentences": ["Problem Statement  VQA Task: Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.", "The paper attempts to fine tune the simple baseline method of Bag-of-Words + Image features (iBOWIMG) to make it competitive against more sophisticated LSTM models.", "Model  VQA modelled as a classification task where the system learns to choose among one of the top k most prominent answers.", "Text Features - Convert input question to a one-hot vector and then transform to word vectors using a word embedding.", "Image Features - Last layer activations from GoogLeNet.", "Text features are concatenated with image features and fed into a softmax.", "Different learning rates and weight clipping for word embedding layer and softmax layer with the learning rate for embedding layer much higher than that of softmax layer.", "Results  iBOWIMG model reports an accuracy of 55.89% for Open-ended questions and 61.97% for Multiple-Choice questions which is comparable to the performance of other, more sophisticated models.", "Interpretation of the model  Since the model is very simple, it is possible to interpret the model to know what exactly is the model learning.", "This is the greatest strength of the paper even though the model is very simple and naive.", "The model attempts to memorise the correlation between the answer class and the informative words (in the question) and image features.", "Question words generally can influence the answer given the bias in images occurring in COCO dataset.", "Given the simple linear transformation being used, it is possible to quantify the importance of each single words (in the question) to the answer.", "The paper uses the Class Activation Mapping (CAM) approach (which uses the linear relation between softmax and final image feature map) to highlight the informative image regions relevant to the predicted answer.", "While the results reported by the paper are not themselves so significant, the described approach provides a way to interpret the strengths and weakness of different VQA datasets."], "summary_text": "Problem Statement  VQA Task: Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image. The paper attempts to fine tune the simple baseline method of Bag-of-Words + Image features (iBOWIMG) to make it competitive against more sophisticated LSTM models. Model  VQA modelled as a classification task where the system learns to choose among one of the top k most prominent answers. Text Features - Convert input question to a one-hot vector and then transform to word vectors using a word embedding. Image Features - Last layer activations from GoogLeNet. Text features are concatenated with image features and fed into a softmax. Different learning rates and weight clipping for word embedding layer and softmax layer with the learning rate for embedding layer much higher than that of softmax layer. Results  iBOWIMG model reports an accuracy of 55.89% for Open-ended questions and 61.97% for Multiple-Choice questions which is comparable to the performance of other, more sophisticated models. Interpretation of the model  Since the model is very simple, it is possible to interpret the model to know what exactly is the model learning. This is the greatest strength of the paper even though the model is very simple and naive. The model attempts to memorise the correlation between the answer class and the informative words (in the question) and image features. Question words generally can influence the answer given the bias in images occurring in COCO dataset. Given the simple linear transformation being used, it is possible to quantify the importance of each single words (in the question) to the answer. The paper uses the Class Activation Mapping (CAM) approach (which uses the linear relation between softmax and final image feature map) to highlight the informative image regions relevant to the predicted answer. While the results reported by the paper are not themselves so significant, the described approach provides a way to interpret the strengths and weakness of different VQA datasets.", "pdf_url": "http://arxiv.org/pdf/1512.02167.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/simple-baseline-for-visual-question-answering.json"}
{"id": "70691684", "bin": "300_400", "summary_sentences": ["This paper presents a method for training feed-forward neural networks with stochastic hidden units (e.g. sigmoid belief networks), to optimize the expectation (over the stochastic units) of some arbitrary loss function.", "While the proposed method is applicable to any type of stochastic units, it is most interesting for the case of discrete stochastic units, since the reparametrization trick of variational autoencoders cannot be applied to backprop through the sampling step.", "In short, the method builds on the likelihood ratio method (of which REINFORCE is a special case) and proposes a baseline (also known as control variate) which, according to the authors, is such that an unbiased gradient is obtained.", "Specifically, the baseline corresponds to the first-order Taylor expansion of the loss function around some deterministic value of the hidden units (x̄) that doesn't depend on the stochastic hidden units (noted x in the paper).", "For a likelihood ratio method to be unbiased, it is required that the expectation of the baseline (times the gradient of the model's log distribution) with respect to the model's distribution be tractable.", "For the proposed baseline, it can be shown that computing this expectation requires the gradient of the mean (μ) of each stochastic unit in the network with respect to each parameter.", "The key idea behind the proposed method is that 1) an estimate of this expectation can be obtained simply using mean-field and 2)  since mean-field is estimated by a feedforward deterministic pass over the network, it is thus possible to compute the gradients of μ by backpropagation through the mean-field pass (hence the name of the method, MuProp).", "Experiments show that this method converges much faster than previously proposed unbiased methods and often performs better.", "Experiments also show that the method obtains competitive performance compared to biased methods (such as the \"straight through\" method)."], "summary_text": "This paper presents a method for training feed-forward neural networks with stochastic hidden units (e.g. sigmoid belief networks), to optimize the expectation (over the stochastic units) of some arbitrary loss function. While the proposed method is applicable to any type of stochastic units, it is most interesting for the case of discrete stochastic units, since the reparametrization trick of variational autoencoders cannot be applied to backprop through the sampling step. In short, the method builds on the likelihood ratio method (of which REINFORCE is a special case) and proposes a baseline (also known as control variate) which, according to the authors, is such that an unbiased gradient is obtained. Specifically, the baseline corresponds to the first-order Taylor expansion of the loss function around some deterministic value of the hidden units (x̄) that doesn't depend on the stochastic hidden units (noted x in the paper). For a likelihood ratio method to be unbiased, it is required that the expectation of the baseline (times the gradient of the model's log distribution) with respect to the model's distribution be tractable. For the proposed baseline, it can be shown that computing this expectation requires the gradient of the mean (μ) of each stochastic unit in the network with respect to each parameter. The key idea behind the proposed method is that 1) an estimate of this expectation can be obtained simply using mean-field and 2)  since mean-field is estimated by a feedforward deterministic pass over the network, it is thus possible to compute the gradients of μ by backpropagation through the mean-field pass (hence the name of the method, MuProp). Experiments show that this method converges much faster than previously proposed unbiased methods and often performs better. Experiments also show that the method obtains competitive performance compared to biased methods (such as the \"straight through\" method).", "pdf_url": "http://arxiv.org/pdf/1511.05176", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/gulsm15.json"}
{"id": "74164301", "bin": "400_500", "summary_sentences": ["NMT(Neural Machine Translation) systems perform poorly with respect to OOV(out-of-vocabulary) words or rare words.", "The paper presents a word-alignment based technique for translating such rare words.", "Technique  Annotate the training corpus with information about what do different OOV words (in the target sentence) correspond to in the source sentence.", "NMT learns to track the alignment of rare words across source and target sentences and emits such alignments for the test sentences.", "As a post-processing step, use a dictionary to map rare words from the source language to target language.", "Annotating the Corpus  Copy Model  Annotate the OOV words in the source sentence with tokens unk1, unk2,..., etc such that repeated words get the same token.", "In target language, each OOV word, that is aligned to some OOV word in the source language, is assigned the same token as the word in the source language.", "The OOV word in the target language, which has no alignment or is aligned with a known word in the source language.", "is assigned the null token.", "Pros  Very straightforward  Cons  Misses out on words which are not labelled as OOV in the source language.", "PosAll - Positional All Model  All OOV words in the source language are assigned a single unk token.", "All words in the target sentences are assigned positional tokens which denote that the jth word in the target sentence is aligned to the ith word in the source sentence.", "Aligned words that are too far apart, or are unaligned, are assigned a null token.", "Pros  Captures complete alignment between source and target sentences.", "Cons  It doubles the length of target sentences.", "PosUnk - Positional Unknown Model  All OOV words in the source language are assigned a single unk token.", "All OOV words in the target sentences are assigned unk token with the position which gives the relative position of the word in the target language with respect to its aligned source word.", "Pros:  Faster than PosAll model.", "Cons  Does not capture alignment for all words.", "Experiments  Dataset  Subset of WMT'14 dataset  Alignment computed using the Berkeley Aligner  Used architecture from Sequence to Sequence Learning with Neural Networks paper .", "Results  All the 3 approaches (more specifically the PosUnk approach) improve the performance of existing NMTs in the order PosUnk > PosAll > Copy.", "Ensemble models benefit more than individual models as the ensemble of NMT models works better at aligning the OOV words.", "Performance gains are more when using smaller vocabulary.", "Rare word analysis shows that performance gains are more when proposition of OOV words is higher."], "summary_text": "NMT(Neural Machine Translation) systems perform poorly with respect to OOV(out-of-vocabulary) words or rare words. The paper presents a word-alignment based technique for translating such rare words. Technique  Annotate the training corpus with information about what do different OOV words (in the target sentence) correspond to in the source sentence. NMT learns to track the alignment of rare words across source and target sentences and emits such alignments for the test sentences. As a post-processing step, use a dictionary to map rare words from the source language to target language. Annotating the Corpus  Copy Model  Annotate the OOV words in the source sentence with tokens unk1, unk2,..., etc such that repeated words get the same token. In target language, each OOV word, that is aligned to some OOV word in the source language, is assigned the same token as the word in the source language. The OOV word in the target language, which has no alignment or is aligned with a known word in the source language. is assigned the null token. Pros  Very straightforward  Cons  Misses out on words which are not labelled as OOV in the source language. PosAll - Positional All Model  All OOV words in the source language are assigned a single unk token. All words in the target sentences are assigned positional tokens which denote that the jth word in the target sentence is aligned to the ith word in the source sentence. Aligned words that are too far apart, or are unaligned, are assigned a null token. Pros  Captures complete alignment between source and target sentences. Cons  It doubles the length of target sentences. PosUnk - Positional Unknown Model  All OOV words in the source language are assigned a single unk token. All OOV words in the target sentences are assigned unk token with the position which gives the relative position of the word in the target language with respect to its aligned source word. Pros:  Faster than PosAll model. Cons  Does not capture alignment for all words. Experiments  Dataset  Subset of WMT'14 dataset  Alignment computed using the Berkeley Aligner  Used architecture from Sequence to Sequence Learning with Neural Networks paper . Results  All the 3 approaches (more specifically the PosUnk approach) improve the performance of existing NMTs in the order PosUnk > PosAll > Copy. Ensemble models benefit more than individual models as the ensemble of NMT models works better at aligning the OOV words. Performance gains are more when using smaller vocabulary. Rare word analysis shows that performance gains are more when proposition of OOV words is higher.", "pdf_url": "https://arxiv.org/pdf/1410.8206", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/8fe14b74c7292129c6c5ecb37f33b5.json"}
{"id": "66681632", "bin": "400_500", "summary_sentences": ["What  They suggest a modified network architecture for object detectors (i.e. bounding box detectors).", "The architecture aggregates features from many scales (i.e. before each pooling layer) to detect both small and large object.", "The network is shaped similar to an hourglass.", "How  Architecture  They have two branches.", "The first one is similar to any normal network: Convolutions and pooling.", "The exact choice of convolutions (e.g. how many) and pooling is determined by the used base network (e.g.", "~50 convolutions with ~5x pooling in ResNet-50).", "The second branch starts at the first one's output.", "It uses nearest neighbour upsampling to re-increase the resolution back to the original one.", "It does not contain convolutions.", "All layers have 256 channels.", "There are connections between the layers of the first and second branch.", "These connections are simply 1x1 convolutions followed by an addition (similar to residual connections).", "Only layers with similar height and width are connected.", "Visualization:  Integration with Faster R-CNN  They base the RPN on their second branch.", "While usually an RPN is applied to a single feature map of one scale, in their case it is applied to many feature maps of varying scales.", "The RPN uses the same parameters for all scales.", "They use anchor boxes, but only of different aspect ratios, not of different scales (as scales are already covered by their feature map heights/widths).", "Ground truth bounding boxes are associated with the best matching anchor box (i.e. one box among all scales).", "Everything else is the same as in Faster R-CNN.", "Integration with Fast R-CNN  Fast R-CNN does not use an RPN, but instead usually uses Selective Search to find region proposals (and applies RoI-Pooling to them).", "Here, they simply RoI-Pool from the FPN's output of the second branch.", "They do not pool over all scales.", "Instead they pick only the scale/layer that matches the region proposal's size (based on its height/width).", "They process each pooled RoI using two 1024-dimensional fully connected layers (initalizes randomly).", "Everything else is the same as in Fast R-CNN.", "Results  Faster R-CNN  FPN improves recall on COCO by about 8 points, compared to using standard RPN.", "Improvement is stronger for small objects (about 12 points).", "For some reason no AP values here, only recall.", "The RPN uses some convolutions to transform each feature map into region proposals.", "Sharing the features of these convolutions marginally improves results.", "Fast R-CNN  FPN improves AP on COCO by about 2 points.", "Improvement is stronger for small objects (about 2.1 points)."], "summary_text": "What  They suggest a modified network architecture for object detectors (i.e. bounding box detectors). The architecture aggregates features from many scales (i.e. before each pooling layer) to detect both small and large object. The network is shaped similar to an hourglass. How  Architecture  They have two branches. The first one is similar to any normal network: Convolutions and pooling. The exact choice of convolutions (e.g. how many) and pooling is determined by the used base network (e.g. ~50 convolutions with ~5x pooling in ResNet-50). The second branch starts at the first one's output. It uses nearest neighbour upsampling to re-increase the resolution back to the original one. It does not contain convolutions. All layers have 256 channels. There are connections between the layers of the first and second branch. These connections are simply 1x1 convolutions followed by an addition (similar to residual connections). Only layers with similar height and width are connected. Visualization:  Integration with Faster R-CNN  They base the RPN on their second branch. While usually an RPN is applied to a single feature map of one scale, in their case it is applied to many feature maps of varying scales. The RPN uses the same parameters for all scales. They use anchor boxes, but only of different aspect ratios, not of different scales (as scales are already covered by their feature map heights/widths). Ground truth bounding boxes are associated with the best matching anchor box (i.e. one box among all scales). Everything else is the same as in Faster R-CNN. Integration with Fast R-CNN  Fast R-CNN does not use an RPN, but instead usually uses Selective Search to find region proposals (and applies RoI-Pooling to them). Here, they simply RoI-Pool from the FPN's output of the second branch. They do not pool over all scales. Instead they pick only the scale/layer that matches the region proposal's size (based on its height/width). They process each pooled RoI using two 1024-dimensional fully connected layers (initalizes randomly). Everything else is the same as in Fast R-CNN. Results  Faster R-CNN  FPN improves recall on COCO by about 8 points, compared to using standard RPN. Improvement is stronger for small objects (about 12 points). For some reason no AP values here, only recall. The RPN uses some convolutions to transform each feature map into region proposals. Sharing the features of these convolutions marginally improves results. Fast R-CNN  FPN improves AP on COCO by about 2 points. Improvement is stronger for small objects (about 2.1 points).", "pdf_url": "https://arxiv.org/pdf/1612.03144", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/feature_pyramid_networks_for_object_detection.json"}
{"id": "68635574", "bin": "400_500", "summary_sentences": ["The paper explores “if a well behaved RNN can be replaced by a feed-forward network of comparable size without loss in performance.”  “Well behaved” is defined in terms of control-theoretic notion of stability.", "This roughly requires that the gradients do not explode over time.", "The paper shows that under the stability assumption, feedforward networks can approximate RNNs for both training and inference.", "The results are empirically validated as well.", "Problem Setting  Consider a general, non linear dynamical system given by a differential state transition map Φw.", "The hidden ht = Φw(ht-1, xt).", "Assumptions:  Φ is smooth in w and h.  h0 = 0  Φw(0, 0) = 0 (can be ensured by translation)  Stable models are the ones where Φ is contractive ie Φw(h, x) - Φw(h’, x) is less than Λ * (h - h’)  For example, in RNN, stability would require that norm(w) is less than (Lp)-1 where Lp is the Lipschitz constant of the point-wise non linearity used.", "The feedforward approximation uses a finite context (of length k) and is a truncated model.", "A non-parametric function f maps the output of the recurrent model to prediction.", "If f is desired to be a parametric model, its parameters can be pushed to the recurrent model.", "Theoretical Results  For a Λ-contractive system, it can be proved that for a large k (and additional Lipschitz assumptions) the difference in prediction between the recurrent and truncated mode is negligible.", "If the recurrent model and truncated feed-forward network are initialized at the same point and trained over the same input for N-step, then for an optimal k, the weights of the two models would be very close in the Euclidean space.", "It can be shown that this small difference does not lead to large gradient differences during subsequent update steps.", "This can be roughly interpreted as - if the gradient descent can train a stable recurrent network, it can also train a feedforward model and vice-versa.", "The stability condition is important as, without that, truncated models would be bad (even for large values of k).", "Further, it is difficult to show that gradient descent converges to a stationary point."], "summary_text": "The paper explores “if a well behaved RNN can be replaced by a feed-forward network of comparable size without loss in performance.”  “Well behaved” is defined in terms of control-theoretic notion of stability. This roughly requires that the gradients do not explode over time. The paper shows that under the stability assumption, feedforward networks can approximate RNNs for both training and inference. The results are empirically validated as well. Problem Setting  Consider a general, non linear dynamical system given by a differential state transition map Φw. The hidden ht = Φw(ht-1, xt). Assumptions:  Φ is smooth in w and h.  h0 = 0  Φw(0, 0) = 0 (can be ensured by translation)  Stable models are the ones where Φ is contractive ie Φw(h, x) - Φw(h’, x) is less than Λ * (h - h’)  For example, in RNN, stability would require that norm(w) is less than (Lp)-1 where Lp is the Lipschitz constant of the point-wise non linearity used. The feedforward approximation uses a finite context (of length k) and is a truncated model. A non-parametric function f maps the output of the recurrent model to prediction. If f is desired to be a parametric model, its parameters can be pushed to the recurrent model. Theoretical Results  For a Λ-contractive system, it can be proved that for a large k (and additional Lipschitz assumptions) the difference in prediction between the recurrent and truncated mode is negligible. If the recurrent model and truncated feed-forward network are initialized at the same point and trained over the same input for N-step, then for an optimal k, the weights of the two models would be very close in the Euclidean space. It can be shown that this small difference does not lead to large gradient differences during subsequent update steps. This can be roughly interpreted as - if the gradient descent can train a stable recurrent network, it can also train a feedforward model and vice-versa. The stability condition is important as, without that, truncated models would be bad (even for large values of k). Further, it is difficult to show that gradient descent converges to a stationary point.", "pdf_url": "https://arxiv.org/pdf/1805.10369", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/when-recurrent-models-don-t-need-to-be-recurrent.json"}
{"id": "81916449", "bin": "400_500", "summary_sentences": ["The paper proposes a new RNN Encoder-Decoder architecture that can improve the performance of statistical machine translation (SMT) systems.", "RNN Encoder-Decoder  Model consists of two RNNs  Encoder  Learns to encode a variable-length input sequence into a fixed-length vector representation.", "Decoder  Learns to decode a given fixed-length vector representation into a variable-length target sequence.", "Two networks are trained jointly to maximise the conditional probability of the target sequence given the input source sequence.", "Trained model can be used to:  generate a target sequence, given an input sequence.", "score a given pair of input and output sequences.", "Hidden Unit that adaptively remembers and forgets.", "Hidden unit updated to have a  reset gate that adaptively drop any hidden state information that it finds irrelevant.", "update gate that controls how much information from the previous state to carry over.", "Each hidden unit has separate reset and update gates which improve the memory capacity and makes it easier to train.", "Statistical Machine Translation (SMT)  In the phrase-based SMT framework, the translation model is factorised into the translation probabilities of matching phrases in the source and target sentences.", "RNN Encoder-Decoder can be used to rescore the phrase pairs in the phrase table  Experiments  Details  1000 hidden units.", "Activation function in proposed hidden unit - hyperbolic tangent function  Non-recurrent weights initialized by sampling from an isotropic Gaussian distribution (mean = 0, sd = 0.01)  Recurrent weights initialized by sampling from white Gaussian distribution and using its left singular vectors.", "Adadelta and SGD  Observations  Train the model to translate an English phrase to French phrase.", "Using the model to score phrase pairs in the standard phrase-based SMT system improves the translation performance.", "Train a CSLM (Continuous Space Language Model) and compare phrase scores from trained model with those given by CSLM.", "RNN Encoder–Decoder is better at capturing the linguistic regularities in the phrase table.", "RNN Encoder-Decoder learns a continuous space representation for phrases that preserves both the semantic and syntactic structure.", "This comment has been minimized.", "Sign in to view  Copy link  Quote reply  shahbazsyed commented  Mar 23, 2017  Hi,  Thanks for the gist!", "Can you kindly explain what is the \"fixed-length vector representation\" of the input sequence which is generated by the encoder?", "Is it a concatenation of all the word vectors in a given sentence ?", "This comment has been minimized.", "Sign in to view  Copy link  Quote reply  varunjanga commented  Apr 27, 2017  Hi shahbazsyed, if I understand it correctly input sequence can be variable length sequence of words which will be provided as an input to encoder to output fixed-length vector representation which is basically some fixed length(say 128) vector of numbers."], "summary_text": "The paper proposes a new RNN Encoder-Decoder architecture that can improve the performance of statistical machine translation (SMT) systems. RNN Encoder-Decoder  Model consists of two RNNs  Encoder  Learns to encode a variable-length input sequence into a fixed-length vector representation. Decoder  Learns to decode a given fixed-length vector representation into a variable-length target sequence. Two networks are trained jointly to maximise the conditional probability of the target sequence given the input source sequence. Trained model can be used to:  generate a target sequence, given an input sequence. score a given pair of input and output sequences. Hidden Unit that adaptively remembers and forgets. Hidden unit updated to have a  reset gate that adaptively drop any hidden state information that it finds irrelevant. update gate that controls how much information from the previous state to carry over. Each hidden unit has separate reset and update gates which improve the memory capacity and makes it easier to train. Statistical Machine Translation (SMT)  In the phrase-based SMT framework, the translation model is factorised into the translation probabilities of matching phrases in the source and target sentences. RNN Encoder-Decoder can be used to rescore the phrase pairs in the phrase table  Experiments  Details  1000 hidden units. Activation function in proposed hidden unit - hyperbolic tangent function  Non-recurrent weights initialized by sampling from an isotropic Gaussian distribution (mean = 0, sd = 0.01)  Recurrent weights initialized by sampling from white Gaussian distribution and using its left singular vectors. Adadelta and SGD  Observations  Train the model to translate an English phrase to French phrase. Using the model to score phrase pairs in the standard phrase-based SMT system improves the translation performance. Train a CSLM (Continuous Space Language Model) and compare phrase scores from trained model with those given by CSLM. RNN Encoder–Decoder is better at capturing the linguistic regularities in the phrase table. RNN Encoder-Decoder learns a continuous space representation for phrases that preserves both the semantic and syntactic structure. This comment has been minimized. Sign in to view  Copy link  Quote reply  shahbazsyed commented  Mar 23, 2017  Hi,  Thanks for the gist! Can you kindly explain what is the \"fixed-length vector representation\" of the input sequence which is generated by the encoder? Is it a concatenation of all the word vectors in a given sentence ? This comment has been minimized. Sign in to view  Copy link  Quote reply  varunjanga commented  Apr 27, 2017  Hi shahbazsyed, if I understand it correctly input sequence can be variable length sequence of words which will be provided as an input to encoder to output fixed-length vector representation which is basically some fixed length(say 128) vector of numbers.", "pdf_url": "https://arxiv.org/pdf/1406.1078", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/ccec626e68e495fd4577ecdca36b7b.json"}
{"id": "39570633", "bin": "400_500", "summary_sentences": ["The authors train a character-RNN (using mLSTM units) over Amazon Product Reviews (82 million reviews) and use the char-RNN as the feature extractor for sentiment analysis.", "These unsupervised features beat state of the art results for the dataset while are outperformed by supervised approaches on other datasets.", "Most important observation is that the authors find a single neuron (called as the sentiment neuron) which alone achieves a test accuracy of 92.3% thus giving the impression that the sentiment concept has been captured in that single neuron.", "Switching this neuron on (or off) during the generative process produces positive (or negative) reviews.", "Notes  The paper aims to evaluate if the low level features captured by char-RNN can support learning of high-level representations.", "Link to the blog by OpenAI  The paper mentions two possible reasons for weak performance of purely unsupervised networks:  Distributional issues - Sentence vectors trained on books may not generalise to product reviews.", "Limited Capacity of models - Resulting in representational underfitting.", "Single layer with 4096 units.", "Multiplicative LSTM units are used instead of standard LSTM units as they are observed to converge faster.", "Compact model with a high ratio of compute to total params (1.12 buts per byte)  L1 penalty is used instead of L2 as it reduces sample complexity when there are many irrelevant features.", "Found a single neuron (sentiment neuron) which alone captures most of the sentiment concept.", "Capacity Ceiling  Even increasing the dataset by 4 orders of magnitude leads to a very small improvement in accuracy (~1%).", "One possbile reason could be the change in data distribution - trained on Amazon Reviews and tested on Yelp Reviews.", "Similary, the linear model (trained on top of feature vectors) has its own limitations in terms of capacity.", "The model does not work well on out of domain tasks like semantic relatedness over image descriptions.", "The paper shows that positive (or negative) reviews can be generated by switching the sentiment neuron on (or off) during the generative process.", "A tweet by @AlecRad says that zeroing the sentiment neuron drops the performance only by 2% on SST and 10% on IMDB indicating that the network has still learnt a distributed representation.", "Open Questions  Is this phenomenon of disentangling of high level concepts specific to sentiment analysis?", "How do we explain the compression of almost all the sentiment in a single unit?", "Use of hierarchial models for increasing the capacity of char-RNN."], "summary_text": "The authors train a character-RNN (using mLSTM units) over Amazon Product Reviews (82 million reviews) and use the char-RNN as the feature extractor for sentiment analysis. These unsupervised features beat state of the art results for the dataset while are outperformed by supervised approaches on other datasets. Most important observation is that the authors find a single neuron (called as the sentiment neuron) which alone achieves a test accuracy of 92.3% thus giving the impression that the sentiment concept has been captured in that single neuron. Switching this neuron on (or off) during the generative process produces positive (or negative) reviews. Notes  The paper aims to evaluate if the low level features captured by char-RNN can support learning of high-level representations. Link to the blog by OpenAI  The paper mentions two possible reasons for weak performance of purely unsupervised networks:  Distributional issues - Sentence vectors trained on books may not generalise to product reviews. Limited Capacity of models - Resulting in representational underfitting. Single layer with 4096 units. Multiplicative LSTM units are used instead of standard LSTM units as they are observed to converge faster. Compact model with a high ratio of compute to total params (1.12 buts per byte)  L1 penalty is used instead of L2 as it reduces sample complexity when there are many irrelevant features. Found a single neuron (sentiment neuron) which alone captures most of the sentiment concept. Capacity Ceiling  Even increasing the dataset by 4 orders of magnitude leads to a very small improvement in accuracy (~1%). One possbile reason could be the change in data distribution - trained on Amazon Reviews and tested on Yelp Reviews. Similary, the linear model (trained on top of feature vectors) has its own limitations in terms of capacity. The model does not work well on out of domain tasks like semantic relatedness over image descriptions. The paper shows that positive (or negative) reviews can be generated by switching the sentiment neuron on (or off) during the generative process. A tweet by @AlecRad says that zeroing the sentiment neuron drops the performance only by 2% on SST and 10% on IMDB indicating that the network has still learnt a distributed representation. Open Questions  Is this phenomenon of disentangling of high level concepts specific to sentiment analysis? How do we explain the compression of almost all the sentiment in a single unit? Use of hierarchial models for increasing the capacity of char-RNN.", "pdf_url": "https://arxiv.org/pdf/1704.01444", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/4dbe1aa678188399254bb3d0078e1d.json"}
{"id": "47723293", "bin": "500_600", "summary_sentences": ["Note: This paper felt rather hard to read.", "The summary might not have hit exactly what the authors tried to explain.", "What  The authors describe multiple architectures that can model the distributions of images.", "These networks can be used to generate new images or to complete existing ones.", "The networks are mostly based on RNNs.", "How  They define three architectures:  Row LSTM:  Predicts a pixel value based on all previous pixels in the image.", "It applies 1D convolutions (with kernel size 3) to the current and previous rows of the image.", "It uses the convolution results as features to predict a pixel value.", "Diagonal BiLSTM:  Predicts a pixel value based on all previous pixels in the image.", "Instead of applying convolutions in a row-wise fashion, they apply them to the diagonals towards the top left and top right of the pixel.", "Diagonal convolutions can be applied by padding the n-th row with n-1 pixels from the left (diagonal towards top left) or from the right (diagonal towards the top right), then apply a 3x1 column convolution.", "PixelCNN:  Applies convolutions to the region around a pixel to predict its values.", "Uses masks to zero out pixels that follow after the target pixel.", "They use no pooling layers.", "While for the LSTMs each pixel is conditioned on all previous pixels, the dependency range of the CNN is bounded.", "They use up to 12 LSTM layers.", "They use residual connections between their LSTM layers.", "All architectures predict pixel values as a softmax over 255 distinct values (per channel).", "According to the authors that leads to better results than just using one continuous output (i.e. sigmoid) per channel.", "They also try a multi-scale approach: First, one network generates a small image.", "Then a second networks generates the full scale image while being conditioned on the small image.", "Results  The softmax layers learn reasonable distributions.", "E.g. neighboring colors end up with similar probabilities.", "Values 0 and 255 tend to have higher probabilities than others, especially for the very first pixel.", "In the 12-layer LSTM row model, residual and skip connections seem to have roughly the same effect on the network's results.", "Using both yields a tiny improvement over just using one of the techniques alone.", "They achieve a slightly better result on MNIST than DRAW did.", "Their negative log likelihood results for CIFAR-10 improve upon previous models.", "The diagonal BiLSTM model performs best, followed by the row LSTM model, followed by PixelCNN.", "Their generated images for CIFAR-10 and Imagenet capture real local spatial dependencies.", "The multi-scale model produces better looking results.", "The images do not appear blurry.", "Overall they still look very unreal.", "Generated ImageNet 64x64 images.", "Completing partially occluded images."], "summary_text": "Note: This paper felt rather hard to read. The summary might not have hit exactly what the authors tried to explain. What  The authors describe multiple architectures that can model the distributions of images. These networks can be used to generate new images or to complete existing ones. The networks are mostly based on RNNs. How  They define three architectures:  Row LSTM:  Predicts a pixel value based on all previous pixels in the image. It applies 1D convolutions (with kernel size 3) to the current and previous rows of the image. It uses the convolution results as features to predict a pixel value. Diagonal BiLSTM:  Predicts a pixel value based on all previous pixels in the image. Instead of applying convolutions in a row-wise fashion, they apply them to the diagonals towards the top left and top right of the pixel. Diagonal convolutions can be applied by padding the n-th row with n-1 pixels from the left (diagonal towards top left) or from the right (diagonal towards the top right), then apply a 3x1 column convolution. PixelCNN:  Applies convolutions to the region around a pixel to predict its values. Uses masks to zero out pixels that follow after the target pixel. They use no pooling layers. While for the LSTMs each pixel is conditioned on all previous pixels, the dependency range of the CNN is bounded. They use up to 12 LSTM layers. They use residual connections between their LSTM layers. All architectures predict pixel values as a softmax over 255 distinct values (per channel). According to the authors that leads to better results than just using one continuous output (i.e. sigmoid) per channel. They also try a multi-scale approach: First, one network generates a small image. Then a second networks generates the full scale image while being conditioned on the small image. Results  The softmax layers learn reasonable distributions. E.g. neighboring colors end up with similar probabilities. Values 0 and 255 tend to have higher probabilities than others, especially for the very first pixel. In the 12-layer LSTM row model, residual and skip connections seem to have roughly the same effect on the network's results. Using both yields a tiny improvement over just using one of the techniques alone. They achieve a slightly better result on MNIST than DRAW did. Their negative log likelihood results for CIFAR-10 improve upon previous models. The diagonal BiLSTM model performs best, followed by the row LSTM model, followed by PixelCNN. Their generated images for CIFAR-10 and Imagenet capture real local spatial dependencies. The multi-scale model produces better looking results. The images do not appear blurry. Overall they still look very unreal. Generated ImageNet 64x64 images. Completing partially occluded images.", "pdf_url": "http://arxiv.org/pdf/1601.06759", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/pixel_recurrent_neural_networks.json"}
{"id": "42731227", "bin": "500_600", "summary_sentences": ["Uniform sampling from replay memories is not an efficient way to learn.", "Rather, using a clever prioritization scheme to label the experiences in replay memory, learning can be carried out much faster and more effectively.", "However, certain biases are introduced by this non-uniform sampling; hence, weighted importance sampling must be employed in order to correct for this.", "It is shown through experimentation with the Atari Learning Environment that prioritized sampling with Double DQN significantly outperforms the previous state-of-the-art Atari results.", "Evidence  Implemented Double DQN with main changes being the addition of prioritized experience replay sampling and importance-sampling  Tested on Atari Learning Environment  Strengths  Lots of insight about the repercussions of this research and plenty of discussion on extensions  Notes  The magnitude of the TD-error indicates how unexpected a certain transition was  The TD-error can be a poor estimate about the amount an agent can learn from a transition when rewards are noisy  Problems with greedily selecting experiences:  High-error transitions are replayed too frequently  Low-error transitions are almost entirely ignored  Expensive to update entire replay memory, so errors are only updated for transitions that are replayed  Lack of diversity leads to over-fitting  A stochastic sampling method is introduced which finds a balance between greedy prioritization and random sampling (current method)  Two variants of  were studied, where $P$ is the probability of sampling transition $i$, $p_i > 0$ is the priority of transition $i$, and the exponent $\\alpha$ determines how much prioritization is used, with $\\alpha = 0$ the uniform case  Variant 1: proportional prioritization, where $p_i = | \\delta_i| + \\epsilon$ is used and $\\epsilon$ is a small positive constant that prevents the edge-case of transitions not being revisited once their error is zero.", "$\\delta$ is the TD-error  Variant 2: rank-based prioritization, with $p_i = \\frac{1}{rank(i)}$ where $rank(i)$ is the rank of transition $i$ when the replay memory is sorted according to $\\delta_i$  Key insight The estimation of the expected value of the total discounted reward with stochastic updates requires that the updates correspond to the same distribution as the expectation.", "Prioritized replay introduces a bias that changes this distribution uncontrollably.", "This can be corrected by using importance-sampling (IS) weights $ w_i = (\\frac{1}{N} \\frac{1}{P(i)})^{\\beta} $ that fully compensate for the non-uniform probabilities $P(i)$ if $\\beta = 1$.", "These weights are folded into the Q-learning update by using $w_i \\times \\delta_i$, which is normalized by $\\frac{1}{\\max_i w_i}$  IS is annealed from $\\beta_0$ to 1, which means its affect is felt more strongly at the end of the stochastic process; this is because the unbiased nature of the updates in RL is most important near convergence  IS also reduces the gradient magnitudes which is good for optimization; allows the algorithm to follow the curvature of highly non-linear optimization landscapes because the Taylor expansion (gradient descent) is constantly re-approximated"], "summary_text": "Uniform sampling from replay memories is not an efficient way to learn. Rather, using a clever prioritization scheme to label the experiences in replay memory, learning can be carried out much faster and more effectively. However, certain biases are introduced by this non-uniform sampling; hence, weighted importance sampling must be employed in order to correct for this. It is shown through experimentation with the Atari Learning Environment that prioritized sampling with Double DQN significantly outperforms the previous state-of-the-art Atari results. Evidence  Implemented Double DQN with main changes being the addition of prioritized experience replay sampling and importance-sampling  Tested on Atari Learning Environment  Strengths  Lots of insight about the repercussions of this research and plenty of discussion on extensions  Notes  The magnitude of the TD-error indicates how unexpected a certain transition was  The TD-error can be a poor estimate about the amount an agent can learn from a transition when rewards are noisy  Problems with greedily selecting experiences:  High-error transitions are replayed too frequently  Low-error transitions are almost entirely ignored  Expensive to update entire replay memory, so errors are only updated for transitions that are replayed  Lack of diversity leads to over-fitting  A stochastic sampling method is introduced which finds a balance between greedy prioritization and random sampling (current method)  Two variants of  were studied, where $P$ is the probability of sampling transition $i$, $p_i > 0$ is the priority of transition $i$, and the exponent $\\alpha$ determines how much prioritization is used, with $\\alpha = 0$ the uniform case  Variant 1: proportional prioritization, where $p_i = | \\delta_i| + \\epsilon$ is used and $\\epsilon$ is a small positive constant that prevents the edge-case of transitions not being revisited once their error is zero. $\\delta$ is the TD-error  Variant 2: rank-based prioritization, with $p_i = \\frac{1}{rank(i)}$ where $rank(i)$ is the rank of transition $i$ when the replay memory is sorted according to $\\delta_i$  Key insight The estimation of the expected value of the total discounted reward with stochastic updates requires that the updates correspond to the same distribution as the expectation. Prioritized replay introduces a bias that changes this distribution uncontrollably. This can be corrected by using importance-sampling (IS) weights $ w_i = (\\frac{1}{N} \\frac{1}{P(i)})^{\\beta} $ that fully compensate for the non-uniform probabilities $P(i)$ if $\\beta = 1$. These weights are folded into the Q-learning update by using $w_i \\times \\delta_i$, which is normalized by $\\frac{1}{\\max_i w_i}$  IS is annealed from $\\beta_0$ to 1, which means its affect is felt more strongly at the end of the stochastic process; this is because the unbiased nature of the updates in RL is most important near convergence  IS also reduces the gradient magnitudes which is good for optimization; allows the algorithm to follow the curvature of highly non-linear optimization landscapes because the Taylor expansion (gradient descent) is constantly re-approximated", "pdf_url": "http://arxiv.org/pdf/1511.05952.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/prioritizing-experience-replay.json"}
{"id": "24950879", "bin": "500_600", "summary_sentences": ["The paper describes a general purpose neural embedding model where different type of entities (described in terms of discrete features) are embedded in a common vector space.", "A similarity function is learnt to compare these entities in a meaningful way and score their similarity.", "The definition of the similarity function could depend on the downstream task where the embeddings are used.", "Link to the implementation  Approach  Each entity is described as a set of discrete features.", "For example, for the recommendation use case, the users may be described as a bag-of-words of movies they have liked.", "For the search use case, the document may be described as a bag-of-words of words they are made up of.", "Given a dataset and a task at hand, generate a set of positive samples E = (a, b) such that a is the input to the task (from the dataset) and b is the expected label(answer/entity) for the given task.", "Similarly, generate another set of negative samples E - = (a, bi-) such that bi- is one of the incorrect label(answer/entity) for the given task.", "The incorrect entity can be sampled randomly from the set of candidate entities.", "Multiple incorrect samples could be generated for each positive example.", "These incorrect samples are indexed using i.", "For example, in case of supervised learning problem like document classification, a would be one of the documents (probably described in terms of words), b is the correct label and bi-) is one of the randomly sampled label from set of all the labels (excluding the correct label).", "In case of collaborative filtering, a would be the user (either described as a discrete entity like a userid or in terms of items purchased so far), b is the next item the user purchases and bi-) is one of the randomly sampled item from the set of all the items.", "A similarity function is chosen to compare the representation of entities of type a and b.", "The paper considered cosine similarity and inner product and observed that cosine similarity works better for the case with a large number of entities.", "A loss function compares the similarity between positive pairs (a, b) and (a, bi-).", "The paper considered margin ranking loss and negative log loss of softmax and reported that margin ranking loss works better.", "The norm of embeddings is capped at 1.", "Observations  The same model architecture is applied to a variety of tasks including multi-class classification, multi-label classification, collaborative filtering, content-based recommendation, link prediction, information retrieval, word embeddings and sentence embeddings.", "The model provides a strong baseline on all the tasks and performs at par with much more complicated and task-specific networks."], "summary_text": "The paper describes a general purpose neural embedding model where different type of entities (described in terms of discrete features) are embedded in a common vector space. A similarity function is learnt to compare these entities in a meaningful way and score their similarity. The definition of the similarity function could depend on the downstream task where the embeddings are used. Link to the implementation  Approach  Each entity is described as a set of discrete features. For example, for the recommendation use case, the users may be described as a bag-of-words of movies they have liked. For the search use case, the document may be described as a bag-of-words of words they are made up of. Given a dataset and a task at hand, generate a set of positive samples E = (a, b) such that a is the input to the task (from the dataset) and b is the expected label(answer/entity) for the given task. Similarly, generate another set of negative samples E - = (a, bi-) such that bi- is one of the incorrect label(answer/entity) for the given task. The incorrect entity can be sampled randomly from the set of candidate entities. Multiple incorrect samples could be generated for each positive example. These incorrect samples are indexed using i. For example, in case of supervised learning problem like document classification, a would be one of the documents (probably described in terms of words), b is the correct label and bi-) is one of the randomly sampled label from set of all the labels (excluding the correct label). In case of collaborative filtering, a would be the user (either described as a discrete entity like a userid or in terms of items purchased so far), b is the next item the user purchases and bi-) is one of the randomly sampled item from the set of all the items. A similarity function is chosen to compare the representation of entities of type a and b. The paper considered cosine similarity and inner product and observed that cosine similarity works better for the case with a large number of entities. A loss function compares the similarity between positive pairs (a, b) and (a, bi-). The paper considered margin ranking loss and negative log loss of softmax and reported that margin ranking loss works better. The norm of embeddings is capped at 1. Observations  The same model architecture is applied to a variety of tasks including multi-class classification, multi-label classification, collaborative filtering, content-based recommendation, link prediction, information retrieval, word embeddings and sentence embeddings. The model provides a strong baseline on all the tasks and performs at par with much more complicated and task-specific networks.", "pdf_url": "https://arxiv.org/pdf/1709.03856", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/starspace-embed-all-the-things.json"}
{"id": "99878837", "bin": "500_600", "summary_sentences": ["The paper explores the strengths and weaknesses of different evaluation metrics for end-to-end dialogue systems(in unsupervised setting).", "Evaluation Metrics Considered  Word Based Similarity Metric  BLEU  Analyses the co-occurrences of n-grams in the ground truth and the proposed responses.", "BLEU-N: N-gram precision for the entire dataset.", "Brevity penalty added to avoid bias towards short sentences.", "METEOR  Create explicit alignment between candidate and target response (using Wordnet, stemmed token etc).", "Compute the harmonic mean of precision and recall between proposed and ground truth.", "ROGUE  F-measure based on Longest Common Subsequence (LCS) between candidate and target response.", "Embedding Based Metric  Greedy Matching  Each token in actual response is greedily matched with each token in predicted response based on cosine similarity of word embedding (and vice-versa).", "Total score is averaged over all words.", "Embedding Average  Calculate sentence level embedding by averaging word level embeddings  Compare sentence level embeddings between candidate and target sentences.", "Vector Extrema  For each dimension in the word vector, take the most extreme value amongst all word vectors in the sentence, and use that value in the sentence-level embedding.", "Idea is that by taking the maxima along each dimension, we can ignore the common words (which will be pulled towards the origin in the vector space).", "Dialogue Models Considered  Retrieval Models  TF-IDF  Compute the TF-IDF vectors for each context and response in the corpus.", "C-TFIDF computes the cosine similarity between an input context and all other contexts in the corpus and returns the response with the highest score.", "R-TFIDF computes the cosine similarity between the input context and each response directly.", "Dual Encoder  Two RNNs which respectively compute the vector representation of the input context and response.", "Then calculate the probability that given response is the ground truth response given the context.", "Generative Models  LSTM language model  LSTM model trained to predict the next word in the (context, response) pair.", "Given a context, model encodes it with the LSTM and generates a response using a greedy beam search procedure.", "Hierarchical Recurrent Encoder-Decoder (HRED)  Uses a hierarchy of encoders.", "Each utterance in the context passes through an ‘utterance-level’ encoder and the output of these encoders is passed through another 'context-level' decoder.", "Better handling of long-term dependencies as compared to the conventional Encoder-Decoder.", "Observations  Human survey to determine the correlation between human judgement on the quality of responses, and the score assigned by each metric.", "Metrics (especially BLEU-4 and BLEU-3) correlate poorly with human evaluation.", "Best performing metric:  Using word-overlaps - BLEU-2 score  Using word embeddings - vector average  Embedding-based metrics would benefit from a weighting of word saliency.", "BLEU could still be a good evaluation metric in constrained tasks like mapping dialogue acts to natural language sentences."], "summary_text": "The paper explores the strengths and weaknesses of different evaluation metrics for end-to-end dialogue systems(in unsupervised setting). Evaluation Metrics Considered  Word Based Similarity Metric  BLEU  Analyses the co-occurrences of n-grams in the ground truth and the proposed responses. BLEU-N: N-gram precision for the entire dataset. Brevity penalty added to avoid bias towards short sentences. METEOR  Create explicit alignment between candidate and target response (using Wordnet, stemmed token etc). Compute the harmonic mean of precision and recall between proposed and ground truth. ROGUE  F-measure based on Longest Common Subsequence (LCS) between candidate and target response. Embedding Based Metric  Greedy Matching  Each token in actual response is greedily matched with each token in predicted response based on cosine similarity of word embedding (and vice-versa). Total score is averaged over all words. Embedding Average  Calculate sentence level embedding by averaging word level embeddings  Compare sentence level embeddings between candidate and target sentences. Vector Extrema  For each dimension in the word vector, take the most extreme value amongst all word vectors in the sentence, and use that value in the sentence-level embedding. Idea is that by taking the maxima along each dimension, we can ignore the common words (which will be pulled towards the origin in the vector space). Dialogue Models Considered  Retrieval Models  TF-IDF  Compute the TF-IDF vectors for each context and response in the corpus. C-TFIDF computes the cosine similarity between an input context and all other contexts in the corpus and returns the response with the highest score. R-TFIDF computes the cosine similarity between the input context and each response directly. Dual Encoder  Two RNNs which respectively compute the vector representation of the input context and response. Then calculate the probability that given response is the ground truth response given the context. Generative Models  LSTM language model  LSTM model trained to predict the next word in the (context, response) pair. Given a context, model encodes it with the LSTM and generates a response using a greedy beam search procedure. Hierarchical Recurrent Encoder-Decoder (HRED)  Uses a hierarchy of encoders. Each utterance in the context passes through an ‘utterance-level’ encoder and the output of these encoders is passed through another 'context-level' decoder. Better handling of long-term dependencies as compared to the conventional Encoder-Decoder. Observations  Human survey to determine the correlation between human judgement on the quality of responses, and the score assigned by each metric. Metrics (especially BLEU-4 and BLEU-3) correlate poorly with human evaluation. Best performing metric:  Using word-overlaps - BLEU-2 score  Using word embeddings - vector average  Embedding-based metrics would benefit from a weighting of word saliency. BLEU could still be a good evaluation metric in constrained tasks like mapping dialogue acts to natural language sentences.", "pdf_url": "https://arxiv.org/pdf/1603.08023", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/5748b6339ceff26420ceecfc79d58d.json"}
{"id": "91812941", "bin": "500_600", "summary_sentences": ["The paper presents a benchmark and experimental protocol (environments, metrics, baselines, training/testing setup) to evaluate RL algorithms for generalization.", "Several RL algorithms are evaluated and the key takeaway is that the “vanilla” RL algorithms can generalize better than the RL algorithms that are specifically designed to generalize, given enough diversity in the distribution of the training environments.", "The focus is on evaluating generalization to environmental changes that affect the system dynamics (and not the goal or rewards).", "Two generalization regimes are considered:  Interpolation - parameters of the test environment are similar to the parameters of the training environment.", "Extrapolation - parameters of the test environment are different from the parameters of the training environment.", "Following algorithms are considered as part of the benchmark:  “Vanilla” RL algorithms - A2C, PPO  RL algorithms that are designed to generalize:  EPOpt - Learn a (robust) policy that maximizes the expected reward over the most difficult distribution of environments (ones with the worst expected reward).", "RL2 - Learn an (adaptive) policy that can adapt to the current environment/task by considering the trajectory and not just the state transition sequence.", "These specially designed RL algorithms can be optimized using either A2C or PPO leading to combinations like EPOpt-A2C or EPOpt-PPO etc.", "The models are either composed of feedforward networks completely or feedforward + recurrent networks.", "Environments  CartPole, MountainCar, Acrobot, and Pendulum from OpenAI Gym.", "HalfCheetah and Hopper from OpenAI Roboschool.", "Three versions of each environment are considered:  Deterministic: Environment parameters are fixed.", "This case corresponds to the standard environment setup in classical RL.", "Random: Environment parameters are sampled randomly.", "This case corresponds to sampling from a distribution of environments.", "Extreme: Environment parameters are sampled from their extreme values.", "This case corresponds to the edge-case environments which would not be encountered during training generally.", "Performance Metrics  Average total reward per episode.", "Success percentage: Percentage of episodes where a certain goal (or reward) is obtained.", "Evaluation Metrics/Setups  Default: success percentage when training and evaluating the deterministic version of the environment.", "Interpolation: success percentage when training and evaluating on the random version of the environment.", "Extrapolation: the geometric mean of the success percentage of following three versions:  Train on deterministic and evaluate on the random version.", "Train on deterministic and evaluate on extreme version.", "Train on random and evaluate on the extreme version.", "Observations  Extrapolation is harder than interpolation.", "Increasing the diversity in the training environments improves the interpolation generalization of vanilla RL methods.", "EPOpt improves generalization only for continuous control environments and only with PPO.", "RL2 is difficult to train on the environments considered and did not provide a clear advantage in terms of generalization.", "EPOpt-PPO outperforms PPO on only 3 environments and EPOpt-A2C does not"], "summary_text": "The paper presents a benchmark and experimental protocol (environments, metrics, baselines, training/testing setup) to evaluate RL algorithms for generalization. Several RL algorithms are evaluated and the key takeaway is that the “vanilla” RL algorithms can generalize better than the RL algorithms that are specifically designed to generalize, given enough diversity in the distribution of the training environments. The focus is on evaluating generalization to environmental changes that affect the system dynamics (and not the goal or rewards). Two generalization regimes are considered:  Interpolation - parameters of the test environment are similar to the parameters of the training environment. Extrapolation - parameters of the test environment are different from the parameters of the training environment. Following algorithms are considered as part of the benchmark:  “Vanilla” RL algorithms - A2C, PPO  RL algorithms that are designed to generalize:  EPOpt - Learn a (robust) policy that maximizes the expected reward over the most difficult distribution of environments (ones with the worst expected reward). RL2 - Learn an (adaptive) policy that can adapt to the current environment/task by considering the trajectory and not just the state transition sequence. These specially designed RL algorithms can be optimized using either A2C or PPO leading to combinations like EPOpt-A2C or EPOpt-PPO etc. The models are either composed of feedforward networks completely or feedforward + recurrent networks. Environments  CartPole, MountainCar, Acrobot, and Pendulum from OpenAI Gym. HalfCheetah and Hopper from OpenAI Roboschool. Three versions of each environment are considered:  Deterministic: Environment parameters are fixed. This case corresponds to the standard environment setup in classical RL. Random: Environment parameters are sampled randomly. This case corresponds to sampling from a distribution of environments. Extreme: Environment parameters are sampled from their extreme values. This case corresponds to the edge-case environments which would not be encountered during training generally. Performance Metrics  Average total reward per episode. Success percentage: Percentage of episodes where a certain goal (or reward) is obtained. Evaluation Metrics/Setups  Default: success percentage when training and evaluating the deterministic version of the environment. Interpolation: success percentage when training and evaluating on the random version of the environment. Extrapolation: the geometric mean of the success percentage of following three versions:  Train on deterministic and evaluate on the random version. Train on deterministic and evaluate on extreme version. Train on random and evaluate on the extreme version. Observations  Extrapolation is harder than interpolation. Increasing the diversity in the training environments improves the interpolation generalization of vanilla RL methods. EPOpt improves generalization only for continuous control environments and only with PPO. RL2 is difficult to train on the environments considered and did not provide a clear advantage in terms of generalization. EPOpt-PPO outperforms PPO on only 3 environments and EPOpt-A2C does not", "pdf_url": "https://arxiv.org/pdf/1810.12282", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/assessing-generalization-in-deep-reinforcement-learning.json"}
{"id": "59485457", "bin": "500_600", "summary_sentences": ["What  They suggest a method to generate ensembles of models requiring less total training time.", "The method is based on \"saving\" intermediary versions of models.", "How  They save every M epochs an intermediary version of the model (i.e. they save the parameters).", "Then they combine the last n models to an ensemble.", "To make each version more dissimilar from the other ones, they cycle the learning rate using a cosine function.", "That means that they increase the learning rate significantly, then keep it at the high level for a short time, then decrease it fast, then keep it at a low level for a short time.", "Formula for the (cycled) learning rate:  alpha_0 is the initial learning rate,  T the total number of training iterations  M the number of training iterations per cycle (after each cycle, the learning rate is increased)  Visualization of the cycled learning rate:  (Note that after the third cycle the model is already at nearly optimal accuracy, despite having suffered two times from increasing the learning rate.", "With a better learning rate schedule it might have been able to reach the optimum in 2-3 cycles.", "So it is kinda unhonest to say that this ensembling method adds no training it, it just probably adds less than \"normal\" ensembling from scratch.)", "(Also note here that the blue learning rate schedule that they are comparing against is probably far away from being optimal.)", "They argue that cycling the learning rate is also useful to \"jump\" between local minima.", "I.e. the model reaches a local minima, then escapes it using a high learning rate, then descends into a new local minima using the lowered learning rate.", "Then the ensemble would consist of models in different local minima.", "(Note here though that the current state of science is that there aren't really local minima in deep NNs, only saddle points.)", "(Also note that this means that the ensembled models probably are often fairly similar.", "A proper ensemble consisting only of models trained from scratch might perform better.)", "Results  Using roughly the 3 last snapshots for the ensemble seems to be the best compromise (at alpha_0=0.1).", "Using too many snaphots can worsen the results.", "Using alpha_0=0.2 seems to be a better choice than alpha_0=0.1.", "They argue that the high learning rate between cycles leads to more diverse local minima.", "Only running one learning rate cycle and collecting the ensemble models from that leads to worse results (as opposed to running multiple cycles).", "The following visualization shows the effect of using a single cycle vs. multiple (in relations to the training iterations).", "They perform overall better than models without ensembling.", "True ensembles reach quite a bit better accuracy still.", "Running models with interpolated snaphots (e.g. set each weight to 30% of snapshot 1 and 70% of snaphot 5), the test accuracy is improved if the snapshots are close to each other (e.g.", "snaphot 4 and 5).", "This indicates that the parameters change more and more with each cycle."], "summary_text": "What  They suggest a method to generate ensembles of models requiring less total training time. The method is based on \"saving\" intermediary versions of models. How  They save every M epochs an intermediary version of the model (i.e. they save the parameters). Then they combine the last n models to an ensemble. To make each version more dissimilar from the other ones, they cycle the learning rate using a cosine function. That means that they increase the learning rate significantly, then keep it at the high level for a short time, then decrease it fast, then keep it at a low level for a short time. Formula for the (cycled) learning rate:  alpha_0 is the initial learning rate,  T the total number of training iterations  M the number of training iterations per cycle (after each cycle, the learning rate is increased)  Visualization of the cycled learning rate:  (Note that after the third cycle the model is already at nearly optimal accuracy, despite having suffered two times from increasing the learning rate. With a better learning rate schedule it might have been able to reach the optimum in 2-3 cycles. So it is kinda unhonest to say that this ensembling method adds no training it, it just probably adds less than \"normal\" ensembling from scratch.) (Also note here that the blue learning rate schedule that they are comparing against is probably far away from being optimal.) They argue that cycling the learning rate is also useful to \"jump\" between local minima. I.e. the model reaches a local minima, then escapes it using a high learning rate, then descends into a new local minima using the lowered learning rate. Then the ensemble would consist of models in different local minima. (Note here though that the current state of science is that there aren't really local minima in deep NNs, only saddle points.) (Also note that this means that the ensembled models probably are often fairly similar. A proper ensemble consisting only of models trained from scratch might perform better.) Results  Using roughly the 3 last snapshots for the ensemble seems to be the best compromise (at alpha_0=0.1). Using too many snaphots can worsen the results. Using alpha_0=0.2 seems to be a better choice than alpha_0=0.1. They argue that the high learning rate between cycles leads to more diverse local minima. Only running one learning rate cycle and collecting the ensemble models from that leads to worse results (as opposed to running multiple cycles). The following visualization shows the effect of using a single cycle vs. multiple (in relations to the training iterations). They perform overall better than models without ensembling. True ensembles reach quite a bit better accuracy still. Running models with interpolated snaphots (e.g. set each weight to 30% of snapshot 1 and 70% of snaphot 5), the test accuracy is improved if the snapshots are close to each other (e.g. snaphot 4 and 5). This indicates that the parameters change more and more with each cycle.", "pdf_url": "https://arxiv.org/pdf/1704.00109", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/snapshot_ensembles.json"}
{"id": "93544751", "bin": "600_700", "summary_sentences": ["Standard unsupervised learning aims to learn transferable features.", "The paper proposes to learn a transferable learning rule (in an unsupervised manner) that can generalize across tasks and architectures.", "Paper  Approach  Consider training the model with supervised learning - φt+1 = SupervisedUpdate(φt, xt, yt, θ).", "Here t denotes the step, (x, y) denotes the data points, θ denotes the hyperparameters of the optimizer.", "Extending this formulation for meta-learning, one could say that t is the step of the inner loop, θ are the parameters of the meta learning model.", "Further, the paper proposes to use φt+1 = UnsupervisedUpdate(φt, xt, θ) ie yt is not used (or even assumed to be available as this is unsupervised learning).", "The meta update rule is used to learn the weights of a meta-model by performing SGD on the sum of MetaObjective over the distribution of tasks (over the course of inner loop training).", "Model  Base model: MLP with parameters φt  To ensure that it generalizes across architectures, the update rule is designed to be neural-local ie updates are a function of pre and postsynaptic neurons though, in practice, this constraint is relaxed to decorrelate neurons by using cross neural information.", "Each neuron i in every layer l (in the base model) has an update network (MLP) which takes as input the feedforward activations, feedback weights and error signals.", "ie hbl(i) = MLP(xbl(i), zbl(i), vl+1, δl(i), θ)  b - index of the minibatch  xl - pre non-linearity activations  zl - post non-linearity activations  vl - feedback weights  δl - error signal  All the update networks share the meta parameters θ  The model is run in a standard feed-forward manner and the update network (corresponding to each unit) is used to generate the error signal δlb(i) = lin(hbl(i)).", "This loss is backpropogated using the set of learned backward weights vl instead of the forward weights wl.", "The weight update Δwl is also generated using a per-neuron update network.", "Meta Objective  The MetaObjective is based on fitting a linear regression model to labeled examples with a small number of data points.", "Given the emphasis on learning generalizable features, the weights (of linear regression) are estimated on one batch and evaluated on another batch.", "The MetaObjective is to reduce the cosine distance between yb and vTxbL  yb - Actual lables on the evaluation batch  xbL - Features of the evaluation batch (using the base model)  v - parameters of the linear regression model (learned on train batch)  Practical Considerations  Meta gradients are approximated using truncated backdrop through time.", "Increasing variation in the training dataset helps the meta optimization process.", "Data is augmented with shifts, rotations, and noise.", "Predicting these coefficients is an auxiliary (regression) task for training the meta-objective.", "Training the system requires a lot of resources - 8 days with 512 workers.", "Results  With standard unsupervised learning, the performance (on transfer task) starts declining after some time even though the performance (on the unsupervised task) is improving.", "This suggests that the objective function for the two tasks starts to mismatch.", "UnsupervisedUpdate leads to a better generalization as compared to both VAE and supervised learning (followed by transfer).", "UnsupervisedUpdate also leads to a positive transfer across domains (vision to language) when trained for a shorter duration of time (to ensure that the meta-objective does not overfit).", "UnsupervisedUpdate also generalizes to larger model architectures and different activation functions."], "summary_text": "Standard unsupervised learning aims to learn transferable features. The paper proposes to learn a transferable learning rule (in an unsupervised manner) that can generalize across tasks and architectures. Paper  Approach  Consider training the model with supervised learning - φt+1 = SupervisedUpdate(φt, xt, yt, θ). Here t denotes the step, (x, y) denotes the data points, θ denotes the hyperparameters of the optimizer. Extending this formulation for meta-learning, one could say that t is the step of the inner loop, θ are the parameters of the meta learning model. Further, the paper proposes to use φt+1 = UnsupervisedUpdate(φt, xt, θ) ie yt is not used (or even assumed to be available as this is unsupervised learning). The meta update rule is used to learn the weights of a meta-model by performing SGD on the sum of MetaObjective over the distribution of tasks (over the course of inner loop training). Model  Base model: MLP with parameters φt  To ensure that it generalizes across architectures, the update rule is designed to be neural-local ie updates are a function of pre and postsynaptic neurons though, in practice, this constraint is relaxed to decorrelate neurons by using cross neural information. Each neuron i in every layer l (in the base model) has an update network (MLP) which takes as input the feedforward activations, feedback weights and error signals. ie hbl(i) = MLP(xbl(i), zbl(i), vl+1, δl(i), θ)  b - index of the minibatch  xl - pre non-linearity activations  zl - post non-linearity activations  vl - feedback weights  δl - error signal  All the update networks share the meta parameters θ  The model is run in a standard feed-forward manner and the update network (corresponding to each unit) is used to generate the error signal δlb(i) = lin(hbl(i)). This loss is backpropogated using the set of learned backward weights vl instead of the forward weights wl. The weight update Δwl is also generated using a per-neuron update network. Meta Objective  The MetaObjective is based on fitting a linear regression model to labeled examples with a small number of data points. Given the emphasis on learning generalizable features, the weights (of linear regression) are estimated on one batch and evaluated on another batch. The MetaObjective is to reduce the cosine distance between yb and vTxbL  yb - Actual lables on the evaluation batch  xbL - Features of the evaluation batch (using the base model)  v - parameters of the linear regression model (learned on train batch)  Practical Considerations  Meta gradients are approximated using truncated backdrop through time. Increasing variation in the training dataset helps the meta optimization process. Data is augmented with shifts, rotations, and noise. Predicting these coefficients is an auxiliary (regression) task for training the meta-objective. Training the system requires a lot of resources - 8 days with 512 workers. Results  With standard unsupervised learning, the performance (on transfer task) starts declining after some time even though the performance (on the unsupervised task) is improving. This suggests that the objective function for the two tasks starts to mismatch. UnsupervisedUpdate leads to a better generalization as compared to both VAE and supervised learning (followed by transfer). UnsupervisedUpdate also leads to a positive transfer across domains (vision to language) when trained for a shorter duration of time (to ensure that the meta-objective does not overfit). UnsupervisedUpdate also generalizes to larger model architectures and different activation functions.", "pdf_url": "https://www.mitpressjournals.org/doi/pdf/10.1162/089976602753712972", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/meta-learning-update-rules-for-unsupervised-representation-learning.json"}
{"id": "3855110", "bin": "600_700", "summary_sentences": ["Major points made by the article:  “Our view is that to make genomic medicine a reality, we must develop computer systems that can accurately interpret the text of the genome just as the machinery inside the cell does”.", "“Protein-coding exons are the most understood regions in the genome (re: “start” and “stop” codons”).", "A long standing open problem is predicting whether a mutation will disrupt the stability or structure of the final protein molecule  “Predicting phenotypes (e.g., traits and disease risks) from biomarkers such as the genome is, in principle, a supervised machine learning problem”.", "The correct approach is not so simple; the computational model should be trained to predict measurable intermediate cell variables, also known as molecular phenotypes, and then these variables can be linked to phenotypes.", "Alternative Splicing (AS) is the selection and ligation of specific exons during post-transcriptional modification.", "On average, each protein-coding gene has approximately four transcripts (# of ways of selecting and combining available exons).", "We would like to be able to predict splicing by discovering the instructions that control splicing  Computational Model of Splicing  By accurately modeling splicing and AS computationally, researchers have been able to predict how it is affected by variations in the genome, and then to assess whether a mutation in the genome affects disease risk.", "Computational Model of Protein-DNA and Protein-RNA binding  “Accurate models of protein-sequence binding are essential for interpreting the genome and for predicting the effects of mutations…Biologists have developed high-throughput experiments that measure the sequence specificity of individual proteins.”  Example computational model: inputs = genomic sequence, outputs is a binding score.", "One would like to predict the “motifs”, or patterns, that a particular protein binds to.", "Specific Discussion Related to Deep Learning  Deep Learning has been used to improve predictive performance- see Feedforward NNs for AS patterns .", "CNNs have been used to improve predictive performance for binding specificity .", "Cellular processes are highly stochastic and hence the genotype of an individual may not be sufficient to completely determine their phenotype  Measuring hundreds of thousands of cell variable measurements per patient for a small group of people potentially gives a better chance at deciphering the genomic instructions of the cell.", "More data for a model to learn from.", "Necessary to use “large-scale machine learning”  RNNs can be useful for the following  genome annotation  Modelling of cell variable dynamics through time  Creating a sequential state model of protein binding based on RNNs or LSTMs  Imputation of epigenomic tracks - seq2seq  Machine Learning models need to be more interpret-able for genomics!", "Notes  Since this is my first foray into computational biology, I’m going to keep track of a lot of terminology here:  1.", "Protein-coding genes describe how to build large molecules made from amino-acid chains (human genome contains ~20,000) 2.", "Non-coding genes describe how to build small molecules made from ribonucleic acid (RNA) chains (human genome contains ~25,000) 3.", "Information structures making up alternating regions on a typical gene are known as Introns and Exons  4.", "Protein-sequence binding is the binding of proteins to nucleotide sequences 5.", "Position-Frequency Matrix - \"workhorse of binding site modeling\"  Strengths  Excellent paper for Machine Learning researchers to get a first look at diving into genomics."], "summary_text": "Major points made by the article:  “Our view is that to make genomic medicine a reality, we must develop computer systems that can accurately interpret the text of the genome just as the machinery inside the cell does”. “Protein-coding exons are the most understood regions in the genome (re: “start” and “stop” codons”). A long standing open problem is predicting whether a mutation will disrupt the stability or structure of the final protein molecule  “Predicting phenotypes (e.g., traits and disease risks) from biomarkers such as the genome is, in principle, a supervised machine learning problem”. The correct approach is not so simple; the computational model should be trained to predict measurable intermediate cell variables, also known as molecular phenotypes, and then these variables can be linked to phenotypes. Alternative Splicing (AS) is the selection and ligation of specific exons during post-transcriptional modification. On average, each protein-coding gene has approximately four transcripts (# of ways of selecting and combining available exons). We would like to be able to predict splicing by discovering the instructions that control splicing  Computational Model of Splicing  By accurately modeling splicing and AS computationally, researchers have been able to predict how it is affected by variations in the genome, and then to assess whether a mutation in the genome affects disease risk. Computational Model of Protein-DNA and Protein-RNA binding  “Accurate models of protein-sequence binding are essential for interpreting the genome and for predicting the effects of mutations…Biologists have developed high-throughput experiments that measure the sequence specificity of individual proteins.”  Example computational model: inputs = genomic sequence, outputs is a binding score. One would like to predict the “motifs”, or patterns, that a particular protein binds to. Specific Discussion Related to Deep Learning  Deep Learning has been used to improve predictive performance- see Feedforward NNs for AS patterns . CNNs have been used to improve predictive performance for binding specificity . Cellular processes are highly stochastic and hence the genotype of an individual may not be sufficient to completely determine their phenotype  Measuring hundreds of thousands of cell variable measurements per patient for a small group of people potentially gives a better chance at deciphering the genomic instructions of the cell. More data for a model to learn from. Necessary to use “large-scale machine learning”  RNNs can be useful for the following  genome annotation  Modelling of cell variable dynamics through time  Creating a sequential state model of protein binding based on RNNs or LSTMs  Imputation of epigenomic tracks - seq2seq  Machine Learning models need to be more interpret-able for genomics! Notes  Since this is my first foray into computational biology, I’m going to keep track of a lot of terminology here:  1. Protein-coding genes describe how to build large molecules made from amino-acid chains (human genome contains ~20,000) 2. Non-coding genes describe how to build small molecules made from ribonucleic acid (RNA) chains (human genome contains ~25,000) 3. Information structures making up alternating regions on a typical gene are known as Introns and Exons  4. Protein-sequence binding is the binding of proteins to nucleotide sequences 5. Position-Frequency Matrix - \"workhorse of binding site modeling\"  Strengths  Excellent paper for Machine Learning researchers to get a first look at diving into genomics.", "pdf_url": "https://persagen.com/files/misc/leung2016machine.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/ml-for-genomics.json"}
{"id": "1907968", "bin": "600_700", "summary_sentences": ["What  They present a variation of Faster R-CNN.", "Faster R-CNN is a model that detects bounding boxes in images.", "Their variation is about as accurate as the best performing versions of Faster R-CNN.", "Their variation is significantly faster than these variations (roughly 50ms per image).", "How  PVANET reuses the standard Faster R-CNN architecture:  A base network that transforms an image into a feature map.", "A region proposal network (RPN) that uses the feature map to predict bounding box candidates.", "A classifier that uses the feature map and the bounding box candidates to predict the final bounding boxes.", "PVANET modifies the base network and keeps the RPN and classifier the same.", "Inception  Their base network uses eight Inception modules.", "They argue that these are good choices here, because they are able to represent an image at different scales (aka at different receptive field sizes) due to their mixture of 3x3 and 1x1 convolutions.", "Representing an image at different scales is useful here in order to detect both large and small bounding boxes.", "Inception modules are also reasonably fast.", "Visualization of their Inception modules:  Concatenated ReLUs  Before the eight Inception modules, they start the network with eight convolutions using concatenated ReLUs.", "These CReLUs compute both the classic ReLU result (max(0, x)) and concatenate to that the negated result, i.e. something like f(x) = max(0, x <concat> (-1)*x).", "That is done, because among the early one can often find pairs of convolution filters that are the negated variations of each other.", "So by adding CReLUs, the network does not have to compute these any more, instead they are created (almost) for free, reducing the computation time by up to 50%.", "Visualization of their final CReLU block:  TODO  Multi-Scale output  Usually one would generate the final feature map simply from the output of the last convolution.", "They instead combine the outputs of three different convolutions, each resembling a different scale (or level of abstraction).", "They take one from an early point of the network (downscaled), one from the middle part (kept the same) and one from the end (upscaled).", "They concatenate these and apply a 1x1 convolution to generate the final output.", "Other stuff  Most of their network uses residual connections (including the Inception modules) to facilitate learning.", "They pretrain on ILSVRC2012 and then perform fine-tuning on MSCOCO, VOC 2007 and VOC 2012.", "They use plateau detection for their learning rate, i.e. if a moving average of the loss does not improve any more, they decrease the learning rate.", "They say that this increases accuracy significantly.", "The classifier in Faster R-CNN consists of fully connected layers.", "They compress these via Truncated SVD to speed things up.", "(That was already part of Fast R-CNN, I think.)", "Results  On Pascal VOC 2012 they achieve 82.5% mAP at 46ms/image (Titan X GPU).", "Faster R-CNN + ResNet-101: 83.8% at 2.2s/image.", "Faster R-CNN + VGG16: 75.9% at 110ms/image.", "R-FCN + ResNet-101: 82.0% at 133ms/image.", "Decreasing the number of region proposals from 300 per image to 50 almost doubles the speed (to 27ms/image) at a small loss of 1.5 percentage points mAP.", "Using Truncated SVD for the classifier reduces the required timer per image by about 30% at roughly 1 percentage point of mAP loss."], "summary_text": "What  They present a variation of Faster R-CNN. Faster R-CNN is a model that detects bounding boxes in images. Their variation is about as accurate as the best performing versions of Faster R-CNN. Their variation is significantly faster than these variations (roughly 50ms per image). How  PVANET reuses the standard Faster R-CNN architecture:  A base network that transforms an image into a feature map. A region proposal network (RPN) that uses the feature map to predict bounding box candidates. A classifier that uses the feature map and the bounding box candidates to predict the final bounding boxes. PVANET modifies the base network and keeps the RPN and classifier the same. Inception  Their base network uses eight Inception modules. They argue that these are good choices here, because they are able to represent an image at different scales (aka at different receptive field sizes) due to their mixture of 3x3 and 1x1 convolutions. Representing an image at different scales is useful here in order to detect both large and small bounding boxes. Inception modules are also reasonably fast. Visualization of their Inception modules:  Concatenated ReLUs  Before the eight Inception modules, they start the network with eight convolutions using concatenated ReLUs. These CReLUs compute both the classic ReLU result (max(0, x)) and concatenate to that the negated result, i.e. something like f(x) = max(0, x <concat> (-1)*x). That is done, because among the early one can often find pairs of convolution filters that are the negated variations of each other. So by adding CReLUs, the network does not have to compute these any more, instead they are created (almost) for free, reducing the computation time by up to 50%. Visualization of their final CReLU block:  TODO  Multi-Scale output  Usually one would generate the final feature map simply from the output of the last convolution. They instead combine the outputs of three different convolutions, each resembling a different scale (or level of abstraction). They take one from an early point of the network (downscaled), one from the middle part (kept the same) and one from the end (upscaled). They concatenate these and apply a 1x1 convolution to generate the final output. Other stuff  Most of their network uses residual connections (including the Inception modules) to facilitate learning. They pretrain on ILSVRC2012 and then perform fine-tuning on MSCOCO, VOC 2007 and VOC 2012. They use plateau detection for their learning rate, i.e. if a moving average of the loss does not improve any more, they decrease the learning rate. They say that this increases accuracy significantly. The classifier in Faster R-CNN consists of fully connected layers. They compress these via Truncated SVD to speed things up. (That was already part of Fast R-CNN, I think.) Results  On Pascal VOC 2012 they achieve 82.5% mAP at 46ms/image (Titan X GPU). Faster R-CNN + ResNet-101: 83.8% at 2.2s/image. Faster R-CNN + VGG16: 75.9% at 110ms/image. R-FCN + ResNet-101: 82.0% at 133ms/image. Decreasing the number of region proposals from 300 per image to 50 almost doubles the speed (to 27ms/image) at a small loss of 1.5 percentage points mAP. Using Truncated SVD for the classifier reduces the required timer per image by about 30% at roughly 1 percentage point of mAP loss.", "pdf_url": "https://arxiv.org/pdf/1608.08021", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/pvanet.json"}
{"id": "27086937", "bin": "600_700", "summary_sentences": ["In this paper they prior the representation a logistic regression model using known protein-protein interactions.", "They do so by regularizing the weights of the model using the Laplacian encoding of a graph.", "Here is a regularization term of this form:  $$\\lambda ||w||_1 + \\eta w^T L w,$$  #### A small example:  Given a small graph of three nodes A, B, and C with one edge: {A-B} we have the following Laplacian:  $$ L = D - A =  \\left[\\array{ 1 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 0}\\right] - \\left[\\array{ 0 & 1 & 0 \\\\ 1 & 0 & 0\\\\ 0 & 0 & 0}\\right]$$  $$L =  \\left[\\array{ 1 & -1 & 0 \\\\ -1 & 1 & 0\\\\ 0 & 0 & 0}\\right] $$  If we have a small linear regression of the form:  $$y = x_Aw_A + x_Bw_B + x_Cw_C$$  Then we can look at how $w^TLw$ will impact the weights to gain insight:  $$w^TLw $$  $$= \\left[\\array{ w_A & w_B & w_C}\\right] \\left[\\array{ 1 & -1 & 0 \\\\ -1 & 1 & 0\\\\ 0 & 0 & 0}\\right] \\left[\\array{ w_A \\\\ w_B \\\\ w_C}\\right]  $$  $$=  \\left[\\array{ w_A & w_B & w_C}\\right] \\left[\\array{ w_A -w_B \\\\ -w_A + w_B \\\\ 0}\\right]  $$    $$ =  (w_A^2 -w_Aw_B ) +  (-w_Aw_B + w_B^2) $$  So because all terms are squared we can remove them from consideration to look at what is the real impact of regularization.", "$$ =  (-w_Aw_B ) +  (-w_Aw_B) $$  $$ = -2w_Aw_B$$  The Laplacian regularization seems to increase the weight values of edges which are connected.", "Along with the squared terms and the $L1$ penalty that is also used the weights cannot grow without bound.", "#### A few more experiments:  If we perform the same computation for a graph with two edges: {A-B, B-C} we have the following term which increases the weights of both pairwise interactions:  $$ = -2w_Aw_B -2w_Bw_C$$  If we perform the same computation for a graph with two edges: {A-B, A-C} we have no surprises:   $$ = -2w_Aw_B -2w_Aw_C$$  Another thing to think about is if there are no edges.", "If by default there are self-loops then the degree matrix will have 1 on the diagonal and it will be the identity which will be an $L2$ term.", "If no self loops are defined then the result is a 0 matrix yielding no regularization at all.", "#### Contribution:  A contribution of this paper is to use the absolute value of the weights to make training easier.", "$$|w|^T L |w|$$  TODO: Add more about how this impacts learning.", "#### Overview  Here a high level figure shows the data and targets together with a graph prior.", "It looks nice so I wanted to include it.", "[url]"], "summary_text": "In this paper they prior the representation a logistic regression model using known protein-protein interactions. They do so by regularizing the weights of the model using the Laplacian encoding of a graph. Here is a regularization term of this form:  $$\\lambda ||w||_1 + \\eta w^T L w,$$  #### A small example:  Given a small graph of three nodes A, B, and C with one edge: {A-B} we have the following Laplacian:  $$ L = D - A =  \\left[\\array{ 1 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 0}\\right] - \\left[\\array{ 0 & 1 & 0 \\\\ 1 & 0 & 0\\\\ 0 & 0 & 0}\\right]$$  $$L =  \\left[\\array{ 1 & -1 & 0 \\\\ -1 & 1 & 0\\\\ 0 & 0 & 0}\\right] $$  If we have a small linear regression of the form:  $$y = x_Aw_A + x_Bw_B + x_Cw_C$$  Then we can look at how $w^TLw$ will impact the weights to gain insight:  $$w^TLw $$  $$= \\left[\\array{ w_A & w_B & w_C}\\right] \\left[\\array{ 1 & -1 & 0 \\\\ -1 & 1 & 0\\\\ 0 & 0 & 0}\\right] \\left[\\array{ w_A \\\\ w_B \\\\ w_C}\\right]  $$  $$=  \\left[\\array{ w_A & w_B & w_C}\\right] \\left[\\array{ w_A -w_B \\\\ -w_A + w_B \\\\ 0}\\right]  $$    $$ =  (w_A^2 -w_Aw_B ) +  (-w_Aw_B + w_B^2) $$  So because all terms are squared we can remove them from consideration to look at what is the real impact of regularization. $$ =  (-w_Aw_B ) +  (-w_Aw_B) $$  $$ = -2w_Aw_B$$  The Laplacian regularization seems to increase the weight values of edges which are connected. Along with the squared terms and the $L1$ penalty that is also used the weights cannot grow without bound. #### A few more experiments:  If we perform the same computation for a graph with two edges: {A-B, B-C} we have the following term which increases the weights of both pairwise interactions:  $$ = -2w_Aw_B -2w_Bw_C$$  If we perform the same computation for a graph with two edges: {A-B, A-C} we have no surprises:   $$ = -2w_Aw_B -2w_Aw_C$$  Another thing to think about is if there are no edges. If by default there are self-loops then the degree matrix will have 1 on the diagonal and it will be the identity which will be an $L2$ term. If no self loops are defined then the result is a 0 matrix yielding no regularization at all. #### Contribution:  A contribution of this paper is to use the absolute value of the weights to make training easier. $$|w|^T L |w|$$  TODO: Add more about how this impacts learning. #### Overview  Here a high level figure shows the data and targets together with a graph prior. It looks nice so I wanted to include it. [url]", "pdf_url": "http://arxiv.org/pdf/1609.06480v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/1609.06480.json"}
{"id": "77684000", "bin": "600_700", "summary_sentences": ["Build a supervised reading comprehension data set using news corpus.", "Compare the performance of neural models and state-of-the-art natural language processing model on reading comprehension task.", "Reading Comprehension  Estimate conditional probability p(a|c, q), where c is a context document, q is a query related to the document, and a is the answer to that query.", "Dataset Generation  Use online newspapers (CNN and DailyMail) and their matching summaries.", "Parse summaries and bullet points into Cloze style questions.", "Generate corpus of document-query-answer triplets by replacing one entity at a time with a placeholder.", "Data anonymized and randomised using coreference systems, abstract entity markers and random permutation of the entity markers.", "The processed data set is more focused in terms of evaluating reading comprehension as models can not exploit co-occurrence.", "Models  Baseline Models  Majority Baseline  Picks the most frequently observed entity in the context document.", "Exclusive Majority  Picks the most frequently observed entity in the context document which is not observed in the query.", "Symbolic Matching Models  Frame-Semantic Parsing  Parse the sentence to find predicates to answer questions like \"who did what to whom\".", "Extracting entity-predicate triples (e1,V, e2) from query q and context document d  Resolve queries using rules like exact match, matching entity etc.", "Word Distance Benchmark  Align placeholder of Cloze form questions with each possible entity in the context document and calculate the distance between the question and the context around the aligned entity.", "Sum the distance of every word in q to their nearest aligned word in d  Neural Network Models  Deep LSTM Reader  Test the ability of Deep LSTM encoders to handle significantly longer sequences.", "Feed the document query pair as a single large document, one word at a time.", "Use Deep LSTM cell with skip connections from input to hidden layers and hidden layer to output.", "Attentive Reader  Employ attention model to overcome the bottleneck of fixed width hidden vector.", "Encode the document and the query using separate bidirectional single layer LSTM.", "Query encoding is obtained by concatenating the final forward and backwards outputs.", "Document encoding is obtained by a weighted sum of output vectors (obtained by concatenating the forward and backwards outputs).", "The weights can be interpreted as the degree to which the network attends to a particular token in the document.", "Model completed by defining a non-linear combination of document and query embedding.", "Impatient Reader  As an add-on to the attentive reader, the model can re-read the document as each query token is read.", "Model accumulates the information from the document as each query token is seen and finally outputs a joint document query representation in the form of a non-linear combination of document embedding and query embedding.", "Result  Attentive and Impatient Readers outperform all other models highlighting the benefits of attention modelling.", "Frame-Semantic pipeline does not scale to cases where several methods are needed to answer a query.", "Moreover, they provide poor coverage as a lot of relations do not adhere to the default predicate-argument structure.", "Word Distance approach outperformed the Frame-Semantic approach as there was significant lexical overlap between the query and the document.", "The paper also includes heat maps over the context documents to visualise the attention mechanism.", "This comment has been minimized.", "Sign in to view  Copy link  Quote reply  yauhen-info commented  Apr 28, 2017  Thank you for sharing a good piece of work.", "Let me also ask if you had found a link to an implementation of the Attentive and Impatient Readers?"], "summary_text": "Build a supervised reading comprehension data set using news corpus. Compare the performance of neural models and state-of-the-art natural language processing model on reading comprehension task. Reading Comprehension  Estimate conditional probability p(a|c, q), where c is a context document, q is a query related to the document, and a is the answer to that query. Dataset Generation  Use online newspapers (CNN and DailyMail) and their matching summaries. Parse summaries and bullet points into Cloze style questions. Generate corpus of document-query-answer triplets by replacing one entity at a time with a placeholder. Data anonymized and randomised using coreference systems, abstract entity markers and random permutation of the entity markers. The processed data set is more focused in terms of evaluating reading comprehension as models can not exploit co-occurrence. Models  Baseline Models  Majority Baseline  Picks the most frequently observed entity in the context document. Exclusive Majority  Picks the most frequently observed entity in the context document which is not observed in the query. Symbolic Matching Models  Frame-Semantic Parsing  Parse the sentence to find predicates to answer questions like \"who did what to whom\". Extracting entity-predicate triples (e1,V, e2) from query q and context document d  Resolve queries using rules like exact match, matching entity etc. Word Distance Benchmark  Align placeholder of Cloze form questions with each possible entity in the context document and calculate the distance between the question and the context around the aligned entity. Sum the distance of every word in q to their nearest aligned word in d  Neural Network Models  Deep LSTM Reader  Test the ability of Deep LSTM encoders to handle significantly longer sequences. Feed the document query pair as a single large document, one word at a time. Use Deep LSTM cell with skip connections from input to hidden layers and hidden layer to output. Attentive Reader  Employ attention model to overcome the bottleneck of fixed width hidden vector. Encode the document and the query using separate bidirectional single layer LSTM. Query encoding is obtained by concatenating the final forward and backwards outputs. Document encoding is obtained by a weighted sum of output vectors (obtained by concatenating the forward and backwards outputs). The weights can be interpreted as the degree to which the network attends to a particular token in the document. Model completed by defining a non-linear combination of document and query embedding. Impatient Reader  As an add-on to the attentive reader, the model can re-read the document as each query token is read. Model accumulates the information from the document as each query token is seen and finally outputs a joint document query representation in the form of a non-linear combination of document embedding and query embedding. Result  Attentive and Impatient Readers outperform all other models highlighting the benefits of attention modelling. Frame-Semantic pipeline does not scale to cases where several methods are needed to answer a query. Moreover, they provide poor coverage as a lot of relations do not adhere to the default predicate-argument structure. Word Distance approach outperformed the Frame-Semantic approach as there was significant lexical overlap between the query and the document. The paper also includes heat maps over the context documents to visualise the attention mechanism. This comment has been minimized. Sign in to view  Copy link  Quote reply  yauhen-info commented  Apr 28, 2017  Thank you for sharing a good piece of work. Let me also ask if you had found a link to an implementation of the Attentive and Impatient Readers?", "pdf_url": "http://arxiv.org/pdf/1506.03340v3", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/63eb099bb7a1ab4831cd37bffffb04.json"}
{"id": "87460557", "bin": "700_800", "summary_sentences": ["Enabling blockchain innovations with pegged sidechains – Back et al. 2014  A very topical choice today.", "Last week a number of key players in the Bitcoin ecosystem published a paper (see link above) discussing a mechanism (‘pegged sidechains’) to allowed continued innovation and evolution of Bitcoin and related blockchain-based solutions.", "From the abstract:  Since the introduction of Bitcoin in 2009, and the multiple computer science and electronic cash innovations it brought, there has been great interest in the potential of decentralised cryptocurrencies.", "At the same time,  implementation changes to the consensus-critical parts of Bitcoin must necessarily be handled very conservatively.", "As a result, Bitcoin has greater difficulty than other internet protocols adapting to new demands and accommodating new innovation.", "An early solution to the problem of innovation was the development of alternative blockchains, altchains, which modify the bitcoin codebase in some way.", "The deep security expertise needed to do this right has lead to a situation where  we have seen a volatile, non-navigable environment develop, where the most visible projects may be the least technically sound.", "What kinds of innovation are being held back by this current situation?", "Back et al. list six different categories, including:  * exploration of trade-offs between block size and transaction rate, and between security and cost – it would be nice to be able to make these trade-offs per transaction as transactions vary in value and risk-profile  * trading of assets other than currencies on blockchains  * enhanced privacy and censorship-resistance  privacy and censorship-resistance could be improved by use of cryptographic accumulators,  ring signatures, or Chaumian blinding  (that’s a few ‘unknown unknowns’ that just became ‘known unknowns’ for me ;).", "Perhaps something to investigate in future editions of #themorningpaper ).", "We desire a world in which interoperable altchains can be easily created and used, but without unnecessarily fragmenting markets and development….", "we argue it is possible to achieve these seemingly contradictory goals.", "The key insight is that Bitcoin the blockchain is conceptually independent of ‘bitcoin’ the asset.", "If we had technology to support the movement of assets between blockchains, new systems could be developed which users could adopt by simply reusing the existing Bitcoin currency.", "A principled manner of transferring assets between blockchains is introduced, the pegged sidechain.", "Sidechains are firewalled so that a problem in one sidechain does not affect other chains.", "…because sidechains are still blockchains independent of Bitcoin, they are free to experiment with new transaction designs, trust models, economic models, asset issuance semantics, or cryptographic features.", "and furthermore,  …these technologies can also be used in complementary currencies.", "Examples include community currencies, which are designed to preferentially boost local businesses; business barter associations, which support social programs like education or elderly care; and limited-purpose tokens which are used wihin organisations such as massive multiplayer games, loyalty programs, and online communities.", "Using pegged sidechains to manage in-game assets definitely sounds like an interesting area to me.", "The actual pegged sidechain mechanism is described in detail in the paper, which I encourage you to read (as always!).", "At the core is the idea of a DMMS:  We observe that Bitcoin’s blockheaders can be regarded as an example of a dynamic-membership multi-party signature (or DMMS), which we consider to be of independent interest as a new type of group signature.", "Unless you believe that Bitcoin as we know it today is the be-all and end-all of crypto-currencies and trustless distributed systems, then a mechanism that allows innovation to thrive on top without being held back by the necessary slow rate of change of Bitcoin itself is an important contribution.", "Related reading from previous editions of #themorningpaper:  Something a little different for today: \"Bitcoin: A peer to peer electronic cash system,\" Nakamoto.", "[url]"], "summary_text": "Enabling blockchain innovations with pegged sidechains – Back et al. 2014  A very topical choice today. Last week a number of key players in the Bitcoin ecosystem published a paper (see link above) discussing a mechanism (‘pegged sidechains’) to allowed continued innovation and evolution of Bitcoin and related blockchain-based solutions. From the abstract:  Since the introduction of Bitcoin in 2009, and the multiple computer science and electronic cash innovations it brought, there has been great interest in the potential of decentralised cryptocurrencies. At the same time,  implementation changes to the consensus-critical parts of Bitcoin must necessarily be handled very conservatively. As a result, Bitcoin has greater difficulty than other internet protocols adapting to new demands and accommodating new innovation. An early solution to the problem of innovation was the development of alternative blockchains, altchains, which modify the bitcoin codebase in some way. The deep security expertise needed to do this right has lead to a situation where  we have seen a volatile, non-navigable environment develop, where the most visible projects may be the least technically sound. What kinds of innovation are being held back by this current situation? Back et al. list six different categories, including:  * exploration of trade-offs between block size and transaction rate, and between security and cost – it would be nice to be able to make these trade-offs per transaction as transactions vary in value and risk-profile  * trading of assets other than currencies on blockchains  * enhanced privacy and censorship-resistance  privacy and censorship-resistance could be improved by use of cryptographic accumulators,  ring signatures, or Chaumian blinding  (that’s a few ‘unknown unknowns’ that just became ‘known unknowns’ for me ;). Perhaps something to investigate in future editions of #themorningpaper ). We desire a world in which interoperable altchains can be easily created and used, but without unnecessarily fragmenting markets and development…. we argue it is possible to achieve these seemingly contradictory goals. The key insight is that Bitcoin the blockchain is conceptually independent of ‘bitcoin’ the asset. If we had technology to support the movement of assets between blockchains, new systems could be developed which users could adopt by simply reusing the existing Bitcoin currency. A principled manner of transferring assets between blockchains is introduced, the pegged sidechain. Sidechains are firewalled so that a problem in one sidechain does not affect other chains. …because sidechains are still blockchains independent of Bitcoin, they are free to experiment with new transaction designs, trust models, economic models, asset issuance semantics, or cryptographic features. and furthermore,  …these technologies can also be used in complementary currencies. Examples include community currencies, which are designed to preferentially boost local businesses; business barter associations, which support social programs like education or elderly care; and limited-purpose tokens which are used wihin organisations such as massive multiplayer games, loyalty programs, and online communities. Using pegged sidechains to manage in-game assets definitely sounds like an interesting area to me. The actual pegged sidechain mechanism is described in detail in the paper, which I encourage you to read (as always!). At the core is the idea of a DMMS:  We observe that Bitcoin’s blockheaders can be regarded as an example of a dynamic-membership multi-party signature (or DMMS), which we consider to be of independent interest as a new type of group signature. Unless you believe that Bitcoin as we know it today is the be-all and end-all of crypto-currencies and trustless distributed systems, then a mechanism that allows innovation to thrive on top without being held back by the necessary slow rate of change of Bitcoin itself is an important contribution. Related reading from previous editions of #themorningpaper:  Something a little different for today: \"Bitcoin: A peer to peer electronic cash system,\" Nakamoto. [url]", "pdf_url": "http://www.blockstream.com/sidechains.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/enabling-blockchain-innovations-with-pegged-sidechains.json"}
{"id": "53269747", "bin": "700_800", "summary_sentences": ["How good are query optimizers, really?", "Leis et al., VLBD 2015  Last week we looked at cardinality estimation using index-based sampling , evaluated using the Join Order Benchmark.", "Today’s choice is the paper that introduces the Join Order Benchmark (JOB) itself.", "It’s a great evaluation paper , and along the way we’ll learn a lot about mainstream query optimisers.", "The evaluated databases are PostgresQL, HyPer, and three commercial databases named ‘DBMS A’, B, and C. We don’t know what those databases are, but if I had to make an educated guess I’d say they’re likely to be SQLServer, DB/2, and Oracle (in some order) based on a few hints scattered through the paper.", "The goal of this paper is to investigate the contribution of all relevant query optimizer components to end-to-end query performance in a realistic setting.", "The heart of the paper is an investigation into the performance of industrial-strength cardinality estimators: the authors show that cardinality estimation is the most important factor in producing good query plans.", "Cost models (that consume those estimates) are also important, but not as significant.", "Finally, and perhaps unsurprisingly, the more query plans a DBMS considers, the better the overall result.", "PostgreSQL optimiser  For background, let’s start out by looking at how the PostgreSQL query optimizer works.", "Cardinalities of base tables are estimated using histograms (quantile statistics), most common values with their frequencies, and domain cardinalities (distinct value counts).", "These per-attribute statistics are computed by the analyze command using a sample of the relation.", "Join sizes are estimated using:  |T1 ⋈x=y T2| = (|T1||T2|) / max(dom(x),dom(y))  Join orders, including bushy trees but excluding trees with cross products, are enumerated using dynamic programming.", "The cost model used to determine which plan is cheapest is comprised of over 4000 lines of C code and takes into account many subtle factors.", "At the core, it combines CPU and I/O costs with certain weights.", "“Specifically, the cost of an operator is defined as a weighted sum of the number of accessed disk pages (both sequential and random) and the amount of data processed in memory.” Setting the weights of those cost variables is a dark art.", "The Join Order Benchmark  Many research papers on query processing and optimization use standard benchmarks like TPC-H, TPC-DS, or the Star Schema Benchmark (SSB)… we argue they are not good benchmarks for the cardinality estimation component of query optimizers.", "The reason is that in order to easily be able to scale the benchmark data, the data generators are using the very same simplifying assumptions (uniformity, independence, principle of inclusion) that query optimizers make.", "To reinforce the point, take a look at the cardinality estimation errors in PostgreSQL for four representative queries from JOB vs three from TPC-H (note the log scale, and significant underestimation on ‘real data’ cardinalities):  The Join Order Benchmark is based on the Internet Movie Data Base (IMDB).", "“Like most real-world data sets IMDB is full of correlations and non-uniform data distributions, and is therefore much more challenging than most synthetic data sets.”  JOB consists of a total of 113 queries over IMDB, with between 3 and 16 joins per query and an average of 8.", "The queries all answer questions that may reasonably have been asked by a movie enthusiast.", "For cardinality estimators the queries are challenging due to the significant number of joins and the correlations contained in the data set.", "However, we did not try to “trick” the query optimizer, e.g., by picking attributes with extreme correlations.", "The JOB query set is available online at  [url]"], "summary_text": "How good are query optimizers, really? Leis et al., VLBD 2015  Last week we looked at cardinality estimation using index-based sampling , evaluated using the Join Order Benchmark. Today’s choice is the paper that introduces the Join Order Benchmark (JOB) itself. It’s a great evaluation paper , and along the way we’ll learn a lot about mainstream query optimisers. The evaluated databases are PostgresQL, HyPer, and three commercial databases named ‘DBMS A’, B, and C. We don’t know what those databases are, but if I had to make an educated guess I’d say they’re likely to be SQLServer, DB/2, and Oracle (in some order) based on a few hints scattered through the paper. The goal of this paper is to investigate the contribution of all relevant query optimizer components to end-to-end query performance in a realistic setting. The heart of the paper is an investigation into the performance of industrial-strength cardinality estimators: the authors show that cardinality estimation is the most important factor in producing good query plans. Cost models (that consume those estimates) are also important, but not as significant. Finally, and perhaps unsurprisingly, the more query plans a DBMS considers, the better the overall result. PostgreSQL optimiser  For background, let’s start out by looking at how the PostgreSQL query optimizer works. Cardinalities of base tables are estimated using histograms (quantile statistics), most common values with their frequencies, and domain cardinalities (distinct value counts). These per-attribute statistics are computed by the analyze command using a sample of the relation. Join sizes are estimated using:  |T1 ⋈x=y T2| = (|T1||T2|) / max(dom(x),dom(y))  Join orders, including bushy trees but excluding trees with cross products, are enumerated using dynamic programming. The cost model used to determine which plan is cheapest is comprised of over 4000 lines of C code and takes into account many subtle factors. At the core, it combines CPU and I/O costs with certain weights. “Specifically, the cost of an operator is defined as a weighted sum of the number of accessed disk pages (both sequential and random) and the amount of data processed in memory.” Setting the weights of those cost variables is a dark art. The Join Order Benchmark  Many research papers on query processing and optimization use standard benchmarks like TPC-H, TPC-DS, or the Star Schema Benchmark (SSB)… we argue they are not good benchmarks for the cardinality estimation component of query optimizers. The reason is that in order to easily be able to scale the benchmark data, the data generators are using the very same simplifying assumptions (uniformity, independence, principle of inclusion) that query optimizers make. To reinforce the point, take a look at the cardinality estimation errors in PostgreSQL for four representative queries from JOB vs three from TPC-H (note the log scale, and significant underestimation on ‘real data’ cardinalities):  The Join Order Benchmark is based on the Internet Movie Data Base (IMDB). “Like most real-world data sets IMDB is full of correlations and non-uniform data distributions, and is therefore much more challenging than most synthetic data sets.”  JOB consists of a total of 113 queries over IMDB, with between 3 and 16 joins per query and an average of 8. The queries all answer questions that may reasonably have been asked by a movie enthusiast. For cardinality estimators the queries are challenging due to the significant number of joins and the correlations contained in the data set. However, we did not try to “trick” the query optimizer, e.g., by picking attributes with extreme correlations. The JOB query set is available online at  [url]", "pdf_url": "http://www.vldb.org/pvldb/vol9/p204-leis.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/how-good-are-query-optimizers-really.json"}
{"id": "3953515", "bin": "700_800", "summary_sentences": ["What  They suggest a new bounding box detector.", "Their detector works without an RPN and RoI-Pooling, making it very fast (almost 60fps).", "Their detector works at multiple scales, making it better at detecting small and large objects.", "They achieve scores similar to Faster R-CNN.", "How  Architecture  Similar to Faster R-CNN, they use a base network (modified version of VGG16) to transform images to feature maps.", "They do not use an RPN.", "They predict via convolutions for each location in the feature maps:  (a) one confidence value per class (high confidence indicates that there is a bounding box of that class at the given location)  (b) x/y offsets that indicate where exactly the center of the bounding box is (e.g. a bit to the left or top of the feature map cell's center)  (c) height/width values that reflect the (logarithm of) the height/width of the bounding box  Similar to Faster R-CNN, they also use the concept of anchor boxes.", "So they generate the described values not only once per location, but several times for several anchor boxes (they use six anchor boxes).", "Each anchor box has different height/width and optionally scale.", "Visualization of the predictions and anchor boxes:  They generate these predictions not only for the final feature map, but also for various feature maps in between (e.g. before pooling layers).", "This makes it easier for the network to detect small (as well as large) bounding boxes (multi-scale detection).", "Visualization of the multi-scale architecture:  Training  Ground truth bounding boxes have to be matched with anchor boxes (at multiple scales) to determine correct outputs.", "To do this, anchor boxes and ground truth bounding boxes are matched if their jaccard overlap is 0.5 or higher.", "Any unmatched ground truth bounding box is matched to the anchor box with highest jaccard overlap.", "Note that this means that a ground truth bounding box can be assigned to multiple anchor boxes (in Faster R-CNN it is always only one).", "The loss function is similar to Faster R-CNN, i.e. a mixture of confidence loss (classification) and location loss (regression).", "They use softmax with crossentropy for the confidence loss and smooth L1 loss for the location.", "Similar to Faster R-CNN, they perform hard negative mining.", "Instead of training every anchor box at every scale they only train the ones with the highest loss (per example image).", "While doing that, they also pick the anchor boxes to be trained so that 3 in 4 boxes are negative examples (and 1 in 4 positive).", "Data Augmentation: They sample patches from images using a wide range of possible sizes and aspect ratios.", "They also horizontally flip images, perform cropping and padding and perform some photo-metric distortions.", "Non-Maximum-Suppression (NMS)  Upon inference, they remove all bounding boxes that have a confidence below 0.01.", "They then apply NMS, removing bounding boxes if there is already a similar one (measured by jaccard overlap of 0.45 or more).", "Results  Pascal VOC 2007  They achieve around 1-3 points mAP better results than Faster R-CNN.", "Despite the multi-scale method, the model's performance is still significantly worse for small objects than for large ones.", "Adding data augmentation significantly improved the results compared to no data augmentation (around 6 points mAP).", "Using more than one anchor box also had noticeable effects on the results (around 2 mAP or more).", "Using multiple feature maps to predict outputs (multi-scale architecture) significantly improves the results (around 10 mAP).", "Though adding very coarse (high-level) feature maps seems to rather hurt than help.", "Pascal VOC 2012  Around 4 mAP better results than Faster R-CNN.", "COCO  Between 1 and 4 mAP better results than Faster R-CNN.", "Times  At a batch size of 1, SSD runs at about 46 fps at input resolution 300x300 (74.3 mAP on Pascal VOC) and 19 fps at input resolution 512x512 (76.8 mAP on Pascal VOC)."], "summary_text": "What  They suggest a new bounding box detector. Their detector works without an RPN and RoI-Pooling, making it very fast (almost 60fps). Their detector works at multiple scales, making it better at detecting small and large objects. They achieve scores similar to Faster R-CNN. How  Architecture  Similar to Faster R-CNN, they use a base network (modified version of VGG16) to transform images to feature maps. They do not use an RPN. They predict via convolutions for each location in the feature maps:  (a) one confidence value per class (high confidence indicates that there is a bounding box of that class at the given location)  (b) x/y offsets that indicate where exactly the center of the bounding box is (e.g. a bit to the left or top of the feature map cell's center)  (c) height/width values that reflect the (logarithm of) the height/width of the bounding box  Similar to Faster R-CNN, they also use the concept of anchor boxes. So they generate the described values not only once per location, but several times for several anchor boxes (they use six anchor boxes). Each anchor box has different height/width and optionally scale. Visualization of the predictions and anchor boxes:  They generate these predictions not only for the final feature map, but also for various feature maps in between (e.g. before pooling layers). This makes it easier for the network to detect small (as well as large) bounding boxes (multi-scale detection). Visualization of the multi-scale architecture:  Training  Ground truth bounding boxes have to be matched with anchor boxes (at multiple scales) to determine correct outputs. To do this, anchor boxes and ground truth bounding boxes are matched if their jaccard overlap is 0.5 or higher. Any unmatched ground truth bounding box is matched to the anchor box with highest jaccard overlap. Note that this means that a ground truth bounding box can be assigned to multiple anchor boxes (in Faster R-CNN it is always only one). The loss function is similar to Faster R-CNN, i.e. a mixture of confidence loss (classification) and location loss (regression). They use softmax with crossentropy for the confidence loss and smooth L1 loss for the location. Similar to Faster R-CNN, they perform hard negative mining. Instead of training every anchor box at every scale they only train the ones with the highest loss (per example image). While doing that, they also pick the anchor boxes to be trained so that 3 in 4 boxes are negative examples (and 1 in 4 positive). Data Augmentation: They sample patches from images using a wide range of possible sizes and aspect ratios. They also horizontally flip images, perform cropping and padding and perform some photo-metric distortions. Non-Maximum-Suppression (NMS)  Upon inference, they remove all bounding boxes that have a confidence below 0.01. They then apply NMS, removing bounding boxes if there is already a similar one (measured by jaccard overlap of 0.45 or more). Results  Pascal VOC 2007  They achieve around 1-3 points mAP better results than Faster R-CNN. Despite the multi-scale method, the model's performance is still significantly worse for small objects than for large ones. Adding data augmentation significantly improved the results compared to no data augmentation (around 6 points mAP). Using more than one anchor box also had noticeable effects on the results (around 2 mAP or more). Using multiple feature maps to predict outputs (multi-scale architecture) significantly improves the results (around 10 mAP). Though adding very coarse (high-level) feature maps seems to rather hurt than help. Pascal VOC 2012  Around 4 mAP better results than Faster R-CNN. COCO  Between 1 and 4 mAP better results than Faster R-CNN. Times  At a batch size of 1, SSD runs at about 46 fps at input resolution 300x300 (74.3 mAP on Pascal VOC) and 19 fps at input resolution 512x512 (76.8 mAP on Pascal VOC).", "pdf_url": "https://arxiv.org/pdf/1512.02325", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/ssd.json"}
{"id": "61360879", "bin": "800_900", "summary_sentences": ["Finn, Christiano, Abbeel, Levine, 2016  In generative modeling, there is a trade-off between maximum-likelihood approaches that produce a moment-matching distribution that try to “cover” all the modes of the unknown data distribution as opposed to approaches that estimate the unknown data distribution by “filling in” as many modes as possible.", "The latter effectively allows one to produce more realistic samples but with lower diversity, while the former leads to a solution with probability mass in parts of the space that have negligible probability under the true distribution.", "Hence, the authors make the claim that for imitation learning and generative modeling, having both a “discriminator” as well as a “generator” encourages mode-seeking behavior.", "They show a connection between IRL and GANs by theoretically motivating an optimal IRL discriminator that learns the cost function, and an optimal IRL generator that is able to generate high-quality trajectories.", "It is assumed that one can sample from the generator density, i.e., that it is computable and can be held fixed while training the discriminator.", "The discriminator is modeled as a Boltzmann distribution with a parameterized energy function.", "Since energy-based models are a more general form of the Maximum Entropy IRL problem, the authors also were able to show a direct connection between GANs and EBMs.", "No experiments are provided in this paper, so the efficacy of using GANs for IRL remain to be seen.", "Guided Cost Learning Demo:  Notes  In Section 2.3.2.", "Guided Cost Learning, we have the following importance sampling formulation of the cost function, where the data is modeled as a Boltzmann distribution:  We want our biased distribution $q(\\tau) \\propto | \\exp(-c_{\\theta})(\\tau)) | = \\exp(-c_{\\theta}(\\tau))$.", "This is the optimal importance sampling distribution that produces the importance sampling estimate of some function of a random variable $f(X)$ with minimal variance.", "It can be shown that $q*(x) \\propto f(x) * p(x)$.", "Importance sampling estimates suffer from high variance if the sampling distribution $q$ is biased.", "In Guided Cost Learning, to ensure $q$ samples from all trajectories $\\tau$ with high values of $\\exp(-c_{\\theta}(\\tau))$, the demonstration data samples (low cost as result of IRL objective) are mixed with the generated samples from $q$.", "Hence, $q$ is replaced with $\\mu = \\frac{1}{2}p + \\frac{1}{2}q$ in the cost function.", "In Section 3, the author’s theoretical argument for comparing GANs and IRL begins by assuming that the discriminator can be written as  To see that is similar to the standard sigmoid binary classification loss, recall the identity  Let $\\log Z$ be the bias of the sigmoid and notice that $\\log q(\\tau)$ is subtracted from the input.", "The author’s argument continues by showing that this specific form of a GAN optimizes the same thing that MaximumEnt IRL does (pg.", "6).", "Questions  Being able to compute the generator’s density and evaluate it cheaply enables this method, since you can then realistically learn an unbiased estimate of the partition function.", "What happens when the partition function remains biased?", "In Guided Cost Learning, what form does $q(\\tau)$ take?", "Since it attaches a probability to a trajectory, it should have the same form as the demonstration distribution $p$…i.e., the input is a sequence of ($x_i$, $u_i$) and the output is a probability.", "The GAN training procedure minimizes the Jensen-Shannon divergence , which works sort of like the reverse-KL divergence.", "However, GANs don’t try to fit as many modes of the data distribution as the model is able to- see Section 3.2.5 of Goodfellow’s 2016 NIPS tutorial.", "In fact, this is in part a symptom of the mode collapse problem that GANs have.", "So, does training Guided Cost Learning/EBMs with GANs make them susceptible to this problem?", "The authors don’t really discuss this, but it may only become apparent in practice.", "The motivation for the mixed sampling distribution for the importance sampling formulation seems to be realted to this."], "summary_text": "Finn, Christiano, Abbeel, Levine, 2016  In generative modeling, there is a trade-off between maximum-likelihood approaches that produce a moment-matching distribution that try to “cover” all the modes of the unknown data distribution as opposed to approaches that estimate the unknown data distribution by “filling in” as many modes as possible. The latter effectively allows one to produce more realistic samples but with lower diversity, while the former leads to a solution with probability mass in parts of the space that have negligible probability under the true distribution. Hence, the authors make the claim that for imitation learning and generative modeling, having both a “discriminator” as well as a “generator” encourages mode-seeking behavior. They show a connection between IRL and GANs by theoretically motivating an optimal IRL discriminator that learns the cost function, and an optimal IRL generator that is able to generate high-quality trajectories. It is assumed that one can sample from the generator density, i.e., that it is computable and can be held fixed while training the discriminator. The discriminator is modeled as a Boltzmann distribution with a parameterized energy function. Since energy-based models are a more general form of the Maximum Entropy IRL problem, the authors also were able to show a direct connection between GANs and EBMs. No experiments are provided in this paper, so the efficacy of using GANs for IRL remain to be seen. Guided Cost Learning Demo:  Notes  In Section 2.3.2. Guided Cost Learning, we have the following importance sampling formulation of the cost function, where the data is modeled as a Boltzmann distribution:  We want our biased distribution $q(\\tau) \\propto | \\exp(-c_{\\theta})(\\tau)) | = \\exp(-c_{\\theta}(\\tau))$. This is the optimal importance sampling distribution that produces the importance sampling estimate of some function of a random variable $f(X)$ with minimal variance. It can be shown that $q*(x) \\propto f(x) * p(x)$. Importance sampling estimates suffer from high variance if the sampling distribution $q$ is biased. In Guided Cost Learning, to ensure $q$ samples from all trajectories $\\tau$ with high values of $\\exp(-c_{\\theta}(\\tau))$, the demonstration data samples (low cost as result of IRL objective) are mixed with the generated samples from $q$. Hence, $q$ is replaced with $\\mu = \\frac{1}{2}p + \\frac{1}{2}q$ in the cost function. In Section 3, the author’s theoretical argument for comparing GANs and IRL begins by assuming that the discriminator can be written as  To see that is similar to the standard sigmoid binary classification loss, recall the identity  Let $\\log Z$ be the bias of the sigmoid and notice that $\\log q(\\tau)$ is subtracted from the input. The author’s argument continues by showing that this specific form of a GAN optimizes the same thing that MaximumEnt IRL does (pg. 6). Questions  Being able to compute the generator’s density and evaluate it cheaply enables this method, since you can then realistically learn an unbiased estimate of the partition function. What happens when the partition function remains biased? In Guided Cost Learning, what form does $q(\\tau)$ take? Since it attaches a probability to a trajectory, it should have the same form as the demonstration distribution $p$…i.e., the input is a sequence of ($x_i$, $u_i$) and the output is a probability. The GAN training procedure minimizes the Jensen-Shannon divergence , which works sort of like the reverse-KL divergence. However, GANs don’t try to fit as many modes of the data distribution as the model is able to- see Section 3.2.5 of Goodfellow’s 2016 NIPS tutorial. In fact, this is in part a symptom of the mode collapse problem that GANs have. So, does training Guided Cost Learning/EBMs with GANs make them susceptible to this problem? The authors don’t really discuss this, but it may only become apparent in practice. The motivation for the mixed sampling distribution for the importance sampling formulation seems to be realted to this.", "pdf_url": "https://arxiv.org/pdf/1611.03852", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/gans-irl-ebm.json"}
{"id": "70335565", "bin": "800_900", "summary_sentences": ["Cuckoo Search via Lévy Flights – Yang et al. 2010  Another nature inspired optimisation algorithm today – and this time it’s the turn of the cuckoos coupled with the flight pattern of fruit flies (which follow a Lévy flight) A Lévy flight is a random walk in which the step-lengths follow a heavy-tailed probability distribution.", "A recent study by Reynolds and Frye shows that fruit flies or Drosophila melanogaster, explore their landscape using a series of straight flight paths punctuated by a sudden 90o turn, leading to a Lévy-flight-style intermittent scale free search pattern.", "Studies on human behaviour such as the Ju/’hoansi hunter-gatherer foraging patterns also show the typical feature of Lévy flights.", "Even light can be related to Lévy flights.", "Subsequently, such behaviour has been applied to optimization and optimal search, and preliminary results show its promising capability.", "Cuckoos lay their eggs in the nests of other birds (host birds).", "Some host birds can engage direct conflict with the intruding cuckoos.", "If a host bird discovers the eggs are not their own, they will either throw these alien eggs away or simply abandon its nest and build a new nest elsewhere.", "Some cuckoo species such as the New World brood-parasitic Tapera have evolved in such a way that female parasitic cuckoos are often very specialized in the mimicry in colour and pattern of the eggs of a few chosen host species.", "This reduces the probability of their eggs being abandoned and thus increases their reproductivity.", "In addition, the timing of egg-laying of some species is also amazing.", "Parasitic cuckoos often choose a nest where the host bird just laid its own eggs.", "In general, the cuckoo eggs hatch slightly earlier than their host eggs.", "Once the first cuckoo chick is hatched, the first instinct action it will take is to evict the host eggs by blindly propelling the eggs out of the nest, which increases the cuckoo chick’s share of food provided by its host bird.", "Studies also show that a cuckoo chick can also mimic the call of host chicks to gain access to more feeding opportunity.", "Each egg in a nest will represent a solution.", "A cuckoo egg represents a candidate new solution, and the aim is to use the new and potentially better solutions to replace not-so-good solutions in the nests.", "To keep things simple we assume that each nest has only one egg.", "When generating a new solution for a cuckoo i, a  Lévy flight is performed:  xit+1 = xit + α ⊕Lévy(λ)  …where α > 0 is the step size which should be related to the scales of the problem of interest.", "In most cases, we can use α = 1.", "The above equation is essentially the stochastic equation for random walk.", "In general, a random walk is a Markov chain whose next status/location only depends on the current location (the first term in the above equation) and the transition probability (the second term).", "The product⊕means entrywise multiplications.", "This entrywise product is similar to those used in PSO, but here the random walk via Lévy  flight is more efficient in exploring the search space as its step length is much longer in the long run.", "We start by generating an initial population of host nests, and then each iteration proceeds as follows:  Get a cuckoo randomly by Lévy flights, evaluate its fitness function  Choose a nest randomly, and if the new cuckoo performs better than the egg in the nest, replace the egg in the nest with the new cuckoo  Choose some fraction pa of the worst nests: abandon those nests and build new ones.", "Cuckoo search compares favourably against genetic algorithms and PSO (as did bats and fireflies – I wish we could get some comparison of these against each other… ).", "Simulations and comparison show that CS is superior to these existing algorithms for multimodal objective functions.", "This is partly due to the fact that there are fewer parameters to be fine-tuned in CS than in PSO and genetic algorithms.", "In fact, apart from the population size n, there is essentially one parameter pa.", "Furthermore, our simulations also indicate that the convergence rate is insensitive to the parameter pa.", "This also means that we do not have to fine tune these parameters for a specific problem.", "Subsequently, CS is more generic and robust for many optimization problems, comparing with other meta-heuristic algorithms.", "It seems to me that the fruit fly Lévy flight behaviour is more significant in this solution than the cuckoo part.", "So we could easily have called this the Fruit Fly Algorithm too."], "summary_text": "Cuckoo Search via Lévy Flights – Yang et al. 2010  Another nature inspired optimisation algorithm today – and this time it’s the turn of the cuckoos coupled with the flight pattern of fruit flies (which follow a Lévy flight) A Lévy flight is a random walk in which the step-lengths follow a heavy-tailed probability distribution. A recent study by Reynolds and Frye shows that fruit flies or Drosophila melanogaster, explore their landscape using a series of straight flight paths punctuated by a sudden 90o turn, leading to a Lévy-flight-style intermittent scale free search pattern. Studies on human behaviour such as the Ju/’hoansi hunter-gatherer foraging patterns also show the typical feature of Lévy flights. Even light can be related to Lévy flights. Subsequently, such behaviour has been applied to optimization and optimal search, and preliminary results show its promising capability. Cuckoos lay their eggs in the nests of other birds (host birds). Some host birds can engage direct conflict with the intruding cuckoos. If a host bird discovers the eggs are not their own, they will either throw these alien eggs away or simply abandon its nest and build a new nest elsewhere. Some cuckoo species such as the New World brood-parasitic Tapera have evolved in such a way that female parasitic cuckoos are often very specialized in the mimicry in colour and pattern of the eggs of a few chosen host species. This reduces the probability of their eggs being abandoned and thus increases their reproductivity. In addition, the timing of egg-laying of some species is also amazing. Parasitic cuckoos often choose a nest where the host bird just laid its own eggs. In general, the cuckoo eggs hatch slightly earlier than their host eggs. Once the first cuckoo chick is hatched, the first instinct action it will take is to evict the host eggs by blindly propelling the eggs out of the nest, which increases the cuckoo chick’s share of food provided by its host bird. Studies also show that a cuckoo chick can also mimic the call of host chicks to gain access to more feeding opportunity. Each egg in a nest will represent a solution. A cuckoo egg represents a candidate new solution, and the aim is to use the new and potentially better solutions to replace not-so-good solutions in the nests. To keep things simple we assume that each nest has only one egg. When generating a new solution for a cuckoo i, a  Lévy flight is performed:  xit+1 = xit + α ⊕Lévy(λ)  …where α > 0 is the step size which should be related to the scales of the problem of interest. In most cases, we can use α = 1. The above equation is essentially the stochastic equation for random walk. In general, a random walk is a Markov chain whose next status/location only depends on the current location (the first term in the above equation) and the transition probability (the second term). The product⊕means entrywise multiplications. This entrywise product is similar to those used in PSO, but here the random walk via Lévy  flight is more efficient in exploring the search space as its step length is much longer in the long run. We start by generating an initial population of host nests, and then each iteration proceeds as follows:  Get a cuckoo randomly by Lévy flights, evaluate its fitness function  Choose a nest randomly, and if the new cuckoo performs better than the egg in the nest, replace the egg in the nest with the new cuckoo  Choose some fraction pa of the worst nests: abandon those nests and build new ones. Cuckoo search compares favourably against genetic algorithms and PSO (as did bats and fireflies – I wish we could get some comparison of these against each other… ). Simulations and comparison show that CS is superior to these existing algorithms for multimodal objective functions. This is partly due to the fact that there are fewer parameters to be fine-tuned in CS than in PSO and genetic algorithms. In fact, apart from the population size n, there is essentially one parameter pa. Furthermore, our simulations also indicate that the convergence rate is insensitive to the parameter pa. This also means that we do not have to fine tune these parameters for a specific problem. Subsequently, CS is more generic and robust for many optimization problems, comparing with other meta-heuristic algorithms. It seems to me that the fruit fly Lévy flight behaviour is more significant in this solution than the cuckoo part. So we could easily have called this the Fruit Fly Algorithm too.", "pdf_url": "http://www.cs.tufts.edu/comp/150GA/homeworks/hw3/_reading7%20Cuckoo%20search.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/cuckoo-search-via-levy-flights.json"}
{"id": "74193008", "bin": "900_1000", "summary_sentences": ["The authors present a solution to the problem of exploring the state-action space of a continuous control task efficiently.", "Their solution stems from prior work on curiousity-driven exploration, which makes use of key ideas taken from information theory.", "The general idea is to have the agent select actions that maximize the information gain about the agent’s internal belief of the dynamics of the model.", "They use variational inference (VI) to measure information gain and Bayesian neural networks to represent the agent’s belief of the environment’s dynamics.", "The algorithm presented in Section 2.5 is referred to as VIME.", "VIME  VIME uses VI to compute the posterior probability of the parameters of the environment dynamics.", "That is, for the model $ p(s_{t+1}|s_{t}, a_{t}; \\theta) $, parameterized by the random variable $\\Theta$ with values $\\theta \\in \\Theta$, $p(\\theta | \\mathcal{D}) \\approx q(\\theta; \\phi)$.", "In this setting, $q(\\theta; \\phi)$ is represented as factorized distribution, as is common in VI.", "In particular, they use a Bayesian Neural Network parameterized by a fully factorized Gaussian distrubtion.", "Key point: The agent is encouraged to take actions that lead to states that are maximally informative about the dynamics model.", "Letting the history of the agent up to time step $t$ be $\\xi_{t} = \\{s_{0}, a_{0}, s_{1}, a_{1}, …, s_{t}\\} $, we can derive the mutual information of the dynamics model before and after taking action $a_{t}$ as:  Using the variational distribution $q$, we can approximate our posterior distribution by minimizing $ D_{KL} [q(\\theta|\\phi) \\hspace{2pt} || \\hspace{2pt} p(\\theta| \\mathcal{D})] $.", "This is done through the maximization of the variational lower bound $L[q]$:  A clear derivation of the variational lower bound is available on wikipedia .", "Note that in this case, the log evidence term $ \\log p(\\mathcal{D}) $ is computed as an expectation over the model parameters $\\theta$ w.r.t.", "the variational distribution $q$.", "The variational distribution is then used to compute a “bonus” for the external reward function as follows:  The KL-divergence between the new approximate posterior and the old approximate posterior thereby represents the information gained by having taken $a_{t}$, ended up in state $s_{t+1}$.", "The hyperparameter $\\eta$ controls the amount of incentive to explore (the curiosity).", "See Section 2.4 of the paper for an analogy drawn to model compression (the intuition is that the most informative state-action sequence up to time $t$ is the one with the shortest description length).", "Section 2.5 contains implementation details and an outline of the overall algorithm.", "One of the key points of Section 2.5 is that the use of a fully factorized Gaussian allows the KL-divergence in Eq.", "3 to be computedly simply.", "Broader Impact on the RL Community  The authors conducted experiments with VIME to show that, when augmenting an RL algorithm, there are significant improvements in the face of sparse reward signals.", "They specifically tested it with Trust-Region Policy Optimization (TRPO), REINFORCE, and ERWR.", "As a baseline, they used these algorithms sans VIME.", "They included a neat figure of the state space exploration in MountainCar of TRPO with and without VIME.", "With VIME, the exploration is much more diffused, whereas with naive Gaussian exploration it’s condensed and ball-shaped around the origin.", "In my opinion, this is one of the major avenues for further research in RL.", "We should be focused on developing strategies for learning with sparse or nonexistant reward signals.", "This is extremely important for bringing RL into new problem domains.", "As can be seen so far, information theory offers a promising starting point.", "Similar work by Mohamed and Rezende in 2015 presented an algorithm inspired by intrinsically motivated RL that focused on the notion of empowerment.", "This is a broad term that attempts to formalize the notion of having “internal drives” that agents can experience to learn about their environment in an unsupervised fashion.", "They developed a stochastic variational information maximization algorithm.", "The formulation as presented in this paper is useful when explicit external reward functions are not available to an agent.", "The computed empowerment can be used in a closed-loop planner such as Q-learning; agents can then learn information-maximizing behaviors this way.", "The paper contains some cool examples of this.", "A major distinction with VIME, however, is that empowerment doesn’t necessarily favor exploration- as stated by Mohamed and Rezende, agents are only ‘curious’ about parts of its environment that can be reached within its internal planning horizon.", "Despite the significance of the recent work in the intersection of VI and intrinsically motivated RL, these methods are non-trivial and hence will most likely catch on slower.", "Notes  Bayesian RL and PAC-MDP  Boltzmann exploration requires a training time exponential in the number of states in order to solve an n-chain MDP..!"], "summary_text": "The authors present a solution to the problem of exploring the state-action space of a continuous control task efficiently. Their solution stems from prior work on curiousity-driven exploration, which makes use of key ideas taken from information theory. The general idea is to have the agent select actions that maximize the information gain about the agent’s internal belief of the dynamics of the model. They use variational inference (VI) to measure information gain and Bayesian neural networks to represent the agent’s belief of the environment’s dynamics. The algorithm presented in Section 2.5 is referred to as VIME. VIME  VIME uses VI to compute the posterior probability of the parameters of the environment dynamics. That is, for the model $ p(s_{t+1}|s_{t}, a_{t}; \\theta) $, parameterized by the random variable $\\Theta$ with values $\\theta \\in \\Theta$, $p(\\theta | \\mathcal{D}) \\approx q(\\theta; \\phi)$. In this setting, $q(\\theta; \\phi)$ is represented as factorized distribution, as is common in VI. In particular, they use a Bayesian Neural Network parameterized by a fully factorized Gaussian distrubtion. Key point: The agent is encouraged to take actions that lead to states that are maximally informative about the dynamics model. Letting the history of the agent up to time step $t$ be $\\xi_{t} = \\{s_{0}, a_{0}, s_{1}, a_{1}, …, s_{t}\\} $, we can derive the mutual information of the dynamics model before and after taking action $a_{t}$ as:  Using the variational distribution $q$, we can approximate our posterior distribution by minimizing $ D_{KL} [q(\\theta|\\phi) \\hspace{2pt} || \\hspace{2pt} p(\\theta| \\mathcal{D})] $. This is done through the maximization of the variational lower bound $L[q]$:  A clear derivation of the variational lower bound is available on wikipedia . Note that in this case, the log evidence term $ \\log p(\\mathcal{D}) $ is computed as an expectation over the model parameters $\\theta$ w.r.t. the variational distribution $q$. The variational distribution is then used to compute a “bonus” for the external reward function as follows:  The KL-divergence between the new approximate posterior and the old approximate posterior thereby represents the information gained by having taken $a_{t}$, ended up in state $s_{t+1}$. The hyperparameter $\\eta$ controls the amount of incentive to explore (the curiosity). See Section 2.4 of the paper for an analogy drawn to model compression (the intuition is that the most informative state-action sequence up to time $t$ is the one with the shortest description length). Section 2.5 contains implementation details and an outline of the overall algorithm. One of the key points of Section 2.5 is that the use of a fully factorized Gaussian allows the KL-divergence in Eq. 3 to be computedly simply. Broader Impact on the RL Community  The authors conducted experiments with VIME to show that, when augmenting an RL algorithm, there are significant improvements in the face of sparse reward signals. They specifically tested it with Trust-Region Policy Optimization (TRPO), REINFORCE, and ERWR. As a baseline, they used these algorithms sans VIME. They included a neat figure of the state space exploration in MountainCar of TRPO with and without VIME. With VIME, the exploration is much more diffused, whereas with naive Gaussian exploration it’s condensed and ball-shaped around the origin. In my opinion, this is one of the major avenues for further research in RL. We should be focused on developing strategies for learning with sparse or nonexistant reward signals. This is extremely important for bringing RL into new problem domains. As can be seen so far, information theory offers a promising starting point. Similar work by Mohamed and Rezende in 2015 presented an algorithm inspired by intrinsically motivated RL that focused on the notion of empowerment. This is a broad term that attempts to formalize the notion of having “internal drives” that agents can experience to learn about their environment in an unsupervised fashion. They developed a stochastic variational information maximization algorithm. The formulation as presented in this paper is useful when explicit external reward functions are not available to an agent. The computed empowerment can be used in a closed-loop planner such as Q-learning; agents can then learn information-maximizing behaviors this way. The paper contains some cool examples of this. A major distinction with VIME, however, is that empowerment doesn’t necessarily favor exploration- as stated by Mohamed and Rezende, agents are only ‘curious’ about parts of its environment that can be reached within its internal planning horizon. Despite the significance of the recent work in the intersection of VI and intrinsically motivated RL, these methods are non-trivial and hence will most likely catch on slower. Notes  Bayesian RL and PAC-MDP  Boltzmann exploration requires a training time exponential in the number of states in order to solve an n-chain MDP..!", "pdf_url": "http://arxiv.org/pdf/1605.09674v2.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/vime.json"}
{"id": "82043898", "bin": "900_1000", "summary_sentences": ["Detecting Discontinuities in Large-Scale Systems – Malik et al 2014.", "The 7th IEEE/ACM International Conference on Utility and Cloud Computing is coming to London in a couple of weeks time.", "Many of the papers don’t seem to be online yet, but here’s one that is.", "Malik et al. tackle the problem of long-term forecasting for infrastructure provisioning, and in particular identifying discontinuities in performance data so that models are trained on the most relevant data.", "One of the fundamental problems faced by analysts in preparing data for use in forecasting is the timely identification of data discontinuities.", "A discontinuity is an abrupt change in a time-series pattern of a performance counter that persists but does not recur.", "Analysts need to identify discontinuities in performance data so that they can a) remove the discontinuities from the data before building a forecast model and b) retrain an existing forecast model on the performance data from the point in time where a discontinuity occurred.", "We’re also treated to a good overview of the forecasting process in general.", "Practitioners and data scientists spend considerable time (e.g. up to 80%) in preparing data for their forecast algorithms!", "Where does all this time go?", "The accuracy of forecasting results depends on the quality of the performance data (i.e., performance counters; such as CPU utilization, bandwidth consumption, network traffic and Disk IOPS) fed to the forecasting algorithms, i.e., missing value imputation, calculating and adjusting times stamp drifts of logged performance data across hundreds of VMs, identification and removal of outliers and anomalies and in cases, scaling and standardizing the data to remove bias among performance counters.", "One of the fundamental problems faced by analysts in preparing data for long-term forecast is the identification and removal of data discontinuities.", "Discontinuities, like anomalies, are abrupt changes in time-series patterns.", "Unlike anomalies, which are temporary, discontinuities persist.", "They do not appear instantaneousy, but over a brief period called a transition period.", "detecting a discontinuity provides analysts a reference point to retrain their forecasting models and make necessary adjustments.", "After cleaning the logs (e.g. dealing with missing or empty counter values), Principle Component Analysis is used to “select the least correlated subset of performance counters that can still explain the maximum variations in the data.” The performance data needs to be normalized for PCA to work well…  To eliminate PCA bias towards those variables with a larger variance, we standardized the performance counters via Unit Variance scaling, i.e., by dividing the observations of each counter variable by the variable’s standard deviation.", "Scaled performance counter data are then further mean centered to reduce the risk of collinearity.", "PCA was chosen due to its “superior performance in identifying performance counters that are sensitive to minute changes in both workload and environment as compared to many other supervised and unsupervised machine learning techniques.”  The determined principle component performance counters are then fed into an anomaly detector.", "This phase works by finding changes in the data that cannot be easily represented (approximated) by a quadratic function:  When working with training data, we discover (potential) discontinuities by presuming that discontinuities cannot be well modelled by a low order polynomial function.", "Given a performance counter time series data {v[t]}, we approximate the series by the quadratic function _f(t) = c+bt+at² that performance counter time series data {v[t]}, we approximate the series by the quadratic function f(t) = c+bt+at² that minimizes the least squared error (LSE).", "We presume that series containing sudden dramatic changes, anomalies, or discontinuities will not be fit as well by this approximation and so have a larger LSE.", "From this set of discovered anomalies, discontinuities are then identified by looking at the distribution of the performance counter in question before and after the anomaly transition period.", "For discontinuities, the change will persist, whereas for ordinary anamolies it will not.", "The Wilcoxon rank-sum test is used for this comparison.", "A disappointment in the paper (for this reader) is that much of the testing was done based on deliberately injecting anomalies and discontinuities into existing data.", "As a side-effect though, that means we are treated to a discussion on the most common causes of anomalies and discontinuities IRL:  Causes of temporary anomalies:  80% of the performance anomalies in large software systems are due to software inconsistencies and human errors  the most common anomaly occuring in the field is related to transient memory issues (memory spikes)  large enterprises report that periodic CPU saturation is one of the fundamental field problems  interfering workloads are a major cause of performance degradation in data centers (resulting from competition for resources).", "Causes of discontinuities:  Increase in workload due to promotions, new products, mergers and acquisitions  Change in transaction patterns, where a ‘transaction’ in this context is a sequence of events.", "Most likely caused by a new version of the software deployed in the data center.", "Upgrades to infrastructure hardware or software  Finally, the approach was back-tested against 7 years worth of real production logs for which analysts had already identified the discontinuities.", "Precision and recall were both at 92% with the optimum algorithm settings for this data set."], "summary_text": "Detecting Discontinuities in Large-Scale Systems – Malik et al 2014. The 7th IEEE/ACM International Conference on Utility and Cloud Computing is coming to London in a couple of weeks time. Many of the papers don’t seem to be online yet, but here’s one that is. Malik et al. tackle the problem of long-term forecasting for infrastructure provisioning, and in particular identifying discontinuities in performance data so that models are trained on the most relevant data. One of the fundamental problems faced by analysts in preparing data for use in forecasting is the timely identification of data discontinuities. A discontinuity is an abrupt change in a time-series pattern of a performance counter that persists but does not recur. Analysts need to identify discontinuities in performance data so that they can a) remove the discontinuities from the data before building a forecast model and b) retrain an existing forecast model on the performance data from the point in time where a discontinuity occurred. We’re also treated to a good overview of the forecasting process in general. Practitioners and data scientists spend considerable time (e.g. up to 80%) in preparing data for their forecast algorithms! Where does all this time go? The accuracy of forecasting results depends on the quality of the performance data (i.e., performance counters; such as CPU utilization, bandwidth consumption, network traffic and Disk IOPS) fed to the forecasting algorithms, i.e., missing value imputation, calculating and adjusting times stamp drifts of logged performance data across hundreds of VMs, identification and removal of outliers and anomalies and in cases, scaling and standardizing the data to remove bias among performance counters. One of the fundamental problems faced by analysts in preparing data for long-term forecast is the identification and removal of data discontinuities. Discontinuities, like anomalies, are abrupt changes in time-series patterns. Unlike anomalies, which are temporary, discontinuities persist. They do not appear instantaneousy, but over a brief period called a transition period. detecting a discontinuity provides analysts a reference point to retrain their forecasting models and make necessary adjustments. After cleaning the logs (e.g. dealing with missing or empty counter values), Principle Component Analysis is used to “select the least correlated subset of performance counters that can still explain the maximum variations in the data.” The performance data needs to be normalized for PCA to work well…  To eliminate PCA bias towards those variables with a larger variance, we standardized the performance counters via Unit Variance scaling, i.e., by dividing the observations of each counter variable by the variable’s standard deviation. Scaled performance counter data are then further mean centered to reduce the risk of collinearity. PCA was chosen due to its “superior performance in identifying performance counters that are sensitive to minute changes in both workload and environment as compared to many other supervised and unsupervised machine learning techniques.”  The determined principle component performance counters are then fed into an anomaly detector. This phase works by finding changes in the data that cannot be easily represented (approximated) by a quadratic function:  When working with training data, we discover (potential) discontinuities by presuming that discontinuities cannot be well modelled by a low order polynomial function. Given a performance counter time series data {v[t]}, we approximate the series by the quadratic function _f(t) = c+bt+at² that performance counter time series data {v[t]}, we approximate the series by the quadratic function f(t) = c+bt+at² that minimizes the least squared error (LSE). We presume that series containing sudden dramatic changes, anomalies, or discontinuities will not be fit as well by this approximation and so have a larger LSE. From this set of discovered anomalies, discontinuities are then identified by looking at the distribution of the performance counter in question before and after the anomaly transition period. For discontinuities, the change will persist, whereas for ordinary anamolies it will not. The Wilcoxon rank-sum test is used for this comparison. A disappointment in the paper (for this reader) is that much of the testing was done based on deliberately injecting anomalies and discontinuities into existing data. As a side-effect though, that means we are treated to a discussion on the most common causes of anomalies and discontinuities IRL:  Causes of temporary anomalies:  80% of the performance anomalies in large software systems are due to software inconsistencies and human errors  the most common anomaly occuring in the field is related to transient memory issues (memory spikes)  large enterprises report that periodic CPU saturation is one of the fundamental field problems  interfering workloads are a major cause of performance degradation in data centers (resulting from competition for resources). Causes of discontinuities:  Increase in workload due to promotions, new products, mergers and acquisitions  Change in transaction patterns, where a ‘transaction’ in this context is a sequence of events. Most likely caused by a new version of the software deployed in the data center. Upgrades to infrastructure hardware or software  Finally, the approach was back-tested against 7 years worth of real production logs for which analysts had already identified the discontinuities. Precision and recall were both at 92% with the optimum algorithm settings for this data set.", "pdf_url": "http://129.97.186.80/~migod/papers/2014/ucc14.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/detecting-discontinuities-in-large-scale-systems.json"}
