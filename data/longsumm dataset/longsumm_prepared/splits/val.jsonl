{"id": "18000122", "bin": "0_100", "summary_sentences": ["This paper demonstrates that Word2Vec  [ref]  can extract relationships between words and produce latent representations useful for medical data.", "They explore this model on different datasets which yield different relationships between words.", "[url]"], "summary_text": "This paper demonstrates that Word2Vec  [ref]  can extract relationships between words and produce latent representations useful for medical data. They explore this model on different datasets which yield different relationships between words. [url]", "pdf_url": "http://arxiv.org/pdf/1802.00400v2", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/1802.00400.json"}
{"id": "81623492", "bin": "0_100", "summary_sentences": ["\"The SE module can learn some nonlinear global interactions already known to be useful, such as spatial normalization.", "The channel wise weights make it somewhat more powerful than divisive normalization as it can learn feature-specific inhibitions (ie: if we see a lot of flower parts, the probability of boat features should be diminished).", "It also has some similarity to bio inhibitory circuits.\"", "By jcannell on reddit  Slides:  [url]"], "summary_text": "\"The SE module can learn some nonlinear global interactions already known to be useful, such as spatial normalization. The channel wise weights make it somewhat more powerful than divisive normalization as it can learn feature-specific inhibitions (ie: if we see a lot of flower parts, the probability of boat features should be diminished). It also has some similarity to bio inhibitory circuits.\" By jcannell on reddit  Slides:  [url]", "pdf_url": "http://arxiv.org/pdf/1709.01507v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/1709.01507.json"}
{"id": "1050101", "bin": "0_100", "summary_sentences": ["A *Batch Normalization* applied immediately after fully connected layers and adjusts the values of the feedforward output so that they are centered to a zero mean and have unit variance.", "It has been used by famous Convolutional Neural Networks such as GoogLeNet  [ref]  and ResNet  [ref]"], "summary_text": "A *Batch Normalization* applied immediately after fully connected layers and adjusts the values of the feedforward output so that they are centered to a zero mean and have unit variance. It has been used by famous Convolutional Neural Networks such as GoogLeNet  [ref]  and ResNet  [ref]", "pdf_url": "http://proceedings.mlr.press/v37/ioffe15.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/0_100/ioffes15.json"}
{"id": "64175585", "bin": "1000_1100", "summary_sentences": ["AI2: Training a big data machine to defend Veeramachaneni et al. IEEE International conference on Big Data Security, 2016  Will machines take over?", "The lesson of today’s paper is that we’re better off together.", "Combining AI with HI (human intelligence, I felt like we deserved an acronym of our own ;) ) yields much better results than a system that uses only unsupervised learning.", "The context is information security, scanning millions of log entries per day to detect suspicious activity and prevent attacks.", "Examples of attacks include account takeovers, new account fraud (opening a new account using stolen credit card information), and terms of service abuse (e.g. abusing promotional codes, or manipulating cookies for advantage).", "A typical attack has a behavioral signature, which comprises the series of steps involved in commiting it.", "The information necessary to quantify these signatures is buried deep in the raw data, and is often delivered as logs.", "The usual problem with such outlier/anomaly detection systems is that they trigger lots of false positive alarms, that take substantial time and effort to investigate.", "After the system has ‘cried wolf’ enough times they can become distrusted and of limited use.", "AI2 combines the experience and intuition of analysts with machine learning techniques.", "An ensemble of unsupervised learning models generates a set of k events to be analysed per day (where the daily budget k of events that can be analysed is a configurable parameter).", "The human judgements on these k events are used to train a supervised model, the results of which are combined with the unsupervised ensemble results to refine the k events to be presented to the analyst on the next day.", "And so it goes on.", "The end result looks a bit like this:  With a daily investigation budget (k) of 200 events, AI2 detects 86.8% of attacks with a false positive rate of 4.4%.", "Using only unsupervised learning, on 7.9% of attacks are detected.", "If the investigation budget is upped to 1000 events/day, unsupervised learning can detect 73.7% of attacks with a false positive rate of 22%.", "At this level, the unsupervised system is generating 5x the false positives of AI2, and still not detecting as many attacks.", "Detecting attacks is a true ‘needle-in-a-haystack’ problem as the following table shows:  Entities in the above refers to the number of unique IP addresses, users, sessions etc.", "anaysed on a daily basis.", "The very small relative number of true attacks results in extreme class imbalance when trying to learn a supervised model.", "AI2 tracks activity based on ingested log records and aggregates activities over intervals of time (for example,counters, indicators – did this happen in the window at all?", "– elapsed time between events, number of unique values and so on).", "These features are passed into an ensemble of three unsupervised outlier detection models:  A Principle Component Analysis (PCA) based model.", "The basic idea is to use PCA to determine the most significant features (those that explain most of the variance in the data).", "Given an input take its PCA projection, and then from the projection, reconstruct the original variables.", "The reconstruction error will be small for the majority of examples, but will remain high for outliers.", "A Replicator Neural Network (not to be confused with a Recurrent Neural Network – both get abbreviated to RNN).", "This works on a very similar principal.", "The input and output layers have the same number of nodes, and intermediate layers have fewer nodes.", "The goal is to train the network to recreate the input at the output layer – which means it must learn an efficient compressed representation in the lower-dimensional hidden layers.", "Once the RNN has been trained, the reconstruction error can be used as the outlier score.", "The third unsupervised model uses copula functions to build a joint probability function that can be used to detect rare events.", "A copula framework provides a means of inference after modeling a multivariate joint probability distribution from training data.", "Because copula frameworks are less well known than other forms of estimation, we will now briefly review copula theory…  (If you’re interested in that review, and how copula functions are used to form a multivariate density function, see section 6.3 in the paper).", "The scores from each of the models are translated into probabilities using a Weibull distribution, “which is flexible and can model a wide variety of shapes.” This translation means that we can compare like-with-like when combining the results from the three models.", "Here’s an example of the combination process using one-day’s worth of data:  The whole AI2 system cycles through training, deployment, and feedback collection/model updating phases on a daily basis.", "The system trains unsupervised and supervised models based on all the available data, applies those models to the incoming data, identifies k entities as extreme events or attacks, and brings these to the analyst’s attention.", "The analysts deductions are used to build a new predictive model for the next day.", "This combined approach makes effective use of the limited available analyst bandwidth, can overcome some of the weaknesses of pure unsupervised learning, and actively adapts and synthesizes new models.", "This setup captures the cascading effect of the human-machine interaction: the more attacks the predictive system detects, the more feeback it will receive from the analysts; this feedback, in turn, will improve the accuracy of future predictions."], "summary_text": "AI2: Training a big data machine to defend Veeramachaneni et al. IEEE International conference on Big Data Security, 2016  Will machines take over? The lesson of today’s paper is that we’re better off together. Combining AI with HI (human intelligence, I felt like we deserved an acronym of our own ;) ) yields much better results than a system that uses only unsupervised learning. The context is information security, scanning millions of log entries per day to detect suspicious activity and prevent attacks. Examples of attacks include account takeovers, new account fraud (opening a new account using stolen credit card information), and terms of service abuse (e.g. abusing promotional codes, or manipulating cookies for advantage). A typical attack has a behavioral signature, which comprises the series of steps involved in commiting it. The information necessary to quantify these signatures is buried deep in the raw data, and is often delivered as logs. The usual problem with such outlier/anomaly detection systems is that they trigger lots of false positive alarms, that take substantial time and effort to investigate. After the system has ‘cried wolf’ enough times they can become distrusted and of limited use. AI2 combines the experience and intuition of analysts with machine learning techniques. An ensemble of unsupervised learning models generates a set of k events to be analysed per day (where the daily budget k of events that can be analysed is a configurable parameter). The human judgements on these k events are used to train a supervised model, the results of which are combined with the unsupervised ensemble results to refine the k events to be presented to the analyst on the next day. And so it goes on. The end result looks a bit like this:  With a daily investigation budget (k) of 200 events, AI2 detects 86.8% of attacks with a false positive rate of 4.4%. Using only unsupervised learning, on 7.9% of attacks are detected. If the investigation budget is upped to 1000 events/day, unsupervised learning can detect 73.7% of attacks with a false positive rate of 22%. At this level, the unsupervised system is generating 5x the false positives of AI2, and still not detecting as many attacks. Detecting attacks is a true ‘needle-in-a-haystack’ problem as the following table shows:  Entities in the above refers to the number of unique IP addresses, users, sessions etc. anaysed on a daily basis. The very small relative number of true attacks results in extreme class imbalance when trying to learn a supervised model. AI2 tracks activity based on ingested log records and aggregates activities over intervals of time (for example,counters, indicators – did this happen in the window at all? – elapsed time between events, number of unique values and so on). These features are passed into an ensemble of three unsupervised outlier detection models:  A Principle Component Analysis (PCA) based model. The basic idea is to use PCA to determine the most significant features (those that explain most of the variance in the data). Given an input take its PCA projection, and then from the projection, reconstruct the original variables. The reconstruction error will be small for the majority of examples, but will remain high for outliers. A Replicator Neural Network (not to be confused with a Recurrent Neural Network – both get abbreviated to RNN). This works on a very similar principal. The input and output layers have the same number of nodes, and intermediate layers have fewer nodes. The goal is to train the network to recreate the input at the output layer – which means it must learn an efficient compressed representation in the lower-dimensional hidden layers. Once the RNN has been trained, the reconstruction error can be used as the outlier score. The third unsupervised model uses copula functions to build a joint probability function that can be used to detect rare events. A copula framework provides a means of inference after modeling a multivariate joint probability distribution from training data. Because copula frameworks are less well known than other forms of estimation, we will now briefly review copula theory…  (If you’re interested in that review, and how copula functions are used to form a multivariate density function, see section 6.3 in the paper). The scores from each of the models are translated into probabilities using a Weibull distribution, “which is flexible and can model a wide variety of shapes.” This translation means that we can compare like-with-like when combining the results from the three models. Here’s an example of the combination process using one-day’s worth of data:  The whole AI2 system cycles through training, deployment, and feedback collection/model updating phases on a daily basis. The system trains unsupervised and supervised models based on all the available data, applies those models to the incoming data, identifies k entities as extreme events or attacks, and brings these to the analyst’s attention. The analysts deductions are used to build a new predictive model for the next day. This combined approach makes effective use of the limited available analyst bandwidth, can overcome some of the weaknesses of pure unsupervised learning, and actively adapts and synthesizes new models. This setup captures the cascading effect of the human-machine interaction: the more attacks the predictive system detects, the more feeback it will receive from the analysts; this feedback, in turn, will improve the accuracy of future predictions.", "pdf_url": "https://people.csail.mit.edu/kalyan/AI2_Paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/ai2-training-a-big-data-machine-to-defend.json"}
{"id": "80770343", "bin": "1000_1100", "summary_sentences": ["What  The original GAN approach used one Generator (G) to generate images and one Discriminator (D) to rate these images.", "The laplacian pyramid GAN uses multiple pairs of G and D.  It starts with an ordinary GAN that generates small images (say, 4x4).", "Each following pair learns to generate plausible upscalings of the image, usually by a factor of 2.", "(So e.g. from 4x4 to 8x8.)", "This scaling from coarse to fine resembles a laplacian pyramid, hence the name.", "How  The first pair of G and D is just like an ordinary GAN.", "For each pair afterwards, G recieves the output of the previous step, upscaled to the desired size.", "Due to the upscaling, the image will be blurry.", "G has to learn to generate a plausible sharpening of that blurry image.", "G outputs a difference image, not the full sharpened image.", "D recieves the upscaled/blurry image.", "D also recieves either the optimal difference image (for images from the training set) or G's generated difference image.", "D adds the difference image to the blurry image as its first step.", "Afterwards it applies convolutions to the image and ends in one sigmoid unit.", "The training procedure is just like in the ordinary GAN setting.", "Each upscaling pair of G and D can be trained on its own.", "The first G recieves a \"normal\" noise vector, just like in the ordinary GAN setting.", "Later Gs recieve noise as one plane, so each image has four channels: R, G, B, noise.", "Results  Images are rated as looking more realistic than the ones from ordinary GANs.", "The approximated log likelihood is significantly lower (improved) compared to ordinary GANs.", "The generated images do however still look distorted compared to real images.", "They also tried to add class conditional information to G and D (just a one hot vector for the desired class of the image).", "G and D learned successfully to adapt to that information (e.g. to only generate images that seem to show birds).", "Basic training and sampling process.", "The first image is generated directly from noise.", "Everything afterwards is de-blurring of upscaled images.", "Rough chapter-wise notes  Introduction  Instead of just one big generative model, they build multiple ones.", "They start with one model at a small image scale (e.g. 4x4) and then add multiple generative models that increase the image size (e.g.", "from 4x4 to 8x8).", "This scaling from coarse to fine (low frequency to high frequency components) resembles a laplacian pyramid, hence the name of the paper.", "Related Works  Types of generative image models:  Non-Parametric: Models copy patches from training set (e.g. texture synthesis, super-resolution)  Parametric: E.g.", "Deep Boltzmann machines or denoising auto-encoders  Novel approaches: e.g. DRAW, diffusion-based processes, LSTMs  This work is based on (conditional) GANs  Approach  They start with a Gaussian and a Laplacian pyramid.", "They build the Gaussian pyramid by repeatedly decreasing the image height/width by 2: [full size image, half size image, quarter size image, ...]  They build a Laplacian pyramid by taking pairs of images in the gaussian pyramid, upscaling the smaller one and then taking the difference.", "In the laplacian GAN approach, an image at scale k is created by first upscaling the image at scale k-1 and then adding a refinement to it (de-blurring).", "The refinement is created with a GAN that recieves the upscaled image as input.", "Note that the refinement is a difference image (between the upscaled image and the optimal upscaled image).", "The very first (small scale) image is generated by an ordinary GAN.", "D recieves an upscaled image and a difference image.", "It then adds them together to create an upscaled and de-blurred image.", "Then D applies ordinary convolutions to the result and ends in a quality rating (sigmoid).", "Model Architecture and Training  Datasets: CIFAR-10 (32x32, 100k images), STL (96x96, 100k), LSUN (64x64, 10M)  They use a uniform distribution of [-1, 1] for their noise vectors.", "For the upscaling Generators they add the noise as a fourth plane (to the RGB image).", "CIFAR-10: 8->14->28 (height/width), STL: 8->16->32->64->96, LSUN: 4->8->16->32->64  CIFAR-10: G=3 layers, D=2 layers, STL: G=3 layers, D=2 layers, LSUN: G=5 layers, D=3 layers.", "Experiments  Evaluation methods:  Computation of log-likelihood on a held out image set  They use a Gaussian window based Parzen estimation to approximate the probability of an image (note: not very accurate).", "They adapt their estimation method to the special case of the laplacian pyramid.", "Their laplacian pyramid model seems to perform significantly better than ordinary GANs.", "Subjective evaluation of generated images  Their model seems to learn the rough structure and color correlations of images to generate.", "They add class conditional information to G and D. G indeed learns to generate different classes of images.", "All images still have noticeable distortions.", "Subjective evaluation of generated images by other people  15 volunteers.", "They show generated or real images in an interface for 50-2000ms.", "Volunteer then has to decide whether the image is fake or real.", "10k ratings were collected.", "At 2000ms, around 50% of the generated images were considered real, ~90 of the true real ones and <10% of the images generated by an ordinary GAN."], "summary_text": "What  The original GAN approach used one Generator (G) to generate images and one Discriminator (D) to rate these images. The laplacian pyramid GAN uses multiple pairs of G and D.  It starts with an ordinary GAN that generates small images (say, 4x4). Each following pair learns to generate plausible upscalings of the image, usually by a factor of 2. (So e.g. from 4x4 to 8x8.) This scaling from coarse to fine resembles a laplacian pyramid, hence the name. How  The first pair of G and D is just like an ordinary GAN. For each pair afterwards, G recieves the output of the previous step, upscaled to the desired size. Due to the upscaling, the image will be blurry. G has to learn to generate a plausible sharpening of that blurry image. G outputs a difference image, not the full sharpened image. D recieves the upscaled/blurry image. D also recieves either the optimal difference image (for images from the training set) or G's generated difference image. D adds the difference image to the blurry image as its first step. Afterwards it applies convolutions to the image and ends in one sigmoid unit. The training procedure is just like in the ordinary GAN setting. Each upscaling pair of G and D can be trained on its own. The first G recieves a \"normal\" noise vector, just like in the ordinary GAN setting. Later Gs recieve noise as one plane, so each image has four channels: R, G, B, noise. Results  Images are rated as looking more realistic than the ones from ordinary GANs. The approximated log likelihood is significantly lower (improved) compared to ordinary GANs. The generated images do however still look distorted compared to real images. They also tried to add class conditional information to G and D (just a one hot vector for the desired class of the image). G and D learned successfully to adapt to that information (e.g. to only generate images that seem to show birds). Basic training and sampling process. The first image is generated directly from noise. Everything afterwards is de-blurring of upscaled images. Rough chapter-wise notes  Introduction  Instead of just one big generative model, they build multiple ones. They start with one model at a small image scale (e.g. 4x4) and then add multiple generative models that increase the image size (e.g. from 4x4 to 8x8). This scaling from coarse to fine (low frequency to high frequency components) resembles a laplacian pyramid, hence the name of the paper. Related Works  Types of generative image models:  Non-Parametric: Models copy patches from training set (e.g. texture synthesis, super-resolution)  Parametric: E.g. Deep Boltzmann machines or denoising auto-encoders  Novel approaches: e.g. DRAW, diffusion-based processes, LSTMs  This work is based on (conditional) GANs  Approach  They start with a Gaussian and a Laplacian pyramid. They build the Gaussian pyramid by repeatedly decreasing the image height/width by 2: [full size image, half size image, quarter size image, ...]  They build a Laplacian pyramid by taking pairs of images in the gaussian pyramid, upscaling the smaller one and then taking the difference. In the laplacian GAN approach, an image at scale k is created by first upscaling the image at scale k-1 and then adding a refinement to it (de-blurring). The refinement is created with a GAN that recieves the upscaled image as input. Note that the refinement is a difference image (between the upscaled image and the optimal upscaled image). The very first (small scale) image is generated by an ordinary GAN. D recieves an upscaled image and a difference image. It then adds them together to create an upscaled and de-blurred image. Then D applies ordinary convolutions to the result and ends in a quality rating (sigmoid). Model Architecture and Training  Datasets: CIFAR-10 (32x32, 100k images), STL (96x96, 100k), LSUN (64x64, 10M)  They use a uniform distribution of [-1, 1] for their noise vectors. For the upscaling Generators they add the noise as a fourth plane (to the RGB image). CIFAR-10: 8->14->28 (height/width), STL: 8->16->32->64->96, LSUN: 4->8->16->32->64  CIFAR-10: G=3 layers, D=2 layers, STL: G=3 layers, D=2 layers, LSUN: G=5 layers, D=3 layers. Experiments  Evaluation methods:  Computation of log-likelihood on a held out image set  They use a Gaussian window based Parzen estimation to approximate the probability of an image (note: not very accurate). They adapt their estimation method to the special case of the laplacian pyramid. Their laplacian pyramid model seems to perform significantly better than ordinary GANs. Subjective evaluation of generated images  Their model seems to learn the rough structure and color correlations of images to generate. They add class conditional information to G and D. G indeed learns to generate different classes of images. All images still have noticeable distortions. Subjective evaluation of generated images by other people  15 volunteers. They show generated or real images in an interface for 50-2000ms. Volunteer then has to decide whether the image is fake or real. 10k ratings were collected. At 2000ms, around 50% of the generated images were considered real, ~90 of the true real ones and <10% of the images generated by an ordinary GAN.", "pdf_url": "http://arxiv.org/pdf/1506.05751", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1000_1100/deep_generative_image_models_using_a_laplacian_pyramid_of_adversarial_networks.json"}
{"id": "80004741", "bin": "100_200", "summary_sentences": ["In this work they take a different approach to the GAN model  [ref] .", "In the traditionally GAN model a neural network is trained to up-sample from random noise in a feed forward fashion to generate samples from the data distribution.", "This work instead iteratively permutes an image of random noise similar to Artistic Style Transfer  [ref] .", "The image is permuted in order to fool a set of discriminators.", "To obtain the set of discriminators each is trained starting from random noise until some max $t$ step.", "1.", "At first a discriminator is trained to discriminate between the true data and random noise .", "2.", "Images is then permuted using gradients which aim to fool the discriminator and included in the data distribution as a negative example.", "3.", "The discriminator is trained on the true data + random noise + fake data from the previous steps  The images generated at each step are shown below:   [url]"], "summary_text": "In this work they take a different approach to the GAN model  [ref] . In the traditionally GAN model a neural network is trained to up-sample from random noise in a feed forward fashion to generate samples from the data distribution. This work instead iteratively permutes an image of random noise similar to Artistic Style Transfer  [ref] . The image is permuted in order to fool a set of discriminators. To obtain the set of discriminators each is trained starting from random noise until some max $t$ step. 1. At first a discriminator is trained to discriminate between the true data and random noise . 2. Images is then permuted using gradients which aim to fool the discriminator and included in the data distribution as a negative example. 3. The discriminator is trained on the true data + random noise + fake data from the previous steps  The images generated at each step are shown below:   [url]", "pdf_url": "http://arxiv.org/pdf/1704.07820v1", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/1704.07820.json"}
{"id": "97592875", "bin": "100_200", "summary_sentences": ["What is stopping us from applying meta-learning to new tasks?", "Where do the tasks come from?", "Designing task distribution is laborious.", "We should automatically learn tasks!", "Unsupervised Learning via Meta-Learning: The idea is to use a distance metric in an out-of-the-box unsupervised embedding space created by BiGAN/ALI or DeepCluster to construct tasks in an unsupervised way.", "If you cluster points to randomly define classes (e.g. random k-means) you can then sample tasks of 2 or 3 classes and use them to train a model.", "Where does the extra information come from?", "The metric space used for k-means asserts specific distances.", "The intuition why this works is that it is useful model initialization for downstream tasks.", "This summary was written with the help of Chelsea Finn."], "summary_text": "What is stopping us from applying meta-learning to new tasks? Where do the tasks come from? Designing task distribution is laborious. We should automatically learn tasks! Unsupervised Learning via Meta-Learning: The idea is to use a distance metric in an out-of-the-box unsupervised embedding space created by BiGAN/ALI or DeepCluster to construct tasks in an unsupervised way. If you cluster points to randomly define classes (e.g. random k-means) you can then sample tasks of 2 or 3 classes and use them to train a model. Where does the extra information come from? The metric space used for k-means asserts specific distances. The intuition why this works is that it is useful model initialization for downstream tasks. This summary was written with the help of Chelsea Finn.", "pdf_url": "http://arxiv.org/pdf/1810.02334v2", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/100_200/1810.02334.json"}
{"id": "22744554", "bin": "1100_1200", "summary_sentences": ["Knowledge Bases (KBs) are effective tools for Question Answering (QA) but are often too restrictive (due to fixed schema) and too sparse (due to limitations of Information Extraction (IE) systems).", "The paper proposes Key-Value Memory Networks, a neural network architecture based on Memory Networks that can leverage both KBs and raw data for QA.", "The paper also introduces MOVIEQA, a new QA dataset that can be answered by a perfect KB, by Wikipedia pages and by an imperfect KB obtained using IE techniques thereby allowing a comparison between systems using any of the three sources.", ".", "Related Work  TRECQA and WIKIQA are two benchmarks where systems need to select the sentence containing the correct answer, given a question and a list of candidate sentences.", "These datasets are small and make it difficult to compare the  systems using different sources.", "Best results on these benchmarks are reported by CNNs and RNNs with attention mechanism.", "Key-Value Memory Networks  Extension of Memory Networks Model .", "Generalises the way context is stored in memory.", "Comprises of a memory made of slots in the form of pair of vectors (k1, v1)...(km, vm) to encode long-term and short-term context.", "Reading the Memory  Key Hashing - Question, x is used to preselect a subset of array (kh1, vh1)...(khN, vhN) where the key shares atleast one word with x and frequency of the words is less than 1000.", "Key Addresing - Each candidate memory is assigned a relevance probability:  phi = softmax(AφX(x).AφK(khi))  φ is a feature map of dimension D and A is a dxD matrix.", "Value Reading - Value of memories are read by taking their weighted sum using addressing probabilites and a vector o is returned.", "o = sum(phiAφV(vhi))  Memory access process conducted by \"controller\" neural network using q = AφX(x) as the query.", "Query is updated using  q2 = R1(q+o)  Addressing and reading steps are repeated using new Ri matrices to retrive more pertinent information in subsequent access.", "After a fixed number of hops, H, resulting state of controller is used to compute a final prediction.", "a = argmax(softmax(qH+1TBφY(yi))) where yi are the possible candidate outputs and B is a dXD matrix.", "The network is trained end-to-end using a cross entropy loss, backpropogation and stochastic gradient.", "End-to-End Memory Networks can be viewed as a special case of Key-Value Memory Networks by setting key and value to be the same for all the memories.", "Variants of Key-Value Memories  φx and φy - feature map corresponding to query and answer are fixed as bag-of-words representation.", "KB Triple  Triplets of the form \"subject relation object\" can be represented in Key-Value Memory Networks with subject and relation as the key and object as the value.", "In standard Memory Networks, the whole triplet would have to be embedded in the same memory slot.", "The reversed relations \"object is_related_to subject\" can also be stored.", "Sentence Level  A document can be split into sentences with each sentence encoded in the key-value pair of the memory slot as a bag-of-words.", "Window Level  Split the document in the windows of W words and represent it as bag-of-words.", "The window becomes the key and the central word becomes the value.", "Window + Centre Encoding  Instead of mixing the window centre with the rest of the words, double the size of the dictionary and encode the centre of the window and the value using the second dictionary.", "Window + Title  Since title of the document could contain useful information, the word window can be encoded as the key and document title as the value.", "The key could be augmented with features like \"window\" and \"title\" to distinguish between different cases.", "MOVIEQA Benchmark  Knowledge Representation  Doc - Raw documents (from Wikipedia) related to movies.", "KB - Graph based KB made of entities and relations.", "IE - Performing Information Extraction on Wikipedia to create a KB.", "The QA pairs should be answerable by both raw document and KB so that the three approaches can be compared and the gap between the three solutions can be closed.", "The dataset has more than 100000 QA pairs, making it much larger than most existing datasets.", "Experiments  MOVIEQA  Systems Compared  Bordes et al's QA system  Supervised Embeddings(without KB)  Memory Networks  Key-Value Memory Networks  Observations  Key-Value Memory Networks outperforms all methods on all data sources.", "KB > Doc > IE  The best memory representation for directly reading documents uses \"Window Level + Centre Encoding + Title\".", "KB vs Synthetic Document Analysis  Given KB triplets, construct synthetic \"Wikipedia\" articles using templates, conjuctions and coreferences to determine the causes for gap in performance when using KB vs doc.", "Loss in One Template sentences are due to difficulty of extracting subject, relation and object from the artifical docs.", "Using multiple templates does not detoriate performance much.", "But conjuctions and coreferences cause a dip in performance.", "WIKIQA  Given a question, select the sentence (from Wikipedia document) that best answers the question.", "Key-Value Memory Networks outperforms all other solutions though it is only marginally better than LDC (Sentence Similarity Learning by Lexical Decomposition and Composition) and attentive models based on CNNs and RNNs.", "This comment has been minimized.", "Sign in to view  Copy link  Quote reply  ylytju commented  Mar 7, 2018  In the sentence level, what do play the role of the key and value?", "In this case, what's the difference between the Key-value MM and the traditional MM?", "This comment has been minimized.", "Sign in to view  Copy link  Quote reply  Owner Author  shagunsodhani commented  Mar 8, 2018  In some cases, where your key is same as your value, there is no difference between key-value and traditional memory networks.", "Now let us say that in the sentence case, the key and memory both are the same bag of words, then there is no difference.", "But now you could have the key different from the memory."], "summary_text": "Knowledge Bases (KBs) are effective tools for Question Answering (QA) but are often too restrictive (due to fixed schema) and too sparse (due to limitations of Information Extraction (IE) systems). The paper proposes Key-Value Memory Networks, a neural network architecture based on Memory Networks that can leverage both KBs and raw data for QA. The paper also introduces MOVIEQA, a new QA dataset that can be answered by a perfect KB, by Wikipedia pages and by an imperfect KB obtained using IE techniques thereby allowing a comparison between systems using any of the three sources. . Related Work  TRECQA and WIKIQA are two benchmarks where systems need to select the sentence containing the correct answer, given a question and a list of candidate sentences. These datasets are small and make it difficult to compare the  systems using different sources. Best results on these benchmarks are reported by CNNs and RNNs with attention mechanism. Key-Value Memory Networks  Extension of Memory Networks Model . Generalises the way context is stored in memory. Comprises of a memory made of slots in the form of pair of vectors (k1, v1)...(km, vm) to encode long-term and short-term context. Reading the Memory  Key Hashing - Question, x is used to preselect a subset of array (kh1, vh1)...(khN, vhN) where the key shares atleast one word with x and frequency of the words is less than 1000. Key Addresing - Each candidate memory is assigned a relevance probability:  phi = softmax(AφX(x).AφK(khi))  φ is a feature map of dimension D and A is a dxD matrix. Value Reading - Value of memories are read by taking their weighted sum using addressing probabilites and a vector o is returned. o = sum(phiAφV(vhi))  Memory access process conducted by \"controller\" neural network using q = AφX(x) as the query. Query is updated using  q2 = R1(q+o)  Addressing and reading steps are repeated using new Ri matrices to retrive more pertinent information in subsequent access. After a fixed number of hops, H, resulting state of controller is used to compute a final prediction. a = argmax(softmax(qH+1TBφY(yi))) where yi are the possible candidate outputs and B is a dXD matrix. The network is trained end-to-end using a cross entropy loss, backpropogation and stochastic gradient. End-to-End Memory Networks can be viewed as a special case of Key-Value Memory Networks by setting key and value to be the same for all the memories. Variants of Key-Value Memories  φx and φy - feature map corresponding to query and answer are fixed as bag-of-words representation. KB Triple  Triplets of the form \"subject relation object\" can be represented in Key-Value Memory Networks with subject and relation as the key and object as the value. In standard Memory Networks, the whole triplet would have to be embedded in the same memory slot. The reversed relations \"object is_related_to subject\" can also be stored. Sentence Level  A document can be split into sentences with each sentence encoded in the key-value pair of the memory slot as a bag-of-words. Window Level  Split the document in the windows of W words and represent it as bag-of-words. The window becomes the key and the central word becomes the value. Window + Centre Encoding  Instead of mixing the window centre with the rest of the words, double the size of the dictionary and encode the centre of the window and the value using the second dictionary. Window + Title  Since title of the document could contain useful information, the word window can be encoded as the key and document title as the value. The key could be augmented with features like \"window\" and \"title\" to distinguish between different cases. MOVIEQA Benchmark  Knowledge Representation  Doc - Raw documents (from Wikipedia) related to movies. KB - Graph based KB made of entities and relations. IE - Performing Information Extraction on Wikipedia to create a KB. The QA pairs should be answerable by both raw document and KB so that the three approaches can be compared and the gap between the three solutions can be closed. The dataset has more than 100000 QA pairs, making it much larger than most existing datasets. Experiments  MOVIEQA  Systems Compared  Bordes et al's QA system  Supervised Embeddings(without KB)  Memory Networks  Key-Value Memory Networks  Observations  Key-Value Memory Networks outperforms all methods on all data sources. KB > Doc > IE  The best memory representation for directly reading documents uses \"Window Level + Centre Encoding + Title\". KB vs Synthetic Document Analysis  Given KB triplets, construct synthetic \"Wikipedia\" articles using templates, conjuctions and coreferences to determine the causes for gap in performance when using KB vs doc. Loss in One Template sentences are due to difficulty of extracting subject, relation and object from the artifical docs. Using multiple templates does not detoriate performance much. But conjuctions and coreferences cause a dip in performance. WIKIQA  Given a question, select the sentence (from Wikipedia document) that best answers the question. Key-Value Memory Networks outperforms all other solutions though it is only marginally better than LDC (Sentence Similarity Learning by Lexical Decomposition and Composition) and attentive models based on CNNs and RNNs. This comment has been minimized. Sign in to view  Copy link  Quote reply  ylytju commented  Mar 7, 2018  In the sentence level, what do play the role of the key and value? In this case, what's the difference between the Key-value MM and the traditional MM? This comment has been minimized. Sign in to view  Copy link  Quote reply  Owner Author  shagunsodhani commented  Mar 8, 2018  In some cases, where your key is same as your value, there is no difference between key-value and traditional memory networks. Now let us say that in the sentence case, the key and memory both are the same bag of words, then there is no difference. But now you could have the key different from the memory.", "pdf_url": "https://arxiv.org/pdf/1606.03126", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/e0baa075b4a917c0a69edc575772a8.json"}
{"id": "37010180", "bin": "1100_1200", "summary_sentences": ["See also  Video explanation by authors  What  They describe a method that can be used for two problems:  (1) Choose a style image and apply that style to other images.", "(2) Choose an example texture image and create new texture images that look similar.", "In contrast to previous methods their method can be applied very fast to images (style transfer) or noise (texture creation).", "However, per style/texture a single (expensive) initial training session is still necessary.", "Their method builds upon their previous paper \" Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis \".", "How  Rough overview of their previous method:  Transfer styles using three losses:  Content loss: MSE between VGG representations.", "Regularization loss: Sum of x-gradient and y-gradients (encouraging smooth areas).", "MRF-based style loss: Sample k x k patches from VGG representations of content image and style image.", "For each patch from content image find the nearest neighbor (based on normalized cross correlation) from style patches.", "Loss is then the sum of squared errors of euclidean distances between content patches and their nearest neighbors.", "Generation of new images is done by starting with noise and then iteratively applying changes that minimize the loss function.", "They introduce mostly two major changes:  (a) Get rid of the costly nearest neighbor search for the MRF loss.", "Instead, use a discriminator-network that receives a patch and rates how real that patch looks.", "This discriminator-network is costly to train, but that only has to be done once (per style/texture).", "(b) Get rid of the slow, iterative generation of images.", "Instead, start with the content image (style transfer) or noise image (texture generation) and feed that through a single generator-network to create the output image (with transfered style or generated texture).", "This generator-network is costly to train, but that only has to be done once (per style/texture).", "MDANs  They implement change (a) to the standard architecture and call that an \"MDAN\" (Markovian Deconvolutional Adversarial Networks).", "So the architecture of the MDAN is:  Input: Image (RGB pixels)  Branch 1: Markovian Patch Quality Rater (aka Discriminator)  Starts by feeding the image through VGG19 until layer relu3_1.", "(Note: VGG weights are fixed/not trained.)", "Then extracts k x k patches from the generated representations.", "Feeds each patch through a shallow ConvNet (convolution with BN then fully connected layer).", "Training loss is a hinge loss, i.e. max margin between classes +1 (real looking patch) and -1 (fake looking patch).", "(Could also take a single sigmoid output, but they argue that hinge loss isn't as likely to saturate.)", "This branch will be trained continuously while synthesizing a new image.", "Branch 2: Content Estimation/Guidance  Note: This branch is only used for style transfer, i.e if using an content image and not for texture generation.", "Starts by feeding the currently synthesized image through VGG19 until layer relu5_1.", "(Note: VGG weights are fixed/not trained.)", "Also feeds the content image through VGG19 until layer relu5_1.", "Then uses a MSE loss between both representations (so similar to a MSE on RGB pixels that is often used in autoencoders).", "Nothing in this branch needs to trained, the loss only affects the synthesizing of the image.", "MGANs  The MGAN is like the MDAN, but additionally implements change (b), i.e. they add a generator that takes an image and stylizes it.", "The generator's architecture is:  Input: Image (RGB pixels) or noise (for texture synthesis)  Output: Image (RGB pixels) (stylized input image or generated texture)  The generator takes the image (pixels) and feeds that through VGG19 until layer relu4_1.", "Similar to the DCGAN generator, they then apply a few fractionally strided convolutions (with BN and LeakyReLUs) to that, ending in a Tanh output.", "(Fractionally strided convolutions increase the height/width of the images, here to compensate the VGG pooling layers.)", "The output after the Tanh is the output image (RGB pixels).", "They train the generator with pairs of (input image, stylized image or texture).", "These pairs can be gathered by first running the MDAN alone on several images.", "(With significant augmentation a few dozen pairs already seem to be enough.)", "One of two possible loss functions can then be used:  Simple standard choice: MSE on the euclidean distance between expected output pixels and generated output pixels.", "Can cause blurriness.", "Better choice: MSE on a higher VGG representation.", "Simply feed the generated output pixels through VGG19 until relu4_1 and the reuse the already generated (see above) VGG-representation of the input image.", "This is very similar to the pixel-wise comparison, but tends to cause less blurriness.", "Note: For some reason the authors call their generator a VAE, but don't mention any typical VAE technique, so it's not described like one here.", "They use Adam to train their networks.", "For texture generation they use Perlin Noise instead of simple white noise.", "In Perlin Noise, lower frequency components dominate more than higher frequency components.", "White noise didn't work well with the VGG representations in the generator (activations were close to zero).", "Results  Similar quality like previous methods, but much faster (compared to most methods).", "For the Markovian Patch Quality Rater (MDAN branch 1):  They found that the weights of this branch can be used as initialization for other training sessions (e.g. other texture styles), leading to a decrease in required iterations/epochs.", "Using VGG for feature extraction seems to be crucial.", "Training from scratch generated in worse results.", "Using larger patch sizes preserves more structure of the structure of the style image/texture.", "Smaller patches leads to more flexibility in generated patterns.", "They found that using more than 3 convolutional layers or more than 64 filters per layer provided no visible benefit in quality.", "Result of their method, compared to other methods.", "Architecture of their model."], "summary_text": "See also  Video explanation by authors  What  They describe a method that can be used for two problems:  (1) Choose a style image and apply that style to other images. (2) Choose an example texture image and create new texture images that look similar. In contrast to previous methods their method can be applied very fast to images (style transfer) or noise (texture creation). However, per style/texture a single (expensive) initial training session is still necessary. Their method builds upon their previous paper \" Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis \". How  Rough overview of their previous method:  Transfer styles using three losses:  Content loss: MSE between VGG representations. Regularization loss: Sum of x-gradient and y-gradients (encouraging smooth areas). MRF-based style loss: Sample k x k patches from VGG representations of content image and style image. For each patch from content image find the nearest neighbor (based on normalized cross correlation) from style patches. Loss is then the sum of squared errors of euclidean distances between content patches and their nearest neighbors. Generation of new images is done by starting with noise and then iteratively applying changes that minimize the loss function. They introduce mostly two major changes:  (a) Get rid of the costly nearest neighbor search for the MRF loss. Instead, use a discriminator-network that receives a patch and rates how real that patch looks. This discriminator-network is costly to train, but that only has to be done once (per style/texture). (b) Get rid of the slow, iterative generation of images. Instead, start with the content image (style transfer) or noise image (texture generation) and feed that through a single generator-network to create the output image (with transfered style or generated texture). This generator-network is costly to train, but that only has to be done once (per style/texture). MDANs  They implement change (a) to the standard architecture and call that an \"MDAN\" (Markovian Deconvolutional Adversarial Networks). So the architecture of the MDAN is:  Input: Image (RGB pixels)  Branch 1: Markovian Patch Quality Rater (aka Discriminator)  Starts by feeding the image through VGG19 until layer relu3_1. (Note: VGG weights are fixed/not trained.) Then extracts k x k patches from the generated representations. Feeds each patch through a shallow ConvNet (convolution with BN then fully connected layer). Training loss is a hinge loss, i.e. max margin between classes +1 (real looking patch) and -1 (fake looking patch). (Could also take a single sigmoid output, but they argue that hinge loss isn't as likely to saturate.) This branch will be trained continuously while synthesizing a new image. Branch 2: Content Estimation/Guidance  Note: This branch is only used for style transfer, i.e if using an content image and not for texture generation. Starts by feeding the currently synthesized image through VGG19 until layer relu5_1. (Note: VGG weights are fixed/not trained.) Also feeds the content image through VGG19 until layer relu5_1. Then uses a MSE loss between both representations (so similar to a MSE on RGB pixels that is often used in autoencoders). Nothing in this branch needs to trained, the loss only affects the synthesizing of the image. MGANs  The MGAN is like the MDAN, but additionally implements change (b), i.e. they add a generator that takes an image and stylizes it. The generator's architecture is:  Input: Image (RGB pixels) or noise (for texture synthesis)  Output: Image (RGB pixels) (stylized input image or generated texture)  The generator takes the image (pixels) and feeds that through VGG19 until layer relu4_1. Similar to the DCGAN generator, they then apply a few fractionally strided convolutions (with BN and LeakyReLUs) to that, ending in a Tanh output. (Fractionally strided convolutions increase the height/width of the images, here to compensate the VGG pooling layers.) The output after the Tanh is the output image (RGB pixels). They train the generator with pairs of (input image, stylized image or texture). These pairs can be gathered by first running the MDAN alone on several images. (With significant augmentation a few dozen pairs already seem to be enough.) One of two possible loss functions can then be used:  Simple standard choice: MSE on the euclidean distance between expected output pixels and generated output pixels. Can cause blurriness. Better choice: MSE on a higher VGG representation. Simply feed the generated output pixels through VGG19 until relu4_1 and the reuse the already generated (see above) VGG-representation of the input image. This is very similar to the pixel-wise comparison, but tends to cause less blurriness. Note: For some reason the authors call their generator a VAE, but don't mention any typical VAE technique, so it's not described like one here. They use Adam to train their networks. For texture generation they use Perlin Noise instead of simple white noise. In Perlin Noise, lower frequency components dominate more than higher frequency components. White noise didn't work well with the VGG representations in the generator (activations were close to zero). Results  Similar quality like previous methods, but much faster (compared to most methods). For the Markovian Patch Quality Rater (MDAN branch 1):  They found that the weights of this branch can be used as initialization for other training sessions (e.g. other texture styles), leading to a decrease in required iterations/epochs. Using VGG for feature extraction seems to be crucial. Training from scratch generated in worse results. Using larger patch sizes preserves more structure of the structure of the style image/texture. Smaller patches leads to more flexibility in generated patterns. They found that using more than 3 convolutional layers or more than 64 filters per layer provided no visible benefit in quality. Result of their method, compared to other methods. Architecture of their model.", "pdf_url": "http://arxiv.org/pdf/1604.04382", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1100_1200/markovian_gans.json"}
{"id": "21050319", "bin": "1200_1300", "summary_sentences": ["The Design Philosophy of the DARPA Internet Protocols – Clark 1988  While there have been papers and specifications that describe how the (internet) protocols work, it is sometimes difficult to deduce from these why the protocol is as it is.", "For example, the Internet protocol is based on a connectionless or datagram mode of service.", "The motivation for this has been greatly misunderstood.", "This paper attempts to capture some of the early reasoning which shaped the Internet protocols.", "Understanding the underlying principles behind something can turn what might on the surface seem to be simply a collection of facts into a chain of causes and consequences that makes it much easier to see how those parts fit together.", "Clark provides us with some of those insights for the design of the Internet Protocols, working from the goals towards the implementation consequences.", "The top level goal for the DARPA Internet Architecture was to develop an effective technique for multiplexed utilization of existing interconnected networks.", "This implied integrating networks spanning different administrative boundaries of control.", "And since the intial networks to be connected used packet switching, packet switching was adopted as a fundamental component of the internet architecture.", "From the ARPANET project the notion of store-and-forward packet switching for interconnects was well understood.", "From these assumptions comes the fundamental structure of the Internet: a packet switched communications facility in which a number of distinguishable networks are connected together using packet communications processors called gateways which implement a store and forward packet forwarding algorithm.", "The top level goal says ‘what’ is to be achieved, but says very little about the desired characteristics of a system that accomplishes it.", "There were seven second-level goals, which are presented below in priority order.", "Internet communication must continue despite loss of networks or gateways.", "The Internet must support multiple types of communications service.", "The Internet architecture must accommodate a variety of networks.", "The Internet architecture must permit distributed management of its resources.", "The Internet architecture must be cost effective.", "The Internet architecture must permit host attachment with a low level of effort.", "The resources used in the internet architecture must be accountable.", "These goals are in order of importance, and an entirely different network architecture would result if the order were changed.", "For example, since this network was designed to operate in a military context, which implied the possibility of a hostile environment, survivability was put as a first goal, and accountability as a last goal.", "It turns out that the top three goals on the list had the most influence on the resulting design.", "See the full paper (link at the top) for reflections on the remaining four.", "Surviving in the face of failure  If two entities are communicating over the Internet, and some failure causes the Internet to be temporarily disrupted and reconfigured to reconstitute the service, then the entities communicating should be able to continue without having to reestablish or reset the high level state of their conversation.", "The only error the communicating parties should ever see is the case of total partition.", "If the application(s) on either end of the connection are not required to resolve any other failures, then the state necessary for recovery must be held in the lower layers – but where?", "One option is to put it in the intermediate nodes in the network, and of course to protect it from loss it must be replicated.", "I think the knee-jerk reaction of many system designers today might be to distribute the state in some such manner, maybe using a gossip-protocol.", "But the original designers of the internet had a insight which enabled a much simpler solution, and they called it ‘fate sharing.’  The alternative, which this architecture chose, is to take this information and gather it at the endpoint of the net, at the entity which is utilizing the service of the network.", "I call this approach to reliability “fate-sharing.” The fate-sharing model suggests that it is acceptable to lose the state information associated with an entity if, at the same time, the entity itself is lost.", "Specifically, information about transport level synchronization is stored in the host which is attached to the net and using its communication service.", "Two consequences of this are that the intermediate nodes must not store any (essential) state – leading to a datagram (stateless packet switching) based design, and that the host becomes an important trusted part of the overall solution.", "Handling multiple types of communications service  Debugging protocols and VOIP were the first two use cases that suggested something more than just TCP might be needed.", "You most want the debugger to work precisely when things are going wrong – so a model that says it first requires a fully reliable transport is not a good one!", "It’s much better to make do with whatever you can get.", "When it comes to VOIP, regular delivery of packets (even if it means losing some) is more important than reliability for a good user experience.", "A surprising observation about the control of variation in delay is that the most serious source of delay in networks is the mechanism to provide reliable delivery!", "It was thus decided… to split TCP and IP into two layers.", "TCP provided one particular type of service, the reliable sequenced data stream, while IP attempted to provide a basic building block out of which a variety of types of service could be built… The User Datagram Protocol (UDP) was created to provide a application-level interface to the basic datagram service of Internet.", "Accommodating a variety of networks.", "The easiest way to accommodate a wide variety of networks, is to make the requirements for integrating a network as simple as possible.", "This boils down to:  being able to transport a packet or datagram of reasonable size (e.g. 100 bytes),  reasonable but not reliable delivery, and some form of addressing.", "There are a number of services which are explicitly not assumed from the network.", "These include reliable or sequenced delivery, network level broadcast or multicast, priority ranking of transmitted packet, support for multiple types of service, and internal knowledge of failures, speeds, or delays.", "On datagrams  There is a mistaken assumption often associated with datagrams, which is that the motivation for datagrams is the support of a higher level service which is essentially equivalent to the datagram.", "In other words, it has sometimes been suggested that the datagram is provided because the transport service which the application requires is a datagram service.", "In fact, this is seldom the case.", "The importance of datagrams instead stems from:  Eliminating the need for connection state in intermediate nodes  Providing a building block on top of which a variety of services can be built  Representing the minimum network server assumption, enabling a wide variety of networks to be easily incorporated."], "summary_text": "The Design Philosophy of the DARPA Internet Protocols – Clark 1988  While there have been papers and specifications that describe how the (internet) protocols work, it is sometimes difficult to deduce from these why the protocol is as it is. For example, the Internet protocol is based on a connectionless or datagram mode of service. The motivation for this has been greatly misunderstood. This paper attempts to capture some of the early reasoning which shaped the Internet protocols. Understanding the underlying principles behind something can turn what might on the surface seem to be simply a collection of facts into a chain of causes and consequences that makes it much easier to see how those parts fit together. Clark provides us with some of those insights for the design of the Internet Protocols, working from the goals towards the implementation consequences. The top level goal for the DARPA Internet Architecture was to develop an effective technique for multiplexed utilization of existing interconnected networks. This implied integrating networks spanning different administrative boundaries of control. And since the intial networks to be connected used packet switching, packet switching was adopted as a fundamental component of the internet architecture. From the ARPANET project the notion of store-and-forward packet switching for interconnects was well understood. From these assumptions comes the fundamental structure of the Internet: a packet switched communications facility in which a number of distinguishable networks are connected together using packet communications processors called gateways which implement a store and forward packet forwarding algorithm. The top level goal says ‘what’ is to be achieved, but says very little about the desired characteristics of a system that accomplishes it. There were seven second-level goals, which are presented below in priority order. Internet communication must continue despite loss of networks or gateways. The Internet must support multiple types of communications service. The Internet architecture must accommodate a variety of networks. The Internet architecture must permit distributed management of its resources. The Internet architecture must be cost effective. The Internet architecture must permit host attachment with a low level of effort. The resources used in the internet architecture must be accountable. These goals are in order of importance, and an entirely different network architecture would result if the order were changed. For example, since this network was designed to operate in a military context, which implied the possibility of a hostile environment, survivability was put as a first goal, and accountability as a last goal. It turns out that the top three goals on the list had the most influence on the resulting design. See the full paper (link at the top) for reflections on the remaining four. Surviving in the face of failure  If two entities are communicating over the Internet, and some failure causes the Internet to be temporarily disrupted and reconfigured to reconstitute the service, then the entities communicating should be able to continue without having to reestablish or reset the high level state of their conversation. The only error the communicating parties should ever see is the case of total partition. If the application(s) on either end of the connection are not required to resolve any other failures, then the state necessary for recovery must be held in the lower layers – but where? One option is to put it in the intermediate nodes in the network, and of course to protect it from loss it must be replicated. I think the knee-jerk reaction of many system designers today might be to distribute the state in some such manner, maybe using a gossip-protocol. But the original designers of the internet had a insight which enabled a much simpler solution, and they called it ‘fate sharing.’  The alternative, which this architecture chose, is to take this information and gather it at the endpoint of the net, at the entity which is utilizing the service of the network. I call this approach to reliability “fate-sharing.” The fate-sharing model suggests that it is acceptable to lose the state information associated with an entity if, at the same time, the entity itself is lost. Specifically, information about transport level synchronization is stored in the host which is attached to the net and using its communication service. Two consequences of this are that the intermediate nodes must not store any (essential) state – leading to a datagram (stateless packet switching) based design, and that the host becomes an important trusted part of the overall solution. Handling multiple types of communications service  Debugging protocols and VOIP were the first two use cases that suggested something more than just TCP might be needed. You most want the debugger to work precisely when things are going wrong – so a model that says it first requires a fully reliable transport is not a good one! It’s much better to make do with whatever you can get. When it comes to VOIP, regular delivery of packets (even if it means losing some) is more important than reliability for a good user experience. A surprising observation about the control of variation in delay is that the most serious source of delay in networks is the mechanism to provide reliable delivery! It was thus decided… to split TCP and IP into two layers. TCP provided one particular type of service, the reliable sequenced data stream, while IP attempted to provide a basic building block out of which a variety of types of service could be built… The User Datagram Protocol (UDP) was created to provide a application-level interface to the basic datagram service of Internet. Accommodating a variety of networks. The easiest way to accommodate a wide variety of networks, is to make the requirements for integrating a network as simple as possible. This boils down to:  being able to transport a packet or datagram of reasonable size (e.g. 100 bytes),  reasonable but not reliable delivery, and some form of addressing. There are a number of services which are explicitly not assumed from the network. These include reliable or sequenced delivery, network level broadcast or multicast, priority ranking of transmitted packet, support for multiple types of service, and internal knowledge of failures, speeds, or delays. On datagrams  There is a mistaken assumption often associated with datagrams, which is that the motivation for datagrams is the support of a higher level service which is essentially equivalent to the datagram. In other words, it has sometimes been suggested that the datagram is provided because the transport service which the application requires is a datagram service. In fact, this is seldom the case. The importance of datagrams instead stems from:  Eliminating the need for connection state in intermediate nodes  Providing a building block on top of which a variety of services can be built  Representing the minimum network server assumption, enabling a wide variety of networks to be easily incorporated.", "pdf_url": "http://ccr.sigcomm.org/archive/1995/jan95/ccr-9501-clark.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/the-design-philosophy-of-the-darpa-internet-protocols.json"}
{"id": "11773091", "bin": "1200_1300", "summary_sentences": ["Musketeer: all for one, one for all in data processing systems – Gog et al. 2015  For between 40-80% of the jobs submitted to MapReduce systems, you’d be better off just running them on a single machine…  It was Eurosys 2015 last week, and a great new crop of papers were presented.", "Gog et al. from the Cambridge Systems at Scale ( CamSaS ) initiative published today’s choice, ‘Musketeer’.", "In fact, it’s going to be tomorrow’s choice as well since there’s more good material here than I can do justice to in one write-up.", "Today I want to focus on the motivation section from the Musketeer paper, which sheds a lot of light on the question “what’s the best big data processing system?” Tomorrow we’ll look at Musketeer itself.", "What’s the best big data processing system?", "“For what?” should probably be your first question here.", "No-one system is universally best.", "The Musketeers conducted a very interesting study that looked into this:  We evaluated a range of contemporary data processing systems – Hadoop, Spark, Naiad, PowerGraph, Metis and GraphChi – under controlled and comparable conditions.", "We found that (i) their performance varies widely depending on the high-level workflow; (ii) no single system always out-performs all others; and (iii) almost every system performs best under some circumstances  It’s common sense that each system must have a design point, and therefore you should expect it to work best with workloads close to that design point.", "But it’s easy to lose sight of this in religious wars over the ‘best’ data processing system – which can often take place without any context.", "Let’s assume you do have a particular workload in mind, so that we can ask the much better question: “what’s the best data processing system for this workload?”.", "Even then…  Choosing the “right” parallel data processing system is difficult.", "It requires significant expert knowledge about the programming paradigm, design goals and implementation of the many available systems.", "Tomorrow we’ll see how Musketeer can help make this choice for you, and even retarget your workflow to the back-end for which it is best suited.", "Even if you don’t use Musketeer though, the analysis from section 2 of the paper is of interest.", "Gog et al. examined makespan – the entire time to execute a workflow including not only the computation itself, but also any data loading, pre-processing, and output materialization.", "From this, they determine four key factors that influence system performance:  The size of the input data.", "Single machine frameworks outperform distributed ones for smaller inputs.", "The structure of the data.", "Skew and selectivity impact I/O performance and work distribution.", "Engineering decisions made during the constructing of the data processing system itself.", "For example, how efficiently it can load data.", "The computation type, since specialized systems operate more efficiently.", "In all systems they studied, the ultimate source and sink of data is files in HDFS.", "Do you really need that fancy distributed framework?", "You might recall the story from last year of awk and grep on the command-line of a single machine outperforming a Hadoop cluster by a factor of 235 .", "Gog et al. studied the effect on input size on framework performance.", "Take a look at their figure 2a, below:  For small inputs (≤0.5GB), the Metis single-machine MapReduce system performs best.", "This matters, as small inputs are common in practice: 40– 80% of Cloudera customers’ MapReduce jobs and 70% of jobs in a Facebook trace have ≤ 1GB of input.", "This last point bears repeating, and if I can generalize slightly: for between 40-80% of the jobs submitted to MapReduce systems, you’d be better off just running them on a single machine.", "Likewise a join workflow producing 1.9GB of data runs best on a single machine.", "A larger join producing 29GB works best on Hadoop.", "See figure 2b below.", "Do you really need that new shiny thing?", "Well, maybe!", "But likewise there is no universal guarantee that e.g. Spark is better than Hadoop MR.", "It depends on what you’re trying to do…  Once the data size grows, Hive, Spark and Hadoop all surpass the single-machine Metis, not least since they can stream data from and to HDFS in parallel.", "However, since there is no data re-use in this workflow, Spark performs worse than Hadoop: it loads all data into a distributed in-memory RDD before performing the projection.", "What are you optimising for?", "For workflows involving iterative computations on graphs, it won’t surprise you to learn that specialized graph processing systems do well.", "It is evident that graph-oriented paradigms have significant advantages for this computation: a GraphLINQ implementation running on Naiad outperforms all other systems.", "PowerGraph also performs very well, since its vertex-centric sharding reduces the communication overhead that dominates PageRank… However, the fastest system is not always the most efficient.", "Look at figure 3a below.", "With smaller graphs the 100 node clusters may be the fastest, but you’re getting nowhere near a 100x speed-up for all that investment (we’re on the RHS of the Universal Scalability Law curve).", "If you prepared to wait just a little longer for results, you can get your answer with dramatically less compute power (also compare e.g. PowerGraph on 16 nodes with GraphChi on one).", "Yes, but what’s the best data processing system?", "Our experiments show that the “best” system for a given workflow varies considerably.", "The right choice – i.e., the fastest or most efficient system – depends on the workflow, the input data size and the scale of parallelism available.", "If you think a little carefully about what you’re trying to achieve – when you really need fully precise results vs. good approximations ; when you really need to run on a distributed framework vs. a single machine; when you really need results quickly vs. waiting a little bit longer but being much more efficient – you can significantly improve the overall effectiveness of your data platform.", "Tomorrow we’ll see how Musketeer can help to make all this more practical and manageable by enabling workflows to be written once and mapped to many systems – even combining systems within a workflow.", "Postscript  The authors did something very neat with this paper – in the pdf version, every figure is actually a link to a webpage describing the experiments and data sets behind it.", "Really great idea, thanks!"], "summary_text": "Musketeer: all for one, one for all in data processing systems – Gog et al. 2015  For between 40-80% of the jobs submitted to MapReduce systems, you’d be better off just running them on a single machine…  It was Eurosys 2015 last week, and a great new crop of papers were presented. Gog et al. from the Cambridge Systems at Scale ( CamSaS ) initiative published today’s choice, ‘Musketeer’. In fact, it’s going to be tomorrow’s choice as well since there’s more good material here than I can do justice to in one write-up. Today I want to focus on the motivation section from the Musketeer paper, which sheds a lot of light on the question “what’s the best big data processing system?” Tomorrow we’ll look at Musketeer itself. What’s the best big data processing system? “For what?” should probably be your first question here. No-one system is universally best. The Musketeers conducted a very interesting study that looked into this:  We evaluated a range of contemporary data processing systems – Hadoop, Spark, Naiad, PowerGraph, Metis and GraphChi – under controlled and comparable conditions. We found that (i) their performance varies widely depending on the high-level workflow; (ii) no single system always out-performs all others; and (iii) almost every system performs best under some circumstances  It’s common sense that each system must have a design point, and therefore you should expect it to work best with workloads close to that design point. But it’s easy to lose sight of this in religious wars over the ‘best’ data processing system – which can often take place without any context. Let’s assume you do have a particular workload in mind, so that we can ask the much better question: “what’s the best data processing system for this workload?”. Even then…  Choosing the “right” parallel data processing system is difficult. It requires significant expert knowledge about the programming paradigm, design goals and implementation of the many available systems. Tomorrow we’ll see how Musketeer can help make this choice for you, and even retarget your workflow to the back-end for which it is best suited. Even if you don’t use Musketeer though, the analysis from section 2 of the paper is of interest. Gog et al. examined makespan – the entire time to execute a workflow including not only the computation itself, but also any data loading, pre-processing, and output materialization. From this, they determine four key factors that influence system performance:  The size of the input data. Single machine frameworks outperform distributed ones for smaller inputs. The structure of the data. Skew and selectivity impact I/O performance and work distribution. Engineering decisions made during the constructing of the data processing system itself. For example, how efficiently it can load data. The computation type, since specialized systems operate more efficiently. In all systems they studied, the ultimate source and sink of data is files in HDFS. Do you really need that fancy distributed framework? You might recall the story from last year of awk and grep on the command-line of a single machine outperforming a Hadoop cluster by a factor of 235 . Gog et al. studied the effect on input size on framework performance. Take a look at their figure 2a, below:  For small inputs (≤0.5GB), the Metis single-machine MapReduce system performs best. This matters, as small inputs are common in practice: 40– 80% of Cloudera customers’ MapReduce jobs and 70% of jobs in a Facebook trace have ≤ 1GB of input. This last point bears repeating, and if I can generalize slightly: for between 40-80% of the jobs submitted to MapReduce systems, you’d be better off just running them on a single machine. Likewise a join workflow producing 1.9GB of data runs best on a single machine. A larger join producing 29GB works best on Hadoop. See figure 2b below. Do you really need that new shiny thing? Well, maybe! But likewise there is no universal guarantee that e.g. Spark is better than Hadoop MR. It depends on what you’re trying to do…  Once the data size grows, Hive, Spark and Hadoop all surpass the single-machine Metis, not least since they can stream data from and to HDFS in parallel. However, since there is no data re-use in this workflow, Spark performs worse than Hadoop: it loads all data into a distributed in-memory RDD before performing the projection. What are you optimising for? For workflows involving iterative computations on graphs, it won’t surprise you to learn that specialized graph processing systems do well. It is evident that graph-oriented paradigms have significant advantages for this computation: a GraphLINQ implementation running on Naiad outperforms all other systems. PowerGraph also performs very well, since its vertex-centric sharding reduces the communication overhead that dominates PageRank… However, the fastest system is not always the most efficient. Look at figure 3a below. With smaller graphs the 100 node clusters may be the fastest, but you’re getting nowhere near a 100x speed-up for all that investment (we’re on the RHS of the Universal Scalability Law curve). If you prepared to wait just a little longer for results, you can get your answer with dramatically less compute power (also compare e.g. PowerGraph on 16 nodes with GraphChi on one). Yes, but what’s the best data processing system? Our experiments show that the “best” system for a given workflow varies considerably. The right choice – i.e., the fastest or most efficient system – depends on the workflow, the input data size and the scale of parallelism available. If you think a little carefully about what you’re trying to achieve – when you really need fully precise results vs. good approximations ; when you really need to run on a distributed framework vs. a single machine; when you really need results quickly vs. waiting a little bit longer but being much more efficient – you can significantly improve the overall effectiveness of your data platform. Tomorrow we’ll see how Musketeer can help to make all this more practical and manageable by enabling workflows to be written once and mapped to many systems – even combining systems within a workflow. Postscript  The authors did something very neat with this paper – in the pdf version, every figure is actually a link to a webpage describing the experiments and data sets behind it. Really great idea, thanks!", "pdf_url": "http://www.cl.cam.ac.uk/research/srg/netos/camsas/pubs/eurosys15-musketeer.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1200_1300/musketeer-part-i-whats-the-best-data-processing-system.json"}
{"id": "45765062", "bin": "1300_1400", "summary_sentences": ["Economic factors of vulnerability trade and exploitation Allodi, CCS’17  Today we’re going on a tour inside a prominent Russian cybercrime market, identified in the paper as ‘RuMarket’ (not its real name).", "The author infiltrated the market using a fake identity, and was subsequently able to collect data on market activities from 2010 to 2017.", "RuMarket is a forum-based market that can be reached from the open Internet.", "Access to the market requires explicit admission by the market administrators, who validate the access request by performing background checks on the requester… As members of the market, we have access to all the (history of) product information, trades and prices available to active participants.", "This analysis spans seven years of market activity (July 2010 – April 2017).", "The study particularly looks at the trade in vulnerabilities.", "By far the most activity (by an order of magnitude) surrounds vulnerabilities that are marketed in reference to one or more CVEs.", "Searching and filtering on vulnerabilities traded by CVE resulted in a set of 89 traded exploits over 57 unique vulnerabilities, that were embedded in 38 different packages.", "The packages can be one of three different types:  Standalone packages sell stand-alone exploit that are then personalised by the buyer (e.g., to add their own shellcode).", "Malware packages are exploits embedded in malware packaging services.", "EKITS are exploit packages offered under an ‘exploit-as-a-service’ model in which the seller maintains a service which customers rent to deliver their own attacks.", "The authors find 26 standalone packages, 6 malware packages, and 6 exploit kits.", "The exploit kits of course contain an exploit portfolio within them that the kit owners keep up to date.", "Over time, the number of vendors trading in the market has been steadily growing:  The top EKIT packages remain active in the market board for more than 3 years, whereas standalone and malware packages have a shorter shelf-life.", "Overall, the average package remains active for a year following initial publication.", "The lifecycle of an exploit  Most new exploits appear in the market as standalone products, and later on these get re-packaged into malware and exploit kit distributions.", "The overall trend is for exploit kits to specialise using fewer, more reliable exploits that at their original introduction.", "Standalone exploits therefore seem to play a role in the ‘innovation’ process in RuMarket; this may indicate the presence of an ‘exploit chain’ whereby the most reliable and effective standalone exploits are selected for future inclusion in exploit kit products for deployment at scale.", "The top 2.5% of standalone exploits are published in the market within a week of the public disclosure of the corresponding CVE, the top 25% within 40 days, and the top 50% within 2.5 months.", "The gap between CVE disclosure and exploits packaged in malware or exploit kits is much longer – about half a year on average.", "Overall, most packaged exploits take around four months from disclosure date to be published.", "If we look at the speed of exploit packaging by year though, a trend emerges:  We observe that the mean exploit age decreases steadily for more recent publication dates, indicating that exploit vendors are becoming faster in releasing exploits for newly disclosed vulnerabilities.", "The coefficient of linear regression indicates that exploits appear at an approximately 30% faster rate every year.", "Exploit targets  The favourite exploit targets are Microsoft, Adobe, and Oracle products.", "Microsoft vulnerabilities account for more than half of the standalone products.", "After 2013, with the release of plug-in blocking in browsers and improvements to the default security settings in Java, the packaging of Oracle exploits drops off.", "How much is that exploit in the window?", "Standalone packages mostly bundle a single exploit, and trade for up to 8000 USD, with a mean price of 2000 USD.", "Malware packages trade at between 400 and 4000 USD, with most in the 1000-2000 USD range.", "Exploit kits are typically rented for a period of two to three weeks.", "Basic exploit kits go for 150-400 USD, and those embedding a wider range of exploits rent for 400-2000 USD.", "The prices for standalone and malware packages show an increasing trend, whereas exploit kit prices are steadily decreasing.", "If we break the figures down by exploit target, we see that Microsoft exploits are the most expensive, and within the Microsoft category, exploits targeting Office and Windows are the most expensive of all.", "(Which makes sense, because these give the attacker the most potential targets).", "We find that baseline prices for exploits vary widely by software vendor, and are negatively correlated with the age of the exploit; Adobe and Microsoft exploits retain their value significantly longer than Oracle exploits.", "This may indicate a prolonged economic interest in the exploitation of Microsoft and Adobe vulnerabilities, a finding consistent with related work on the persistence of vulnerabilities on end-user systems.", "Are traded exploits used in the wild?", "Symantec’s threat explorer and attack signature databases contain information on exploits in the wild, with associated CVE-IDs.", "Thus it becomes possible to look at trading activity of packaged exploits, and compare it to attacks in the wild.", "Exploits with more active forum discussions correlate with a higher chance of exploitation in the wild:  Moreover, the lower the cost of the exploit package, the more likely it is to see the exploit used in the wild:  …everything else being equal, exploits bundled in more expensive packages are less likely to be detected in the wild than comparable exploits bundled in less expensive packages.", "These findings weigh favorably on the existence of a relation between market activity and exploit deployment in the wild.", "Exploit kits play an important role here.", "The exploit kit model allows vendors to reduce their exploit development and deployment costs, thus offering exploits at lower prices.", "The combination of lower prices and ‘as-a-service’ delivery makes the exploits available to a wider audience, generating more attacks overall.", "This suggests that the criminal business model may play a central role in the diffusion of cyber-attacks, and calls for additional studies characterizing this effect.", "The severity of the vulnerability also matters.", "Critical vulnerabilities with exploits traded in RuMarket have a much higher chance of exploitation (93%) than non-critical vulnerabilities (62%).", "Bug bounties  The prices available for exploits in modern bug-bounty programs are roughly in line or below the prices for the same exploits on RuMarket.", "Thus it is still more profitable for a vendor to sell their exploits to different buyers multiple times on the RuMarket than to move to ‘legitimate’ vulnerability markets as in the latter case you can trade the exploit only once.", "(And what’s stopping a vendor from trading an exploit in both markets, on the assumption that patching in the wild takes time???).", "The last word  Our findings quantify a strong correlation between market activities and likelihood of exploit.", "We find that the analyzed market shows signs of expansion, and that exploit-as-a-service models may allow for drastic cuts in exploit development costs.", "Further, we find that exploit prices are aligned with or above those of ‘legitimate’ vulnerability markets, supporting work on the identification of incentives for responsible vulnerability disclosure and attack economics."], "summary_text": "Economic factors of vulnerability trade and exploitation Allodi, CCS’17  Today we’re going on a tour inside a prominent Russian cybercrime market, identified in the paper as ‘RuMarket’ (not its real name). The author infiltrated the market using a fake identity, and was subsequently able to collect data on market activities from 2010 to 2017. RuMarket is a forum-based market that can be reached from the open Internet. Access to the market requires explicit admission by the market administrators, who validate the access request by performing background checks on the requester… As members of the market, we have access to all the (history of) product information, trades and prices available to active participants. This analysis spans seven years of market activity (July 2010 – April 2017). The study particularly looks at the trade in vulnerabilities. By far the most activity (by an order of magnitude) surrounds vulnerabilities that are marketed in reference to one or more CVEs. Searching and filtering on vulnerabilities traded by CVE resulted in a set of 89 traded exploits over 57 unique vulnerabilities, that were embedded in 38 different packages. The packages can be one of three different types:  Standalone packages sell stand-alone exploit that are then personalised by the buyer (e.g., to add their own shellcode). Malware packages are exploits embedded in malware packaging services. EKITS are exploit packages offered under an ‘exploit-as-a-service’ model in which the seller maintains a service which customers rent to deliver their own attacks. The authors find 26 standalone packages, 6 malware packages, and 6 exploit kits. The exploit kits of course contain an exploit portfolio within them that the kit owners keep up to date. Over time, the number of vendors trading in the market has been steadily growing:  The top EKIT packages remain active in the market board for more than 3 years, whereas standalone and malware packages have a shorter shelf-life. Overall, the average package remains active for a year following initial publication. The lifecycle of an exploit  Most new exploits appear in the market as standalone products, and later on these get re-packaged into malware and exploit kit distributions. The overall trend is for exploit kits to specialise using fewer, more reliable exploits that at their original introduction. Standalone exploits therefore seem to play a role in the ‘innovation’ process in RuMarket; this may indicate the presence of an ‘exploit chain’ whereby the most reliable and effective standalone exploits are selected for future inclusion in exploit kit products for deployment at scale. The top 2.5% of standalone exploits are published in the market within a week of the public disclosure of the corresponding CVE, the top 25% within 40 days, and the top 50% within 2.5 months. The gap between CVE disclosure and exploits packaged in malware or exploit kits is much longer – about half a year on average. Overall, most packaged exploits take around four months from disclosure date to be published. If we look at the speed of exploit packaging by year though, a trend emerges:  We observe that the mean exploit age decreases steadily for more recent publication dates, indicating that exploit vendors are becoming faster in releasing exploits for newly disclosed vulnerabilities. The coefficient of linear regression indicates that exploits appear at an approximately 30% faster rate every year. Exploit targets  The favourite exploit targets are Microsoft, Adobe, and Oracle products. Microsoft vulnerabilities account for more than half of the standalone products. After 2013, with the release of plug-in blocking in browsers and improvements to the default security settings in Java, the packaging of Oracle exploits drops off. How much is that exploit in the window? Standalone packages mostly bundle a single exploit, and trade for up to 8000 USD, with a mean price of 2000 USD. Malware packages trade at between 400 and 4000 USD, with most in the 1000-2000 USD range. Exploit kits are typically rented for a period of two to three weeks. Basic exploit kits go for 150-400 USD, and those embedding a wider range of exploits rent for 400-2000 USD. The prices for standalone and malware packages show an increasing trend, whereas exploit kit prices are steadily decreasing. If we break the figures down by exploit target, we see that Microsoft exploits are the most expensive, and within the Microsoft category, exploits targeting Office and Windows are the most expensive of all. (Which makes sense, because these give the attacker the most potential targets). We find that baseline prices for exploits vary widely by software vendor, and are negatively correlated with the age of the exploit; Adobe and Microsoft exploits retain their value significantly longer than Oracle exploits. This may indicate a prolonged economic interest in the exploitation of Microsoft and Adobe vulnerabilities, a finding consistent with related work on the persistence of vulnerabilities on end-user systems. Are traded exploits used in the wild? Symantec’s threat explorer and attack signature databases contain information on exploits in the wild, with associated CVE-IDs. Thus it becomes possible to look at trading activity of packaged exploits, and compare it to attacks in the wild. Exploits with more active forum discussions correlate with a higher chance of exploitation in the wild:  Moreover, the lower the cost of the exploit package, the more likely it is to see the exploit used in the wild:  …everything else being equal, exploits bundled in more expensive packages are less likely to be detected in the wild than comparable exploits bundled in less expensive packages. These findings weigh favorably on the existence of a relation between market activity and exploit deployment in the wild. Exploit kits play an important role here. The exploit kit model allows vendors to reduce their exploit development and deployment costs, thus offering exploits at lower prices. The combination of lower prices and ‘as-a-service’ delivery makes the exploits available to a wider audience, generating more attacks overall. This suggests that the criminal business model may play a central role in the diffusion of cyber-attacks, and calls for additional studies characterizing this effect. The severity of the vulnerability also matters. Critical vulnerabilities with exploits traded in RuMarket have a much higher chance of exploitation (93%) than non-critical vulnerabilities (62%). Bug bounties  The prices available for exploits in modern bug-bounty programs are roughly in line or below the prices for the same exploits on RuMarket. Thus it is still more profitable for a vendor to sell their exploits to different buyers multiple times on the RuMarket than to move to ‘legitimate’ vulnerability markets as in the latter case you can trade the exploit only once. (And what’s stopping a vendor from trading an exploit in both markets, on the assumption that patching in the wild takes time???). The last word  Our findings quantify a strong correlation between market activities and likelihood of exploit. We find that the analyzed market shows signs of expansion, and that exploit-as-a-service models may allow for drastic cuts in exploit development costs. Further, we find that exploit prices are aligned with or above those of ‘legitimate’ vulnerability markets, supporting work on the identification of incentives for responsible vulnerability disclosure and attack economics.", "pdf_url": "https://arxiv.org/pdf/1708.04866", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/economic-factors-of-vulnerability-trade-and-exploitation.json"}
{"id": "12239056", "bin": "1300_1400", "summary_sentences": ["What  They suggest a variation of Faster R-CNN.", "Their network detects bounding boxes (e.g. of people, cars) in images and also segments the objects within these bounding boxes (i.e.", "classifies for each pixel whether it is part of the object or background).", "The model runs roughly at the same speed as Faster R-CNN.", "How  The architecture and training is mostly the same as in Faster R-CNN:  Input is an image.", "The backbone network transforms the input image into feature maps.", "It consists of convolutions, e.g. initialized with ResNet's weights.", "The RPN (Region Proposal Network) takes the feature maps and classifies for each location whether there is a bounding box at that point (with some other stuff to regress height/width and offsets).", "This leads to a large number of bounding box candidates (region proposals) per image.", "RoIAlign: Each region proposal's \"area\" is extracted from the feature maps and converted into a fixed-size 7x7xF feature map (with F input filters).", "(See below.)", "The head uses the region proposal's features to perform  Classification: \"is the bounding box of a person/car/.../background\"  Regression: \"bounding box should have width/height/offset so and so\"  Segmentation: \"pixels so and so are part of this object's mask\"  Rough visualization of the architecture:  RoIAlign  This is very similar to RoIPooling in Faster R-CNN.", "For each RoI, RoIPooling first \"finds\" the features in the feature maps that lie within the RoI's rectangle.", "Then it max-pools them to create a fixed size vector.", "Problem: The coordinates where an RoI starts and ends may be non-integers.", "E.g. the top left corner might have coordinates (x=2.5, y=4.7).", "RoIPooling simply rounds these values to the nearest integers (e.g. (x=2, y=5)).", "But that can create pooled RoIs that are significantly off, as the feature maps with which RoIPooling works have high (total) stride (e.g. 32 pixels in standard ResNets).", "So being just one cell off can easily lead to being 32 pixels off on the input image.", "For classification, being some pixels off is usually not that bad.", "For masks however it can significantly worsen the results, as these have to be pixel-accurate.", "In RoIAlign this is compensated by not rounding the coordinates and instead using bilinear interpolation to interpolate between the feature map's cells.", "Each RoI is pooled by RoIAlign to a fixed sized feature map of size (H, W, F), with H and W usually being 7 or 14.", "(It can also generate different sizes, e.g. 7x7xF for classification and more accurate 14x14xF for masks.)", "If H and W are 7, this leads to 49 cells within each plane of the pooled feature maps.", "Each cell again is a rectangle -- similar to the RoIs -- and pooled with bilinear interpolation.", "More exactly, each cell is split up into four sub-cells (top left, top right, bottom right, bottom left).", "Each of these sub-cells is pooled via bilinear interpolation, leading to four values per cell.", "The final cell value is then computed using either an average or a maximum over the four sub-values.", "Segmentation  They add an additional branch to the head that gets pooled RoI as inputs and processes them seperately from the classification and regression (no connections between the branches).", "That branch does segmentation.", "It is fully convolutional, similar to many segmentation networks.", "The result is one mask per class.", "There is no softmax per pixel over the classes, as classification is done by a different branch.", "Base networks  Their backbone networks are either ResNet or ResNeXt (in the 50 or 102 layer variations).", "Their head is either the fourth/fifth module from ResNet/ResNeXt (called C4 (fourth) or C5 (fifth)) or they use the second half from the FPN network (called FPN).", "They denote their networks via backbone-head, i.e. ResNet-101-FPN means that their backbone is ResNet-101 and their head is FPN.", "Visualization of the different heads:  Training  Training happens in basically the same way as Faster R-CNN.", "They just add an additional loss term to the total loss (L = L_classification + L_regression + L_mask).", "L_mask is based on binary cross-entropy.", "For each predicted RoI, the correct mask is the intersection between that RoI's area and the correct mask.", "They only train masks for RoIs that are positive (overlap with ground truth bounding boxes).", "They train for 120k iterations at learning rate 0.02 and 40k at 0.002 with weight decay 0.0002 and momentum 0.9.", "Test  For the C4-head they sample up to 300 region proposals from the RPN (those with highest confidence values).", "For the FPN head they sample up to 1000, as FPN is faster.", "They sample masks only for the 100 proposals with highest confidence values.", "Each mask is turned into a binary mask using a threshold of 0.5.", "Results  Instance Segmentation  They train and test on COCO.", "They can outperform the best competitor by a decent margin (AP 37.1 vs 33.6 for FCIS+++ with OHEM).", "Their model especially performs much better when there is overlap between bounding boxes.", "Ranking of their models: ResNeXt-101-FPN > ResNet-101-FPN > ResNet-50-FPN > ResNet-101-C4 > ResNet-50-C4.", "Using sigmoid instead of softmax (over classes) for the mask prediction significantly improves results by 5.5 to 7.1 points AP (depending on measurement method).", "Predicting only one mask per RoI (class-agnostic) instead of C masks (where C is the number of classes) only has a small negative effect on AP (about 0.6 points).", "Using RoIAlign instead of RoIPooling has significant positive effects on the AP of around 5 to 10 points (if a network with C5 head is chosen, which has a high stride of 32).", "Effects are smaller for small strides and FPN head.", "Using fully convolutional networks for the mask branch performs better than fully connected layers (1-3 points AP).", "Examples results on COCO vs FCIS (note the better handling of overlap):  Bounding-Box-Detection  Training additionally on masks seems to improve AP for bounding boxes by around 1 point (benefit from multi-task learning).", "Timing  Around 200ms for ResNet-101-FPN.", "(M40 GPU)  Around 400ms for ResNet-101-C4.", "Human Pose Estimation  The mask branch can be used to predict keypoint (landmark) locations on human bodies (i.e. locations of hands, feet etc.).", "This is done by using one mask per keypoint, initializing it to 0 and setting the keypoint location to 1.", "By doing this, Mask R-CNN can predict keypoints roughly as good as the current leading models (on COCO), while running at 5fps.", "Cityscapes  They test their model on the cityscapes dataset.", "They beat previous models with significant margins.", "This is largely due to their better handling of overlapping instances.", "They get their best scores using a model that was pre-trained on COCO.", "Examples results on cityscapes:"], "summary_text": "What  They suggest a variation of Faster R-CNN. Their network detects bounding boxes (e.g. of people, cars) in images and also segments the objects within these bounding boxes (i.e. classifies for each pixel whether it is part of the object or background). The model runs roughly at the same speed as Faster R-CNN. How  The architecture and training is mostly the same as in Faster R-CNN:  Input is an image. The backbone network transforms the input image into feature maps. It consists of convolutions, e.g. initialized with ResNet's weights. The RPN (Region Proposal Network) takes the feature maps and classifies for each location whether there is a bounding box at that point (with some other stuff to regress height/width and offsets). This leads to a large number of bounding box candidates (region proposals) per image. RoIAlign: Each region proposal's \"area\" is extracted from the feature maps and converted into a fixed-size 7x7xF feature map (with F input filters). (See below.) The head uses the region proposal's features to perform  Classification: \"is the bounding box of a person/car/.../background\"  Regression: \"bounding box should have width/height/offset so and so\"  Segmentation: \"pixels so and so are part of this object's mask\"  Rough visualization of the architecture:  RoIAlign  This is very similar to RoIPooling in Faster R-CNN. For each RoI, RoIPooling first \"finds\" the features in the feature maps that lie within the RoI's rectangle. Then it max-pools them to create a fixed size vector. Problem: The coordinates where an RoI starts and ends may be non-integers. E.g. the top left corner might have coordinates (x=2.5, y=4.7). RoIPooling simply rounds these values to the nearest integers (e.g. (x=2, y=5)). But that can create pooled RoIs that are significantly off, as the feature maps with which RoIPooling works have high (total) stride (e.g. 32 pixels in standard ResNets). So being just one cell off can easily lead to being 32 pixels off on the input image. For classification, being some pixels off is usually not that bad. For masks however it can significantly worsen the results, as these have to be pixel-accurate. In RoIAlign this is compensated by not rounding the coordinates and instead using bilinear interpolation to interpolate between the feature map's cells. Each RoI is pooled by RoIAlign to a fixed sized feature map of size (H, W, F), with H and W usually being 7 or 14. (It can also generate different sizes, e.g. 7x7xF for classification and more accurate 14x14xF for masks.) If H and W are 7, this leads to 49 cells within each plane of the pooled feature maps. Each cell again is a rectangle -- similar to the RoIs -- and pooled with bilinear interpolation. More exactly, each cell is split up into four sub-cells (top left, top right, bottom right, bottom left). Each of these sub-cells is pooled via bilinear interpolation, leading to four values per cell. The final cell value is then computed using either an average or a maximum over the four sub-values. Segmentation  They add an additional branch to the head that gets pooled RoI as inputs and processes them seperately from the classification and regression (no connections between the branches). That branch does segmentation. It is fully convolutional, similar to many segmentation networks. The result is one mask per class. There is no softmax per pixel over the classes, as classification is done by a different branch. Base networks  Their backbone networks are either ResNet or ResNeXt (in the 50 or 102 layer variations). Their head is either the fourth/fifth module from ResNet/ResNeXt (called C4 (fourth) or C5 (fifth)) or they use the second half from the FPN network (called FPN). They denote their networks via backbone-head, i.e. ResNet-101-FPN means that their backbone is ResNet-101 and their head is FPN. Visualization of the different heads:  Training  Training happens in basically the same way as Faster R-CNN. They just add an additional loss term to the total loss (L = L_classification + L_regression + L_mask). L_mask is based on binary cross-entropy. For each predicted RoI, the correct mask is the intersection between that RoI's area and the correct mask. They only train masks for RoIs that are positive (overlap with ground truth bounding boxes). They train for 120k iterations at learning rate 0.02 and 40k at 0.002 with weight decay 0.0002 and momentum 0.9. Test  For the C4-head they sample up to 300 region proposals from the RPN (those with highest confidence values). For the FPN head they sample up to 1000, as FPN is faster. They sample masks only for the 100 proposals with highest confidence values. Each mask is turned into a binary mask using a threshold of 0.5. Results  Instance Segmentation  They train and test on COCO. They can outperform the best competitor by a decent margin (AP 37.1 vs 33.6 for FCIS+++ with OHEM). Their model especially performs much better when there is overlap between bounding boxes. Ranking of their models: ResNeXt-101-FPN > ResNet-101-FPN > ResNet-50-FPN > ResNet-101-C4 > ResNet-50-C4. Using sigmoid instead of softmax (over classes) for the mask prediction significantly improves results by 5.5 to 7.1 points AP (depending on measurement method). Predicting only one mask per RoI (class-agnostic) instead of C masks (where C is the number of classes) only has a small negative effect on AP (about 0.6 points). Using RoIAlign instead of RoIPooling has significant positive effects on the AP of around 5 to 10 points (if a network with C5 head is chosen, which has a high stride of 32). Effects are smaller for small strides and FPN head. Using fully convolutional networks for the mask branch performs better than fully connected layers (1-3 points AP). Examples results on COCO vs FCIS (note the better handling of overlap):  Bounding-Box-Detection  Training additionally on masks seems to improve AP for bounding boxes by around 1 point (benefit from multi-task learning). Timing  Around 200ms for ResNet-101-FPN. (M40 GPU)  Around 400ms for ResNet-101-C4. Human Pose Estimation  The mask branch can be used to predict keypoint (landmark) locations on human bodies (i.e. locations of hands, feet etc.). This is done by using one mask per keypoint, initializing it to 0 and setting the keypoint location to 1. By doing this, Mask R-CNN can predict keypoints roughly as good as the current leading models (on COCO), while running at 5fps. Cityscapes  They test their model on the cityscapes dataset. They beat previous models with significant margins. This is largely due to their better handling of overlapping instances. They get their best scores using a model that was pre-trained on COCO. Examples results on cityscapes:", "pdf_url": "https://arxiv.org/pdf/1703.06870", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/mask_r-cnn.json"}
{"id": "13655370", "bin": "1300_1400", "summary_sentences": ["What  ELUs are an activation function  The are most similar to LeakyReLUs and PReLUs  How (formula)  f(x):  if x >= 0: x  else: alpha(exp(x)-1)  f'(x) / Derivative:  if x >= 0: 1  else: f(x) + alpha  alpha defines at which negative value the ELU saturates.", "E. g. alpha=1.0 means that the minimum value that the ELU can reach is -1.0  LeakyReLUs however can go to -Infinity, ReLUs can't go below 0.", "Form of ELUs(alpha=1.0) vs LeakyReLUs vs ReLUs.", "Why  They derive from the unit natural gradient that a network learns faster, if the mean activation of each neuron is close to zero.", "ReLUs can go above 0, but never below.", "So their mean activation will usually be quite a bit above 0, which should slow down learning.", "ELUs, LeakyReLUs and PReLUs all have negative slopes, so their mean activations should be closer to 0.", "In contrast to LeakyReLUs and PReLUs, ELUs saturate at a negative value (usually -1.0).", "The authors think that is good, because it lets ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence.", "So ELUs can measure the presence of concepts quantitatively, but the absence only qualitatively.", "They think that this makes ELUs more robust to noise.", "Results  In their tests on MNIST, CIFAR-10, CIFAR-100 and ImageNet, ELUs perform (nearly always) better than ReLUs and LeakyReLUs.", "However, they don't test PReLUs at all and use an alpha of 0.1 for LeakyReLUs (even though 0.33 is afaik standard) and don't test LeakyReLUs on ImageNet (only ReLUs).", "Comparison of ELUs, LeakyReLUs, ReLUs on CIFAR-100.", "ELUs ends up with best values, beaten during the early epochs by LeakyReLUs.", "(Learning rates were optimized for ReLUs.)", "Rough chapter-wise notes  Introduction  Currently popular choice: ReLUs  ReLU: max(0, x)  ReLUs are sparse and avoid the vanishing gradient problem, because their derivate is 1 when they are active.", "ReLUs have a mean activation larger than zero.", "Non-zero mean activation causes a bias shift in the next layer, especially if multiple of them are correlated.", "The natural gradient (?)", "corrects for the bias shift by adjusting the weight update.", "Having less bias shift would bring the standard gradient closer to the natural gradient, which would lead to faster learning.", "Suggested solutions:  Centering activation functions at zero, which would keep the off-diagonal entries of the Fisher information matrix small.", "Batch Normalization  Projected Natural Gradient Descent (implicitly whitens the activations)  These solutions have the problem, that they might end up taking away previous learning steps, which would slow down learning unnecessarily.", "Chosing a good activation function would be a better solution.", "Previously, tanh was prefered over sigmoid for that reason (pushed mean towards zero).", "Recent new activation functions:  LeakyReLUs: x if x > 0, else alpha*x  PReLUs: Like LeakyReLUs, but alpha is learned  RReLUs: Slope of part < 0 is sampled randomly  Such activation functions with non-zero slopes for negative values seemed to improve results.", "The deactivation state of such units is not very robust to noise, can get very negative.", "They suggest an activation function that can return negative values, but quickly saturates (for negative values, not for positive ones).", "So the model can make a quantitative assessment for positive statements (there is an amount X of A in the image), but only a qualitative negative one (something indicates that B is not in the image).", "They argue that this makes their activation function more robust to noise.", "Their activation function still has activations with a mean close to zero.", "Zero Mean Activations Speed Up Learning  Natural Gradient = Update direction which corrects the gradient direction with the Fisher Information Matrix  Hessian-Free Optimization techniques use an extended Gauss-Newton approximation of Hessians and therefore can be interpreted as versions of natural gradient descent.", "Computing the Fisher matrix is too expensive for neural networks.", "Methods to approximate the Fisher matrix or to perform natural gradient descent have been developed.", "Natural gradient = inverse(FisherMatrix) * gradientOfWeights  Lots of formulas.", "Apparently first explaining how the natural gradient descent works, then proofing that natural gradient descent can deal well with non-zero-mean activations.", "Natural gradient descent auto-corrects bias shift (i.e. non-zero-mean activations).", "If that auto-correction does not exist, oscillations (?)", "can occur, which slow down learning.", "Two ways to push means towards zero:  Unit zero mean normalization (e.g. Batch Normalization)  Activation functions with negative parts  Exponential Linear Units (ELUs)  Formula  f(x):  if x >= 0: x  else: alpha(exp(x)-1)  f'(x) / Derivative:  if x >= 0: 1  else: f(x) + alpha  alpha defines at which negative value the ELU saturates.", "alpha=0.5 => minimum value is -0.5 (?)", "ELUs avoid the vanishing gradient problem, because their positive part is the identity function (like e.g. ReLUs)  The negative values of ELUs push the mean activation towards zero.", "Mean activations closer to zero resemble more the natural gradient, therefore they should speed up learning.", "ELUs are more noise robust than PReLUs and LeakyReLUs, because their negative values saturate and thus should create a small gradient.", "\"ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence\"  Experiments Using ELUs  They compare ELUs to ReLUs and LeakyReLUs, but not to PReLUs (no explanation why).", "They seem to use a negative slope of 0.1 for LeakyReLUs, even though 0.33 is standard afaik.", "They use an alpha of 1.0 for their ELUs (i.e. minimum value is -1.0).", "MNIST classification:  ELUs achieved lower mean activations than ReLU/LeakyReLU  ELUs achieved lower cross entropy loss than ReLU/LeakyReLU (and also seemed to learn faster)  They used 5 hidden layers of 256 units each (no explanation why so many)  (No convolutions)  MNIST Autoencoder:  ELUs performed consistently best (at different learning rates)  Usually ELU > LeakyReLU > ReLU  LeakyReLUs not far off, so if they had used a 0.33 value maybe these would have won  CIFAR-100 classification:  Convolutional network, 11 conv layers  LeakyReLUs performed better during the first ~50 epochs, ReLUs mostly on par with ELUs  LeakyReLUs about on par for epochs 50-100  ELUs win in the end (the learning rates used might not be optimal for ELUs, were designed for ReLUs)  CIFER-100, CIFAR-10 (big convnet):  6.55% error on CIFAR-10, 24.28% on CIFAR-100  No comparison with ReLUs and LeakyReLUs for same architecture  ImageNet  Big convnet with spatial pyramid pooling (?)", "before the fully connected layers  Network with ELUs performed better than ReLU network (better score at end, faster learning)  Networks were still learning at the end, they didn't run till convergence  No comparison to LeakyReLUs"], "summary_text": "What  ELUs are an activation function  The are most similar to LeakyReLUs and PReLUs  How (formula)  f(x):  if x >= 0: x  else: alpha(exp(x)-1)  f'(x) / Derivative:  if x >= 0: 1  else: f(x) + alpha  alpha defines at which negative value the ELU saturates. E. g. alpha=1.0 means that the minimum value that the ELU can reach is -1.0  LeakyReLUs however can go to -Infinity, ReLUs can't go below 0. Form of ELUs(alpha=1.0) vs LeakyReLUs vs ReLUs. Why  They derive from the unit natural gradient that a network learns faster, if the mean activation of each neuron is close to zero. ReLUs can go above 0, but never below. So their mean activation will usually be quite a bit above 0, which should slow down learning. ELUs, LeakyReLUs and PReLUs all have negative slopes, so their mean activations should be closer to 0. In contrast to LeakyReLUs and PReLUs, ELUs saturate at a negative value (usually -1.0). The authors think that is good, because it lets ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence. So ELUs can measure the presence of concepts quantitatively, but the absence only qualitatively. They think that this makes ELUs more robust to noise. Results  In their tests on MNIST, CIFAR-10, CIFAR-100 and ImageNet, ELUs perform (nearly always) better than ReLUs and LeakyReLUs. However, they don't test PReLUs at all and use an alpha of 0.1 for LeakyReLUs (even though 0.33 is afaik standard) and don't test LeakyReLUs on ImageNet (only ReLUs). Comparison of ELUs, LeakyReLUs, ReLUs on CIFAR-100. ELUs ends up with best values, beaten during the early epochs by LeakyReLUs. (Learning rates were optimized for ReLUs.) Rough chapter-wise notes  Introduction  Currently popular choice: ReLUs  ReLU: max(0, x)  ReLUs are sparse and avoid the vanishing gradient problem, because their derivate is 1 when they are active. ReLUs have a mean activation larger than zero. Non-zero mean activation causes a bias shift in the next layer, especially if multiple of them are correlated. The natural gradient (?) corrects for the bias shift by adjusting the weight update. Having less bias shift would bring the standard gradient closer to the natural gradient, which would lead to faster learning. Suggested solutions:  Centering activation functions at zero, which would keep the off-diagonal entries of the Fisher information matrix small. Batch Normalization  Projected Natural Gradient Descent (implicitly whitens the activations)  These solutions have the problem, that they might end up taking away previous learning steps, which would slow down learning unnecessarily. Chosing a good activation function would be a better solution. Previously, tanh was prefered over sigmoid for that reason (pushed mean towards zero). Recent new activation functions:  LeakyReLUs: x if x > 0, else alpha*x  PReLUs: Like LeakyReLUs, but alpha is learned  RReLUs: Slope of part < 0 is sampled randomly  Such activation functions with non-zero slopes for negative values seemed to improve results. The deactivation state of such units is not very robust to noise, can get very negative. They suggest an activation function that can return negative values, but quickly saturates (for negative values, not for positive ones). So the model can make a quantitative assessment for positive statements (there is an amount X of A in the image), but only a qualitative negative one (something indicates that B is not in the image). They argue that this makes their activation function more robust to noise. Their activation function still has activations with a mean close to zero. Zero Mean Activations Speed Up Learning  Natural Gradient = Update direction which corrects the gradient direction with the Fisher Information Matrix  Hessian-Free Optimization techniques use an extended Gauss-Newton approximation of Hessians and therefore can be interpreted as versions of natural gradient descent. Computing the Fisher matrix is too expensive for neural networks. Methods to approximate the Fisher matrix or to perform natural gradient descent have been developed. Natural gradient = inverse(FisherMatrix) * gradientOfWeights  Lots of formulas. Apparently first explaining how the natural gradient descent works, then proofing that natural gradient descent can deal well with non-zero-mean activations. Natural gradient descent auto-corrects bias shift (i.e. non-zero-mean activations). If that auto-correction does not exist, oscillations (?) can occur, which slow down learning. Two ways to push means towards zero:  Unit zero mean normalization (e.g. Batch Normalization)  Activation functions with negative parts  Exponential Linear Units (ELUs)  Formula  f(x):  if x >= 0: x  else: alpha(exp(x)-1)  f'(x) / Derivative:  if x >= 0: 1  else: f(x) + alpha  alpha defines at which negative value the ELU saturates. alpha=0.5 => minimum value is -0.5 (?) ELUs avoid the vanishing gradient problem, because their positive part is the identity function (like e.g. ReLUs)  The negative values of ELUs push the mean activation towards zero. Mean activations closer to zero resemble more the natural gradient, therefore they should speed up learning. ELUs are more noise robust than PReLUs and LeakyReLUs, because their negative values saturate and thus should create a small gradient. \"ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence\"  Experiments Using ELUs  They compare ELUs to ReLUs and LeakyReLUs, but not to PReLUs (no explanation why). They seem to use a negative slope of 0.1 for LeakyReLUs, even though 0.33 is standard afaik. They use an alpha of 1.0 for their ELUs (i.e. minimum value is -1.0). MNIST classification:  ELUs achieved lower mean activations than ReLU/LeakyReLU  ELUs achieved lower cross entropy loss than ReLU/LeakyReLU (and also seemed to learn faster)  They used 5 hidden layers of 256 units each (no explanation why so many)  (No convolutions)  MNIST Autoencoder:  ELUs performed consistently best (at different learning rates)  Usually ELU > LeakyReLU > ReLU  LeakyReLUs not far off, so if they had used a 0.33 value maybe these would have won  CIFAR-100 classification:  Convolutional network, 11 conv layers  LeakyReLUs performed better during the first ~50 epochs, ReLUs mostly on par with ELUs  LeakyReLUs about on par for epochs 50-100  ELUs win in the end (the learning rates used might not be optimal for ELUs, were designed for ReLUs)  CIFER-100, CIFAR-10 (big convnet):  6.55% error on CIFAR-10, 24.28% on CIFAR-100  No comparison with ReLUs and LeakyReLUs for same architecture  ImageNet  Big convnet with spatial pyramid pooling (?) before the fully connected layers  Network with ELUs performed better than ReLU network (better score at end, faster learning)  Networks were still learning at the end, they didn't run till convergence  No comparison to LeakyReLUs", "pdf_url": "http://arxiv.org/pdf/1511.07289", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1300_1400/elus.json"}
{"id": "17754953", "bin": "1400_1500", "summary_sentences": ["Adding concurrency to smart contracts Dickerson et al., PODC’17  Yesterday we looked at how analogies from concurrent objects could help us understand smart contract behaviour.", "In today’s paper choice from PODC’17 (which also has one Maurice Herlihy on the author list) we get to borrow some ideas from concurrent objects to increase the concurrency of smart contracts.", "Back in 2008 Herlihy & Koskinen published a paper on ‘ Transactional boosting: a methodology for highly-concurrent transactional objects.", "‘ In the context of software transactional memory (STM), transactional boosting showed how to transform highly-concurrent base objects implemented without any notion of transactions into equally concurrent transaction objects that can safely be used with STM.", "Taking some ideas from transaction boosting, …  … This paper presents a novel way to permit miners and validators to execute smart contracts in parallel, based on techniques adapted from software transactional memory.", "Miners execute smart contracts speculatively in parallel, allowing non-conflicting contracts to proceed concurrently, and “discovering” a serialized concurrent schedule for a block’s transactions.", "Why do we want more concurrency?", "When a miner creates a block, includes a sequence of transactions, computing the new state for the transactions’ smart contracts serially, in the order in which they appear in the block.", "If the block is subsequently successfully appended to the blockchain, that block’s transactions are re-executed by every node to confirm that the state transitions were computed honestly and correctly.", "To summarize, a transaction is executed in two contexts: once by miners before attempting to append a block to the blockchain, and many times afterward by validators checking that each block in the blockchain is honest.", "In both contexts, each block’s transactions are executed sequentially in block order.", "Miners are rewarded for blocks that they successfully append to the blockchain, so they have an incentive to increase throughput by parallelizing smart contract executions.", "But simply executing contracts in parallel without any special precautions won’t work as the contracts may perform conflicting accesses to shared data leading to an inconsistent final state.", "Validators on the other hand end up performing the vast majority of contract executions and parallel execution here could bring significant benefits if it could be done safely.", "Speculative smart contracts  We’ve seen previously that writing bug free smart contracts is hard.", "Clearly, even sequential smart contracts must be written with care, and introducing explicit concurrency to contract programming languages would only make the situation worse.", "We conclude that concurrent smart contract executions must be serializable: indistinguishable, except for execution time, from a sequential execution.", "Smart contracts read and modify shared storage, and they are written in Turing-complete languages – so it is impossible in the general case to determine statically whether or not contracts have data conflicts.", "Instead, contracts can be instrumented to detect synchronization conflicts at runtime, in a manner similar to that done in transaction boosting.", "Contracts are executed speculatively, and if a conflict does occur at runtime the conflict is resolved either by delaying one contract until the other completes, or rolling back and restarting one of the conflicting executions.", "Speculation is controlled by two run-time mechanism, invisible to the programmer, and managed by the virtual-machine: abstract locks and inverse logs.", "Storage operations are protected by abstract locks.", "If two storage operations map to distinct locks, then they must commute.", "Or to put it another way, operations that don’t commute must be protected by the same lock.", "It wasn’t clear to me from reading the paper whether this mapping of operations to locks can be performed automatically, or whether it requires human intervention.", "Before executing the operation, a thread must acquire the associated lock.", "When the lock is acquired, it records an inverse operation in a log (think undo log), and then proceeds with the operation.", "If the action commits, its abstract locks are released and its log is discarded.", "If the action aborts, the inverse log is replayed, most recent operations first, to undo the effects of that speculative action.", "When the replay is complete, the actions’ abstract locks are released.", "The advantage of combining abstract locks with inverse logs is that the virtual machine can support very fine-grained concurrency.", "If one contract calls another, a nested speculative action is created.", "At the end of this process, the miner will have discovered a concurrent schedule for a block’s transactions, that is equivalent to some sequential schedule, only faster.", "Validation  So far so good for the miners, but not so great for the validators.", "The problem is that the validators need to produce the same or an equivalent schedule of execution to that discovered by the  miner.", "The solution is for miners to produce and publish extra information concerning the constraints discovered during execution.", "Why would miners make this available?", "… that block [produced by the miner] may be competing with other blocks produced at the same time, and the miner will be rewarded only if the other miners choose to build on that block.", "Publishing a block with a parallel validation schedule makes the block more attractive for validation by other miners.", "Here’s how it works:  Each lock includes a use counter keeping track of the number of times it has been released by a committing action during the construction of the current block.", "When a speculative action commits, it increments the counters for each of the locks it holds, and registers a lock profile with the VM recording the abstract locks and their counter values.", "When all the actions have committed, the common schedule can be reconstructed by comparing lock profiles.", "It is these profiles that the miner includes in the blockchain along with the usual information.", "For example, consider three committed speculative actions, A, B, and C. If A and B have no abstract locks in common, they can run concurrently.", "If an abstract lock has counter value 1 in A’s profile and 2 in C’s profile, then C must be scheduled after A.", "Using the algorithm below, a validator can construct a simple fork-join program that deterministically reproduces the miner’s original speculative schedule.", "Using a work-stealing scheduler, the validator can exploit whatever degree of parallelism it has available.", "The validator keeps a thread-local trace of the abstract locks each thread would have acquired.", "If these traces don’t match the lock profiles provided by the miner the block is rejected.", "Is it safe?", "Correctness is argued by appeal to the analogy with transactional boosting, where serial equivalence has been proven.", "Experimental results  The authors built an implementation based on the JVM for experimental purposes, using the Scala STM library for speculative action execution.", "Examples of smart contracts were translated from Solidity into Scala, the modified to use the concurrency libraries.", "Each function from the Solidy contract is turned into a speculative transaction by wrapping its contents with a ScalaSTM atomic section.", "Solidity mapping objects are implemented as boosted hashtables, where key values are used to index abstract locks.", "The benchmarks are based on Ballot, SimpleAuction, and EtherDoc contracts, as well as a workload mixing all three.", "The experiments used only three concurrent threads, but this was still sufficient to show a benefit:  The charts below give more detail of the speedups obtained at different conflict levels.", "Our proposal for miners only is compatible with current smart contract systems such as Ethereum, but our overall proposal is not, because it requires including scheduling metadata in blocks and incentivizing miners to publish their parallel schedules.", "It may well be compatible with a future “soft fork” (backward compatible change), a subject for future research."], "summary_text": "Adding concurrency to smart contracts Dickerson et al., PODC’17  Yesterday we looked at how analogies from concurrent objects could help us understand smart contract behaviour. In today’s paper choice from PODC’17 (which also has one Maurice Herlihy on the author list) we get to borrow some ideas from concurrent objects to increase the concurrency of smart contracts. Back in 2008 Herlihy & Koskinen published a paper on ‘ Transactional boosting: a methodology for highly-concurrent transactional objects. ‘ In the context of software transactional memory (STM), transactional boosting showed how to transform highly-concurrent base objects implemented without any notion of transactions into equally concurrent transaction objects that can safely be used with STM. Taking some ideas from transaction boosting, …  … This paper presents a novel way to permit miners and validators to execute smart contracts in parallel, based on techniques adapted from software transactional memory. Miners execute smart contracts speculatively in parallel, allowing non-conflicting contracts to proceed concurrently, and “discovering” a serialized concurrent schedule for a block’s transactions. Why do we want more concurrency? When a miner creates a block, includes a sequence of transactions, computing the new state for the transactions’ smart contracts serially, in the order in which they appear in the block. If the block is subsequently successfully appended to the blockchain, that block’s transactions are re-executed by every node to confirm that the state transitions were computed honestly and correctly. To summarize, a transaction is executed in two contexts: once by miners before attempting to append a block to the blockchain, and many times afterward by validators checking that each block in the blockchain is honest. In both contexts, each block’s transactions are executed sequentially in block order. Miners are rewarded for blocks that they successfully append to the blockchain, so they have an incentive to increase throughput by parallelizing smart contract executions. But simply executing contracts in parallel without any special precautions won’t work as the contracts may perform conflicting accesses to shared data leading to an inconsistent final state. Validators on the other hand end up performing the vast majority of contract executions and parallel execution here could bring significant benefits if it could be done safely. Speculative smart contracts  We’ve seen previously that writing bug free smart contracts is hard. Clearly, even sequential smart contracts must be written with care, and introducing explicit concurrency to contract programming languages would only make the situation worse. We conclude that concurrent smart contract executions must be serializable: indistinguishable, except for execution time, from a sequential execution. Smart contracts read and modify shared storage, and they are written in Turing-complete languages – so it is impossible in the general case to determine statically whether or not contracts have data conflicts. Instead, contracts can be instrumented to detect synchronization conflicts at runtime, in a manner similar to that done in transaction boosting. Contracts are executed speculatively, and if a conflict does occur at runtime the conflict is resolved either by delaying one contract until the other completes, or rolling back and restarting one of the conflicting executions. Speculation is controlled by two run-time mechanism, invisible to the programmer, and managed by the virtual-machine: abstract locks and inverse logs. Storage operations are protected by abstract locks. If two storage operations map to distinct locks, then they must commute. Or to put it another way, operations that don’t commute must be protected by the same lock. It wasn’t clear to me from reading the paper whether this mapping of operations to locks can be performed automatically, or whether it requires human intervention. Before executing the operation, a thread must acquire the associated lock. When the lock is acquired, it records an inverse operation in a log (think undo log), and then proceeds with the operation. If the action commits, its abstract locks are released and its log is discarded. If the action aborts, the inverse log is replayed, most recent operations first, to undo the effects of that speculative action. When the replay is complete, the actions’ abstract locks are released. The advantage of combining abstract locks with inverse logs is that the virtual machine can support very fine-grained concurrency. If one contract calls another, a nested speculative action is created. At the end of this process, the miner will have discovered a concurrent schedule for a block’s transactions, that is equivalent to some sequential schedule, only faster. Validation  So far so good for the miners, but not so great for the validators. The problem is that the validators need to produce the same or an equivalent schedule of execution to that discovered by the  miner. The solution is for miners to produce and publish extra information concerning the constraints discovered during execution. Why would miners make this available? … that block [produced by the miner] may be competing with other blocks produced at the same time, and the miner will be rewarded only if the other miners choose to build on that block. Publishing a block with a parallel validation schedule makes the block more attractive for validation by other miners. Here’s how it works:  Each lock includes a use counter keeping track of the number of times it has been released by a committing action during the construction of the current block. When a speculative action commits, it increments the counters for each of the locks it holds, and registers a lock profile with the VM recording the abstract locks and their counter values. When all the actions have committed, the common schedule can be reconstructed by comparing lock profiles. It is these profiles that the miner includes in the blockchain along with the usual information. For example, consider three committed speculative actions, A, B, and C. If A and B have no abstract locks in common, they can run concurrently. If an abstract lock has counter value 1 in A’s profile and 2 in C’s profile, then C must be scheduled after A. Using the algorithm below, a validator can construct a simple fork-join program that deterministically reproduces the miner’s original speculative schedule. Using a work-stealing scheduler, the validator can exploit whatever degree of parallelism it has available. The validator keeps a thread-local trace of the abstract locks each thread would have acquired. If these traces don’t match the lock profiles provided by the miner the block is rejected. Is it safe? Correctness is argued by appeal to the analogy with transactional boosting, where serial equivalence has been proven. Experimental results  The authors built an implementation based on the JVM for experimental purposes, using the Scala STM library for speculative action execution. Examples of smart contracts were translated from Solidity into Scala, the modified to use the concurrency libraries. Each function from the Solidy contract is turned into a speculative transaction by wrapping its contents with a ScalaSTM atomic section. Solidity mapping objects are implemented as boosted hashtables, where key values are used to index abstract locks. The benchmarks are based on Ballot, SimpleAuction, and EtherDoc contracts, as well as a workload mixing all three. The experiments used only three concurrent threads, but this was still sufficient to show a benefit:  The charts below give more detail of the speedups obtained at different conflict levels. Our proposal for miners only is compatible with current smart contract systems such as Ethereum, but our overall proposal is not, because it requires including scheduling metadata in blocks and incentivizing miners to publish their parallel schedules. It may well be compatible with a future “soft fork” (backward compatible change), a subject for future research.", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3087801.3087835?download=true", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/adding-concurrency-to-smart-contracts.json"}
{"id": "16027177", "bin": "1400_1500", "summary_sentences": ["Automated localization for unreproducible builds Ren et al., ICSE’18  Reproducible builds are an important component of integrity in the software supply chain.", "Attacks against package repositories and build environments may compromise binaries and produce packages with backdoors (see this report for a recent prominent example of compromised packages on DockerHub).", "If the same source files always lead to the same binary packages, then an infected binary can be much more easily detected.", "Unfortunately, reproducible builds have not traditionally been the norm.", "Non-determinism creeping into build processes means that rebuilding an application from the exact same source, even within a secure build environment, can often lead to a different binary.", "Due to the significant benefits, many open-source software repositories have initiated their validation processes.", "These repositories include GNU/Linux distributions such as Debian and Guix, as well as software systems like Bitcoin.", "If you have a non-reproducible build, finding out why can be non-trivial.", "It takes time and a lot of effort to hunt down and eradicate  the causes.", "For example, Debian unstable for AMD64 still had 2,342 packages with non-reproducible builds as of August 2017.", "(The number today as I’m writing this is 2,826).", "You can see a stubbornly persistent group of unreproducible builds in this screen grab from tests.reproducible-builds.org :  screenshot  RepLoc is a tool for highlighting the files within a package which are the likely root cause of non-reproducible builds.", "Tested against 671 unreproducible Debian packages, it achieves a Top-1 accuracy rate of 47.09%, and at Top-10 accuracy rate of 79.28%.", "That’s a significant aid to developers looking at cryptic messages from the builds of packages that may include many hundreds of files.", "With the help of RepLoc, the authors also identified and fixed non-reproducibility issues in 6 packages from Debian and Guix.", "Detecting and diagnosing unreproducible builds  The Debian workflow for testing build reproducibility looks like this:  The source is built in two different environments, deliberately constructed with different environment variables and software configurations.", "If the two resulting binaries are not identical, the package is flagged for human inspection, a process called localization which seeks to localise the causes of unreproducible builds.", "One of the major inputs to this process is the diff logs, as generated by diffoscope .", "Those logs produce output that looks like this:  Here we can see diffoscope highlighting a difference in libcompat.a.", "In this case, the root cause is in the Makefile:  Can you spot it?", "The root issue is the unstable ordering of files passed to the ar utility to generate libcompat.a (wildcard libcompat/*.c).", "Here’s a patch that fixes the issue.", "In general there are many possible causes of unreproducible builds including timezones (making e.g. anything that uses the __DATE__ macro to be unreproducible ) and locale settings (making e.g.", "capture of output text unreproducible).", "Introducing RepLoc  RepLoc begins with the build logs and diff log created by diffoscope, and seeks to automatically highlight the most likely files in which the root cause can be found.", "RepLoc’s Query Augmentation (QA) component uses information from the logs to refine queries that help to pinpoint likely causes.", "The Heuristic Filtering component embodies 14 hand-coded heuristic rules that further help to highlight possible causes.", "The combined outputs are passed to ranking component to produce the list of ranked likely-culprit source files as the final output.", "Query Augmentation  The files names highlighted in the diff log are used as the initial seed query set.", "The build logs contain additional information relating to these files, for example:  The QA component splits the build logs into directory sections passed on the ‘Entering / Leaving directory’ messages in the logs.", "Each directory section thus contains a set of commands, which is denoted a ‘command file’ in the paper.", "TF/IDF is used to assign a weight value to each command file to assess it’s relevance to the initial seed queries.", "The commands from the most relevant command files are then added to the query set, to produce an augmented query set.", "(In the example above, the ar cru bin-x86_64/libcompat.a... command causes us to add the content of this command file).", "Heuristic filtering  The authors extract fourteen heuristic rules from Debian’s documentation .", "These rules are encoded as Perl regular expressions(!", "), as summarised in the table below.", "The time and date rules look for uses of the __TIME__ and __DATE__ macros.", "The gzip rule (3) looks for uses of gzip without the -n argument (in which case gzip embeds timestamps in the resulting archive).", "The date cmd rule (4) looks for capture of the current date using the date shell command  PL_LOCALTIME looks for Perl scripts capturing date and time  DATE_IN_TEX looks for date embedding in tex files  SORT_IN_PIPE captures cases where sort is used without a local set  TAR_GZIP_PIPE looks for tar and gzip executed in a pipeline  PL_UNSORTED_KEY catches traversal of unsorted hash keys in Perl  LS_WITHOUT_LOCALE captures cases where ls is used without a locale  UNSORTED_WILDCARD looks for the use of wildcard in Makefiles without sorting  With the rules in hand, it’s a simple matter of running grep over the source tree.", "Heuristic filtering has good recall, but poor precision (i.e., it can produce a lot of false positives).", "Ranking  We can compute the cosine similarity (using TF/IDF) between each package file and the augmented queries to produce a ranked list of candidate files from the QA module.", "These are then combined with the files highlighted by the HF module to give a simple weighted score:  Where QA(f) is the similarity score produced by the QA module, and HF(f) is 1 if the HF module flagged the file, and 0 otherwise.", "α is a configurable parameter to tune the balance between the two terms.", "RepLoc in action  At the time of the study, Debian had 761 packages with accompanying patches that turned unreproducible builds into reproducible ones.", "This constitutes the ground truth dataset for RepLoc.", "The dataset is further divided into four subsets based on the major causes of non-reproducibility.", "The table below summarises how well RepLoc gets on.", "Concentrate on the bottom line (RepLoc) in each row (the other lines show how RepLoc behaves with different subsets of its modules).", "A@N is a top-N accuracy rate score, defined as the percentage of top-N ranked file lists produced by RepLoc that contain at least one problematic file (from the patches).", "P@N is a top-N precision score, defined as the percentage of files reported in a top-N list that are genuinely problematic  R@N is at top-N recall score, defined as the percentage of all problematic files that are successfully identified in a top-N list.", "Overall, RepLoc achieves an average accuracy score of 79% for Top-10 files.", "I.e., if you examine the first ten files RepLoc highlights, you have a 79% chance of finding an issue causing an unreproducible build in at least one of them.", "You will also find on average 75% of all the files with reproducibility problems by the time you have worked through that top-10 list.", "The authors then used RepLoc to see if they could find the root causes of unreproducible builds for packages where no ground truth was available (i.e., there was no-known reproducible build process for them).", "Three packages from Debian are fixed (regina-rexx, fonts-uralic, and manpages-tr).", "The problematic files are right at the top of list produced by RepLoc.", "Three packages from Guix are also fixed (libjpeg-turbo, djvulibre, and skalibs).", "Once more the problematic files are right at the top of the list produced by RepLoc.", "Future work  For the future work, we are interested in the localization of problematic files for tool-chain related issues.", "Also, inspired by record-and-play techniques from crash reproduction based debugging research, it would be interesting to leverage these techniques to detect more accurate correspondence between the build commands executed and the built binaries."], "summary_text": "Automated localization for unreproducible builds Ren et al., ICSE’18  Reproducible builds are an important component of integrity in the software supply chain. Attacks against package repositories and build environments may compromise binaries and produce packages with backdoors (see this report for a recent prominent example of compromised packages on DockerHub). If the same source files always lead to the same binary packages, then an infected binary can be much more easily detected. Unfortunately, reproducible builds have not traditionally been the norm. Non-determinism creeping into build processes means that rebuilding an application from the exact same source, even within a secure build environment, can often lead to a different binary. Due to the significant benefits, many open-source software repositories have initiated their validation processes. These repositories include GNU/Linux distributions such as Debian and Guix, as well as software systems like Bitcoin. If you have a non-reproducible build, finding out why can be non-trivial. It takes time and a lot of effort to hunt down and eradicate  the causes. For example, Debian unstable for AMD64 still had 2,342 packages with non-reproducible builds as of August 2017. (The number today as I’m writing this is 2,826). You can see a stubbornly persistent group of unreproducible builds in this screen grab from tests.reproducible-builds.org :  screenshot  RepLoc is a tool for highlighting the files within a package which are the likely root cause of non-reproducible builds. Tested against 671 unreproducible Debian packages, it achieves a Top-1 accuracy rate of 47.09%, and at Top-10 accuracy rate of 79.28%. That’s a significant aid to developers looking at cryptic messages from the builds of packages that may include many hundreds of files. With the help of RepLoc, the authors also identified and fixed non-reproducibility issues in 6 packages from Debian and Guix. Detecting and diagnosing unreproducible builds  The Debian workflow for testing build reproducibility looks like this:  The source is built in two different environments, deliberately constructed with different environment variables and software configurations. If the two resulting binaries are not identical, the package is flagged for human inspection, a process called localization which seeks to localise the causes of unreproducible builds. One of the major inputs to this process is the diff logs, as generated by diffoscope . Those logs produce output that looks like this:  Here we can see diffoscope highlighting a difference in libcompat.a. In this case, the root cause is in the Makefile:  Can you spot it? The root issue is the unstable ordering of files passed to the ar utility to generate libcompat.a (wildcard libcompat/*.c). Here’s a patch that fixes the issue. In general there are many possible causes of unreproducible builds including timezones (making e.g. anything that uses the __DATE__ macro to be unreproducible ) and locale settings (making e.g. capture of output text unreproducible). Introducing RepLoc  RepLoc begins with the build logs and diff log created by diffoscope, and seeks to automatically highlight the most likely files in which the root cause can be found. RepLoc’s Query Augmentation (QA) component uses information from the logs to refine queries that help to pinpoint likely causes. The Heuristic Filtering component embodies 14 hand-coded heuristic rules that further help to highlight possible causes. The combined outputs are passed to ranking component to produce the list of ranked likely-culprit source files as the final output. Query Augmentation  The files names highlighted in the diff log are used as the initial seed query set. The build logs contain additional information relating to these files, for example:  The QA component splits the build logs into directory sections passed on the ‘Entering / Leaving directory’ messages in the logs. Each directory section thus contains a set of commands, which is denoted a ‘command file’ in the paper. TF/IDF is used to assign a weight value to each command file to assess it’s relevance to the initial seed queries. The commands from the most relevant command files are then added to the query set, to produce an augmented query set. (In the example above, the ar cru bin-x86_64/libcompat.a... command causes us to add the content of this command file). Heuristic filtering  The authors extract fourteen heuristic rules from Debian’s documentation . These rules are encoded as Perl regular expressions(! ), as summarised in the table below. The time and date rules look for uses of the __TIME__ and __DATE__ macros. The gzip rule (3) looks for uses of gzip without the -n argument (in which case gzip embeds timestamps in the resulting archive). The date cmd rule (4) looks for capture of the current date using the date shell command  PL_LOCALTIME looks for Perl scripts capturing date and time  DATE_IN_TEX looks for date embedding in tex files  SORT_IN_PIPE captures cases where sort is used without a local set  TAR_GZIP_PIPE looks for tar and gzip executed in a pipeline  PL_UNSORTED_KEY catches traversal of unsorted hash keys in Perl  LS_WITHOUT_LOCALE captures cases where ls is used without a locale  UNSORTED_WILDCARD looks for the use of wildcard in Makefiles without sorting  With the rules in hand, it’s a simple matter of running grep over the source tree. Heuristic filtering has good recall, but poor precision (i.e., it can produce a lot of false positives). Ranking  We can compute the cosine similarity (using TF/IDF) between each package file and the augmented queries to produce a ranked list of candidate files from the QA module. These are then combined with the files highlighted by the HF module to give a simple weighted score:  Where QA(f) is the similarity score produced by the QA module, and HF(f) is 1 if the HF module flagged the file, and 0 otherwise. α is a configurable parameter to tune the balance between the two terms. RepLoc in action  At the time of the study, Debian had 761 packages with accompanying patches that turned unreproducible builds into reproducible ones. This constitutes the ground truth dataset for RepLoc. The dataset is further divided into four subsets based on the major causes of non-reproducibility. The table below summarises how well RepLoc gets on. Concentrate on the bottom line (RepLoc) in each row (the other lines show how RepLoc behaves with different subsets of its modules). A@N is a top-N accuracy rate score, defined as the percentage of top-N ranked file lists produced by RepLoc that contain at least one problematic file (from the patches). P@N is a top-N precision score, defined as the percentage of files reported in a top-N list that are genuinely problematic  R@N is at top-N recall score, defined as the percentage of all problematic files that are successfully identified in a top-N list. Overall, RepLoc achieves an average accuracy score of 79% for Top-10 files. I.e., if you examine the first ten files RepLoc highlights, you have a 79% chance of finding an issue causing an unreproducible build in at least one of them. You will also find on average 75% of all the files with reproducibility problems by the time you have worked through that top-10 list. The authors then used RepLoc to see if they could find the root causes of unreproducible builds for packages where no ground truth was available (i.e., there was no-known reproducible build process for them). Three packages from Debian are fixed (regina-rexx, fonts-uralic, and manpages-tr). The problematic files are right at the top of list produced by RepLoc. Three packages from Guix are also fixed (libjpeg-turbo, djvulibre, and skalibs). Once more the problematic files are right at the top of the list produced by RepLoc. Future work  For the future work, we are interested in the localization of problematic files for tool-chain related issues. Also, inspired by record-and-play techniques from crash reproduction based debugging research, it would be interesting to leverage these techniques to detect more accurate correspondence between the build commands executed and the built binaries.", "pdf_url": "https://arxiv.org/pdf/1803.06766", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/automated-localization-for-unreproducible-builds.json"}
{"id": "13793499", "bin": "1400_1500", "summary_sentences": ["Three years of the Right To Be Forgotten Bertram et al., 2018  With thanks to Elie Bursztein for bringing this paper to my attention.", "See also Elie’s blog post ‘ Insights about the first three years of the Right To Be Forgotten requests at Google .’  Following on from the GDPR we looked at yesterday, and which comes into force in May of this year, I thought it would be interesting to take a look at another right to be forgotten (RTBF) that has been in force since May 2014.", "Today’s paper choice is a retrospective study from Google of the 2.4M URLs that were requested for delisting from the Google search engine over the last 3 and a half years.", "This particular right to be forgotten enables individuals to request that search engines delist URLs containing “inaccurate, inadequate, irrelevant or excessive” information surfaced by queries containing the name of the requestor.", "Critically, the ruling requires that search engine operators make the determination for whether an individual’s right to privacy outweighs the public’s right to access lawful information when delisting URLs.", "That’s a lot of responsibility to place with private groups within search engine companies!", "Google formed an advisory council drawn from academic scholars, media producers, data protection authorities, civil society, and technologists to establish decision criteria for “particularly challenging delisting requests.”  Google make a RTBF submission form available online.", "Requestors must verify their identity and provide a list of URLs they would like to delist along with the search queries leading to those URLs and a short comment about how the URLs relate to the requestor.", "Every submission is manually reviewed – there is no automation in the decision process.", "The reviewers consider four criteria, designed to weigh public interest against the requestor’s personal privacy:  The validity of the request: is it actionable (e.g., specifies exact URLs) and is the requestor connected with an EU/EEA country.", "The identity of the requestor: for example, if the requestor is a public figure there may be heightened public interest.", "Other categories of interest include minors, politicians, and professionals.", "The content referenced by the URL.", "“For example, information related to a requestor’s business may be of public interest for potential customers.", "Similarly, content related to a violent crime may be of interest to the general public… “  The source of the information: e.g., government site, news site, blog, or forum.", "For the period between May 30th 2014, and December 31st 2017, Google received requests to delist almost 2.4M URLs, from 399,779 unique requestors.", "Only 43% of these URLs were ultimately delisted.", "From January 22nd 2016, requested URLs were additionally annotated with categorical data for purposes of improving transparency around RTBF requests.", "Applying judgement  The paper contains a sprinkling of anonymous requests and the decisions reached, which provide good insight into the challenges the reviewers face.", "Here are some examples:  “… an individual, who was convicted for two separate instances of domestic violence within the previous five years, sent Google a delisting request focusing on the fact that their first conviction was ‘spent’ under local law.", "The requestor did not disclose that the second conviction was not similarly spent, and falsely attributed all the pages sought for delisting to the first conviction.", "Reviewers discovered this as part of the review process and the request was ultimately rejected.”  “In another case, reviewers first delisted 150 URLs submitted by a businessman who was convicted for benefit fraud, after they provided documentation confirming their acquittal.", "When the same person later requested the delisting of URLs related to a conviction for manufacturing court documents about their acquittal, reviewers evaluated the acquittal documentation sent to Google, found it to be a forgery, and reinstated all previously delisted URLs”.", "“…a requestor who held a significant position at a major company sought to delist an article about receiving a long prison sentence for attempted fraud.", "Google rejected this request due to the seriousness of the crime and the professional relevance of the content.”  “…an individual sought to delist an interview they conducted after surviving a terrorist attack.", "Despite the article’s self-authored nature given the requestor was interviewed [note the assumption here that the journalist gave a fair impression of what the interviewee actually said!", "], Google delisted the URL as the requestor was a minor and because of the sensitive nature of the content.”  “… a requestor sought to delist a news article about their acquittal for domestic violence on the grounds that no medical report was presented to the judge confirming the victim’s injuries.", "Given that the requestor was acquitted, Google delisted the article.", "**”  Welcome to the court of social reputation!", "How often are requests made and who makes them?", "Overall, the number of RTBF requests per month has been slowly declining after an initial peak when the facility was first launched.", "The number of previously unseen requestors per month is also declining.", "The most requests originate in France, Germany, and the United Kingdom, though if we look at per capita rates Estonia tops the table.", "Most interestingly, reputation management is clearly a growing business (see also Black Mirror: ‘nosedive’):  The top thousand requesters (0.25% of all requesters) generated 14.6% of requests and 20.8% of delistings.", "These mostly included law firms and reputation management agencies, as well as some requestors with a sizable online presence… while hundreds of thousands of Europeans rely on the RTBF to delist a handful of URLs, there are thousands of entities using the RTBF to alter hundreds of URLs about them or their clients that appear in search results.", "Most requested for delisting, by an order of magnitude,  are URLs concerning private individuals:  Requests predominantly come from private individuals (88%).", "Politicians and government officials requested delisting of 33,937 URLs, and non-governmental public figures another 41,213 URLs.", "Over 77% of requests to delist URLs rooted in a country code top-level domain come from requestors in the same country.", "Requests now take a median of 4 days to process.", "What content do people request delisting for?", "The major categories of sites that contains URLs targeted for delisting are social media sides, directory sites aggregating contact details and personal content, and news sites.", "It seems that people don’t want you to read bad things about them on Facebook!", "Here are the most requested sites by category:  Different countries show different characteristics, which can be explained by, for example, the character of the news organisations in those countries, and the role of the government in publishing information.", "In Italy and the UK, requestors target news media much more than in Germany and France for example.", "Journalists in the former countries are prone to reveal the identity of individuals, whereas those in the latter tend to anonymise parties in their coverage of crimes.", "Requestors in France and Germany target social media and directory services more than average.", "In Spain there is a higher proportion of requests targeting government records.", "Spanish law requires the government to publish ‘edictos’ and ‘indultos.’ “_The former are public notifications to inform missing individuals about a government decision that directly affects them; the latter are government decisions to absolve an individual from a criminal sentence or to commute to a lesser one.”  Looking at the content at the URLs requested for delisting, we find that most pages contain professional information, though it’s interesting to see self-authored content in the number two spot!", "The most commonly requested content related to professional information, which rarely met the criteria for delisting (16.7%).", "Many of these requests pertain to information which is directly relevant or connected to the requestor’s current profession and is therefore in the public interest to have indexed in Google Search."], "summary_text": "Three years of the Right To Be Forgotten Bertram et al., 2018  With thanks to Elie Bursztein for bringing this paper to my attention. See also Elie’s blog post ‘ Insights about the first three years of the Right To Be Forgotten requests at Google .’  Following on from the GDPR we looked at yesterday, and which comes into force in May of this year, I thought it would be interesting to take a look at another right to be forgotten (RTBF) that has been in force since May 2014. Today’s paper choice is a retrospective study from Google of the 2.4M URLs that were requested for delisting from the Google search engine over the last 3 and a half years. This particular right to be forgotten enables individuals to request that search engines delist URLs containing “inaccurate, inadequate, irrelevant or excessive” information surfaced by queries containing the name of the requestor. Critically, the ruling requires that search engine operators make the determination for whether an individual’s right to privacy outweighs the public’s right to access lawful information when delisting URLs. That’s a lot of responsibility to place with private groups within search engine companies! Google formed an advisory council drawn from academic scholars, media producers, data protection authorities, civil society, and technologists to establish decision criteria for “particularly challenging delisting requests.”  Google make a RTBF submission form available online. Requestors must verify their identity and provide a list of URLs they would like to delist along with the search queries leading to those URLs and a short comment about how the URLs relate to the requestor. Every submission is manually reviewed – there is no automation in the decision process. The reviewers consider four criteria, designed to weigh public interest against the requestor’s personal privacy:  The validity of the request: is it actionable (e.g., specifies exact URLs) and is the requestor connected with an EU/EEA country. The identity of the requestor: for example, if the requestor is a public figure there may be heightened public interest. Other categories of interest include minors, politicians, and professionals. The content referenced by the URL. “For example, information related to a requestor’s business may be of public interest for potential customers. Similarly, content related to a violent crime may be of interest to the general public… “  The source of the information: e.g., government site, news site, blog, or forum. For the period between May 30th 2014, and December 31st 2017, Google received requests to delist almost 2.4M URLs, from 399,779 unique requestors. Only 43% of these URLs were ultimately delisted. From January 22nd 2016, requested URLs were additionally annotated with categorical data for purposes of improving transparency around RTBF requests. Applying judgement  The paper contains a sprinkling of anonymous requests and the decisions reached, which provide good insight into the challenges the reviewers face. Here are some examples:  “… an individual, who was convicted for two separate instances of domestic violence within the previous five years, sent Google a delisting request focusing on the fact that their first conviction was ‘spent’ under local law. The requestor did not disclose that the second conviction was not similarly spent, and falsely attributed all the pages sought for delisting to the first conviction. Reviewers discovered this as part of the review process and the request was ultimately rejected.”  “In another case, reviewers first delisted 150 URLs submitted by a businessman who was convicted for benefit fraud, after they provided documentation confirming their acquittal. When the same person later requested the delisting of URLs related to a conviction for manufacturing court documents about their acquittal, reviewers evaluated the acquittal documentation sent to Google, found it to be a forgery, and reinstated all previously delisted URLs”. “…a requestor who held a significant position at a major company sought to delist an article about receiving a long prison sentence for attempted fraud. Google rejected this request due to the seriousness of the crime and the professional relevance of the content.”  “…an individual sought to delist an interview they conducted after surviving a terrorist attack. Despite the article’s self-authored nature given the requestor was interviewed [note the assumption here that the journalist gave a fair impression of what the interviewee actually said! ], Google delisted the URL as the requestor was a minor and because of the sensitive nature of the content.”  “… a requestor sought to delist a news article about their acquittal for domestic violence on the grounds that no medical report was presented to the judge confirming the victim’s injuries. Given that the requestor was acquitted, Google delisted the article. **”  Welcome to the court of social reputation! How often are requests made and who makes them? Overall, the number of RTBF requests per month has been slowly declining after an initial peak when the facility was first launched. The number of previously unseen requestors per month is also declining. The most requests originate in France, Germany, and the United Kingdom, though if we look at per capita rates Estonia tops the table. Most interestingly, reputation management is clearly a growing business (see also Black Mirror: ‘nosedive’):  The top thousand requesters (0.25% of all requesters) generated 14.6% of requests and 20.8% of delistings. These mostly included law firms and reputation management agencies, as well as some requestors with a sizable online presence… while hundreds of thousands of Europeans rely on the RTBF to delist a handful of URLs, there are thousands of entities using the RTBF to alter hundreds of URLs about them or their clients that appear in search results. Most requested for delisting, by an order of magnitude,  are URLs concerning private individuals:  Requests predominantly come from private individuals (88%). Politicians and government officials requested delisting of 33,937 URLs, and non-governmental public figures another 41,213 URLs. Over 77% of requests to delist URLs rooted in a country code top-level domain come from requestors in the same country. Requests now take a median of 4 days to process. What content do people request delisting for? The major categories of sites that contains URLs targeted for delisting are social media sides, directory sites aggregating contact details and personal content, and news sites. It seems that people don’t want you to read bad things about them on Facebook! Here are the most requested sites by category:  Different countries show different characteristics, which can be explained by, for example, the character of the news organisations in those countries, and the role of the government in publishing information. In Italy and the UK, requestors target news media much more than in Germany and France for example. Journalists in the former countries are prone to reveal the identity of individuals, whereas those in the latter tend to anonymise parties in their coverage of crimes. Requestors in France and Germany target social media and directory services more than average. In Spain there is a higher proportion of requests targeting government records. Spanish law requires the government to publish ‘edictos’ and ‘indultos.’ “_The former are public notifications to inform missing individuals about a government decision that directly affects them; the latter are government decisions to absolve an individual from a criminal sentence or to commute to a lesser one.”  Looking at the content at the URLs requested for delisting, we find that most pages contain professional information, though it’s interesting to see self-authored content in the number two spot! The most commonly requested content related to professional information, which rarely met the criteria for delisting (16.7%). Many of these requests pertain to information which is directly relevant or connected to the requestor’s current profession and is therefore in the public interest to have indexed in Google Search.", "pdf_url": "https://www.elie.net/static/files/three-years-of-the-right-to-be-forgotten/three-years-of-the-right-to-be-forgotten-paper.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/three-years-of-the-right-to-be-forgotten.json"}
{"id": "39674358", "bin": "1400_1500", "summary_sentences": ["Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization – Satyanarayan et al. 2015  Today’s paper choice combines Event-driven FRP (E-FRP) with dataflow and stream management techniques from the database community to implement declarative interactive visualisations on top of the existing Vega declarative visualisation grammar and supporting runtime .", "As a good example of what’s possible, take a look at this interactive visualization of US airports in the Live Vega Editor :  (Note the “signals” section in the mark-up).", "In contrast with existing reactive visualization toolkits where only interaction events are modeled as time-varying, Reactive Vega features a unified data model in which the input data, scene graph elements, and interaction events are all treated as first-class streaming data sources.", "Functional Reactive Programming (FRP) models mutable values as continuous, time-varying data streams.", "We focus on a discrete variant called Event-Driven FRP (E-FRP).", "To capture value changes as they occur, E-FRP provides streams, which are infinite time-ordered sequences of discrete events.", "Streams can be composed into signals to build expressions that react to events.", "The E-FRP runtime constructs the necessary dataflow graph such that, when a new event fires, it propagates to corresponding streams.", "Dependent signals are evaluated in a two-phase update: signals reevaluated in the first phase use prior computed values of their dependents, which are subsequently updated in the second phase.", "To efficiently support relational data, Reactive Vega integrates methods from the streaming database literature (Aurora, Eddies, STREAM, TelegraphCQ, Borealis).", "And to support streaming hierarchical data, Reactive Vega’s dataflow graph dynamically rewrites itself at runtime, instantiating new branches to process nested relations.", "In Vega’s declarative visualization design, visual encodings are defined by composing graphical primitives called marks (arcs, bars, lines, symbols and text for example).", "Marks are associated with datasets, and their specifications describe how tuple values map to visual properties such as position and color.", "Scales and guides (i.e., axes and legends) are pro- vided as first-class primitives for mapping a domain of data values to a range of visual properties.", "Special group marks serve as containers to express nested or small multiple displays.", "Child marks and scales can inherit a group mark’s data, or draw from independent datasets.", "Here’s a declarative specification for a brushing interaction:  Our approach draws on Event- Driven Functional Reactive Programming (E-FRP) to abstract input events as time-varying streaming data.", "An event selector syntax facilitates composing and sequencing events together, for example ‘[mousedown, mouseup] > mousemove’ is a single stream of mousemove events that occur between a mousedown and mouseup (i.e., “drag” events).", "Event streams are modeled as first-class data sources and can thus drive visual encoding primitives, or be run through the full gamut of data transformations.", "For added expressivity, event streams can be composed into reactive expressions called signals.", "Signals can be used directly to specify visual primitive properties.", "For example, a signal can dynamically determine a mark’s fill color or a scale’s domain.", "Signals can also parameterize interactive selection rules for visual elements called predicates.", "Predicates define membership within the selection (e.g., by specifying the conditions that must hold true) and can be used within sequences of production rules to drive conditional visual encodings.", "Under the Covers  Dataflow operators are instantiated and connected by the Reactive Vega parser, which traverses a declarative specification containing definitions for input datasets, visual encoding rules, and interaction primitives as described in § 3.", "When data tuples are observed, or when interaction events occur, they are propagated (or “pulsed”) through the graph with each operator being evaluated in turn.", "Propagation ends at the graph’s sole sink: the renderer.", "As each dataset definition is parsed, a corresponding branch in the dataflow graph is constructed.", "The branches contain input and output nodes connected by a pipeline of data transformation operators.", "Input nodes receive raw tuples as a linear stream (tree and graph structures are supported via parent-child or neighbor pointers, respectively).", "Upon data source updates, tuples are flagged as either added, modified, or removed, and each tuple is given a unique identifier.", "Data transformation operators use this metadata to perform targeted computation and, in the process, may derive new tuples from existing ones.", "Derived tuples retain access to their “parent” via prototypal inheritance.", "This relieves operators of the burden of propagating unrelated upstream changes.", "For every low-level event type required by the visualization (e.g. mousedown events) Vega instantiates an event listener node in the dataflow graph and directly connects it to dependent signals.", "In the case of ordered selectors (e.g., a “drag” event specified by ‘[mousedown, mouseup] > mousemove’), each constituent event is connected to an automatically created anonymous signal; an additional anonymous signal connects them to serve as a gatekeeper, and only propagates the final signal value when appropriate.", "Individual signals can be dependent on multiple event nodes and/or other signals, and value propagation follows E-FRP’s two-phase update.", "Generated scene graph elements are themselves modeled as data tuples, and thus can serve as the input data for further downstream visual encoding primitives.", "This enables higher-level layout algorithms to be expressed in a fully declarative fashion.", "The authors describe this as reactive geometry.", "Glitches are avoided through the use of a centralized dataflow graph scheduler that dispatches changesets to appropriate operators in topological order, thus ensuring that an operator is only evaluated after all of its dependencies are up to date.", "This centralization also allows more aggressive pruning of unnecessary computation:  (a) As the scheduler ensures a topological propagation ordering, a branch can be safely pruned for the current propagation if it has already been reflowed… (b) Skipping unchanged operators: Operators identify their dependencies—including signals, data fields, and scale functions—and changesets maintain a tally of updated dependencies as they flow through the graph.", "The scheduler skips evaluation of an individual operator if it is not responsible for deriving new tuples, or if a changeset contains only modified tuples and no dependencies have been updated.", "Both push- and pull-models are used to flow data.", "When an edge connects operators that work with the same data (e.g. a pipeline of data transformations for the same data source) then changesets are pushed along the edge.", "When an edge connects operators with external dependencies such as other data sources, signals, or scale functions then these edges are flagged as reflow changesets.", "External dependencies are connected to Collector nodes along these reflow changeset edges.", "Collectors propagate tuples forward to their dependents, which then request (pull) the latest versions of their dependencies from the scheduler.", "To support streaming nested data structures, operators can dynamically restructure the graph at runtime by extending newbranches, or pruning existing ones, based on observed data.", "These dataflow branches model their corresponding hierarchies as standard relations, thereby enabling subsequent operators to remain agnostic to higher-level structure.", "For example, a Facet operator partitions tuples by key fields; each partiion then propagates down a unique, dynamically-constructed dataflow branch, which can include other operators such as Filter or Sort.", "In order to maintain interactive performance, new branches are queued for evaluation as part of the same propagation in which they were created.", "To ensure changeset propagation continues to occur in topological order, operators are given a rank upon instantiation to uniquely identify their place in the ordering… The most common source of restructuring operations are scene graph operators, as building a nested scene graph is entirely data-driven.", "Reactive Vega is open source and has been merged with the existing Vega project.", "It is available at  [url]"], "summary_text": "Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization – Satyanarayan et al. 2015  Today’s paper choice combines Event-driven FRP (E-FRP) with dataflow and stream management techniques from the database community to implement declarative interactive visualisations on top of the existing Vega declarative visualisation grammar and supporting runtime . As a good example of what’s possible, take a look at this interactive visualization of US airports in the Live Vega Editor :  (Note the “signals” section in the mark-up). In contrast with existing reactive visualization toolkits where only interaction events are modeled as time-varying, Reactive Vega features a unified data model in which the input data, scene graph elements, and interaction events are all treated as first-class streaming data sources. Functional Reactive Programming (FRP) models mutable values as continuous, time-varying data streams. We focus on a discrete variant called Event-Driven FRP (E-FRP). To capture value changes as they occur, E-FRP provides streams, which are infinite time-ordered sequences of discrete events. Streams can be composed into signals to build expressions that react to events. The E-FRP runtime constructs the necessary dataflow graph such that, when a new event fires, it propagates to corresponding streams. Dependent signals are evaluated in a two-phase update: signals reevaluated in the first phase use prior computed values of their dependents, which are subsequently updated in the second phase. To efficiently support relational data, Reactive Vega integrates methods from the streaming database literature (Aurora, Eddies, STREAM, TelegraphCQ, Borealis). And to support streaming hierarchical data, Reactive Vega’s dataflow graph dynamically rewrites itself at runtime, instantiating new branches to process nested relations. In Vega’s declarative visualization design, visual encodings are defined by composing graphical primitives called marks (arcs, bars, lines, symbols and text for example). Marks are associated with datasets, and their specifications describe how tuple values map to visual properties such as position and color. Scales and guides (i.e., axes and legends) are pro- vided as first-class primitives for mapping a domain of data values to a range of visual properties. Special group marks serve as containers to express nested or small multiple displays. Child marks and scales can inherit a group mark’s data, or draw from independent datasets. Here’s a declarative specification for a brushing interaction:  Our approach draws on Event- Driven Functional Reactive Programming (E-FRP) to abstract input events as time-varying streaming data. An event selector syntax facilitates composing and sequencing events together, for example ‘[mousedown, mouseup] > mousemove’ is a single stream of mousemove events that occur between a mousedown and mouseup (i.e., “drag” events). Event streams are modeled as first-class data sources and can thus drive visual encoding primitives, or be run through the full gamut of data transformations. For added expressivity, event streams can be composed into reactive expressions called signals. Signals can be used directly to specify visual primitive properties. For example, a signal can dynamically determine a mark’s fill color or a scale’s domain. Signals can also parameterize interactive selection rules for visual elements called predicates. Predicates define membership within the selection (e.g., by specifying the conditions that must hold true) and can be used within sequences of production rules to drive conditional visual encodings. Under the Covers  Dataflow operators are instantiated and connected by the Reactive Vega parser, which traverses a declarative specification containing definitions for input datasets, visual encoding rules, and interaction primitives as described in § 3. When data tuples are observed, or when interaction events occur, they are propagated (or “pulsed”) through the graph with each operator being evaluated in turn. Propagation ends at the graph’s sole sink: the renderer. As each dataset definition is parsed, a corresponding branch in the dataflow graph is constructed. The branches contain input and output nodes connected by a pipeline of data transformation operators. Input nodes receive raw tuples as a linear stream (tree and graph structures are supported via parent-child or neighbor pointers, respectively). Upon data source updates, tuples are flagged as either added, modified, or removed, and each tuple is given a unique identifier. Data transformation operators use this metadata to perform targeted computation and, in the process, may derive new tuples from existing ones. Derived tuples retain access to their “parent” via prototypal inheritance. This relieves operators of the burden of propagating unrelated upstream changes. For every low-level event type required by the visualization (e.g. mousedown events) Vega instantiates an event listener node in the dataflow graph and directly connects it to dependent signals. In the case of ordered selectors (e.g., a “drag” event specified by ‘[mousedown, mouseup] > mousemove’), each constituent event is connected to an automatically created anonymous signal; an additional anonymous signal connects them to serve as a gatekeeper, and only propagates the final signal value when appropriate. Individual signals can be dependent on multiple event nodes and/or other signals, and value propagation follows E-FRP’s two-phase update. Generated scene graph elements are themselves modeled as data tuples, and thus can serve as the input data for further downstream visual encoding primitives. This enables higher-level layout algorithms to be expressed in a fully declarative fashion. The authors describe this as reactive geometry. Glitches are avoided through the use of a centralized dataflow graph scheduler that dispatches changesets to appropriate operators in topological order, thus ensuring that an operator is only evaluated after all of its dependencies are up to date. This centralization also allows more aggressive pruning of unnecessary computation:  (a) As the scheduler ensures a topological propagation ordering, a branch can be safely pruned for the current propagation if it has already been reflowed… (b) Skipping unchanged operators: Operators identify their dependencies—including signals, data fields, and scale functions—and changesets maintain a tally of updated dependencies as they flow through the graph. The scheduler skips evaluation of an individual operator if it is not responsible for deriving new tuples, or if a changeset contains only modified tuples and no dependencies have been updated. Both push- and pull-models are used to flow data. When an edge connects operators that work with the same data (e.g. a pipeline of data transformations for the same data source) then changesets are pushed along the edge. When an edge connects operators with external dependencies such as other data sources, signals, or scale functions then these edges are flagged as reflow changesets. External dependencies are connected to Collector nodes along these reflow changeset edges. Collectors propagate tuples forward to their dependents, which then request (pull) the latest versions of their dependencies from the scheduler. To support streaming nested data structures, operators can dynamically restructure the graph at runtime by extending newbranches, or pruning existing ones, based on observed data. These dataflow branches model their corresponding hierarchies as standard relations, thereby enabling subsequent operators to remain agnostic to higher-level structure. For example, a Facet operator partitions tuples by key fields; each partiion then propagates down a unique, dynamically-constructed dataflow branch, which can include other operators such as Filter or Sort. In order to maintain interactive performance, new branches are queued for evaluation as part of the same propagation in which they were created. To ensure changeset propagation continues to occur in topological order, operators are given a rank upon instantiation to uniquely identify their place in the ordering… The most common source of restructuring operations are scene graph operators, as building a nested scene graph is entirely data-driven. Reactive Vega is open source and has been merged with the existing Vega project. It is available at  [url]", "pdf_url": "https://idl.cs.washington.edu/files/2015-ReactiveVega-InfoVis.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1400_1500/reactive-vega.json"}
{"id": "87041789", "bin": "1500_1600", "summary_sentences": ["IPA: invariant-preserving applications for weakly consistent replicated databases Balegas et al., VLDB’19  IPA for developers, happy days!", "Last we week looked at automating checks for invariant confluence , and extending the set of cases where we can show that an object is indeed invariant confluent.", "I’m not going to re-cover that background in this write-up, so I suggest you head over there for a quick catch-up before reading on if you missed it first time around.", "Today’s paper is very much in same spirit, building on the same foundation of invariant confluence (I-Confluence) , and also on Indigo which introduced an annotation model for application invariants, a  invariant violation avoidance mechanism using lock reservations and escrows, and limited support for repairing violations that do happen.", "With Invariant-Preserving Applications (IPAs), Balegas et al. introduce new mechanisms for avoiding invariant violations and for repairing them when detected, based on CRDTs.", "There’s also a very nice looking developer workflow to help ensure you’ve got all the bases covered.", "At the end of the day, you get the dual benefit of higher throughput and lower latency (as compared to coordination-based approaches) coupled with knowing that there isn’t some nasty invariant-violating concurrency bug waiting to bite you (so long as you specified your invariants and operation effects correctly of course!).", "Having your cake and eating it too  … it remains difficult to develop applications under weak consistency.", "Several studies show that, in many applications, concurrent executions lead to the violation of application invariants, resulting in inconsistent states.", "At this point you have a few choices:  The common path: turn a blind eye to the possibility and get bitten later on by strange inconsistencies in your data  The belt-and-braces approach: broadly constrain concurrency (i.e. coordinate heavily) in order to avoid the possibility of invariant violations – thus reducing availability and latency  The considered (and time-consuming in development) approach: take time to understand your invariants, figure out which parts of your system can be coordination free, and which parts still need it, and design accordingly.", "For bonus points, revisit your application design so that more parts of it can be coordination free.", "IPA takes the third way of course, and gives you new options for tweaking your application design, so that you can have your cake and eat it too:  This paper proposes a novel approach for preserving application invariants under weak consistency that does not impact the availability and latency of applications… To help programmers adopt our approach, we propose a methodology for modifying applications.", "The key element of the methodology is our invariant-preservation analysis (IPA) and static analysis tool that relies on information about the application, including invariants and operations, to identify which operations might lead to invariant violations and to suggest modifications to the operations to prevent those violations from occurring.", "Coordination avoidance  To avoid coordination overheads we need to ensure that all operations are invariant preserving under merge.", "One of IPA’s neat tricks is allowing invariant violation within a transaction, by also including the logic to repair such a violation if it does occur.", "Our insight is that in many situations the effects to restore the database integrity can be applied preventively alongside the original operations, repairing the invariant violation automatically in a conflicting execution.", "An example makes all this much clearer.", "Consider an e-games platform with players that can take part in tournaments.", "Players can enroll and unenroll for tournaments, and admins can create and delete tournaments.", "But players shouldn’t be able to enroll in tournaments that don’t exist, tournaments may have a maximum capacity for players, and it shouldn’t be possible to delete a tournament that has actively enrolled players.", "What happens if we have concurrent operations where a player enrols for a tournament while an admin is deleting it?", "As-is, the resulting state will violate our invariant.", "But we can ensure any violation will be automatically repaired by adding a new operation and updating merge semantics:  …restoring a tournament to its previous state can be achieved by executing a touch operation in the tournament when executing the enroll, and adopting a conflict resolution policy where the touch wins over a concurrent delete.", "The touch operation has no observable effect, only updating the metadata to guarantee that the concurrent execution is detected and solved according to the defined conflict resolution policy.", "That’s a pretty neat idea!", "Although there are still some open application design questions here for me: for example, how does the admin, who thought they just deleted a tournament, find out that in fact it still exists?", "Shouldn’t they be notified so that they can take some action?", "I.e., there are application reasons why we might want to make compensating actions when a repair occurs.", "So I’d probably want an event I could subscribe to to tell me when such a conflict-repairing merge occurs.", "Compensations / apologies  What about cases where a silent repair isn’t possible or appropriate?", "For that we have compensations.", "With compensations, the idea is to check that the precondition holds when executing the operation in the initial replica, and to check that the invariants hold when operations are integrated remotely or when the state is read.", "Implementations of compensation mechanisms typically require re-executing operations multiple times, or using a leader to order operations, to ensure that replicas converge after applying a compensation.", "We implement compensations without any of these limitations by relying on CRDT convergence rules.", "Applying repairs and compensations  IPA relies on your application being built on top of CRDTs, with operations executed in causal order.", "Additional updates in an operation have to executed atomically  with the rest of the operation to ensure no inconsistencies can be observed.", "“In our prototype, we achieve this by relying on highly available transactions…”  Integrated developer workflow  Onto one of my favourite parts of the paper – the developer workflow!", "It all starts with the developer annotating their object model to specify  the application invariants and operation post-conditions, like this:  Then the IPA tool performs a developer-in-the-loop iterative analysis.", "In each iteration the tool identifies a pair of conflicting operations that might break an invariant when executed concurrently, and then it proposes a set of modifications to the application that will prevent this from happening.", "The developer chooses his or her preferred resolution, and the process repeats until no more conflicting operation pairs remain.", "The analysis returns a new specification of the application, which contains the selected modifications, comprising both the use fo appropriate conflict resolution policies for each object and the modification to operations to avoid invariant violations… Fully patched applications can then execute in any replicated system that provides causal consistency, highly available transactions, and the necessary type-specific conflict resolution policies.", "Here’s the main algorithm used in the tool loop:  Section 5 in the paper goes into a lot more detail on this, and I wish I had the space here to do it justice.", "It’s definitely worth checking out the full paper here if this work interests you.", "Behind the scenes: new CRDTs  To support the CRDT resolutions proposed by IPA, the authors developed new extensions to existing CRDTs:  An extension to Add-wins sets to support the touch operation  An extension to the Rem-wins set to support wildcard values in the remove operation (to support the application model equivalent of  cascading delete)  New compensation CRDTs, for example, Limited Size Set CRDTs that allow a programmer to specify a constraint that must be maintained, and the compensation to execute when the constraint is violated.", "Evaluation  The following table highlights the types of invariants covered by IPA, and their usage in four applications used in the evaluation.", "IPA can make unique identfier, aggregation inclusion, referential integrity and disjunction constraints I-Confluent through repair updates, and can additionally handle numerical and aggregation constraint invariants through compensations.", "Our IPA analysis and tool assist the programmer via static analysis to identify which operations might lead to an invariant violation, when executed concurrently, and by suggesting modifications to the operations.", "Our experimental evaluation shows that the static analysis can handle large applications in reasonable time for an offline process, and that the modified applications have similar performance to their unmodified counterparts that do not preserve invariants."], "summary_text": "IPA: invariant-preserving applications for weakly consistent replicated databases Balegas et al., VLDB’19  IPA for developers, happy days! Last we week looked at automating checks for invariant confluence , and extending the set of cases where we can show that an object is indeed invariant confluent. I’m not going to re-cover that background in this write-up, so I suggest you head over there for a quick catch-up before reading on if you missed it first time around. Today’s paper is very much in same spirit, building on the same foundation of invariant confluence (I-Confluence) , and also on Indigo which introduced an annotation model for application invariants, a  invariant violation avoidance mechanism using lock reservations and escrows, and limited support for repairing violations that do happen. With Invariant-Preserving Applications (IPAs), Balegas et al. introduce new mechanisms for avoiding invariant violations and for repairing them when detected, based on CRDTs. There’s also a very nice looking developer workflow to help ensure you’ve got all the bases covered. At the end of the day, you get the dual benefit of higher throughput and lower latency (as compared to coordination-based approaches) coupled with knowing that there isn’t some nasty invariant-violating concurrency bug waiting to bite you (so long as you specified your invariants and operation effects correctly of course!). Having your cake and eating it too  … it remains difficult to develop applications under weak consistency. Several studies show that, in many applications, concurrent executions lead to the violation of application invariants, resulting in inconsistent states. At this point you have a few choices:  The common path: turn a blind eye to the possibility and get bitten later on by strange inconsistencies in your data  The belt-and-braces approach: broadly constrain concurrency (i.e. coordinate heavily) in order to avoid the possibility of invariant violations – thus reducing availability and latency  The considered (and time-consuming in development) approach: take time to understand your invariants, figure out which parts of your system can be coordination free, and which parts still need it, and design accordingly. For bonus points, revisit your application design so that more parts of it can be coordination free. IPA takes the third way of course, and gives you new options for tweaking your application design, so that you can have your cake and eat it too:  This paper proposes a novel approach for preserving application invariants under weak consistency that does not impact the availability and latency of applications… To help programmers adopt our approach, we propose a methodology for modifying applications. The key element of the methodology is our invariant-preservation analysis (IPA) and static analysis tool that relies on information about the application, including invariants and operations, to identify which operations might lead to invariant violations and to suggest modifications to the operations to prevent those violations from occurring. Coordination avoidance  To avoid coordination overheads we need to ensure that all operations are invariant preserving under merge. One of IPA’s neat tricks is allowing invariant violation within a transaction, by also including the logic to repair such a violation if it does occur. Our insight is that in many situations the effects to restore the database integrity can be applied preventively alongside the original operations, repairing the invariant violation automatically in a conflicting execution. An example makes all this much clearer. Consider an e-games platform with players that can take part in tournaments. Players can enroll and unenroll for tournaments, and admins can create and delete tournaments. But players shouldn’t be able to enroll in tournaments that don’t exist, tournaments may have a maximum capacity for players, and it shouldn’t be possible to delete a tournament that has actively enrolled players. What happens if we have concurrent operations where a player enrols for a tournament while an admin is deleting it? As-is, the resulting state will violate our invariant. But we can ensure any violation will be automatically repaired by adding a new operation and updating merge semantics:  …restoring a tournament to its previous state can be achieved by executing a touch operation in the tournament when executing the enroll, and adopting a conflict resolution policy where the touch wins over a concurrent delete. The touch operation has no observable effect, only updating the metadata to guarantee that the concurrent execution is detected and solved according to the defined conflict resolution policy. That’s a pretty neat idea! Although there are still some open application design questions here for me: for example, how does the admin, who thought they just deleted a tournament, find out that in fact it still exists? Shouldn’t they be notified so that they can take some action? I.e., there are application reasons why we might want to make compensating actions when a repair occurs. So I’d probably want an event I could subscribe to to tell me when such a conflict-repairing merge occurs. Compensations / apologies  What about cases where a silent repair isn’t possible or appropriate? For that we have compensations. With compensations, the idea is to check that the precondition holds when executing the operation in the initial replica, and to check that the invariants hold when operations are integrated remotely or when the state is read. Implementations of compensation mechanisms typically require re-executing operations multiple times, or using a leader to order operations, to ensure that replicas converge after applying a compensation. We implement compensations without any of these limitations by relying on CRDT convergence rules. Applying repairs and compensations  IPA relies on your application being built on top of CRDTs, with operations executed in causal order. Additional updates in an operation have to executed atomically  with the rest of the operation to ensure no inconsistencies can be observed. “In our prototype, we achieve this by relying on highly available transactions…”  Integrated developer workflow  Onto one of my favourite parts of the paper – the developer workflow! It all starts with the developer annotating their object model to specify  the application invariants and operation post-conditions, like this:  Then the IPA tool performs a developer-in-the-loop iterative analysis. In each iteration the tool identifies a pair of conflicting operations that might break an invariant when executed concurrently, and then it proposes a set of modifications to the application that will prevent this from happening. The developer chooses his or her preferred resolution, and the process repeats until no more conflicting operation pairs remain. The analysis returns a new specification of the application, which contains the selected modifications, comprising both the use fo appropriate conflict resolution policies for each object and the modification to operations to avoid invariant violations… Fully patched applications can then execute in any replicated system that provides causal consistency, highly available transactions, and the necessary type-specific conflict resolution policies. Here’s the main algorithm used in the tool loop:  Section 5 in the paper goes into a lot more detail on this, and I wish I had the space here to do it justice. It’s definitely worth checking out the full paper here if this work interests you. Behind the scenes: new CRDTs  To support the CRDT resolutions proposed by IPA, the authors developed new extensions to existing CRDTs:  An extension to Add-wins sets to support the touch operation  An extension to the Rem-wins set to support wildcard values in the remove operation (to support the application model equivalent of  cascading delete)  New compensation CRDTs, for example, Limited Size Set CRDTs that allow a programmer to specify a constraint that must be maintained, and the compensation to execute when the constraint is violated. Evaluation  The following table highlights the types of invariants covered by IPA, and their usage in four applications used in the evaluation. IPA can make unique identfier, aggregation inclusion, referential integrity and disjunction constraints I-Confluent through repair updates, and can additionally handle numerical and aggregation constraint invariants through compensations. Our IPA analysis and tool assist the programmer via static analysis to identify which operations might lead to an invariant violation, when executed concurrently, and by suggesting modifications to the operations. Our experimental evaluation shows that the static analysis can handle large applications in reasonable time for an offline process, and that the modified applications have similar performance to their unmodified counterparts that do not preserve invariants.", "pdf_url": "http://www.vldb.org/pvldb/vol12/p404-balegas.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/ipa.json"}
{"id": "45518901", "bin": "1500_1600", "summary_sentences": ["Secure coding practices in Java: challenges and vulnerabilities Meng et al., ICSE’18  TL;DR : don’t trust everything you read on Stack Overflow.", "Meng et al. conduct a study of Stack Overflow posts relating to secure coding practices in Java to find out the hot topics,  what people struggle with, and whether or not the accepted answers are actually following security best practices.", "We conducted an empirical study on Stack Overflow posts, aiming to understand developer’s concerns on Java secure coding, their programming obstacles, and insecure coding practices.", "We observed a wide adoption of the authentication and authorization features provided by Spring Security — a third-party framework designed to secure enterprise applications…  Well, how could I resist reading that!", "(Some readers may know that I was for many years the CTO of SpringSource).", "Spring Security does come in for some flak in this paper for the high volume of questions that are asked relating to it.", "There’s no calibration though for underlying popularity.", "One of the reasons there are a lot of questions, I posit, is that there are an awful lot of users of Spring Security.", "Spring Boot applications will use Spring Security, and Spring Boot has been growing at an amazing rate these last few years (many millions of downloads every month, and still ticking along at over 300% y-o-y growth ).", "Another reason is that Spring Security has been around a long time, and the survey covers questions going back to 2008, when Spring Security used an older XML-based configuration style.", "Spring Security genuinely was hard to configure in the early days.", "In fact, the origins of Spring Security were in a project called “Acegi” created by the wonderful Ben Alex.", "Ben Alex himself introduced a phrase that became part of the SpringSource folklore when talking about a project to simplify Acegi configuration “whenever someone uses Acegi, a fairy dies.” That was all a long time ago, and the modern Spring Security is a very different beast.", "The authors crawl 22,195 Stack Overflow posts containing the keywords ‘Java’ and ‘security,’ then filtered out those without accepted answers or with negative votes, and those without code snippets.", "After manually inspecting the remaining posts for relevance, the result is a set of 503 posts with dates from 2008 to 2016.", "The posts are then analysed to determine the most common security concerns, programming challenges, and vulnerabilities.", "What are people asking about?", "Most questions are to do with how to get something to work (implementation questions), rather than questions seeking to understand security design.", "The ‘how do I…’ questions were further broken down based on the platform being discussed: Java platform security, Java EE security, or Spring Security.", "Spring Security gets the most questions (56%)!", "Seven major security topics emerge from a second-level classification of the implementation posts:  Java platform: cryptography, access control, & secure communication  Java EE security  Spring Security:  authentication, authorisation, and configuration  We can see the volume of questions in these topic areas growing and the distribution changing over time.", "During 2009-2011, most posts were about Java platform security.", "However, since 2012, the major security concern has shifted to securing enterprise Java applications (including both Java EE security and Spring Security).", "Specifically, Spring Security has taken up over 50% of the posts published every year since 2013.", "Common challenges (over the 8 year period)  Taking the five most popular topic areas (authentication, cryptography, Java EE security, and secure communication), the authors did a further analysis to understand common challenges.", "The biggest topic area by far is authentication (more than all the others combined).", "Recall that the authentication topic is specific to Spring Security.", "Here people want to know (i) how to integrate Spring Security with different application servers and frameworks, (ii) how to configure Spring Security using XML (84 q’s) or Java (42 q’s), and (iii) how to convert XML-based configurations to Java-based ones.", "The Spring family of projects originally all used exclusively XML-based configuration.", "But this has not been true for a long time.", "Today the preferred approach is Java based configuration, which has been supported in Spring Security since the 3.2.0 release in 2013.", "Even so,…  … there are lots of annotations and APIs of classes, methods, and fields available to specify different configuration options… implicit constraints and subtle requirements are not well documented.", "(Here are the latest guides for the 5.0.6 release.", "If you’re starting from scratch, go the Spring Boot way ).", "Developers also struggle converting older XML-based projects to use Java configuration.", "When it comes to cryptography (Java platform), users struggle with poor error messages, implicit constraints, and the difficulties involved in encrypting data using one programming language, and decrypting it in another.", "The cryptography posts were mostly about key generation and usage.", "Developers asked these questions mainly due to clueless error messages, cross-language data handling, and implicit API usage constraints.", "Access control posts mostly concerned how to restrict untrusted code from accessing certain packages, classes, and methods.", "There were also 9 posts on applets, which highlights some of limitations of a study that goes back to 2008.", "The only good answer to a question on applets in 2018 is “don’t use applets!”.", "Security communication posts mainly discussed the process of establishing SSL/TLS connections.", "This process contains so many steps that developers were tempted to accept a broken solution to simply bypass the security verification.", "Vulnerabilities  Does Stack Overflow give good advice?", "Sometimes, yes!", "But… “we identified security vulnerabilities in the accepted answers of three frequently discussed topics: Spring Security’s csrf(), SSL/TLS, and password hashing“.", "A common theme seems to be “my security policy is stopping me doing something,” answered by “disable security!”  Spring Security enables CSRF protection by default, and the corresponding token needs to be included in PATCH, POST, PUT and DELETE methods.", "Fail to do that, and things won’t work as expected.", "Or you could just disable CSRF protection!", "(Don’t).", "In one instance, after accepting the vulnerable solution, an asker commented “Adding csfr().disable() solved the issue!!!", "I have no idea why it was enabled by default.”  In the SSL/TSL topic area certification verification is a pain.", "That doesn’t mean you don’t need to do it though!", "9 out of 10 posts in this area had an accepted answer with an insecure solution bypassing security checks by trusting all certificates and/or allowing all hostnames.", "The implications are not always well understood.", "Disabling the SSL certificate validation process completely destroys the secure communication protocol, leaving clients susceptible to man-in-the-middle (MITM) attacks… A developer justified the verification-bypassing choice by stating “I want my client to accept any certification (because I’m only ever pointing to one server).”  When it comes to password hashing, there’s also a bunch of outdated and wrong advice around.", "3 out of 6 hashing-relevant posts accepted vulnerable solutions as correct answers, indicating that developers were unaware of best secure programming practices.", "Incorrect security information may propagate among Stack Overflow users and negatively influence software development.", "Recommendations  The authors have the following common sense  recommendations to make as a result of their study:  Developers should conduct security testing to check whether features work as expected.", "Security checks should not be disabled – even as a ‘temporary’ fix in dev or test.", "Be careful following advice found on Stack Overflow as some solutions may be out of date or insecure.", "Library designers should deprecate APIs not intended to be used anymore, improve error messages, and design simplified APIs with strong security defenses implemented by default.", "Tool builders can help by creating automatic tools to diagnose security errors, locate buggy code, and suggest security patches or solutions.", "You may not be surprised to hear that…  … our future work is on building automatic or semi-automatic security bug detection and repair tools."], "summary_text": "Secure coding practices in Java: challenges and vulnerabilities Meng et al., ICSE’18  TL;DR : don’t trust everything you read on Stack Overflow. Meng et al. conduct a study of Stack Overflow posts relating to secure coding practices in Java to find out the hot topics,  what people struggle with, and whether or not the accepted answers are actually following security best practices. We conducted an empirical study on Stack Overflow posts, aiming to understand developer’s concerns on Java secure coding, their programming obstacles, and insecure coding practices. We observed a wide adoption of the authentication and authorization features provided by Spring Security — a third-party framework designed to secure enterprise applications…  Well, how could I resist reading that! (Some readers may know that I was for many years the CTO of SpringSource). Spring Security does come in for some flak in this paper for the high volume of questions that are asked relating to it. There’s no calibration though for underlying popularity. One of the reasons there are a lot of questions, I posit, is that there are an awful lot of users of Spring Security. Spring Boot applications will use Spring Security, and Spring Boot has been growing at an amazing rate these last few years (many millions of downloads every month, and still ticking along at over 300% y-o-y growth ). Another reason is that Spring Security has been around a long time, and the survey covers questions going back to 2008, when Spring Security used an older XML-based configuration style. Spring Security genuinely was hard to configure in the early days. In fact, the origins of Spring Security were in a project called “Acegi” created by the wonderful Ben Alex. Ben Alex himself introduced a phrase that became part of the SpringSource folklore when talking about a project to simplify Acegi configuration “whenever someone uses Acegi, a fairy dies.” That was all a long time ago, and the modern Spring Security is a very different beast. The authors crawl 22,195 Stack Overflow posts containing the keywords ‘Java’ and ‘security,’ then filtered out those without accepted answers or with negative votes, and those without code snippets. After manually inspecting the remaining posts for relevance, the result is a set of 503 posts with dates from 2008 to 2016. The posts are then analysed to determine the most common security concerns, programming challenges, and vulnerabilities. What are people asking about? Most questions are to do with how to get something to work (implementation questions), rather than questions seeking to understand security design. The ‘how do I…’ questions were further broken down based on the platform being discussed: Java platform security, Java EE security, or Spring Security. Spring Security gets the most questions (56%)! Seven major security topics emerge from a second-level classification of the implementation posts:  Java platform: cryptography, access control, & secure communication  Java EE security  Spring Security:  authentication, authorisation, and configuration  We can see the volume of questions in these topic areas growing and the distribution changing over time. During 2009-2011, most posts were about Java platform security. However, since 2012, the major security concern has shifted to securing enterprise Java applications (including both Java EE security and Spring Security). Specifically, Spring Security has taken up over 50% of the posts published every year since 2013. Common challenges (over the 8 year period)  Taking the five most popular topic areas (authentication, cryptography, Java EE security, and secure communication), the authors did a further analysis to understand common challenges. The biggest topic area by far is authentication (more than all the others combined). Recall that the authentication topic is specific to Spring Security. Here people want to know (i) how to integrate Spring Security with different application servers and frameworks, (ii) how to configure Spring Security using XML (84 q’s) or Java (42 q’s), and (iii) how to convert XML-based configurations to Java-based ones. The Spring family of projects originally all used exclusively XML-based configuration. But this has not been true for a long time. Today the preferred approach is Java based configuration, which has been supported in Spring Security since the 3.2.0 release in 2013. Even so,…  … there are lots of annotations and APIs of classes, methods, and fields available to specify different configuration options… implicit constraints and subtle requirements are not well documented. (Here are the latest guides for the 5.0.6 release. If you’re starting from scratch, go the Spring Boot way ). Developers also struggle converting older XML-based projects to use Java configuration. When it comes to cryptography (Java platform), users struggle with poor error messages, implicit constraints, and the difficulties involved in encrypting data using one programming language, and decrypting it in another. The cryptography posts were mostly about key generation and usage. Developers asked these questions mainly due to clueless error messages, cross-language data handling, and implicit API usage constraints. Access control posts mostly concerned how to restrict untrusted code from accessing certain packages, classes, and methods. There were also 9 posts on applets, which highlights some of limitations of a study that goes back to 2008. The only good answer to a question on applets in 2018 is “don’t use applets!”. Security communication posts mainly discussed the process of establishing SSL/TLS connections. This process contains so many steps that developers were tempted to accept a broken solution to simply bypass the security verification. Vulnerabilities  Does Stack Overflow give good advice? Sometimes, yes! But… “we identified security vulnerabilities in the accepted answers of three frequently discussed topics: Spring Security’s csrf(), SSL/TLS, and password hashing“. A common theme seems to be “my security policy is stopping me doing something,” answered by “disable security!”  Spring Security enables CSRF protection by default, and the corresponding token needs to be included in PATCH, POST, PUT and DELETE methods. Fail to do that, and things won’t work as expected. Or you could just disable CSRF protection! (Don’t). In one instance, after accepting the vulnerable solution, an asker commented “Adding csfr().disable() solved the issue!!! I have no idea why it was enabled by default.”  In the SSL/TSL topic area certification verification is a pain. That doesn’t mean you don’t need to do it though! 9 out of 10 posts in this area had an accepted answer with an insecure solution bypassing security checks by trusting all certificates and/or allowing all hostnames. The implications are not always well understood. Disabling the SSL certificate validation process completely destroys the secure communication protocol, leaving clients susceptible to man-in-the-middle (MITM) attacks… A developer justified the verification-bypassing choice by stating “I want my client to accept any certification (because I’m only ever pointing to one server).”  When it comes to password hashing, there’s also a bunch of outdated and wrong advice around. 3 out of 6 hashing-relevant posts accepted vulnerable solutions as correct answers, indicating that developers were unaware of best secure programming practices. Incorrect security information may propagate among Stack Overflow users and negatively influence software development. Recommendations  The authors have the following common sense  recommendations to make as a result of their study:  Developers should conduct security testing to check whether features work as expected. Security checks should not be disabled – even as a ‘temporary’ fix in dev or test. Be careful following advice found on Stack Overflow as some solutions may be out of date or insecure. Library designers should deprecate APIs not intended to be used anymore, improve error messages, and design simplified APIs with strong security defenses implemented by default. Tool builders can help by creating automatic tools to diagnose security errors, locate buggy code, and suggest security patches or solutions. You may not be surprised to hear that…  … our future work is on building automatic or semi-automatic security bug detection and repair tools.", "pdf_url": "https://arxiv.org/pdf/1709.09970", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/secure-coding-practices-in-java-challenges-and-vulnerabilities.json"}
{"id": "11791786", "bin": "1500_1600", "summary_sentences": ["Continuous online sequence learning with an unsupervised neural network model Cui et al., Neural Computation, 2016  Yesterday we looked at the biological inspirations for the Hierarchical Temporal Memory (HTM) neural network model .", "Today’s paper demonstrates more of the inner workings, and shows how well HTM networks perform on online sequence learning tasks as compared to other approaches (e.g., LSTM-based networks).", "The HTM model achieves comparable accuracy to other state-of-the-art algorithms.", "The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning.", "The HTM sequence memory model  Recall from yesterday that the HTM sequence memory model is organised around layers of mini-columns of cells.", "The network represents higher-order (dependent on previous time steps) sequences using a composition of two separate sparse representations.", "At the column level, each input element is encoded as a sparse distributed activation of columns: the top 2% of columns that receive the most active feedforward inputs are activated.", "Within an active column, a subset of the cells will be active – if a column contains predicted cells then only these cells will be active, otherwise all cells within the column become active.", "A network has  columns and  neurons per column.", "For each cell in the network we keep track of its activation state and its predictive state at time step  :  The  binary matrix  represents the activation state (on/off) of the cells:  is the activation state of the  th cell in the  th column.", "The  binary matrix  represents the predictive state (on/off) of the cells:  is the predictive state of the  th cell in the  th column.", "Each cell has  dendrite segments.", "To model synapse connections an  matrix  tracks the permanence of the synaptic connection at segment  .", "The values in the matrix are in the range 0 to 1.", "If the permanence value is above some threshold, then the synapse is considered connected.", "For convenience (since it can always be deduced from  ), the binary matrix  denotes the connected synapses.", "At time step  , a cell is in the predictive state if one of its dendritic segments receives enough input (is connected to enough active cells):  Threshold  represents the segment activation threshold and  represents element-wise multiplication.", "At each time  , columns are ordered based on their number of active proximal synapses, and the top 2% are activated.", "This set is denoted as  .", "In theory the proximal  synapse connections can also be adapted continuously during learning, but for this paper the proximal synapse connections are initialised such that each column is randomly connected to 50% of the inputs, and then fixed during learning.", "A cell is considered active if its column is active and it was in predictive state during the previous time step, or its column is active and none of the other cells in the same column were in a predictive state:  Learning is local and straight forward:  The lateral connections in the sequence memory model are learned using a Hebbian-like rule.", "Specifically, if a cell is depolarized and subsequently becomes active, we reinforce the dendritic segment that caused the depolarization.", "If no cell in ac active column is predicted, we select the cell with the most activated segment and reinforce that segment.", "Reinforcement of a dendritic segment involves decreasing permanence values of inactive synapses by a small value  and increasing the permanence for active synapses by a larger value  .", "Where  is a binary matrix with entries set to 1 iff the corresponding entry in  is positive.", "Cell that don’t become active also receive a very small decay.", "The sequence memory operates with sparse distributed representations (SDRs).", "Original data is converted to SDRs using an encoder (in this case, a random encoder for categorical data, and scalar date-time encoders for a taxi data experiment).", "We can’t use a simple one-hot encoding for categorical data because we also want to able to provide noise inputs drawn from a very large distribution.", "Decoding from SDRs is done using classifiers.", "Real-time streaming data analysis  In this paper HTMs are applied to real-time streaming data pattern matching problems where they have the following desirable characteristics:  they can learn online in a single pass without requiring a buffered data set  they can make higher-order predictions across multiple time steps, learning the order (number of previous steps to consider) automatically  they can make multiple simultaneous predictions in the case of ambiguity  they are robust to the loss of synapses and neurons (important for hardware implementations)  they do not require any hyperparameter tuning  Sequence prediction with artificial data  The first test compares the HTM sequence memory model with online sequential extreme machine learning (ELM), a time-delayed neural network (TDNN), and LSTMs.", "A data set is generated with sequences that require a network to maintain a context of at least the first two elements of a sequence in order to correctly predict the last element.", "Since the sequences are presented in a streaming fashion, and predictions are required continuously, this task represents a continuous online learning problem.", "For the LSTMs, retraining is done at regular intervals on a buffered data set of the previous time steps.", "The experiments include several LSTM models with varying buffer sizes.", "In the figure below, we see the prediction accuracy achieved by the various networks when the sequences are generated such that there is a single correct prediction.", "After the one thousandth element, we see how quickly the networks relearn when the sequences change.", "HTM is not the fastest initial learner, but it does achieve full prediction accuracy and recovers accuracy fastest after the sequence change.", "Remember though that HTM is working online seeing each example just once, whereas the LSTM’s are periodically retrained (retraining points indicated by the vertical yellow lines in the figure below).", "HTMs determine the higher-order structure by themselves, whereas ELM and TDNN require the user to determine the number of steps to use as a temporal context.", "In the next experiment scenario the sequences have two or four possible endings, depending on the higher-order context.", "HTMs perform best at this task, and the more possible completions, the better their advantage appears to be.", "HTM sequence memory was tested with varying length sequences, and achieved perfect prediction performance up to 100-order sequences.", "The number of sequences that are required to achieve perfect prediction performance increase linearly as a function of the order of sequences.", "When deliberately damaging the network structure (removal of neurons), HTMs showed no impact with up to 30% cell death, whereas the ELM and LSTM networks declined more rapidly.", "(Techniques such as dropout were not used during training though).", "New York city taxi passenger demand prediction  The HTM sequence memory model was also tested against a variety of other approaches on a task of predicting taxi demand from streaming New York City taxi ride data.", "For this example, the parameters of the ESN, ELM, and LSTM network are extensively hand-tuned.", "The HTM model does not undergo any hyperparameter tuning.", "Limitations of HTMs  We have identified a few limitations of HTM…  As a strict one-pass algorithm with access only to the current input, it may take longer for HTM to learn sequences with very long-term dependencies than algorithms with access to a larger history buffer.", "HTM is robust to spatial noise due to the use of sparse distributed representations, but proved sensitive to temporal noise (replacing elements in a sequence by random symbols).", "LSTMs are more robust to temporal noise.", "To improve the noise robustness of HTM, a hierarchy of sequence models operating on different timescales could be used.", "The HTM model did not perform as well as LSTM on grammar learning tasks – achieving 98.4% accuracy in a test, whereas the LSTM achieves 100%.", "It is future work to determine whether HTM can handle high-dimensional data such as speech and video streams  The work on HTMs also seems to be being reported outside of the usual machine learning conference venues.", "It’s good to bring in ideas from further afield, but this may also mean they have received less scrutiny from the mainstream machine learning community.", "There has been some controversy surrounding this in the past:  [url]"], "summary_text": "Continuous online sequence learning with an unsupervised neural network model Cui et al., Neural Computation, 2016  Yesterday we looked at the biological inspirations for the Hierarchical Temporal Memory (HTM) neural network model . Today’s paper demonstrates more of the inner workings, and shows how well HTM networks perform on online sequence learning tasks as compared to other approaches (e.g., LSTM-based networks). The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. The HTM sequence memory model  Recall from yesterday that the HTM sequence memory model is organised around layers of mini-columns of cells. The network represents higher-order (dependent on previous time steps) sequences using a composition of two separate sparse representations. At the column level, each input element is encoded as a sparse distributed activation of columns: the top 2% of columns that receive the most active feedforward inputs are activated. Within an active column, a subset of the cells will be active – if a column contains predicted cells then only these cells will be active, otherwise all cells within the column become active. A network has  columns and  neurons per column. For each cell in the network we keep track of its activation state and its predictive state at time step  :  The  binary matrix  represents the activation state (on/off) of the cells:  is the activation state of the  th cell in the  th column. The  binary matrix  represents the predictive state (on/off) of the cells:  is the predictive state of the  th cell in the  th column. Each cell has  dendrite segments. To model synapse connections an  matrix  tracks the permanence of the synaptic connection at segment  . The values in the matrix are in the range 0 to 1. If the permanence value is above some threshold, then the synapse is considered connected. For convenience (since it can always be deduced from  ), the binary matrix  denotes the connected synapses. At time step  , a cell is in the predictive state if one of its dendritic segments receives enough input (is connected to enough active cells):  Threshold  represents the segment activation threshold and  represents element-wise multiplication. At each time  , columns are ordered based on their number of active proximal synapses, and the top 2% are activated. This set is denoted as  . In theory the proximal  synapse connections can also be adapted continuously during learning, but for this paper the proximal synapse connections are initialised such that each column is randomly connected to 50% of the inputs, and then fixed during learning. A cell is considered active if its column is active and it was in predictive state during the previous time step, or its column is active and none of the other cells in the same column were in a predictive state:  Learning is local and straight forward:  The lateral connections in the sequence memory model are learned using a Hebbian-like rule. Specifically, if a cell is depolarized and subsequently becomes active, we reinforce the dendritic segment that caused the depolarization. If no cell in ac active column is predicted, we select the cell with the most activated segment and reinforce that segment. Reinforcement of a dendritic segment involves decreasing permanence values of inactive synapses by a small value  and increasing the permanence for active synapses by a larger value  . Where  is a binary matrix with entries set to 1 iff the corresponding entry in  is positive. Cell that don’t become active also receive a very small decay. The sequence memory operates with sparse distributed representations (SDRs). Original data is converted to SDRs using an encoder (in this case, a random encoder for categorical data, and scalar date-time encoders for a taxi data experiment). We can’t use a simple one-hot encoding for categorical data because we also want to able to provide noise inputs drawn from a very large distribution. Decoding from SDRs is done using classifiers. Real-time streaming data analysis  In this paper HTMs are applied to real-time streaming data pattern matching problems where they have the following desirable characteristics:  they can learn online in a single pass without requiring a buffered data set  they can make higher-order predictions across multiple time steps, learning the order (number of previous steps to consider) automatically  they can make multiple simultaneous predictions in the case of ambiguity  they are robust to the loss of synapses and neurons (important for hardware implementations)  they do not require any hyperparameter tuning  Sequence prediction with artificial data  The first test compares the HTM sequence memory model with online sequential extreme machine learning (ELM), a time-delayed neural network (TDNN), and LSTMs. A data set is generated with sequences that require a network to maintain a context of at least the first two elements of a sequence in order to correctly predict the last element. Since the sequences are presented in a streaming fashion, and predictions are required continuously, this task represents a continuous online learning problem. For the LSTMs, retraining is done at regular intervals on a buffered data set of the previous time steps. The experiments include several LSTM models with varying buffer sizes. In the figure below, we see the prediction accuracy achieved by the various networks when the sequences are generated such that there is a single correct prediction. After the one thousandth element, we see how quickly the networks relearn when the sequences change. HTM is not the fastest initial learner, but it does achieve full prediction accuracy and recovers accuracy fastest after the sequence change. Remember though that HTM is working online seeing each example just once, whereas the LSTM’s are periodically retrained (retraining points indicated by the vertical yellow lines in the figure below). HTMs determine the higher-order structure by themselves, whereas ELM and TDNN require the user to determine the number of steps to use as a temporal context. In the next experiment scenario the sequences have two or four possible endings, depending on the higher-order context. HTMs perform best at this task, and the more possible completions, the better their advantage appears to be. HTM sequence memory was tested with varying length sequences, and achieved perfect prediction performance up to 100-order sequences. The number of sequences that are required to achieve perfect prediction performance increase linearly as a function of the order of sequences. When deliberately damaging the network structure (removal of neurons), HTMs showed no impact with up to 30% cell death, whereas the ELM and LSTM networks declined more rapidly. (Techniques such as dropout were not used during training though). New York city taxi passenger demand prediction  The HTM sequence memory model was also tested against a variety of other approaches on a task of predicting taxi demand from streaming New York City taxi ride data. For this example, the parameters of the ESN, ELM, and LSTM network are extensively hand-tuned. The HTM model does not undergo any hyperparameter tuning. Limitations of HTMs  We have identified a few limitations of HTM…  As a strict one-pass algorithm with access only to the current input, it may take longer for HTM to learn sequences with very long-term dependencies than algorithms with access to a larger history buffer. HTM is robust to spatial noise due to the use of sparse distributed representations, but proved sensitive to temporal noise (replacing elements in a sequence by random symbols). LSTMs are more robust to temporal noise. To improve the noise robustness of HTM, a hierarchy of sequence models operating on different timescales could be used. The HTM model did not perform as well as LSTM on grammar learning tasks – achieving 98.4% accuracy in a test, whereas the LSTM achieves 100%. It is future work to determine whether HTM can handle high-dimensional data such as speech and video streams  The work on HTMs also seems to be being reported outside of the usual machine learning conference venues. It’s good to bring in ideas from further afield, but this may also mean they have received less scrutiny from the mainstream machine learning community. There has been some controversy surrounding this in the past:  [url]", "pdf_url": "https://arxiv.org/pdf/1512.05463.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/continuous-online-sequence-learning-with-an-unsupervised-neural-network-model.json"}
{"id": "5550259", "bin": "1500_1600", "summary_sentences": ["LEMNA: explaining deep learning based security applications Guo et al., CCS’18  Understanding why a deep learning model produces the outputs it does is an important part of gaining trust in the model, and in some situations being able to explain decisions is a strong requirement.", "Today’s paper shows that by carefully considering the architectural features of a given model, it’s possible to co-design an explanatory model.", "The idea is applied to deep learning models in the security domain (to detect the start of functions within binaries, and to detect malware) where for reasons we’ll look at next, the assumptions made by black-box explainers such as LIME don’t apply.", "Like LIME, LEMNA approximates a local area of a complex deep learning decision boundary using a simple interpretable model.", "Unlike LIME, LEMNA can handle non-linear local boundaries, and feature dependencies (e.g., for a sequences fed into RNNs, which explicitly model dependencies in sequential data).", "Why explainability matters  While intrigued by the high accuracy, security practitioners are concerned about the lack of transparency of deep learning models, and thus hesitate to widely adopt deep learning classifiers in security and safety-critical areas.", "Explanations that are understandable by security analysts can help to build trust in trained models, and also to troubleshoot classification errors.", "“We argue that classifier reliability and trust do not necessarily come from a high classification accuracy on the training data… (instead) trust is more likely to be established by understanding model behavior .”  Some of the ways that explanations can help to build trust include:  Demonstrating that the model has captured well-know heuristics (i.e., explanations match the knowledge of human experts)  Demonstrating that the model is able to capture new patterns of knowledge, which makes sense to human experts when explained.", "Explaining the reasons behind a false negative or false positive (i.e., highlighting the features that mislead the model), such that a human expert can understand why the mistake was made (and see that the mistake was in some way ‘reasonable’).", "Challenges explaining security DNNs  Unfortunately, existing explanation methods are not directly applicable to security applications… Security applications such as binary reverse engineering and malware analysis either have a high-level feature dependency (e.g., binary code sequences), or require high scalability.", "As a result, Recurrent Neural Networks (RNNs) or Multilayer Perceptron (MLP) models are more widely used.", "A blackbox explanation system such as LIME produces a Locally Interpretable Model Explanation using a linear model to approximate the detection boundary near the input to be explained.", "If the decision boundary is non-linear even in the local area, then this can introduce errors in the explanation process.", "Sampling (looking at points around the input) can easily land in areas beyond the linear region.", "Jumping ahead, a richer decision boundary as obtained by e.g., a mixture regression model will give higher fidelity.", "Furthermore, LIME assumes input features are independent, which does not hold when the input is a sequence (e.g. of byte codes from a binary) fed into an RNN model.", "Here the dependency (i.e., sequencing of instructions) matters very much.", "Handling feature dependencies  The fused lasso penalty term can be added to the loss function of models trained on features with a linear dependency (i.e. ordering dependency) among the input features.", "It works by restricting the coefficient weights of adjacent features to be within some small threshold.", "Consider producing an explanation for sentiment classification in next.", "With fused lasso, words next to each other in a sentence are likely to be grouped together, such that instead of an explanation simply pointing to the word ‘not,’ it can highlight instead a phrase (‘not worth the price’).", "Handling non-linear boundaries  If one line isn’t enough, then use several!", "A mixture regression model is a combination of multiple K linear regression models, where  is the weight assigned to each component model:  Given sufficient data samples, whether the classifier has a linear or non-linear decision boundary, the mixture regression model can nearly perfectly approximate the decision boundary (using a finite set of linear models).", "The LEMNA explanation system  LEMNA (Local Explanation Method using Nonlinear Approximation) combines fused lasso into the learning process of a mixture regression model, so that feature dependencies and decision boundary non-linearities can be handled at the same time.", "The mixture regression model is expressed in the form of probability distributions and trained using Expectation Maximisation (EM).", "To explain the classification of an input x the first step is to synthesize a set of data samples around x using the same approach as described in the LIME paper .", "This corpus of samples is then used to approximate the local decision boundary (for multi-class classification, a set of multiple mixture regression models are trained, each of which performs binary classification for one class).", "From this mixture model, we then identify the linear component that has the best approximation of the local decision boundary.", "The weights (or coefficients) in the linear model can be used to rank features.", "A small set of top features is selected as the explanation result.", "Targeted model patching  One neat application of LEMNA in the paper is classifier ‘patching.’ Given a misclassified instance, LEMNA’s explanation can be used to pinpoint the small set of features  responsible for the miss.", "Often, such instances are outliers in the training data, and do not have enough “counter examples”.", "To this end, our strategy is to augment the training data by adding related “counter examples,” by replacing the feature values of  with random values.", "So long as we only introduce a small (e.g. 2-10) number of such samples for each case we can ‘patch’ the targeted errors without hurting the already high overall accuracy.", "The following table shows the result of patching the top 5 misleading features in a model and retraining for 40 epochs.", "These results demonstrate that by understanding the model behavior, we can identify the weaknesses of the model and enhance the model accordingly.", "Evaluation results  LEMNA is evaluated on an RNN-based model used to find the start of functions when reverse-engineering binaries, and on a malware classifier.", "Here’s an example (on the RHS) of LEMNA highlighting the ‘hot bytes’ in the instruction sequence that led to the classification of x83 as the start of a function.", "LEMNA’s local approximation accuracy for these classifiers has a root mean square error (RMSE) an order of magnitude smaller than LIME’s.", "The saliency of the highlighted features in the explanation can be tested in three different ways.", "Given a set of highlighted features as an explanation for a classification as class C then:  removing those features from the input should lead to a different classification (feature deduction)  adding those features to some other sample not in class C should increase the chances of it being misclassified as a C (feature augmentation)  crafting synthetic inputs including those features should increase the likelihood of those inputs being classified as C  For each instance in the testing set for which an explanation is given, three samples are generated, one for each case above.", "The positive classification rate (PCR) then measures the ratio of samples still classified as the input’s original label.", "In the feature deduction test, removing the top 5 features highlighted by LEMNA drops PCR to 25% or lower, indicating the small set of highlighted features are highly important to the classification.", "In the feature augmentation test, replacing the top 5 features highlighted by LEMNA caused 75% of test case for the PDF malware classifier to flip their labels.", "And using the synthetic inputs with the top 5 inputs, the synthetic instances have a 85%-90% chance of taking the original input’s label.", "Focusing on the reverse-engineering function starts in binaries application, the explanation produced by LEMNA shows that the model does indeed capture well-known heuristics (C.W.H.", "), discovers new knowledge that makes sense to human experts (D.N.K.", "), and can provide insights into misclassification reasons for false negatives (R.F.N.)", "and false positives (R.F.P.", ")."], "summary_text": "LEMNA: explaining deep learning based security applications Guo et al., CCS’18  Understanding why a deep learning model produces the outputs it does is an important part of gaining trust in the model, and in some situations being able to explain decisions is a strong requirement. Today’s paper shows that by carefully considering the architectural features of a given model, it’s possible to co-design an explanatory model. The idea is applied to deep learning models in the security domain (to detect the start of functions within binaries, and to detect malware) where for reasons we’ll look at next, the assumptions made by black-box explainers such as LIME don’t apply. Like LIME, LEMNA approximates a local area of a complex deep learning decision boundary using a simple interpretable model. Unlike LIME, LEMNA can handle non-linear local boundaries, and feature dependencies (e.g., for a sequences fed into RNNs, which explicitly model dependencies in sequential data). Why explainability matters  While intrigued by the high accuracy, security practitioners are concerned about the lack of transparency of deep learning models, and thus hesitate to widely adopt deep learning classifiers in security and safety-critical areas. Explanations that are understandable by security analysts can help to build trust in trained models, and also to troubleshoot classification errors. “We argue that classifier reliability and trust do not necessarily come from a high classification accuracy on the training data… (instead) trust is more likely to be established by understanding model behavior .”  Some of the ways that explanations can help to build trust include:  Demonstrating that the model has captured well-know heuristics (i.e., explanations match the knowledge of human experts)  Demonstrating that the model is able to capture new patterns of knowledge, which makes sense to human experts when explained. Explaining the reasons behind a false negative or false positive (i.e., highlighting the features that mislead the model), such that a human expert can understand why the mistake was made (and see that the mistake was in some way ‘reasonable’). Challenges explaining security DNNs  Unfortunately, existing explanation methods are not directly applicable to security applications… Security applications such as binary reverse engineering and malware analysis either have a high-level feature dependency (e.g., binary code sequences), or require high scalability. As a result, Recurrent Neural Networks (RNNs) or Multilayer Perceptron (MLP) models are more widely used. A blackbox explanation system such as LIME produces a Locally Interpretable Model Explanation using a linear model to approximate the detection boundary near the input to be explained. If the decision boundary is non-linear even in the local area, then this can introduce errors in the explanation process. Sampling (looking at points around the input) can easily land in areas beyond the linear region. Jumping ahead, a richer decision boundary as obtained by e.g., a mixture regression model will give higher fidelity. Furthermore, LIME assumes input features are independent, which does not hold when the input is a sequence (e.g. of byte codes from a binary) fed into an RNN model. Here the dependency (i.e., sequencing of instructions) matters very much. Handling feature dependencies  The fused lasso penalty term can be added to the loss function of models trained on features with a linear dependency (i.e. ordering dependency) among the input features. It works by restricting the coefficient weights of adjacent features to be within some small threshold. Consider producing an explanation for sentiment classification in next. With fused lasso, words next to each other in a sentence are likely to be grouped together, such that instead of an explanation simply pointing to the word ‘not,’ it can highlight instead a phrase (‘not worth the price’). Handling non-linear boundaries  If one line isn’t enough, then use several! A mixture regression model is a combination of multiple K linear regression models, where  is the weight assigned to each component model:  Given sufficient data samples, whether the classifier has a linear or non-linear decision boundary, the mixture regression model can nearly perfectly approximate the decision boundary (using a finite set of linear models). The LEMNA explanation system  LEMNA (Local Explanation Method using Nonlinear Approximation) combines fused lasso into the learning process of a mixture regression model, so that feature dependencies and decision boundary non-linearities can be handled at the same time. The mixture regression model is expressed in the form of probability distributions and trained using Expectation Maximisation (EM). To explain the classification of an input x the first step is to synthesize a set of data samples around x using the same approach as described in the LIME paper . This corpus of samples is then used to approximate the local decision boundary (for multi-class classification, a set of multiple mixture regression models are trained, each of which performs binary classification for one class). From this mixture model, we then identify the linear component that has the best approximation of the local decision boundary. The weights (or coefficients) in the linear model can be used to rank features. A small set of top features is selected as the explanation result. Targeted model patching  One neat application of LEMNA in the paper is classifier ‘patching.’ Given a misclassified instance, LEMNA’s explanation can be used to pinpoint the small set of features  responsible for the miss. Often, such instances are outliers in the training data, and do not have enough “counter examples”. To this end, our strategy is to augment the training data by adding related “counter examples,” by replacing the feature values of  with random values. So long as we only introduce a small (e.g. 2-10) number of such samples for each case we can ‘patch’ the targeted errors without hurting the already high overall accuracy. The following table shows the result of patching the top 5 misleading features in a model and retraining for 40 epochs. These results demonstrate that by understanding the model behavior, we can identify the weaknesses of the model and enhance the model accordingly. Evaluation results  LEMNA is evaluated on an RNN-based model used to find the start of functions when reverse-engineering binaries, and on a malware classifier. Here’s an example (on the RHS) of LEMNA highlighting the ‘hot bytes’ in the instruction sequence that led to the classification of x83 as the start of a function. LEMNA’s local approximation accuracy for these classifiers has a root mean square error (RMSE) an order of magnitude smaller than LIME’s. The saliency of the highlighted features in the explanation can be tested in three different ways. Given a set of highlighted features as an explanation for a classification as class C then:  removing those features from the input should lead to a different classification (feature deduction)  adding those features to some other sample not in class C should increase the chances of it being misclassified as a C (feature augmentation)  crafting synthetic inputs including those features should increase the likelihood of those inputs being classified as C  For each instance in the testing set for which an explanation is given, three samples are generated, one for each case above. The positive classification rate (PCR) then measures the ratio of samples still classified as the input’s original label. In the feature deduction test, removing the top 5 features highlighted by LEMNA drops PCR to 25% or lower, indicating the small set of highlighted features are highly important to the classification. In the feature augmentation test, replacing the top 5 features highlighted by LEMNA caused 75% of test case for the PDF malware classifier to flip their labels. And using the synthetic inputs with the top 5 inputs, the synthetic instances have a 85%-90% chance of taking the original input’s label. Focusing on the reverse-engineering function starts in binaries application, the explanation produced by LEMNA shows that the model does indeed capture well-known heuristics (C.W.H. ), discovers new knowledge that makes sense to human experts (D.N.K. ), and can provide insights into misclassification reasons for false negatives (R.F.N.) and false positives (R.F.P. ).", "pdf_url": "http://people.cs.vt.edu/gangwang/ccs18.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/1500_1600/lemna-explaining-deep-learning-based-security-applications.json"}
{"id": "93142771", "bin": "200_300", "summary_sentences": ["The paper demonstrates how simple CNNs, built on top of word embeddings, can be used for sentence classification tasks.", "Implementation  Architecture  Pad input sentences so that they are of the same length.", "Map words in the padded sentence using word embeddings (which may be either initialized as zero vectors or initialized as word2vec embeddings) to obtain a matrix corresponding to the sentence.", "Apply convolution layer with multiple filter widths and feature maps.", "Apply max-over-time pooling operation over the feature map.", "Concatenate the pooling results from different layers and feed to a fully-connected layer with softmax activation.", "Softmax outputs probabilistic distribution over the labels.", "Use dropout for regularisation.", "Hyperparameters  RELU activation for convolution layers  Filter window of 3, 4, 5 with 100 feature maps each.", "Dropout - 0.5  Gradient clipping at 3  Batch size - 50  Adadelta update rule.", "Variants  CNN-rand  Randomly initialized word vectors.", "CNN-static  Uses pre-trained vectors from word2vec and does not update the word vectors.", "CNN-non-static  Same as CNN-static but updates word vectors during training.", "CNN-multichannel  Uses two set of word vectors (channels).", "One set is updated and other is not updated.", "Datasets  Sentiment analysis datasets for Movie Reviews, Customer Reviews etc.", "Classification data for questions.", "Maximum number of classes for any dataset - 6  Strengths  Good results on benchmarks despite being a simple architecture.", "Word vectors obtained by non-static channel have more meaningful representation.", "Weakness  Small data with few labels.", "Results are not very detailed or exhaustive."], "summary_text": "The paper demonstrates how simple CNNs, built on top of word embeddings, can be used for sentence classification tasks. Implementation  Architecture  Pad input sentences so that they are of the same length. Map words in the padded sentence using word embeddings (which may be either initialized as zero vectors or initialized as word2vec embeddings) to obtain a matrix corresponding to the sentence. Apply convolution layer with multiple filter widths and feature maps. Apply max-over-time pooling operation over the feature map. Concatenate the pooling results from different layers and feed to a fully-connected layer with softmax activation. Softmax outputs probabilistic distribution over the labels. Use dropout for regularisation. Hyperparameters  RELU activation for convolution layers  Filter window of 3, 4, 5 with 100 feature maps each. Dropout - 0.5  Gradient clipping at 3  Batch size - 50  Adadelta update rule. Variants  CNN-rand  Randomly initialized word vectors. CNN-static  Uses pre-trained vectors from word2vec and does not update the word vectors. CNN-non-static  Same as CNN-static but updates word vectors during training. CNN-multichannel  Uses two set of word vectors (channels). One set is updated and other is not updated. Datasets  Sentiment analysis datasets for Movie Reviews, Customer Reviews etc. Classification data for questions. Maximum number of classes for any dataset - 6  Strengths  Good results on benchmarks despite being a simple architecture. Word vectors obtained by non-static channel have more meaningful representation. Weakness  Small data with few labels. Results are not very detailed or exhaustive.", "pdf_url": "https://arxiv.org/pdf/1408.5882", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/e6d2364c278c97b1b2f4ec53255c56.json"}
{"id": "29962756", "bin": "200_300", "summary_sentences": ["The paper presents a domain agnostic approach for conversational modelling based on Sequence to Sequence Learning Framework .", "Model  Neural Conversational Model (NCM)  A Recurrent Neural Network (RNN) reads the input sentence, one token at a time, and predicts the output sequence, one    token at a time.", "Learns by backpropagation.", "The model maximises the cross entropy of correct sequence given its context.", "Greedy inference approach where predicted output token is used as input to predict the next output token.", "Dataset  IT HelpDesk dataset of conversations about computer related issues.", "OpenSubtitles dataset containing movie conversations.", "Results  The paper has reported some samples of conversations generated by the interaction between human actor and the NCM.", "NCM reports lower perplexity as compared to n-grams model.", "NCM outperforms CleverBot in a subjective test involving human evaluators to grade the two systems.", "Strengths  Domain-agnostic.", "End-To-End training without handcrafted rules.", "Underlying architecture (Sequence To Sequence Framework) can be leveraged for machine translation, question answering etc.", "Weakness  The responses are simple, short and at times inconsistent.", "The objective function of Sequence To Sequence Framework is not designed to capture the objective of conversational models."], "summary_text": "The paper presents a domain agnostic approach for conversational modelling based on Sequence to Sequence Learning Framework . Model  Neural Conversational Model (NCM)  A Recurrent Neural Network (RNN) reads the input sentence, one token at a time, and predicts the output sequence, one    token at a time. Learns by backpropagation. The model maximises the cross entropy of correct sequence given its context. Greedy inference approach where predicted output token is used as input to predict the next output token. Dataset  IT HelpDesk dataset of conversations about computer related issues. OpenSubtitles dataset containing movie conversations. Results  The paper has reported some samples of conversations generated by the interaction between human actor and the NCM. NCM reports lower perplexity as compared to n-grams model. NCM outperforms CleverBot in a subjective test involving human evaluators to grade the two systems. Strengths  Domain-agnostic. End-To-End training without handcrafted rules. Underlying architecture (Sequence To Sequence Framework) can be leveraged for machine translation, question answering etc. Weakness  The responses are simple, short and at times inconsistent. The objective function of Sequence To Sequence Framework is not designed to capture the objective of conversational models.", "pdf_url": "http://arxiv.org/pdf/1506.05869", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/6835964df0e49fdef0459c8b334b94.json"}
{"id": "49439664", "bin": "200_300", "summary_sentences": ["The paper introduces GuessWhat - a two-player guessing game where the goal is to locate an object in a rich image scene.", "The game is used to produce a large scale dataset of visual question-answer pairs on the image.", "The paper also describes three tasks based on the game and provides a neural architecture based baselines for each task.", "GuessWhat?!", "Game  One player, called as the oracle, is randomly assigned an object in the given image.", "The second player, called as the questioner, tries to locate the object, given just the image.", "The questioner can ask a series of questions about the object and the oracle can reply in \"yes\" or \"no\" or \"not applicable\".", "Once the questioner is confident of having identified the image, the oracle presents a list of objects to the questioner to choose from.", "A small penalty is added, every time a question is asked, so as to encourage informative questions only.", "Dataset  A filtered subset of images from MSCOCO is used as the image set.", "Two separate tasks create on Amazon Mechanical Turk (AMT) - for the role of oracle and questioner.", "Data was post processed -- both manually and using AMT -- to account for things like spelling mistakes and validation.", "Final dataset comprises of 150K thousand human game iterations with 800K question-answer pairs on 60K images.", "Dataset is available at  [url]"], "summary_text": "The paper introduces GuessWhat - a two-player guessing game where the goal is to locate an object in a rich image scene. The game is used to produce a large scale dataset of visual question-answer pairs on the image. The paper also describes three tasks based on the game and provides a neural architecture based baselines for each task. GuessWhat?! Game  One player, called as the oracle, is randomly assigned an object in the given image. The second player, called as the questioner, tries to locate the object, given just the image. The questioner can ask a series of questions about the object and the oracle can reply in \"yes\" or \"no\" or \"not applicable\". Once the questioner is confident of having identified the image, the oracle presents a list of objects to the questioner to choose from. A small penalty is added, every time a question is asked, so as to encourage informative questions only. Dataset  A filtered subset of images from MSCOCO is used as the image set. Two separate tasks create on Amazon Mechanical Turk (AMT) - for the role of oracle and questioner. Data was post processed -- both manually and using AMT -- to account for things like spelling mistakes and validation. Final dataset comprises of 150K thousand human game iterations with 800K question-answer pairs on 60K images. Dataset is available at  [url]", "pdf_url": "https://arxiv.org/pdf/1611.08481", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/200_300/18238e6aefd7b1e8c922cda9e10488.json"}
{"id": "21919380", "bin": "300_400", "summary_sentences": ["Recurrent Neural Networks (RNNs) are very powerful at modelling sequences but they are not good at learning long-term dependencies.", "The paper discusses the reasons behind this difficulty and some suggestions to mitigate it.", ".", "Optimization Difficulty  RNNs form a deterministic state variable ht as function of input observation and previous state.", "Learnable parameters to decide what will be remembered about the past sequence.", "Using local optimisation techniques like Stochastic Gradient Descent (SGD) are unlikely to find optimal values of tunable parameters  When computations performed by RNN are unfolded through time, a deep Neural Network with shared weights is realised.", "The cost function of this deep network depends on the output of hidden layers.", "Gradient descent updates could \"explode\" (become very large) or \"vanish\" (become very small).", "Training Recurrent Networks  Clip Gradient - when the norm of the gradient vector (g) is above a threshold, update is done in direction of threshold.g/||g||.", "This normalisation implements a simple form of second-order normalisation (the second-order derivate will also be large in regions of exploding gradient).", "Use a leaky integration state-to-state map: ht, i = αiht-1, i + (1-αi)Fi(ht-1, xt)  Different values of α allow a different amount of the previous state to \"leak\" through the unfolded layers to further in time.", "This simply expands the time-scale of vanishing gradients and not totally remove them.", "Use output probability models like Restricted Boltzmann Machine or NADE to capture higher order dependencies between variables in case of multivariate prediction.", "By using rectifier non-linearities, the gradient on hidden units becomes sparse and these sparse gradients help the hidden units to specialise.", "The basic idea is that if the gradient is concentrated in fewer paths (in the unfolded computational graph) the vanishing gradient effect would be limited.", "A simplified Nesterov Momentum rule is proposed to allow storing past velocities for a longer time while actually using these velocities more conservatively.", "The new formulation is also easier to implement.", "Results  SGD with these optimisations outperforms a vanilla SGD."], "summary_text": "Recurrent Neural Networks (RNNs) are very powerful at modelling sequences but they are not good at learning long-term dependencies. The paper discusses the reasons behind this difficulty and some suggestions to mitigate it. . Optimization Difficulty  RNNs form a deterministic state variable ht as function of input observation and previous state. Learnable parameters to decide what will be remembered about the past sequence. Using local optimisation techniques like Stochastic Gradient Descent (SGD) are unlikely to find optimal values of tunable parameters  When computations performed by RNN are unfolded through time, a deep Neural Network with shared weights is realised. The cost function of this deep network depends on the output of hidden layers. Gradient descent updates could \"explode\" (become very large) or \"vanish\" (become very small). Training Recurrent Networks  Clip Gradient - when the norm of the gradient vector (g) is above a threshold, update is done in direction of threshold.g/||g||. This normalisation implements a simple form of second-order normalisation (the second-order derivate will also be large in regions of exploding gradient). Use a leaky integration state-to-state map: ht, i = αiht-1, i + (1-αi)Fi(ht-1, xt)  Different values of α allow a different amount of the previous state to \"leak\" through the unfolded layers to further in time. This simply expands the time-scale of vanishing gradients and not totally remove them. Use output probability models like Restricted Boltzmann Machine or NADE to capture higher order dependencies between variables in case of multivariate prediction. By using rectifier non-linearities, the gradient on hidden units becomes sparse and these sparse gradients help the hidden units to specialise. The basic idea is that if the gradient is concentrated in fewer paths (in the unfolded computational graph) the vanishing gradient effect would be limited. A simplified Nesterov Momentum rule is proposed to allow storing past velocities for a longer time while actually using these velocities more conservatively. The new formulation is also easier to implement. Results  SGD with these optimisations outperforms a vanilla SGD.", "pdf_url": "https://arxiv.org/pdf/1606.03126", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/dc31e3c7999ad4a1edf4f289deaa88.json"}
{"id": "35704901", "bin": "300_400", "summary_sentences": ["The paper introduces a novel architecture that generates an output sequence such that the elements of the output sequence are discrete tokens corresponding to positions in the input sequence.", "Such a problem can not be solved using Seq2Seq or Neural Turing Machines as the size of the output softmax is variable (as it depends on the size of the input sequence).", "Architecture  Traditional attention-base sequence-to-sequence models compute an attention vector for each step of the output decoder and use that to blend the individual context vectors of the input into a single, consolidated attention vector.", "This attention vector is used to compute a fixed size softmax.", "In Pointer Nets, the normalized attention vector (over all the tokens in the input sequence) is normalized and treated as the softmax output over the input tokens.", "So Pointer Net is a very simple modification of the attention model.", "Application  Any problem where the size of the output depends on the size of the input because of which fixed length softmax is ruled out.", "eg combinatorial problems such as planar convex hull where the size of the output would depend on the size of the input.", "Evaluation  The paper considers the following 3 problems:  Convex Hull  Delaunay triangulations  Travelling Salesman Problem (TSP)  Since some of the problems are NP hard, the paper considers approximate solutions whereever the exact solutions are not feasible to compute.", "The authors used the exact same architecture and model parameters of all the instances of the 3 problems to show the generality of the model.", "The proosed Pointer Nets outperforms LSTMs and LSTMs with attention and can generalise quite well for much larger sequences.", "Interestingly, the order in which the inputs are fed to the system affects its performance.", "The authors discussed this apsect in their subsequent paper titled Order Matters: Sequence To Sequence for Sets"], "summary_text": "The paper introduces a novel architecture that generates an output sequence such that the elements of the output sequence are discrete tokens corresponding to positions in the input sequence. Such a problem can not be solved using Seq2Seq or Neural Turing Machines as the size of the output softmax is variable (as it depends on the size of the input sequence). Architecture  Traditional attention-base sequence-to-sequence models compute an attention vector for each step of the output decoder and use that to blend the individual context vectors of the input into a single, consolidated attention vector. This attention vector is used to compute a fixed size softmax. In Pointer Nets, the normalized attention vector (over all the tokens in the input sequence) is normalized and treated as the softmax output over the input tokens. So Pointer Net is a very simple modification of the attention model. Application  Any problem where the size of the output depends on the size of the input because of which fixed length softmax is ruled out. eg combinatorial problems such as planar convex hull where the size of the output would depend on the size of the input. Evaluation  The paper considers the following 3 problems:  Convex Hull  Delaunay triangulations  Travelling Salesman Problem (TSP)  Since some of the problems are NP hard, the paper considers approximate solutions whereever the exact solutions are not feasible to compute. The authors used the exact same architecture and model parameters of all the instances of the 3 problems to show the generality of the model. The proosed Pointer Nets outperforms LSTMs and LSTMs with attention and can generalise quite well for much larger sequences. Interestingly, the order in which the inputs are fed to the system affects its performance. The authors discussed this apsect in their subsequent paper titled Order Matters: Sequence To Sequence for Sets", "pdf_url": "https://arxiv.org/pdf/1506.03134", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/pointer-networks.json"}
{"id": "687757", "bin": "300_400", "summary_sentences": ["This paper describes how to apply the idea of batch normalization (BN) successfully to recurrent neural networks, specifically to LSTM networks.", "The technique involves the 3 following ideas:  **1) Careful initialization of the BN scaling parameter.", "** While standard practice is to initialize it to 1 (to have unit variance), they show that this situation creates problems with the gradient flow through time, which vanishes quickly.", "A value around 0.1 (used in the experiments) preserves gradient flow much better.", "**2) Separate BN for the \"hiddens to hiddens pre-activation and for the \"inputs to hiddens\" pre-activation.", "** In other words, 2 separate BN operators are applied on each contributions to the pre-activation, before summing and passing through the tanh and sigmoid non-linearities.", "**3) Use of largest time-step BN statistics for longer test-time sequences.", "** Indeed, one issue with applying BN to RNNs is that if the input sequences have varying length, and if one uses per-time-step mean/variance statistics in the BN transformation (which is the natural thing to do), it hasn't been clear how do deal with the last time steps of longer sequences seen at test time, for which BN has no statistics from the training set.", "The paper shows evidence that the pre-activation statistics tend to gradually converge to stationary values over time steps, which supports the idea of simply using the training set's last time step statistics.", "Among these ideas, I believe the most impactful idea is 1).", "The papers mentions towards the end that improper initialization of the BN scaling parameter probably explains previous failed attempts to apply BN to recurrent networks.", "Experiments on 4 datasets confirms the method's success.", "**My two cents**  This is an excellent development for LSTMs.", "BN has had an important impact on our success in training deep neural networks, and this approach might very well have a similar impact on the success of LSTMs in practice."], "summary_text": "This paper describes how to apply the idea of batch normalization (BN) successfully to recurrent neural networks, specifically to LSTM networks. The technique involves the 3 following ideas:  **1) Careful initialization of the BN scaling parameter. ** While standard practice is to initialize it to 1 (to have unit variance), they show that this situation creates problems with the gradient flow through time, which vanishes quickly. A value around 0.1 (used in the experiments) preserves gradient flow much better. **2) Separate BN for the \"hiddens to hiddens pre-activation and for the \"inputs to hiddens\" pre-activation. ** In other words, 2 separate BN operators are applied on each contributions to the pre-activation, before summing and passing through the tanh and sigmoid non-linearities. **3) Use of largest time-step BN statistics for longer test-time sequences. ** Indeed, one issue with applying BN to RNNs is that if the input sequences have varying length, and if one uses per-time-step mean/variance statistics in the BN transformation (which is the natural thing to do), it hasn't been clear how do deal with the last time steps of longer sequences seen at test time, for which BN has no statistics from the training set. The paper shows evidence that the pre-activation statistics tend to gradually converge to stationary values over time steps, which supports the idea of simply using the training set's last time step statistics. Among these ideas, I believe the most impactful idea is 1). The papers mentions towards the end that improper initialization of the BN scaling parameter probably explains previous failed attempts to apply BN to recurrent networks. Experiments on 4 datasets confirms the method's success. **My two cents**  This is an excellent development for LSTMs. BN has had an important impact on our success in training deep neural networks, and this approach might very well have a similar impact on the success of LSTMs in practice.", "pdf_url": "http://arxiv.org/pdf/1603.09025", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/cooijmansblc16.json"}
{"id": "24000799", "bin": "300_400", "summary_sentences": ["Continual Learning paradigm focuses on learning from a non-stationary stream of data with additional desiderata - transferring knowledge from previously seen task to unseen tasks and being resilient to catastrophic forgetting - all with a fixed memory and computational budget.", "This is in contrast to the IID (independent and identically distributed) assumption in statistical learning.", "One common example of the non-iid data is setups involving sequential decision making - eg Reinforcement learning.", "Paper  Benchmark  Many existing benchmarks use MNIST as the underlying dataset (eg Permuted MNIST, Split MNIST, etc).", "These benchmarks lack complexity and make it hard to observe positive and negative backward transfer.", "Most works focus only on the catastrophic forgetting challenge and ignore the other issues (like computation and memory footprint, the capacity of the network, etc).", "The paper proposes a new benchmark based on Starcraft II video game to understand the different approaches for lifelong learning.", "The sequence of tasks is designed to be a curriculum - the learning agent stats with learning simple skills and later move to more complex tasks.", "These complex tasks require remembering and composing skills learned in the earlier levels.", "To evaluate for catastrophic forgetting, the tasks are designed such that not all the skills are needed for solving each task.", "Hence the learning agent needs to remember skills even though they are not needed at the current level.", "Each level comes with a fixed computational budget of episodes and each episode has a fixed time limit.", "Once the budget is consumed the agent has to proceed to the next level.", "Hence agents with better sample efficiency would benefit.", "The benchmark supports both RL and supervised learning version.", "In the supervised version, expert agents (pretrained on each level) are also provided.", "Baselines are provided for distillation (using experts): sequential training (fine tuning), Dropout and SER.", "None of the baseline methods achieve positive or negative backward transfer.", "When modeled as a pure RL task, the benchmark is extremely difficult to solve.", "The paper suggests using a metric to record the amount of learning/data required to recover performance on the previous task."], "summary_text": "Continual Learning paradigm focuses on learning from a non-stationary stream of data with additional desiderata - transferring knowledge from previously seen task to unseen tasks and being resilient to catastrophic forgetting - all with a fixed memory and computational budget. This is in contrast to the IID (independent and identically distributed) assumption in statistical learning. One common example of the non-iid data is setups involving sequential decision making - eg Reinforcement learning. Paper  Benchmark  Many existing benchmarks use MNIST as the underlying dataset (eg Permuted MNIST, Split MNIST, etc). These benchmarks lack complexity and make it hard to observe positive and negative backward transfer. Most works focus only on the catastrophic forgetting challenge and ignore the other issues (like computation and memory footprint, the capacity of the network, etc). The paper proposes a new benchmark based on Starcraft II video game to understand the different approaches for lifelong learning. The sequence of tasks is designed to be a curriculum - the learning agent stats with learning simple skills and later move to more complex tasks. These complex tasks require remembering and composing skills learned in the earlier levels. To evaluate for catastrophic forgetting, the tasks are designed such that not all the skills are needed for solving each task. Hence the learning agent needs to remember skills even though they are not needed at the current level. Each level comes with a fixed computational budget of episodes and each episode has a fixed time limit. Once the budget is consumed the agent has to proceed to the next level. Hence agents with better sample efficiency would benefit. The benchmark supports both RL and supervised learning version. In the supervised version, expert agents (pretrained on each level) are also provided. Baselines are provided for distillation (using experts): sequential training (fine tuning), Dropout and SER. None of the baseline methods achieve positive or negative backward transfer. When modeled as a pure RL task, the benchmark is extremely difficult to solve. The paper suggests using a metric to record the amount of learning/data required to recover performance on the previous task.", "pdf_url": "https://www.mitpressjournals.org/doi/pdf/10.1162/089976602753712972", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/300_400/towards-a-natural-benchmark-for-continual-learning.json"}
{"id": "30748495", "bin": "400_500", "summary_sentences": ["   Training RNNs to model long term dependencies is difficult but in some cases, the information about dependencies between elements (of the sequence) may be present in the form of symbolic knowledge.", "For example, when encoding sentences, coreference, and hypernymy relations can be extracted between tokens.", "These elements(tokens) can be connected with each other with different kind of edges resulting in the graph data structure.", "One approach could be to model this knowledge(encoded in the graph) using a graph neural network (GNN).", "The authors prefer to encode the information into 2 DAGs (via topological sorting) as training the GNN could add some extra overhead.", "This results into the Memory as Acyclic Graph Encoding RNN (MAGE-RNN) architecture.", "Its GRU version is referred to as MAGE-GRU.", "Given an input sequence of tokens [x1, x2, …, xT] and information about which tokens relate to other tokens, a graph G is constructed with different (possibly typed) edges.", "Given the graph G, two DFS orderings are computed - forward DFS and backward DFS.", "MAGE-RNN uses separate networks for accessing the forward and backward DFS orders.", "A separate hidden state is maintained for each entity type to separate memory content from addressing.", "For any DFS order (forward or backward), the representation at time t is given as the concatenation of representation of different edge types at that time.", "The hidden states (for different edge types at time t) are updated in the topological order using the current state of all incoming edges at xt.", "The representation of the DFS order is given as the sequence of all the previous representations.", "In some cases, elements across multiple sequences could be related to each other.", "In that case, the graph is decomposed into a collection of DAGs and use MAGE-GRU on the DAGs by taking one random permutation of the sequences and decomposing it into the forward and the backward graphs.", "The model is evaluated on the task of text comprehension with coreference on bAbi dataset (story based QA), LAMBADA dataset (broad context language modeling) and CNN dataset (cloze-style QA).", "MAGE-GRU was used as a replacement for GRU units in bi-directional GRUs and GA-Reader architecture.", "DAG-RNN and shared version of MAGE-GRU (with shared edge types) are the other baselines.", "For all the cases, the model with MAGE-GRU works the best."], "summary_text": "Training RNNs to model long term dependencies is difficult but in some cases, the information about dependencies between elements (of the sequence) may be present in the form of symbolic knowledge. For example, when encoding sentences, coreference, and hypernymy relations can be extracted between tokens. These elements(tokens) can be connected with each other with different kind of edges resulting in the graph data structure. One approach could be to model this knowledge(encoded in the graph) using a graph neural network (GNN). The authors prefer to encode the information into 2 DAGs (via topological sorting) as training the GNN could add some extra overhead. This results into the Memory as Acyclic Graph Encoding RNN (MAGE-RNN) architecture. Its GRU version is referred to as MAGE-GRU. Given an input sequence of tokens [x1, x2, …, xT] and information about which tokens relate to other tokens, a graph G is constructed with different (possibly typed) edges. Given the graph G, two DFS orderings are computed - forward DFS and backward DFS. MAGE-RNN uses separate networks for accessing the forward and backward DFS orders. A separate hidden state is maintained for each entity type to separate memory content from addressing. For any DFS order (forward or backward), the representation at time t is given as the concatenation of representation of different edge types at that time. The hidden states (for different edge types at time t) are updated in the topological order using the current state of all incoming edges at xt. The representation of the DFS order is given as the sequence of all the previous representations. In some cases, elements across multiple sequences could be related to each other. In that case, the graph is decomposed into a collection of DAGs and use MAGE-GRU on the DAGs by taking one random permutation of the sequences and decomposing it into the forward and the backward graphs. The model is evaluated on the task of text comprehension with coreference on bAbi dataset (story based QA), LAMBADA dataset (broad context language modeling) and CNN dataset (cloze-style QA). MAGE-GRU was used as a replacement for GRU units in bi-directional GRUs and GA-Reader architecture. DAG-RNN and shared version of MAGE-GRU (with shared edge types) are the other baselines. For all the cases, the model with MAGE-GRU works the best.", "pdf_url": "https://arxiv.org/pdf/1703.02620", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/linguistic-knowledge-as-memory-for-recurrent-neural-networks.json"}
{"id": "98238949", "bin": "400_500", "summary_sentences": ["For the task of Visual Question Answering , decompose a question into its linguistic substructures and train a neural network module for each substructure.", "Jointly train the modules and dynamically compose them into deep networks which can learn to answer the question.", "Start by analyzing the question and decide what logical units are needed to answer the question and what should be the relationship between them.", "The paper also introduces a new dataset for Visual Question Answering which has challenging, highly compositional questions about abstract shapes.", "Inspiration  Questions tend to be compositional.", "Different architectures are needed for different tasks - CNNs for object detection, RNNs for counting.", "Recurrent and Recursive Neural Networks also use the idea of a different network graph for each input.", "Neural Module Network for VQA  Training samples of form (w, x, y)  w - Natural Language Question  x - Images  y - Answer  Model specified by collection of modules {m} and a network layout predictor P.  Model instantiates a network based on P(w) and uses that to encode a distribution P(y|w, x, model_params)  Modules  Find: Finds objects of interest.", "Transform: Shift regions of attention.", "Combine: Merge two attention maps into a single one.", "Describe: Map a pair of attention and input image to a distribution over the labels.", "Measure: Map attention to a distribution over the labels.", "Natural Language Question to Networks  Map question to the layout which specifies the set of modules and connections between them.", "Assemble the final network using the layout.", "Parse the input question to obtain set of dependencies and obtain a representation similar to combinatory logic.", "eg “what is the colour of the truck?” becomes “colour(truck)”  The symbolic representation is mapped to a layout:  All leaves become find module.", "All internal nodes become transform/combine module.", "All root nodes become describe/measure module.", "Answering Natural Language Question  Final model combines output from a simple LSTM question encoder with the output of the neural module network.", "This helps in modelling the syntactic and semantic regularities of the question.", "Experiments  Since some modules are updated more frequently than others, adaptive per weight learning rates are better.", "The paper introduces a small SHAPES datasets (64 images and 244 unique questions per image).", "Neural Module Network achieves a score of 90% on SHAPES dataset while VIS + LSTM baseline achieves an accuracy of 65.3%.", "Even on natural images (VQA dataset), the neural module network outperforms the VIS + LSTM baseline."], "summary_text": "For the task of Visual Question Answering , decompose a question into its linguistic substructures and train a neural network module for each substructure. Jointly train the modules and dynamically compose them into deep networks which can learn to answer the question. Start by analyzing the question and decide what logical units are needed to answer the question and what should be the relationship between them. The paper also introduces a new dataset for Visual Question Answering which has challenging, highly compositional questions about abstract shapes. Inspiration  Questions tend to be compositional. Different architectures are needed for different tasks - CNNs for object detection, RNNs for counting. Recurrent and Recursive Neural Networks also use the idea of a different network graph for each input. Neural Module Network for VQA  Training samples of form (w, x, y)  w - Natural Language Question  x - Images  y - Answer  Model specified by collection of modules {m} and a network layout predictor P.  Model instantiates a network based on P(w) and uses that to encode a distribution P(y|w, x, model_params)  Modules  Find: Finds objects of interest. Transform: Shift regions of attention. Combine: Merge two attention maps into a single one. Describe: Map a pair of attention and input image to a distribution over the labels. Measure: Map attention to a distribution over the labels. Natural Language Question to Networks  Map question to the layout which specifies the set of modules and connections between them. Assemble the final network using the layout. Parse the input question to obtain set of dependencies and obtain a representation similar to combinatory logic. eg “what is the colour of the truck?” becomes “colour(truck)”  The symbolic representation is mapped to a layout:  All leaves become find module. All internal nodes become transform/combine module. All root nodes become describe/measure module. Answering Natural Language Question  Final model combines output from a simple LSTM question encoder with the output of the neural module network. This helps in modelling the syntactic and semantic regularities of the question. Experiments  Since some modules are updated more frequently than others, adaptive per weight learning rates are better. The paper introduces a small SHAPES datasets (64 images and 244 unique questions per image). Neural Module Network achieves a score of 90% on SHAPES dataset while VIS + LSTM baseline achieves an accuracy of 65.3%. Even on natural images (VQA dataset), the neural module network outperforms the VIS + LSTM baseline.", "pdf_url": "https://arxiv.org/pdf/1511.02799", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/neural-module-networks.json"}
{"id": "66403710", "bin": "400_500", "summary_sentences": ["This paper proposes to learn embeddings of text and/or images according to a dissimilarity metric that is asymmetric and implements the notion of partial order.", "For example, we'd like the metric to capture that the sentence \"a dog in the yard\" is more specific than just \"a dog\".", "Similarly, given the image of a scene and a caption describing it, we'd also like to capture that the image is more specific than the caption, since captions only describe the main elements of the scene.", "We'd also like to capture the hypernym relation between single words, e.g. where \"woman\" is more specific than \"person\".", "To achieve this, they propose to use the following dissimilarity metric:  $$E(x,y) = ||max(0,y-x)||^2$$  where x and y are embedding vectors and the max operation is applied element-wise.", "The way to use this metric is to learn embeddings such that, for a pair x,y where the object (e.g. \"a dog in the yard\") represented by $x$ is more specific than the object (e.g.", "\"a dog\") represented by $y$, then $E(x,y)$ is as small as possible.", "For example, let's assume that $x$ and y are the output of a neural network, where each output dimension detects a certain concept, i.e. is non-zero only if the concept associated with that dimension is present in the input.", "For x representing \"a dog in the yard\", we could expect having only two dimensions that are non-zero: one detecting the concept \"dog\" (let's note it $x_j$) and another detecting the concept \"yard\" ($x_k$).", "For y representing \"a dog\", only the dimension associated with \"dog\" ($y_j$) would be non-zero and have the same value as $x_j$.", "In this situation, it is easy to see that $E(x,y)$ would be 0, but $E(y,x)$ would be greater than zero, thus capturing appropriately the asymmetric relationship between the two.", "The authors show in the paper how to leverage this new asymmetric metric in training losses that are appropriate for 3 problems: hypernym detection, caption-image retrieval and textual entailment.", "They show that the proposed metric yields superior performance on these problems compared to symmetric metrics that have been used by prior work."], "summary_text": "This paper proposes to learn embeddings of text and/or images according to a dissimilarity metric that is asymmetric and implements the notion of partial order. For example, we'd like the metric to capture that the sentence \"a dog in the yard\" is more specific than just \"a dog\". Similarly, given the image of a scene and a caption describing it, we'd also like to capture that the image is more specific than the caption, since captions only describe the main elements of the scene. We'd also like to capture the hypernym relation between single words, e.g. where \"woman\" is more specific than \"person\". To achieve this, they propose to use the following dissimilarity metric:  $$E(x,y) = ||max(0,y-x)||^2$$  where x and y are embedding vectors and the max operation is applied element-wise. The way to use this metric is to learn embeddings such that, for a pair x,y where the object (e.g. \"a dog in the yard\") represented by $x$ is more specific than the object (e.g. \"a dog\") represented by $y$, then $E(x,y)$ is as small as possible. For example, let's assume that $x$ and y are the output of a neural network, where each output dimension detects a certain concept, i.e. is non-zero only if the concept associated with that dimension is present in the input. For x representing \"a dog in the yard\", we could expect having only two dimensions that are non-zero: one detecting the concept \"dog\" (let's note it $x_j$) and another detecting the concept \"yard\" ($x_k$). For y representing \"a dog\", only the dimension associated with \"dog\" ($y_j$) would be non-zero and have the same value as $x_j$. In this situation, it is easy to see that $E(x,y)$ would be 0, but $E(y,x)$ would be greater than zero, thus capturing appropriately the asymmetric relationship between the two. The authors show in the paper how to leverage this new asymmetric metric in training losses that are appropriate for 3 problems: hypernym detection, caption-image retrieval and textual entailment. They show that the proposed metric yields superior performance on these problems compared to symmetric metrics that have been used by prior work.", "pdf_url": "http://arxiv.org/pdf/1511.06361", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/vendrovkfu15.json"}
{"id": "96831204", "bin": "400_500", "summary_sentences": ["The paper proposes a neural network classifier to perform transition-based dependency parsing using dense vector representation for the features.", "Earlier approaches used a large, manually designed sparse feature vector which took a lot of time and effort to compute and was often incomplete.", "Description of the system  The system described in the paper uses arc-standard system (a greedy, transition-based dependency parsing system).", "Words, POS tags and arc labels are represented as d dimensional vectors.", "Sw, St, Sl denote the set of words, POS and labels respectively.", "Neural network takes as input selected words from the 3 sets and uses a single hidden layer followed by Softmax which models the different actions that can be chosen by the arc-standard system.", "Uses a cube activation function to allow interaction between features coming from the set of words, POS and labels in the first layer itself.", "These features come from different embeddings and are not related as such.", "Using separate embedding for POS tags and labels allow for capturing aspects like NN (singular noun) should be closer to NNS (plural noun) than DT (determiner).", "Input to the network contains words on the stack and buffer and their left and right children (read upon transition-based parsing), their labels and corresponding arc labels.", "Output generated by the system is the action to be taken (transition to be performed) when reading each word in the input.", "This sequential and deterministic nature of the input-output mapping allows the problem to be modelled as a supervised learning problem and a cross entropy loss can be used.", "L2-regularization term is also added to the loss.", "During inference, a greedy decoding strategy is used and transition with the highest score is chosen.", "The paper mentions a pre-computation trick where matrix computation of most frequent top 10000 words is performed beforehand and cached.", "Experiments  Dataset  English Penn Treebank (PTB)  Chinese Penn Treebank (CTB)  Two dependency representations used:  CoNLL Syntactic Dependencies (CD)  Stanford Basic Dependencies (SD)  Metrics:  Unlabeled Attached Scores (UAS)  Labeled Attached Scores (LAS)  Benchmarked against:  Greedy arc-eager parser  Greedy arc-standard parser  Malt-Parser  MSTParser  Results  The system proposed in the paper outperforms all other parsers in both speed and accuracy.", "Analysis  Cube function gives a 0.8-1.2% improvement over tanh.", "Pretained embeddings give 0.7-1.7% improvement over training embeddings from scratch.", "Using POS and labels gives an improvement of 1.7% and 0.4% respectively."], "summary_text": "The paper proposes a neural network classifier to perform transition-based dependency parsing using dense vector representation for the features. Earlier approaches used a large, manually designed sparse feature vector which took a lot of time and effort to compute and was often incomplete. Description of the system  The system described in the paper uses arc-standard system (a greedy, transition-based dependency parsing system). Words, POS tags and arc labels are represented as d dimensional vectors. Sw, St, Sl denote the set of words, POS and labels respectively. Neural network takes as input selected words from the 3 sets and uses a single hidden layer followed by Softmax which models the different actions that can be chosen by the arc-standard system. Uses a cube activation function to allow interaction between features coming from the set of words, POS and labels in the first layer itself. These features come from different embeddings and are not related as such. Using separate embedding for POS tags and labels allow for capturing aspects like NN (singular noun) should be closer to NNS (plural noun) than DT (determiner). Input to the network contains words on the stack and buffer and their left and right children (read upon transition-based parsing), their labels and corresponding arc labels. Output generated by the system is the action to be taken (transition to be performed) when reading each word in the input. This sequential and deterministic nature of the input-output mapping allows the problem to be modelled as a supervised learning problem and a cross entropy loss can be used. L2-regularization term is also added to the loss. During inference, a greedy decoding strategy is used and transition with the highest score is chosen. The paper mentions a pre-computation trick where matrix computation of most frequent top 10000 words is performed beforehand and cached. Experiments  Dataset  English Penn Treebank (PTB)  Chinese Penn Treebank (CTB)  Two dependency representations used:  CoNLL Syntactic Dependencies (CD)  Stanford Basic Dependencies (SD)  Metrics:  Unlabeled Attached Scores (UAS)  Labeled Attached Scores (LAS)  Benchmarked against:  Greedy arc-eager parser  Greedy arc-standard parser  Malt-Parser  MSTParser  Results  The system proposed in the paper outperforms all other parsers in both speed and accuracy. Analysis  Cube function gives a 0.8-1.2% improvement over tanh. Pretained embeddings give 0.7-1.7% improvement over training embeddings from scratch. Using POS and labels gives an improvement of 1.7% and 0.4% respectively.", "pdf_url": "https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/a-fast-and-accurate-dependency-parser-using-neural-networks.json"}
{"id": "2905851", "bin": "400_500", "summary_sentences": ["The paper proposes a new algorithm called as Probabilistic Ensemble with Trajectory sampling (PETS) that combines uncertainty aware deep learning models (ensemble of deep learning models that encode uncertainty) with sampling-based uncertainty propagation.", "PETS improves over other probabilistic MBRL approaches by isolating epistemic uncertainty (due to limited training data) and aleatoric uncertainty (inherent in the system).", "Uncertainty-Aware Neural Network Dynamics Model  Aleatoric uncertainty can be accounted for by learning a parameterized distribution (probabilistic neural network) trained with negative log-likelihood.", "Epistemic uncertainty can be accounted for by either having an infinite amount of data or by using ensembles.", "The paper uses a neural network to predict the mean and standard deviation of a gaussian distribution which defines the predictive model.", "This setup is referred to as the “probabilistic” model and denoted by P.  The alternate setup of the deterministic model is where a neural network is used to make a point prediction (and is denoted by D).", "Ensemble of probabilistic models is denoted as PE while that of deterministic models is denoted as DE.", "Planning and Control with learned Dynamics  Model Predictive Control (MPC) is used for planning.", "Given a start state and an action sequence, the probabilistic dynamics model induces a distribution over the trajectories.", "The first action, among the sequence of optimized actions, is executed.", "Instead of random shooting, Cross Entropy Method (CEM) is used.", "Trajectory Sampling  Let us say there are B bootstrap models in the ensemble.", "Given the current state, P particles are created and each particle is propagated using one of the bootstrap models.", "Two variants are considered:  TS1 - At each timestep, each particle samples a bootstrap.", "In this case, particle separation can not be attributed to ti the compounding effects of the bootstraps.", "TS$\\infty$ - The bootstrapped model (per particle) is sampled just once and is not changed after that.", "This setup separates aleatoric and epistemic uncertainty.", "Aleatoric state variance is the average variance of the particles of the same bootstrap while epistemic state variance is the variance of the average of particles of same bootstrap indexes.", "Result  The proposed approach reaches the asymptotic performance of state-of-the-art model-free algorithms in much fewer samples.", "The general performance trend is probabilistic emsemble > probabilisitc model > deterministc ensemble > determinisitc model./.", "Initial experiments for learning policy by propagating gradients through the ensemble of models did not work and has been left as future work."], "summary_text": "The paper proposes a new algorithm called as Probabilistic Ensemble with Trajectory sampling (PETS) that combines uncertainty aware deep learning models (ensemble of deep learning models that encode uncertainty) with sampling-based uncertainty propagation. PETS improves over other probabilistic MBRL approaches by isolating epistemic uncertainty (due to limited training data) and aleatoric uncertainty (inherent in the system). Uncertainty-Aware Neural Network Dynamics Model  Aleatoric uncertainty can be accounted for by learning a parameterized distribution (probabilistic neural network) trained with negative log-likelihood. Epistemic uncertainty can be accounted for by either having an infinite amount of data or by using ensembles. The paper uses a neural network to predict the mean and standard deviation of a gaussian distribution which defines the predictive model. This setup is referred to as the “probabilistic” model and denoted by P.  The alternate setup of the deterministic model is where a neural network is used to make a point prediction (and is denoted by D). Ensemble of probabilistic models is denoted as PE while that of deterministic models is denoted as DE. Planning and Control with learned Dynamics  Model Predictive Control (MPC) is used for planning. Given a start state and an action sequence, the probabilistic dynamics model induces a distribution over the trajectories. The first action, among the sequence of optimized actions, is executed. Instead of random shooting, Cross Entropy Method (CEM) is used. Trajectory Sampling  Let us say there are B bootstrap models in the ensemble. Given the current state, P particles are created and each particle is propagated using one of the bootstrap models. Two variants are considered:  TS1 - At each timestep, each particle samples a bootstrap. In this case, particle separation can not be attributed to ti the compounding effects of the bootstraps. TS$\\infty$ - The bootstrapped model (per particle) is sampled just once and is not changed after that. This setup separates aleatoric and epistemic uncertainty. Aleatoric state variance is the average variance of the particles of the same bootstrap while epistemic state variance is the variance of the average of particles of same bootstrap indexes. Result  The proposed approach reaches the asymptotic performance of state-of-the-art model-free algorithms in much fewer samples. The general performance trend is probabilistic emsemble > probabilisitc model > deterministc ensemble > determinisitc model./. Initial experiments for learning policy by propagating gradients through the ensemble of models did not work and has been left as future work.", "pdf_url": "https://arxiv.org/pdf/1805.12114.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/400_500/deep-reinforcement-learning-in-a-handful-of-trials-using-probabilistic-dynamics-models.json"}
{"id": "7120002", "bin": "500_600", "summary_sentences": ["Catastrophic Forgetting refers to the phenomenon where when a learning system is trained on two tasks in succession, it may forget how to perform the first task.", "The paper investigates this behaviour for different learning activations in presence and absence of dropout.", "Link to the implementation  Experiment Formulation  For each experiment, two tasks are defined - “old” task and “new” task.", "The network is first trained on the “old” task until the validation set error has not improved for the last 100 epochs.", "The “best” performing model is then trained for the “new” task until the combined error on the “old” and the “new” validation datasets has not improved in the last 100 epochs.", "All the tasks used the same model architecture - 2 hidden layers followed by a softmax layer.", "Following activations were tested:  Sigmoid  ReLU  Hard Local Winner Takes It All  Maxout  Models were trained using SGD with or without dropout.", "For each combination of the model, activation and the training mechanism, a random hyper param search was performed with set of 25 hyperparams.", "The authors took care to keep the hyperparams and other settings consistent and comparable across different experiments.", "Deviations, wherever applicable, and their reasons were documented.", "Observations  In terms of the relationship between the “old” and the “new” tasks, three kinds of settings are considered:  The tasks are very very similar but the input is processed in a different format.", "For this setting, MNIST dataset was used with a different permutation of pixels for the “old” and the “new” task.", "The tasks are similar but not exactly the same.", "For this setting, the task was to predict sentiments of reviews across 2 different product categories.", "In the last setting, 2 dissimilar tasks were used.", "One task was to predict sentiment of reviews and another task was to perform classification over MNIST dataset (reduced to 2 classes).", "Using Dropout improved the overall validation performance for all the models for all the tasks.", "Using Dropout also increase the size of the optimal model across all the activations indicating that maybe the increased size of the model could explain the increased resistance to forgetting.", "It would have been interesting to check if dropout always selected the largest model possible given the set of the hyperparams.", "On the dissimilar task, dropout improved the performance while reducing the model size so it might have other properties as well that helps to prevent forgetting.", "As compared to the choice of training technique, the activation function has a less consistent effect on resistance to forgetting.", "The paper recommends performing cross-validation for the choice of the activation function.", "If that is not feasible, maxout activation function with dropout could be used."], "summary_text": "Catastrophic Forgetting refers to the phenomenon where when a learning system is trained on two tasks in succession, it may forget how to perform the first task. The paper investigates this behaviour for different learning activations in presence and absence of dropout. Link to the implementation  Experiment Formulation  For each experiment, two tasks are defined - “old” task and “new” task. The network is first trained on the “old” task until the validation set error has not improved for the last 100 epochs. The “best” performing model is then trained for the “new” task until the combined error on the “old” and the “new” validation datasets has not improved in the last 100 epochs. All the tasks used the same model architecture - 2 hidden layers followed by a softmax layer. Following activations were tested:  Sigmoid  ReLU  Hard Local Winner Takes It All  Maxout  Models were trained using SGD with or without dropout. For each combination of the model, activation and the training mechanism, a random hyper param search was performed with set of 25 hyperparams. The authors took care to keep the hyperparams and other settings consistent and comparable across different experiments. Deviations, wherever applicable, and their reasons were documented. Observations  In terms of the relationship between the “old” and the “new” tasks, three kinds of settings are considered:  The tasks are very very similar but the input is processed in a different format. For this setting, MNIST dataset was used with a different permutation of pixels for the “old” and the “new” task. The tasks are similar but not exactly the same. For this setting, the task was to predict sentiments of reviews across 2 different product categories. In the last setting, 2 dissimilar tasks were used. One task was to predict sentiment of reviews and another task was to perform classification over MNIST dataset (reduced to 2 classes). Using Dropout improved the overall validation performance for all the models for all the tasks. Using Dropout also increase the size of the optimal model across all the activations indicating that maybe the increased size of the model could explain the increased resistance to forgetting. It would have been interesting to check if dropout always selected the largest model possible given the set of the hyperparams. On the dissimilar task, dropout improved the performance while reducing the model size so it might have other properties as well that helps to prevent forgetting. As compared to the choice of training technique, the activation function has a less consistent effect on resistance to forgetting. The paper recommends performing cross-validation for the choice of the activation function. If that is not feasible, maxout activation function with dropout could be used.", "pdf_url": "https://arxiv.org/pdf/1312.6211", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/an-empirical-investigation-of-catastrophic-forgetting-in-gradient-based-neural-networks.json"}
{"id": "11636296", "bin": "500_600", "summary_sentences": ["Relational Reinforcement Learning (RRL) paradigm uses relational state (and action) space and policy representation to leverage the generalization capability of relational learning for reinforcement learning.", "The paper shows that effectiveness of RRL - in terms of generalization, sample efficiency and interplay - using box-world and StarCraft II minigames.", ".", "Architecture  The main idea is to use neural network models that operate on structured representations and perform relational reasoning via iterated, message-passing style methods.", "Use of non-local computations using a shared function (in terms of pairwise interactions between entities) provides a better inductive bias.", "Multi-head dot product attention mechanism is used to model the pairwise interactions (with one or more attention blocks).", "Iterative computations can be used to capture higher-order interactions between entities.", "Entity extraction is based on the assumption that entities are things located at a particular point in space.", "A CNN is used to parse the pixel space observation into k feature maps of size nxn.", "The (x, y) coordinates are concatenated to each k-dimensional pixel feature-vector to indicate the pixel’s position in the map.", "The resulting n2 x k matrix acts as the entity matrix.", "Actor-critic architecture (using distributed agent IMPALA) is used.", "Environment  Box-World  12 x 12-pixel room with keys and boxes placed randomly.", "Agent can move in 4 directions.", "The task is to collect gems by unlocking boxes (which may contain keys to unlock other boxes).", "Each level has a unique sequence in which boxes need to be opened as opening the wrong box could make the level unsolvable.", "Difficulty of a level can be controlled using: (i) Number of boxes in the path to the goal.", "(ii) The number of distractor branches, (iii)  Length of distractor branches.", "StarCraft II minigames  9 mini games designed as specific scenarios in the Starcraft game are used.", "Results  Box-World  RRL agents solve over 98% of the levels while the RL agent solves less than 95% of the levels.", "Visualising the attention scores indicate that:  keys attend to locks they can unlock.", "all objects attend to agent’s location.", "agent and gem attend to each other (and themselves).", "Generalization capacity is tested in two ways:  Performance on levels that require opening a larger sequence of boxes than it is trained on.", "Performance on levels that require key-lock combinations not seen during training.", "In both the scenarios, the RRL agent significantly outperforms the RL agent.", "StarCraft  RLL agent achieves better or equal results that the RL agent in all but one game.", "For testing generalization, the agent, that was trained for controlling two marines, was transferred on the task which requires it to control 5 marines.", "These results are not conclusive given the high variability."], "summary_text": "Relational Reinforcement Learning (RRL) paradigm uses relational state (and action) space and policy representation to leverage the generalization capability of relational learning for reinforcement learning. The paper shows that effectiveness of RRL - in terms of generalization, sample efficiency and interplay - using box-world and StarCraft II minigames. . Architecture  The main idea is to use neural network models that operate on structured representations and perform relational reasoning via iterated, message-passing style methods. Use of non-local computations using a shared function (in terms of pairwise interactions between entities) provides a better inductive bias. Multi-head dot product attention mechanism is used to model the pairwise interactions (with one or more attention blocks). Iterative computations can be used to capture higher-order interactions between entities. Entity extraction is based on the assumption that entities are things located at a particular point in space. A CNN is used to parse the pixel space observation into k feature maps of size nxn. The (x, y) coordinates are concatenated to each k-dimensional pixel feature-vector to indicate the pixel’s position in the map. The resulting n2 x k matrix acts as the entity matrix. Actor-critic architecture (using distributed agent IMPALA) is used. Environment  Box-World  12 x 12-pixel room with keys and boxes placed randomly. Agent can move in 4 directions. The task is to collect gems by unlocking boxes (which may contain keys to unlock other boxes). Each level has a unique sequence in which boxes need to be opened as opening the wrong box could make the level unsolvable. Difficulty of a level can be controlled using: (i) Number of boxes in the path to the goal. (ii) The number of distractor branches, (iii)  Length of distractor branches. StarCraft II minigames  9 mini games designed as specific scenarios in the Starcraft game are used. Results  Box-World  RRL agents solve over 98% of the levels while the RL agent solves less than 95% of the levels. Visualising the attention scores indicate that:  keys attend to locks they can unlock. all objects attend to agent’s location. agent and gem attend to each other (and themselves). Generalization capacity is tested in two ways:  Performance on levels that require opening a larger sequence of boxes than it is trained on. Performance on levels that require key-lock combinations not seen during training. In both the scenarios, the RRL agent significantly outperforms the RL agent. StarCraft  RLL agent achieves better or equal results that the RL agent in all but one game. For testing generalization, the agent, that was trained for controlling two marines, was transferred on the task which requires it to control 5 marines. These results are not conclusive given the high variability.", "pdf_url": "https://arxiv.org/pdf/1806.01830", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/relational-reinforcement-learning.json"}
{"id": "94730041", "bin": "500_600", "summary_sentences": ["The first two sections of this paper provide a decent overview of recent advances in the hierarchical and intrinsic RL literature.", "The authors propose a learning framework for achieving complex goals in the face of sparse rewards.", "Drawing from Singh, et al. , a separation between intrinsic and extrinsic learning is derived, such that external reward signals are generated from the environment and internal reward signals are generated by an internal critic.", "There is a meta-controller that learns an approximation of the optimal goal policy $\\pi_{g} (g | s )$ and a controller that learns an approximation of the optimal action policy $\\pi_{a g} (a|g,s)$.", "The meta-controller operates on a slower time-scale than the controller; it is concerned with selecting the optimal goal for the controller to work towards.", "The internal critic provides incremental feedback for the controller, and the meta-controller receives external feedback from the environment.", "Each controller is represented as a Deep Q-Network, and the usual DRL learning tricks are applied.", "The framework is tested in two settings, a stochastic decision making problem and the Atari game Montezuma’s Revenge.", "The hierarchical agent significantly outperforms DQN on Montezuma’s Revenge (DQN gets 0 points).", "My Notes  The main contribution of this paper is the use of deep Q-networks for hierarchical/intrinsically motivated RL.", "However, the theory of intrinsically motivated RL and hierarchical RL with sub-policies for learning incremental behaviors under sparse extrinsic feedback is not novel; hence, the overall impact of the paper is substantially reduced.", "DQN is used as a baseline in the results/figures; but the reason for using such a questionable baseline is unclear.", "The authors even mention that Gorila DQN achieves a better average reward of 4.16; it would be better to see a more recent DRL algorithm used as a baseline.", "The set of goals, the intrinsic critic, and the external reward all still need to be hand-crafted for every learning problem- not a flaw of the research, since the motivation for this work was to handle sparse rewards.", "The algorithm relies doubly on epsilon-greedy exploration; an epsilon parameter is annealed for both policies.", "Even with the exploration decay schedule, the asymptotic variance of the total reward for the Montezuma experiment is pretty large.", "To solve Montezuma’s Revenge, they implemented a “custom object detector” to identify objects in the game such as the ladder and key.", "However, the authors only mention it in passing.", "Also, the details of the internal critic for the Montezuma agent are unclear.", "It appears that they defined certain relations such as “agent reaches ladder” and “agent reaches key” and used these for intrinsic rewards, but it is unclear exactly how they went about doing it.", "They claim that their method does not require explicit encoding of the relations between objects as well.", "Overall, the idea of using hierarchical DQN to learn a temporal abstraction is a promising one, and it should be explored more.", "Unsupervised discovery of sub-tasks/goals is still an open problem in RL as well.", "An interesting avenue to look into as mentioned by the authors is the use of evolutionary methods to search the space of reward functions ."], "summary_text": "The first two sections of this paper provide a decent overview of recent advances in the hierarchical and intrinsic RL literature. The authors propose a learning framework for achieving complex goals in the face of sparse rewards. Drawing from Singh, et al. , a separation between intrinsic and extrinsic learning is derived, such that external reward signals are generated from the environment and internal reward signals are generated by an internal critic. There is a meta-controller that learns an approximation of the optimal goal policy $\\pi_{g} (g | s )$ and a controller that learns an approximation of the optimal action policy $\\pi_{a g} (a|g,s)$. The meta-controller operates on a slower time-scale than the controller; it is concerned with selecting the optimal goal for the controller to work towards. The internal critic provides incremental feedback for the controller, and the meta-controller receives external feedback from the environment. Each controller is represented as a Deep Q-Network, and the usual DRL learning tricks are applied. The framework is tested in two settings, a stochastic decision making problem and the Atari game Montezuma’s Revenge. The hierarchical agent significantly outperforms DQN on Montezuma’s Revenge (DQN gets 0 points). My Notes  The main contribution of this paper is the use of deep Q-networks for hierarchical/intrinsically motivated RL. However, the theory of intrinsically motivated RL and hierarchical RL with sub-policies for learning incremental behaviors under sparse extrinsic feedback is not novel; hence, the overall impact of the paper is substantially reduced. DQN is used as a baseline in the results/figures; but the reason for using such a questionable baseline is unclear. The authors even mention that Gorila DQN achieves a better average reward of 4.16; it would be better to see a more recent DRL algorithm used as a baseline. The set of goals, the intrinsic critic, and the external reward all still need to be hand-crafted for every learning problem- not a flaw of the research, since the motivation for this work was to handle sparse rewards. The algorithm relies doubly on epsilon-greedy exploration; an epsilon parameter is annealed for both policies. Even with the exploration decay schedule, the asymptotic variance of the total reward for the Montezuma experiment is pretty large. To solve Montezuma’s Revenge, they implemented a “custom object detector” to identify objects in the game such as the ladder and key. However, the authors only mention it in passing. Also, the details of the internal critic for the Montezuma agent are unclear. It appears that they defined certain relations such as “agent reaches ladder” and “agent reaches key” and used these for intrinsic rewards, but it is unclear exactly how they went about doing it. They claim that their method does not require explicit encoding of the relations between objects as well. Overall, the idea of using hierarchical DQN to learn a temporal abstraction is a promising one, and it should be explored more. Unsupervised discovery of sub-tasks/goals is still an open problem in RL as well. An interesting avenue to look into as mentioned by the authors is the use of evolutionary methods to search the space of reward functions .", "pdf_url": "http://arxiv.org/pdf/1604.06057v2.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/hierarchical-drl-intrinsic-motivation.json"}
{"id": "53721399", "bin": "500_600", "summary_sentences": ["When neural networks are trained on images, they tend to learn the same kind of features for the first layer (corresponding to Gabor filters or colour blobs).", "The first layer features are “general” irrespective of the task/optimizer etc.", "The final layer features tend to be “specific” in the sense that they strongly depend on the task.", "The paper studies the transition of generalization property across layers in the network.", "This could be useful in the domain of transfer learning where features are reused across tasks.", "Setup  Degree of generality of a set of features, learned on task A, is defined as the extent to which these features can be used for another task B.  Randomly split 1000 ImageNet classes into 2 groups (corresponding to tasks A and B).", "Each group has 500 classes and half the total number of examples.", "Two 8-layer convolutional networks are trained on the two datasets and labelled as baseA and baseB respectively.", "Now choose a layer numbered n from {1, 2…7}.", "For each layer n, train the following two networks:  Selffer Network BnB  Copy (and freeze) first n layers from baseB.", "The remaining layers are initialized randomly and trained on B.", "This serves as the control group.", "Transfer Network AnB  Copy (and freeze) first n layers from baseA.", "The remaining layers are initialized randomly and trained on B.", "This corresponds to transferring features from A to B.", "If AnB performs well, nth layer features are “general”.", "In another setting, the transferred layers are also fine-tuned (BnB+ and AnB+).", "ImageNet dataset contains a hierarchy of classes which allow for creating the datasets A and B with high and low similarity.", "Observation  Dataset A and B are similar  For n = {1, 2}, the performance of the BnB model is same as baseB model.", "For n = {3, 4, 5, 6}, the performance of BnB model is worse.", "This indicates the presence of “fragile co-adaption” features on successive layers where features interact with each other in a complex way and can not be easily separated across layers.", "This is more prominent across middle layers and less across the first and the last layers.", "For model AnB, the performance of baseB for n = {1, 2}.", "Beyond that, the performance begins to drop.", "Transfer learning of features followed by fine-tuning gives better results than training the network from scratch.", "Dataset A and B are dissimilar  Effectiveness of feature transfer decreases as the two tasks become less similar.", "Random Weights  Instead of using transferred weights in BnB and BnA, the first n layers were initialized randomly.", "The performance falls for layer 1 and 2.", "It further drops to near-random level for layers 3 and beyond.", "Another interesting insight is that even for dissimilar tasks, transferring features is better than using random features."], "summary_text": "When neural networks are trained on images, they tend to learn the same kind of features for the first layer (corresponding to Gabor filters or colour blobs). The first layer features are “general” irrespective of the task/optimizer etc. The final layer features tend to be “specific” in the sense that they strongly depend on the task. The paper studies the transition of generalization property across layers in the network. This could be useful in the domain of transfer learning where features are reused across tasks. Setup  Degree of generality of a set of features, learned on task A, is defined as the extent to which these features can be used for another task B.  Randomly split 1000 ImageNet classes into 2 groups (corresponding to tasks A and B). Each group has 500 classes and half the total number of examples. Two 8-layer convolutional networks are trained on the two datasets and labelled as baseA and baseB respectively. Now choose a layer numbered n from {1, 2…7}. For each layer n, train the following two networks:  Selffer Network BnB  Copy (and freeze) first n layers from baseB. The remaining layers are initialized randomly and trained on B. This serves as the control group. Transfer Network AnB  Copy (and freeze) first n layers from baseA. The remaining layers are initialized randomly and trained on B. This corresponds to transferring features from A to B. If AnB performs well, nth layer features are “general”. In another setting, the transferred layers are also fine-tuned (BnB+ and AnB+). ImageNet dataset contains a hierarchy of classes which allow for creating the datasets A and B with high and low similarity. Observation  Dataset A and B are similar  For n = {1, 2}, the performance of the BnB model is same as baseB model. For n = {3, 4, 5, 6}, the performance of BnB model is worse. This indicates the presence of “fragile co-adaption” features on successive layers where features interact with each other in a complex way and can not be easily separated across layers. This is more prominent across middle layers and less across the first and the last layers. For model AnB, the performance of baseB for n = {1, 2}. Beyond that, the performance begins to drop. Transfer learning of features followed by fine-tuning gives better results than training the network from scratch. Dataset A and B are dissimilar  Effectiveness of feature transfer decreases as the two tasks become less similar. Random Weights  Instead of using transferred weights in BnB and BnA, the first n layers were initialized randomly. The performance falls for layer 1 and 2. It further drops to near-random level for layers 3 and beyond. Another interesting insight is that even for dissimilar tasks, transferring features is better than using random features.", "pdf_url": "http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/how-transferable-are-features-in-deep-neural-networks.json"}
{"id": "74261188", "bin": "500_600", "summary_sentences": ["The research question the authors answered was whether by shifting from an episodic to a “schematic”, or gist-like, memory system, a reinforcement learning agent could learn to achieve its goals in a dynamic environment.", "The authors focused on 2D navigation tasks where the reward locations constantly changed, such that new reward locations were correlated in the short-term but where independent and sampled from a stable distribution in the long-term.", "I found it interesting that the authors claimed the real world is like this, and consequentially they staked a lot of the significance of their work on this fact.", "The main conclusion they came to was that given the existence of a stable long-term distribution for reward location (or whatever random variable the agent is concerned with estimating a distribution for), the optimal strategy for an agent is to shift from utilizing episodic to schematic memories slowly.", "The authors implemented their agent using a novel neural network architecture that consisted of, in general, an episodic memory system, a schematic memory system, a critic to generate a TD-error.", "The episodic memory system was:  a spatial encoder which took in the (x,y)-pair of the current location of the agent,  an autoencoder implemented as a 3-layer recurrent network  a network of place field units  The output of the spatial encoder fed into the autoencoder, and the output of this fed into the place cells.", "“Retrieving” memory from the place cells was implemented as a fully-connected sigmoid layer.", "The use of place field units was quite interesting; the idea behind this was to learn to associate activation patterns of place cells with specific locations within the environment where rewards were recently found.", "The schematic memory was implemented as a Restricted Boltzman Machine.", "The first layer was a direct projection of the place cells from the episodic network.", "The ultimate goal of the RBM was to learn a general statistical model of the reward locations.", "It was trained in an offline manner (i.e., while the agent was “at rest” between trials) by using random activity in the spatial encoder, and propagating that through to the RBM.", "This was curious, but apparently since they also had added a TD-prediction error to the episodic system via a critic, this was more biologically plausible than iid sampling from the episodic memory.", "The agent has a parameter that controls how much it mixes its episodic and schematic memories; the resultant “mixed” memory then influences action-selection.", "Future Directions  How would this compare with an LSTM- literally, a Long Short-Term Memory neural network?", "Can an LSTM learn to adapt to environments with with both short-term and long-term statistical patterns like this?", "We’re seeing a shift towards more complex RL environments that this could be applied to; for example, 3D navigation tasks where there are multiple goals that could potentially move over time.", "Perhaps this could also be applied to modeling of the dynamic behavior of other agents in a multi-agent setting?", "Cool use of unsupervised learning to enhance RL!"], "summary_text": "The research question the authors answered was whether by shifting from an episodic to a “schematic”, or gist-like, memory system, a reinforcement learning agent could learn to achieve its goals in a dynamic environment. The authors focused on 2D navigation tasks where the reward locations constantly changed, such that new reward locations were correlated in the short-term but where independent and sampled from a stable distribution in the long-term. I found it interesting that the authors claimed the real world is like this, and consequentially they staked a lot of the significance of their work on this fact. The main conclusion they came to was that given the existence of a stable long-term distribution for reward location (or whatever random variable the agent is concerned with estimating a distribution for), the optimal strategy for an agent is to shift from utilizing episodic to schematic memories slowly. The authors implemented their agent using a novel neural network architecture that consisted of, in general, an episodic memory system, a schematic memory system, a critic to generate a TD-error. The episodic memory system was:  a spatial encoder which took in the (x,y)-pair of the current location of the agent,  an autoencoder implemented as a 3-layer recurrent network  a network of place field units  The output of the spatial encoder fed into the autoencoder, and the output of this fed into the place cells. “Retrieving” memory from the place cells was implemented as a fully-connected sigmoid layer. The use of place field units was quite interesting; the idea behind this was to learn to associate activation patterns of place cells with specific locations within the environment where rewards were recently found. The schematic memory was implemented as a Restricted Boltzman Machine. The first layer was a direct projection of the place cells from the episodic network. The ultimate goal of the RBM was to learn a general statistical model of the reward locations. It was trained in an offline manner (i.e., while the agent was “at rest” between trials) by using random activity in the spatial encoder, and propagating that through to the RBM. This was curious, but apparently since they also had added a TD-prediction error to the episodic system via a critic, this was more biologically plausible than iid sampling from the episodic memory. The agent has a parameter that controls how much it mixes its episodic and schematic memories; the resultant “mixed” memory then influences action-selection. Future Directions  How would this compare with an LSTM- literally, a Long Short-Term Memory neural network? Can an LSTM learn to adapt to environments with with both short-term and long-term statistical patterns like this? We’re seeing a shift towards more complex RL environments that this could be applied to; for example, 3D navigation tasks where there are multiple goals that could potentially move over time. Perhaps this could also be applied to modeling of the dynamic behavior of other agents in a multi-agent setting? Cool use of unsupervised learning to enhance RL!", "pdf_url": "https://www.jneurosci.org/content/jneuro/36/48/12228.full.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/500_600/memory-transformations.json"}
{"id": "54139177", "bin": "600_700", "summary_sentences": ["Analysis of join-the-shortest queue routing for web server farms – Gupter et al 2007  What’s the best way to balance web requests across a set of servers?", "Round-robin is the simple algorithm that everyone knows best, but there is a better way… This paper analyzes the Join the Shortest Queue (JSQ) routing policy and shows that it delivers near-optimal results.", "It is assumed that the load balancer (dispatcher) immediately routes requests to one of the servers in the farm, and that servers equally split their capacity over the requests they are processing.", "This is known a a processor sharing (PS) scheduling policy.", "We are thus interested in an PS server farm with immediate dispatch.", "Although the paper considers the serving of static web pages, the immediate dispatch and PS server policy are equally applicable in the case of web apps and REST APIs etc.", "The JSQ routing policy is widely used in commercial dispatchers.", "Under JSQ, an incoming request is routed to the server with the least number of unfinished requests.", "Thus, JSQ strives to balance load across the servers reducing the probability of one server having several jobs while another server sits idle.", "From the point of view of a new arrival it is a greedy policy for the case of PS servers, because the arrival would prefer sharing with as few jobs as possible.", "Gupter et al. build a queueing model conditioned by the average request arrival rate (assume a Poisson distribution), number of servers, and (mean) time taken by a job to run on a server in isolation.", "In the typical queueing theory description, this is an M/G/K/JSQ/PS model : Poisson arrival rate M, General distribution of service processing times, K servers, JSQ dispatching, and PS serving.", "Despite the ubiquity of JSQ/PS server farms, no-one has yet analyzed the performance of JSQ in this setting.", "The full analysis is heavy reading at points, but thankfully the key results are all summarized for us.", "Firstly it is shown that a single queue approximation (focusing on what happens at just one of the servers) can be used to understand the behaviour of the system as a whole, and furthermore that this approximation is exact if job-size distribution is exponential.", "Does the behaviour of the system depend on the distribution of job sizes (I.e. the mix of long and short time to service requests)?", "The JSQ/PS system shows near insensitivity to the variability of the job-size distribution… This is a non-trivial result since very similar routing policies for PS server farms, like Least-Work-Left (sending the job to the host with the least total work), or Round-Robin, are highly sensitive to the job-size distribution.", "The model was tested against extensive simulations and found to be highly accurate:  …our analytical approximation method is always within 2.5% of simulation estimates for mean queue length and response time, under all job-size distributions examined.", "(which also means you could use this model for capacity planning and/or response time estimations).", "So JSQ has some very nice properties, but is it actually a good choice for your load-balancing algorithm?", "We show, via simulation, that it is unlikely there is a routing policy that outperforms JSQ by more than about 10%  So there you have it: easy to implement, stable under a wide variety of request distributions, useful for modelling, and near-optimal.", "If you need to implement a load-balancing function for PS servers, JSQ is the way to go…"], "summary_text": "Analysis of join-the-shortest queue routing for web server farms – Gupter et al 2007  What’s the best way to balance web requests across a set of servers? Round-robin is the simple algorithm that everyone knows best, but there is a better way… This paper analyzes the Join the Shortest Queue (JSQ) routing policy and shows that it delivers near-optimal results. It is assumed that the load balancer (dispatcher) immediately routes requests to one of the servers in the farm, and that servers equally split their capacity over the requests they are processing. This is known a a processor sharing (PS) scheduling policy. We are thus interested in an PS server farm with immediate dispatch. Although the paper considers the serving of static web pages, the immediate dispatch and PS server policy are equally applicable in the case of web apps and REST APIs etc. The JSQ routing policy is widely used in commercial dispatchers. Under JSQ, an incoming request is routed to the server with the least number of unfinished requests. Thus, JSQ strives to balance load across the servers reducing the probability of one server having several jobs while another server sits idle. From the point of view of a new arrival it is a greedy policy for the case of PS servers, because the arrival would prefer sharing with as few jobs as possible. Gupter et al. build a queueing model conditioned by the average request arrival rate (assume a Poisson distribution), number of servers, and (mean) time taken by a job to run on a server in isolation. In the typical queueing theory description, this is an M/G/K/JSQ/PS model : Poisson arrival rate M, General distribution of service processing times, K servers, JSQ dispatching, and PS serving. Despite the ubiquity of JSQ/PS server farms, no-one has yet analyzed the performance of JSQ in this setting. The full analysis is heavy reading at points, but thankfully the key results are all summarized for us. Firstly it is shown that a single queue approximation (focusing on what happens at just one of the servers) can be used to understand the behaviour of the system as a whole, and furthermore that this approximation is exact if job-size distribution is exponential. Does the behaviour of the system depend on the distribution of job sizes (I.e. the mix of long and short time to service requests)? The JSQ/PS system shows near insensitivity to the variability of the job-size distribution… This is a non-trivial result since very similar routing policies for PS server farms, like Least-Work-Left (sending the job to the host with the least total work), or Round-Robin, are highly sensitive to the job-size distribution. The model was tested against extensive simulations and found to be highly accurate:  …our analytical approximation method is always within 2.5% of simulation estimates for mean queue length and response time, under all job-size distributions examined. (which also means you could use this model for capacity planning and/or response time estimations). So JSQ has some very nice properties, but is it actually a good choice for your load-balancing algorithm? We show, via simulation, that it is unlikely there is a routing policy that outperforms JSQ by more than about 10%  So there you have it: easy to implement, stable under a wide variety of request distributions, useful for modelling, and near-optimal. If you need to implement a load-balancing function for PS servers, JSQ is the way to go…", "pdf_url": "http://www.cs.cmu.edu/~harchol/Papers/peva07.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/analysis-of-join-the-shortest-queue-routing.json"}
{"id": "52492536", "bin": "600_700", "summary_sentences": ["What  They describe a new architecture for GANs.", "The architecture is based on letting the Generator (G) create images in multiple steps, similar to DRAW.", "They also briefly suggest a method to compare the quality of the results of different generators with each other.", "How  In a classic GAN one samples a noise vector z, feeds that into a Generator (G), which then generates an image x, which is then fed through the Discriminator (D) to estimate its quality.", "Their method operates in basically the same way, but internally G is changed to generate images in multiple time steps.", "Outline of how their G operates:  Time step 0:  Input: Empty image delta C-1, randomly sampled z.", "Feed delta C-1 through a number of downsampling convolutions to create a tensor.", "(Not very useful here, as the image is empty.", "More useful in later timesteps.)", "Feed z through a number of upsampling convolutions to create a tensor (similar to DCGAN).", "Concat the output of the previous two steps.", "Feed that concatenation through a few more convolutions.", "Output: delta C0 (changes to apply to the empty starting canvas).", "Time step 1 (and later):  Input: Previous change delta C0, randomly sampled z (can be the same as in step 0).", "Feed delta C0 through a number of downsampling convolutions to create a tensor.", "Feed z through a number of upsampling convolutions to create a tensor (similar to DCGAN).", "Concat the output of the previous two steps.", "Feed that concatenation through a few more convolutions.", "Output: delta C1 (changes to apply to the empty starting canvas).", "At the end, after all timesteps have been performed:  Create final output image by summing all the changes, i.e. delta C0 + delta C1 + ..., which basically means empty start canvas + changes from time step 0 + changes from time step 1 + ....  Their architecture as an image:  Comparison measure  They suggest a new method to compare GAN results with each other.", "They suggest to train pairs of G and D, e.g. for two pairs (G1, D1), (G2, D2).", "Then they let the pairs compete with each other.", "To estimate the quality of D they suggest r_test = errorRate(D1, testset) / errorRate(D2, testset).", "(\"Which D is better at spotting that the test set images are real images?\")", "To estimate the quality of the generated samples they suggest r_sample = errorRate(D1, images by G2) / errorRate(D2, images by G1).", "(\"Which G is better at fooling an unknown D, i.e. possibly better at generating life-like images?\")", "They suggest to estimate which G is better using r_sample and then to estimate how valid that result is using r_test.", "Results  Generated images of churches, with timesteps 1 to 5:  Overfitting  They saw no indication of overfitting in the sense of memorizing images from the training dataset.", "They however saw some indication of G just interpolating between some good images and of G reusing small image patches in different images.", "Randomness of noise vector z:  Sampling the noise vector once seems to be better than resampling it at every timestep.", "Resampling it at every time step often led to very similar looking output images."], "summary_text": "What  They describe a new architecture for GANs. The architecture is based on letting the Generator (G) create images in multiple steps, similar to DRAW. They also briefly suggest a method to compare the quality of the results of different generators with each other. How  In a classic GAN one samples a noise vector z, feeds that into a Generator (G), which then generates an image x, which is then fed through the Discriminator (D) to estimate its quality. Their method operates in basically the same way, but internally G is changed to generate images in multiple time steps. Outline of how their G operates:  Time step 0:  Input: Empty image delta C-1, randomly sampled z. Feed delta C-1 through a number of downsampling convolutions to create a tensor. (Not very useful here, as the image is empty. More useful in later timesteps.) Feed z through a number of upsampling convolutions to create a tensor (similar to DCGAN). Concat the output of the previous two steps. Feed that concatenation through a few more convolutions. Output: delta C0 (changes to apply to the empty starting canvas). Time step 1 (and later):  Input: Previous change delta C0, randomly sampled z (can be the same as in step 0). Feed delta C0 through a number of downsampling convolutions to create a tensor. Feed z through a number of upsampling convolutions to create a tensor (similar to DCGAN). Concat the output of the previous two steps. Feed that concatenation through a few more convolutions. Output: delta C1 (changes to apply to the empty starting canvas). At the end, after all timesteps have been performed:  Create final output image by summing all the changes, i.e. delta C0 + delta C1 + ..., which basically means empty start canvas + changes from time step 0 + changes from time step 1 + ....  Their architecture as an image:  Comparison measure  They suggest a new method to compare GAN results with each other. They suggest to train pairs of G and D, e.g. for two pairs (G1, D1), (G2, D2). Then they let the pairs compete with each other. To estimate the quality of D they suggest r_test = errorRate(D1, testset) / errorRate(D2, testset). (\"Which D is better at spotting that the test set images are real images?\") To estimate the quality of the generated samples they suggest r_sample = errorRate(D1, images by G2) / errorRate(D2, images by G1). (\"Which G is better at fooling an unknown D, i.e. possibly better at generating life-like images?\") They suggest to estimate which G is better using r_sample and then to estimate how valid that result is using r_test. Results  Generated images of churches, with timesteps 1 to 5:  Overfitting  They saw no indication of overfitting in the sense of memorizing images from the training dataset. They however saw some indication of G just interpolating between some good images and of G reusing small image patches in different images. Randomness of noise vector z:  Sampling the noise vector once seems to be better than resampling it at every timestep. Resampling it at every time step often led to very similar looking output images.", "pdf_url": "http://arxiv.org/pdf/1602.05110v4", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/generating_images_with_recurrent_adversarial_networks.json"}
{"id": "60887948", "bin": "600_700", "summary_sentences": ["The paper explores knowledge distillation (KD) from the perspective of transferring knowledge between 2 networks of identical capacity.", "This is in contrast to much of the previous work in KD which has focused on transferring knowledge from a larger network to a smaller network.", "The paper reports that these Born Again Networks (BANs) outperform their teachers by significant margins in many cases.", "Approach  The standard KD setting is as follows:  Start with an untrained network (or ensemble of networks) and train them for the given task.", "This network is referred to as the teacher network.", "Now start with another untrained network (generally of smaller size than the teacher network) and train it using the output of the teacher network.", "This network is referred to as the student network.", "The paper augments this setting with an extra cross-entropy loss between the output of the teacher and the student networks.", "The student tried to predict the correct answer while matching the output distribution of the teacher.", "The resulting student network is referred to as BAN - Born Again Network.", "The same approach can be used multiple times (with diminishing returns) where the kth generation student is initialized by knowledge transfer from (k-1)th generation student.", "The output of multiple generation BANs are combined via averaging to produce BANE (Born Again Network Ensemble).", "Dark Knowledge  Hinton et al suggested that even when the output of the teacher network is incorrect, it contains useful information about the similarity between the output classes.", "This information is referred to as the “dark knowledge”.", "The current paper observed that the gradient of the correct output dimension during distillation and normal supervised training resembles the original gradient up to a  weight factor.", "This sample specific weight is defined by the value of the teacher’s max output.", "This suggests distillation may be performing some kind of importance weighing.", "To explore this further, the paper considers 2 cases:  Confidence Weighted By Teacher Max (CWTM) - where each example in the student’s loss function is weighted by the confidence that the teacher has on the prediction for that sample.", "The student incurs a higher loss if the teacher was more confident about the example.", "Dark Knowledge with Permuted Predictions (DKPP) - The non-argmax output of teacher’s predictive distribution are permuted thus destroying the information about which output classes are related.", "The key effect of these variations is that the covariance between the output classes is lost and classical knowledge distillation would not be sufficient to explain improvements (if any).", "Experiments  Image Data  Datasets  CIFAR10  CIFAR100  Baselines  ResNets  DenseNets  BAN Variants  BAN-DenseNet and BAN-ResNet  - Train a sequence of 2 or 3 BANs using DenseNets and ResNets.", "Different variants constrain BANs to be similar to their teacher or penalize l2-distance between student and teacher activations etc.", "Two settings with CWTM and DKPP as explained earlier.", "BAN-Resnet with DenseNet teacher and BAN-DenseNet with ResNet teacher  Text Data  Datasets:  PTB Dataset  Baselines  CNN-LSTM model  BAN Variant  LSTM  Results  BAN student models improved over their teachers in most of the configurations.", "Training BANs across multiple generations leads to saturating improvements.", "The student models exhibit improvements even in the control settings (CWTM and DKPP).", "One reason could be that the permutation procedure did not remove the higher order moments of output distribution.", "Improvements in the CWTM model suggests that the pre-trained models can be used to rebalance the training set by giving lesser weight for samples where the teacher’s output distribution is more spread."], "summary_text": "The paper explores knowledge distillation (KD) from the perspective of transferring knowledge between 2 networks of identical capacity. This is in contrast to much of the previous work in KD which has focused on transferring knowledge from a larger network to a smaller network. The paper reports that these Born Again Networks (BANs) outperform their teachers by significant margins in many cases. Approach  The standard KD setting is as follows:  Start with an untrained network (or ensemble of networks) and train them for the given task. This network is referred to as the teacher network. Now start with another untrained network (generally of smaller size than the teacher network) and train it using the output of the teacher network. This network is referred to as the student network. The paper augments this setting with an extra cross-entropy loss between the output of the teacher and the student networks. The student tried to predict the correct answer while matching the output distribution of the teacher. The resulting student network is referred to as BAN - Born Again Network. The same approach can be used multiple times (with diminishing returns) where the kth generation student is initialized by knowledge transfer from (k-1)th generation student. The output of multiple generation BANs are combined via averaging to produce BANE (Born Again Network Ensemble). Dark Knowledge  Hinton et al suggested that even when the output of the teacher network is incorrect, it contains useful information about the similarity between the output classes. This information is referred to as the “dark knowledge”. The current paper observed that the gradient of the correct output dimension during distillation and normal supervised training resembles the original gradient up to a  weight factor. This sample specific weight is defined by the value of the teacher’s max output. This suggests distillation may be performing some kind of importance weighing. To explore this further, the paper considers 2 cases:  Confidence Weighted By Teacher Max (CWTM) - where each example in the student’s loss function is weighted by the confidence that the teacher has on the prediction for that sample. The student incurs a higher loss if the teacher was more confident about the example. Dark Knowledge with Permuted Predictions (DKPP) - The non-argmax output of teacher’s predictive distribution are permuted thus destroying the information about which output classes are related. The key effect of these variations is that the covariance between the output classes is lost and classical knowledge distillation would not be sufficient to explain improvements (if any). Experiments  Image Data  Datasets  CIFAR10  CIFAR100  Baselines  ResNets  DenseNets  BAN Variants  BAN-DenseNet and BAN-ResNet  - Train a sequence of 2 or 3 BANs using DenseNets and ResNets. Different variants constrain BANs to be similar to their teacher or penalize l2-distance between student and teacher activations etc. Two settings with CWTM and DKPP as explained earlier. BAN-Resnet with DenseNet teacher and BAN-DenseNet with ResNet teacher  Text Data  Datasets:  PTB Dataset  Baselines  CNN-LSTM model  BAN Variant  LSTM  Results  BAN student models improved over their teachers in most of the configurations. Training BANs across multiple generations leads to saturating improvements. The student models exhibit improvements even in the control settings (CWTM and DKPP). One reason could be that the permutation procedure did not remove the higher order moments of output distribution. Improvements in the CWTM model suggests that the pre-trained models can be used to rebalance the training set by giving lesser weight for samples where the teacher’s output distribution is more spread.", "pdf_url": "https://arxiv.org/pdf/1805.04770", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/born-again-neural-networks.json"}
{"id": "90600191", "bin": "600_700", "summary_sentences": ["The paper proposes ECM (Emotional Chatting Machine) which can generate both semantically and emotionally appropriate responses in a dialogue setting.", "More specifically, given an input utterance or dialogue and the desired emotional category of the response, ECM is to generate an appropriate response that conforms to the given emotional category.", "Much of the recent, deep learning based work on conversational agents has focused on the use of encoder-decoder framework where the input utterance (given sequence of words) is mapped to a response utterance (target sequence of words).", "This is the so-called seq2seq family of models.", "ECM model can sit within this framework and introduces 3 new components:  Emotion Category Embedding  Embed the emotion categories into a real-valued, low-dimensional vector space.", "These embeddings are used as input to the decoder and are learnt along with rest of the model.", "Internal Memory  Physiological, emotional responses are relatively short-lived and involve changes.", "ECM accounts for this effect by adding an Internal Memory which captures this dynamics of emotions during decoding.", "It starts with “full” emotions in the beginning and keeps decaying the emotion value over time.", "How much of the emotion value is to be decayed is determined by a sigmoid gate.", "By the time the sentence is decoded, the value becomes zero, signifying that the emotion has been completely expressed.", "External Memory  Emotional responses are expected to carry emotionally strong words along with generic, neutral words.", "An external memory is used to include the emotionally strong words explicitly by using 2 non-overlapping vocabularies - generic vocabulary and the emotion vocabulary (read from the external memory).", "Both these vocabularies are assigned different generation probabilities and an output gate controls the weights of generic and emotion words.", "This way the emotion words are included in an otherwise neutral response.", "Loss function  The first component is the cross-entropy loss between predicted and target token distribution.", "A regularization term on internal memory to make sure the emotional state decays to 0 at the end of the decoding process.", "Another regularization term on external memory to supervise the probability of selection of a generic vs emotion word.", "*Dataset  STC Dataset (~220K posts and ~4300K responses) annotated by the emotional classifier.", "Any error on the part of the classifier degrades the quality of the training dataset.", "NLPCC Dataset - Emotion classification dataset with 23105 sentences.", "Metric  Perplexity to evaluate the model at the content level.", "Emotion accuracy to evaluate the model at the emotional level.", "ECM achieves a perplexity of 65.9 and emotional accuracy of 0.773.", "Based on human evaluations, ECM statistically outperforms the seq2seq baselines on both naturalness (likeliness of response being generated by a human) and emotion accuracy.", "Notes  It is an interesting idea to let the sigmoid gate decide how the emotion “value” be spent while decoding.", "It seems similar to the idea of how much do we want to “attend” to the emotion value the key difference being that your total attention is limited.", "It would be interesting to see the shape of the distribution of how much of the emotion value is spent at each decoding time step.", "If the curve is highly biased towards say using most of the emotion value towards the end of the decoding process, maybe another regularisation term is needed to ensure a more balanced distribution of how the emotion is spent."], "summary_text": "The paper proposes ECM (Emotional Chatting Machine) which can generate both semantically and emotionally appropriate responses in a dialogue setting. More specifically, given an input utterance or dialogue and the desired emotional category of the response, ECM is to generate an appropriate response that conforms to the given emotional category. Much of the recent, deep learning based work on conversational agents has focused on the use of encoder-decoder framework where the input utterance (given sequence of words) is mapped to a response utterance (target sequence of words). This is the so-called seq2seq family of models. ECM model can sit within this framework and introduces 3 new components:  Emotion Category Embedding  Embed the emotion categories into a real-valued, low-dimensional vector space. These embeddings are used as input to the decoder and are learnt along with rest of the model. Internal Memory  Physiological, emotional responses are relatively short-lived and involve changes. ECM accounts for this effect by adding an Internal Memory which captures this dynamics of emotions during decoding. It starts with “full” emotions in the beginning and keeps decaying the emotion value over time. How much of the emotion value is to be decayed is determined by a sigmoid gate. By the time the sentence is decoded, the value becomes zero, signifying that the emotion has been completely expressed. External Memory  Emotional responses are expected to carry emotionally strong words along with generic, neutral words. An external memory is used to include the emotionally strong words explicitly by using 2 non-overlapping vocabularies - generic vocabulary and the emotion vocabulary (read from the external memory). Both these vocabularies are assigned different generation probabilities and an output gate controls the weights of generic and emotion words. This way the emotion words are included in an otherwise neutral response. Loss function  The first component is the cross-entropy loss between predicted and target token distribution. A regularization term on internal memory to make sure the emotional state decays to 0 at the end of the decoding process. Another regularization term on external memory to supervise the probability of selection of a generic vs emotion word. *Dataset  STC Dataset (~220K posts and ~4300K responses) annotated by the emotional classifier. Any error on the part of the classifier degrades the quality of the training dataset. NLPCC Dataset - Emotion classification dataset with 23105 sentences. Metric  Perplexity to evaluate the model at the content level. Emotion accuracy to evaluate the model at the emotional level. ECM achieves a perplexity of 65.9 and emotional accuracy of 0.773. Based on human evaluations, ECM statistically outperforms the seq2seq baselines on both naturalness (likeliness of response being generated by a human) and emotion accuracy. Notes  It is an interesting idea to let the sigmoid gate decide how the emotion “value” be spent while decoding. It seems similar to the idea of how much do we want to “attend” to the emotion value the key difference being that your total attention is limited. It would be interesting to see the shape of the distribution of how much of the emotion value is spent at each decoding time step. If the curve is highly biased towards say using most of the emotion value towards the end of the decoding process, maybe another regularisation term is needed to ensure a more balanced distribution of how the emotion is spent.", "pdf_url": "https://arxiv.org/pdf/1704.01074", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/600_700/emotional-chatting-machine-emotional-conversation-generation-with-internal-and-external-memory.json"}
{"id": "96002700", "bin": "700_800", "summary_sentences": ["The paper presents some general characteristics that intelligent machines should possess and a roadmap to develop such intelligent machines in small, realistic steps.", "Ability to Communicate  The intelligent agents should be able to communicate with humans, preferably using language as the medium.", "Such systems can be programmed through natural language and can access much of the human knowledge which is encoded using natural language.", "The learning environment should facilitate interactive communication and the machine should have a minimalistic bit interface for IO to keep the interface simple.", "Further, the machine should be free to use any internal representation for learning tasks.", "Ability to Learn  Learning allows the machine to adapt to the external environment and correct their mistakes.", "Users should be able to control the motivation of the machine via a communication channel.", "This is similar to the notion of rewards in reinforcement learning.", "A simulated ecosystem to educate communication-based intelligent machines  Simulated environment to teach basic linguistic interactions and know-how to operate in the world.", "Though the environment should be challenging enough to force the machine to \"learn how to learn\", its complexity should be manageable.", "Unlike class AI block worlds, the simulated environment is not intended to teach an exhaustive set of functionality to the agent.", "The aim is to teach the machine how to learn efficiently by combining already acquired skills.", "Description  Agent  Learner or actor  Teacher  Assigns tasks and rewards to the learner and provides helpful information.", "Aim is to kick start the learner's efficient learning capabilities without providing enough direct information.", "Environment  Learner explores the environment by giving orders, asking questions and receiving feedback.", "Environment uses a controlled language which is more explicit and restricted.", "Think of learner as a high-level programming language, the teacher as the programmer and the environment as the compiler.", "Interface Channels  Generic input and output channels.", "Teacher and environment write to the input channel.", "Reward is written to input channel.", "Learner writes to the output channel and learns to use ambigous prefixes to address the agents and services it needs to interact with.", "Reward  Way to provide feedback to the learner.", "Rewards should become sparse as the learner's intelligence grows and \"curiosity\" should be a learnt strategy.", "Learner should maximise average reward over time so that faster strategies are preferred in case of equal rewards.", "Incremental Structure  Think of learner progressing through different levels where skills from earlier levels can be used in later levels.", "Tasks need not be ordered within a level.", "Learner starts by performing basic tasks like repeating characters then learns to associate linguistic strings to action sequences.", "Further, the learner learns to ask questions and \"read\" natural text.", "Time Off  Learner is given time to either explore the environment or to interact with the Teacher or to update its internal structure by replaying the previous experience.", "Evaluation  Evaluating the learning agent on only the final behaviour only is not sufficient as it overlooks the number of attempts to reach the optimal behaviour.", "Better approach would be to conduct public competition where developers have access to preprogrammed environment for fixed amount of time and learners are evaluated on tasks that are considerably different from the tasks encountered during training.", "Tasks  A brief overview of the type of tasks is provided here  Types of Learning  Concept of positive and negative rewards.", "Discovery of algorithms.", "Remember facts, skills, and learning strategies.", "Long term memory  To store facts, algorithms and even ability to learn.", "Compositional Learning Skills  Producing new structures by combining together known facts and skills.", "Understanding new concepts should not always require training examples.", "Computational properties of intelligent machines  Computational model should be able to represent any pattern in data (alternatively, represent any algorithm in fixed length).", "Among the various Turning-complete computational systems available, the most natural choice would be a compositional system that can perform computations in parallel.", "Alternatively, a non-growing model with immensely large capacity could be used.", "In a growing model, new cells are connected to ones that spawned them leading to topological structures that can contribute to learning.", "But it is not clear if such topological structures can arise in a large-capacity unstructured model."], "summary_text": "The paper presents some general characteristics that intelligent machines should possess and a roadmap to develop such intelligent machines in small, realistic steps. Ability to Communicate  The intelligent agents should be able to communicate with humans, preferably using language as the medium. Such systems can be programmed through natural language and can access much of the human knowledge which is encoded using natural language. The learning environment should facilitate interactive communication and the machine should have a minimalistic bit interface for IO to keep the interface simple. Further, the machine should be free to use any internal representation for learning tasks. Ability to Learn  Learning allows the machine to adapt to the external environment and correct their mistakes. Users should be able to control the motivation of the machine via a communication channel. This is similar to the notion of rewards in reinforcement learning. A simulated ecosystem to educate communication-based intelligent machines  Simulated environment to teach basic linguistic interactions and know-how to operate in the world. Though the environment should be challenging enough to force the machine to \"learn how to learn\", its complexity should be manageable. Unlike class AI block worlds, the simulated environment is not intended to teach an exhaustive set of functionality to the agent. The aim is to teach the machine how to learn efficiently by combining already acquired skills. Description  Agent  Learner or actor  Teacher  Assigns tasks and rewards to the learner and provides helpful information. Aim is to kick start the learner's efficient learning capabilities without providing enough direct information. Environment  Learner explores the environment by giving orders, asking questions and receiving feedback. Environment uses a controlled language which is more explicit and restricted. Think of learner as a high-level programming language, the teacher as the programmer and the environment as the compiler. Interface Channels  Generic input and output channels. Teacher and environment write to the input channel. Reward is written to input channel. Learner writes to the output channel and learns to use ambigous prefixes to address the agents and services it needs to interact with. Reward  Way to provide feedback to the learner. Rewards should become sparse as the learner's intelligence grows and \"curiosity\" should be a learnt strategy. Learner should maximise average reward over time so that faster strategies are preferred in case of equal rewards. Incremental Structure  Think of learner progressing through different levels where skills from earlier levels can be used in later levels. Tasks need not be ordered within a level. Learner starts by performing basic tasks like repeating characters then learns to associate linguistic strings to action sequences. Further, the learner learns to ask questions and \"read\" natural text. Time Off  Learner is given time to either explore the environment or to interact with the Teacher or to update its internal structure by replaying the previous experience. Evaluation  Evaluating the learning agent on only the final behaviour only is not sufficient as it overlooks the number of attempts to reach the optimal behaviour. Better approach would be to conduct public competition where developers have access to preprogrammed environment for fixed amount of time and learners are evaluated on tasks that are considerably different from the tasks encountered during training. Tasks  A brief overview of the type of tasks is provided here  Types of Learning  Concept of positive and negative rewards. Discovery of algorithms. Remember facts, skills, and learning strategies. Long term memory  To store facts, algorithms and even ability to learn. Compositional Learning Skills  Producing new structures by combining together known facts and skills. Understanding new concepts should not always require training examples. Computational properties of intelligent machines  Computational model should be able to represent any pattern in data (alternatively, represent any algorithm in fixed length). Among the various Turning-complete computational systems available, the most natural choice would be a compositional system that can perform computations in parallel. Alternatively, a non-growing model with immensely large capacity could be used. In a growing model, new cells are connected to ones that spawned them leading to topological structures that can contribute to learning. But it is not clear if such topological structures can arise in a large-capacity unstructured model.", "pdf_url": "https://arxiv.org/pdf/1511.08130", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/28673525b1713c2d41fd0fac38f81f.json"}
{"id": "23426061", "bin": "700_800", "summary_sentences": ["Open-domain Question Answering (Open QA) - efficiently querying large-scale knowledge base(KB) using natural language.", "Two main approaches:  Information Retrieval  Transform question (in natural language) into a valid query(in terms of KB) to get a broad set of candidate answers.", "Perform fine-grained detection on candidate answers.", "Semantic Parsing  Interpret the correct meaning of the question and convert it into an exact query.", "Limitations:  Human intervention to create lexicon, grammar, and schema.", "This work builds upon the previous work where an embedding model learns low dimensional vector representation of words and symbols.", ".", "Task Definition  Input - Training set of questions (paired with answers).", "KB providing a structure among the answers.", "Answers are entities in KB and questions are strings with one identified KB entity.", "The paper has used FREEBASE as the KB.", "Datasets  WebQuestions - Built using FREEBASE, Google Suggest API, and Mechanical Turk.", "FREEBASE triplets transformed into questions.", "Clue Web Extractions dataset with entities linked with FREEBASE triplets.", "Dataset of paraphrased questions using WIKIANSWERS.", "Embedding Questions and Answers  Model learns low-dimensional vector embeddings of words in question entities and relation types of FREEBASE such that questions and their answers are represented close to each other in the joint embedding space.", "Scoring function S(q, a), where q is a question and a is an answer, generates high score if a answers q.", "S(q, a) = f(q)T.g(a)  f(q) maps question to embedding space.", "f(q) = Wφ(q)  W is a matrix of dimension K * N  K - dimension of embedding space (hyper parameter).", "N - total number of words/entities/relation types.", "ψ(q) - Sparse Vector encoding the number of times a word appears in q.", "Similarly, g(a) = Wψ(a) maps answer to embedding space.", "&psi(a) gives answer representation, as discussed below.", "Possible Representations of Candidate Answers  Answer represented as a single entity from FREEBASE and TBD is a one-of-N encoded vector.", "Answer represented as a path from question to answer.", "The paper considers only one or two hop paths resulting in 3-of-N or 4-of-N encoded vectors(middle entities are not recorded).", "Encode the above two representations using subgraph representation which represents both the path and the entire subgraph of entities connected to answer entity as a subgraph.", "Two embedding representations are used to differentiate between entities in path and entities in the subgraph.", "SubGraph approach is based on the hypothesis that including more information about the answers would improve results.", "Training and Loss Function  Minimize margin based ranking loss to learn matrix W.  Stochastic Gradient Descent, multi-threaded with Hogwild.", "Multitask Training of Embeddings  To account for a large number of synthetically generated questions, the paper also multi-tasks the training of model with paraphrased prediction.", "Scoring function Sprp(q1, q2) = f(q1)Tf(q2), where f uses the same weight matrix W as before.", "High score is assigned if q1 and q2 belong to same paraphrase cluster.", "Additionally, the model multitasks the task of mapping embeddings of FREEBASE entities (mids) to actual words.", "Inference  For each question, a candidate set is generated.", "The answer (from candidate set) with the highest set is reported as the correct answer.", "Candidate set generation strategy  C1 - All KB triplets containing the KB entity from the question forms a candidate set.", "Answers would be limited to 1-hop paths.", "C2 - Rank all relation types and keep top 10 types and add only those 2-hop candidates where the selected relations appear in the path.", "Results  C2 strategy outperforms C1 approach supporting the hypothesis that a richer representation for answers can store more information.", "Proposed approach outperforms the baseline methods but is outperformed by an ensemble of proposed approach with semantic parsing via paraphrasing model."], "summary_text": "Open-domain Question Answering (Open QA) - efficiently querying large-scale knowledge base(KB) using natural language. Two main approaches:  Information Retrieval  Transform question (in natural language) into a valid query(in terms of KB) to get a broad set of candidate answers. Perform fine-grained detection on candidate answers. Semantic Parsing  Interpret the correct meaning of the question and convert it into an exact query. Limitations:  Human intervention to create lexicon, grammar, and schema. This work builds upon the previous work where an embedding model learns low dimensional vector representation of words and symbols. . Task Definition  Input - Training set of questions (paired with answers). KB providing a structure among the answers. Answers are entities in KB and questions are strings with one identified KB entity. The paper has used FREEBASE as the KB. Datasets  WebQuestions - Built using FREEBASE, Google Suggest API, and Mechanical Turk. FREEBASE triplets transformed into questions. Clue Web Extractions dataset with entities linked with FREEBASE triplets. Dataset of paraphrased questions using WIKIANSWERS. Embedding Questions and Answers  Model learns low-dimensional vector embeddings of words in question entities and relation types of FREEBASE such that questions and their answers are represented close to each other in the joint embedding space. Scoring function S(q, a), where q is a question and a is an answer, generates high score if a answers q. S(q, a) = f(q)T.g(a)  f(q) maps question to embedding space. f(q) = Wφ(q)  W is a matrix of dimension K * N  K - dimension of embedding space (hyper parameter). N - total number of words/entities/relation types. ψ(q) - Sparse Vector encoding the number of times a word appears in q. Similarly, g(a) = Wψ(a) maps answer to embedding space. &psi(a) gives answer representation, as discussed below. Possible Representations of Candidate Answers  Answer represented as a single entity from FREEBASE and TBD is a one-of-N encoded vector. Answer represented as a path from question to answer. The paper considers only one or two hop paths resulting in 3-of-N or 4-of-N encoded vectors(middle entities are not recorded). Encode the above two representations using subgraph representation which represents both the path and the entire subgraph of entities connected to answer entity as a subgraph. Two embedding representations are used to differentiate between entities in path and entities in the subgraph. SubGraph approach is based on the hypothesis that including more information about the answers would improve results. Training and Loss Function  Minimize margin based ranking loss to learn matrix W.  Stochastic Gradient Descent, multi-threaded with Hogwild. Multitask Training of Embeddings  To account for a large number of synthetically generated questions, the paper also multi-tasks the training of model with paraphrased prediction. Scoring function Sprp(q1, q2) = f(q1)Tf(q2), where f uses the same weight matrix W as before. High score is assigned if q1 and q2 belong to same paraphrase cluster. Additionally, the model multitasks the task of mapping embeddings of FREEBASE entities (mids) to actual words. Inference  For each question, a candidate set is generated. The answer (from candidate set) with the highest set is reported as the correct answer. Candidate set generation strategy  C1 - All KB triplets containing the KB entity from the question forms a candidate set. Answers would be limited to 1-hop paths. C2 - Rank all relation types and keep top 10 types and add only those 2-hop candidates where the selected relations appear in the path. Results  C2 strategy outperforms C1 approach supporting the hypothesis that a richer representation for answers can store more information. Proposed approach outperforms the baseline methods but is outperformed by an ensemble of proposed approach with semantic parsing via paraphrasing model.", "pdf_url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/5e299ff5f79a4f9da4a2e9281a0676.json"}
{"id": "75865335", "bin": "700_800", "summary_sentences": ["What  Previously, methods to detect bounding boxes in images were often based on the combination of manual feature extraction with SVMs.", "They replace the manual feature extraction with a CNN, leading to significantly higher accuracy.", "They use supervised pre-training on auxiliary datasets to deal with the small amount of labeled data (instead of the sometimes used unsupervised pre-training).", "They call their method R-CNN (\"Regions with CNN features\").", "How  Their system has three modules: 1) Region proposal generation, 2) CNN-based feature extraction per region proposal, 3) classification.", "Region proposals generation  A region proposal is a bounding box candidate that might contain an object.", "By default they generate 2000 region proposals per image.", "They suggest \"simple\" (i.e. not learned) algorithms for this step (e.g.", "objectneess, selective search, CPMC).", "They use selective search (makes it comparable to previous systems).", "CNN features  Uses a CNN to extract features, applied to each region proposal (replaces the previously used manual feature extraction).", "So each region proposal ist turned into a fixed length vector.", "They use AlexNet by Krizhevsky et al. as their base CNN (takes 227x227 RGB images, converts them into 4096-dimensional vectors).", "They add p=16 pixels to each side of every region proposal, extract the pixels and then simply resize them to 227x227 (ignoring aspect ratio, so images might end up distorted).", "They generate one 4096d vector per image, which is less than what some previous manual feature extraction methods used.", "That enables faster classification, less memory usage and thus more possible classes.", "Classification  A classifier that receives the extracted feature vectors (one per region proposal) and classifies them into a predefined set of available classes (e.g. \"person\", \"car\", \"bike\", \"background / no object\").", "They use one SVM per available class.", "The regions that were not classified as background might overlap (multiple bounding boxes on the same object).", "They use greedy non-maximum suppresion to fix that problem (for each class individually).", "That method simply rejects regions if they overlap strongly with another region that has higher score.", "Overlap is determined via Intersection of Union (IoU).", "Training method  Pre-Training of CNN  They use AlexNet pretrained on Imagenet (1000 classes).", "They replace the last fully connected layer with a randomly initialized one that leads to C+1 classes (C object classes, +1 for background).", "Fine-Tuning of CNN  The use SGD with learning rate 0.001.", "Batch size is 128 (32 positive windows, 96 background windows).", "A region proposal is considered positive, if its IoU with any ground-truth bounding box is >=0.5.", "SVM  They train one SVM per class via hard negative mining.", "For positive examples they use here an IoU threshold of >=0.3, which performed better than 0.5.", "Results  Pascal VOC 2010  They: 53.7% mAP  Closest competitor (SegDPM): 40.4% mAP  Closest competitor that uses the same region proposal method (UVA): 35.1% mAP  ILSVRC2013 detection  They: 31.4% mAP  Closest competitor (OverFeat): 24.3% mAP  The feed a large number of region proposals through the network and log for each filter in the last conv-layer which images activated it the most:  Usefulness of layers:  They remove later layers of the network and retrain in order to find out which layers are the most useful ones.", "Their result is that both fully connected layers of AlexNet seemed to be very domain-specific and profit most from fine-tuning.", "Using VGG16:  Using VGG16 instead of AlexNet increased mAP from 58.5% to 66.0% on Pascal VOC 2007.", "Computation time was 7 times higher.", "They train a linear regression model that improves the bounding box dimensions based on the extracted features of the last pooling layer.", "That improved their mAP by 3-4 percentage points.", "The region proposals generated by selective search have a recall of 98% on Pascal VOC and 91.6% on ILSVRC2013 (measured by IoU of >=0.5)."], "summary_text": "What  Previously, methods to detect bounding boxes in images were often based on the combination of manual feature extraction with SVMs. They replace the manual feature extraction with a CNN, leading to significantly higher accuracy. They use supervised pre-training on auxiliary datasets to deal with the small amount of labeled data (instead of the sometimes used unsupervised pre-training). They call their method R-CNN (\"Regions with CNN features\"). How  Their system has three modules: 1) Region proposal generation, 2) CNN-based feature extraction per region proposal, 3) classification. Region proposals generation  A region proposal is a bounding box candidate that might contain an object. By default they generate 2000 region proposals per image. They suggest \"simple\" (i.e. not learned) algorithms for this step (e.g. objectneess, selective search, CPMC). They use selective search (makes it comparable to previous systems). CNN features  Uses a CNN to extract features, applied to each region proposal (replaces the previously used manual feature extraction). So each region proposal ist turned into a fixed length vector. They use AlexNet by Krizhevsky et al. as their base CNN (takes 227x227 RGB images, converts them into 4096-dimensional vectors). They add p=16 pixels to each side of every region proposal, extract the pixels and then simply resize them to 227x227 (ignoring aspect ratio, so images might end up distorted). They generate one 4096d vector per image, which is less than what some previous manual feature extraction methods used. That enables faster classification, less memory usage and thus more possible classes. Classification  A classifier that receives the extracted feature vectors (one per region proposal) and classifies them into a predefined set of available classes (e.g. \"person\", \"car\", \"bike\", \"background / no object\"). They use one SVM per available class. The regions that were not classified as background might overlap (multiple bounding boxes on the same object). They use greedy non-maximum suppresion to fix that problem (for each class individually). That method simply rejects regions if they overlap strongly with another region that has higher score. Overlap is determined via Intersection of Union (IoU). Training method  Pre-Training of CNN  They use AlexNet pretrained on Imagenet (1000 classes). They replace the last fully connected layer with a randomly initialized one that leads to C+1 classes (C object classes, +1 for background). Fine-Tuning of CNN  The use SGD with learning rate 0.001. Batch size is 128 (32 positive windows, 96 background windows). A region proposal is considered positive, if its IoU with any ground-truth bounding box is >=0.5. SVM  They train one SVM per class via hard negative mining. For positive examples they use here an IoU threshold of >=0.3, which performed better than 0.5. Results  Pascal VOC 2010  They: 53.7% mAP  Closest competitor (SegDPM): 40.4% mAP  Closest competitor that uses the same region proposal method (UVA): 35.1% mAP  ILSVRC2013 detection  They: 31.4% mAP  Closest competitor (OverFeat): 24.3% mAP  The feed a large number of region proposals through the network and log for each filter in the last conv-layer which images activated it the most:  Usefulness of layers:  They remove later layers of the network and retrain in order to find out which layers are the most useful ones. Their result is that both fully connected layers of AlexNet seemed to be very domain-specific and profit most from fine-tuning. Using VGG16:  Using VGG16 instead of AlexNet increased mAP from 58.5% to 66.0% on Pascal VOC 2007. Computation time was 7 times higher. They train a linear regression model that improves the bounding box dimensions based on the extracted features of the last pooling layer. That improved their mAP by 3-4 percentage points. The region proposals generated by selective search have a recall of 98% on Pascal VOC and 91.6% on ILSVRC2013 (measured by IoU of >=0.5).", "pdf_url": "https://arxiv.org/pdf/1311.2524", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/700_800/rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation.json"}
{"id": "17675091", "bin": "800_900", "summary_sentences": ["What  They suggest a new version of YOLO, a model to detect bounding boxes in images.", "Their new version is more accurate, faster and is trained to recognize up to 9000 classes.", "How  Their base model is the previous YOLOv1, which they improve here.", "Accuracy improvements  They add batch normalization to the network.", "Pretraining usually happens on ImageNet at 224x224, fine tuning for bounding box detection then on another dataset, say Pascal VOC 2012, at higher resolutions, e.g. 448x448 in the case of YOLOv1.", "This is problematic, because the pretrained network has to learn to deal with higher resolutions and a new task at the same time.", "They instead first pretrain on low resolution ImageNet examples, then on higher resolution ImegeNet examples and only then switch to bounding box detection.", "That improves their accuracy by about 4 percentage points mAP.", "They switch to anchor boxes, similar to Faster R-CNN.", "That's largely the same as in YOLOv1.", "Classification is now done per tested anchor box shape, instead of per grid cell.", "The regression of x/y-coordinates is now a bit smarter and uses sigmoids to only translate a box within a grid cell.", "In Faster R-CNN the anchor box shapes are manually chosen (e.g. small squared boxes, large squared boxes, thin but high boxes, ...).", "Here instead they learn these shapes from data.", "That is done by applying k-Means to the bounding boxes in a dataset.", "They cluster them into k=5 clusters and then use the centroids as anchor box shapes.", "Their accuracy this way is the same as with 9 manually chosen anchor boxes.", "(Using k=9 further increases their accuracy significantly, but also increases model complexity.", "As they want to predict 9000 classes they stay with k=5.)", "To better predict small bounding boxes, they add a pass-through connection from a higher resolution layer to the end of the network.", "They train their network now at multiple scales.", "(As the network is now fully convolutional, they can easily do that.)", "Speed improvements  They get rid of their fully connected layers.", "Instead the network is now fully convolutional.", "They have also removed a handful or so of their convolutional layers.", "Capability improvement (weakly supervised learning)  They suggest a method to predict bounding boxes of the 9000 most common classes in ImageNet.", "They add a few more abstract classes to that (e.g. dog for all breeds of dogs) and arrive at over 9000 classes (9418 to be precise).", "They train on ImageNet and MSCOCO.", "ImageNet only contains class labels, no bounding boxes.", "MSCOCO only contains general classes (e.g. \"dog\" instead of the specific breed).", "They train iteratively on both datasets.", "MSCOCO is used for detection and classification, while ImageNet is only used for classification.", "For an ImageNet example of class c, they search among the predicted bounding boxes for the one that has highest predicted probability of being c and backpropagate only the classification loss for that box.", "In order to compensate the problem of different abstraction levels on the classes (e.g. \"dog\" vs a specific breed), they make use of WordNet.", "Based on that data they generate a hierarchy/tree of classes, e.g. one path through that tree could be: object -> animal -> canine -> dog -> hunting dog -> terrier -> yorkshire terrier.", "They let the network predict paths in that hierarchy, so that the prediction \"dog\" for a specific dog breed is not completely wrong.", "Visualization of the hierarchy:  They predict many small softmaxes for the paths in the hierarchy, one per node:  Results  Accuracy  They reach about 73.4 mAP when training on Pascal VOC 2007 and 2012.", "That's slightly behind Faster R-CNN with VGG16 with 75.9 mAP, trained on MSCOCO+2007+2012.", "Speed  They reach 91 fps (10ms/image) at image resolution 288x288 and 40 fps (25ms/image) at 544x544.", "Weakly supervised learning  They test their 9000-class-detection on ImageNet's detection task, which contains bounding boxes for 200 object classes.", "They achieve 19.7 mAP for all classes and 16.0% mAP for the 156 classes which are not part of MSCOCO.", "For some classes they get 0 mAP accuracy.", "The system performs well for all kinds of animals, but struggles with not-living objects, like sunglasses.", "Example images (notice the class labels):"], "summary_text": "What  They suggest a new version of YOLO, a model to detect bounding boxes in images. Their new version is more accurate, faster and is trained to recognize up to 9000 classes. How  Their base model is the previous YOLOv1, which they improve here. Accuracy improvements  They add batch normalization to the network. Pretraining usually happens on ImageNet at 224x224, fine tuning for bounding box detection then on another dataset, say Pascal VOC 2012, at higher resolutions, e.g. 448x448 in the case of YOLOv1. This is problematic, because the pretrained network has to learn to deal with higher resolutions and a new task at the same time. They instead first pretrain on low resolution ImageNet examples, then on higher resolution ImegeNet examples and only then switch to bounding box detection. That improves their accuracy by about 4 percentage points mAP. They switch to anchor boxes, similar to Faster R-CNN. That's largely the same as in YOLOv1. Classification is now done per tested anchor box shape, instead of per grid cell. The regression of x/y-coordinates is now a bit smarter and uses sigmoids to only translate a box within a grid cell. In Faster R-CNN the anchor box shapes are manually chosen (e.g. small squared boxes, large squared boxes, thin but high boxes, ...). Here instead they learn these shapes from data. That is done by applying k-Means to the bounding boxes in a dataset. They cluster them into k=5 clusters and then use the centroids as anchor box shapes. Their accuracy this way is the same as with 9 manually chosen anchor boxes. (Using k=9 further increases their accuracy significantly, but also increases model complexity. As they want to predict 9000 classes they stay with k=5.) To better predict small bounding boxes, they add a pass-through connection from a higher resolution layer to the end of the network. They train their network now at multiple scales. (As the network is now fully convolutional, they can easily do that.) Speed improvements  They get rid of their fully connected layers. Instead the network is now fully convolutional. They have also removed a handful or so of their convolutional layers. Capability improvement (weakly supervised learning)  They suggest a method to predict bounding boxes of the 9000 most common classes in ImageNet. They add a few more abstract classes to that (e.g. dog for all breeds of dogs) and arrive at over 9000 classes (9418 to be precise). They train on ImageNet and MSCOCO. ImageNet only contains class labels, no bounding boxes. MSCOCO only contains general classes (e.g. \"dog\" instead of the specific breed). They train iteratively on both datasets. MSCOCO is used for detection and classification, while ImageNet is only used for classification. For an ImageNet example of class c, they search among the predicted bounding boxes for the one that has highest predicted probability of being c and backpropagate only the classification loss for that box. In order to compensate the problem of different abstraction levels on the classes (e.g. \"dog\" vs a specific breed), they make use of WordNet. Based on that data they generate a hierarchy/tree of classes, e.g. one path through that tree could be: object -> animal -> canine -> dog -> hunting dog -> terrier -> yorkshire terrier. They let the network predict paths in that hierarchy, so that the prediction \"dog\" for a specific dog breed is not completely wrong. Visualization of the hierarchy:  They predict many small softmaxes for the paths in the hierarchy, one per node:  Results  Accuracy  They reach about 73.4 mAP when training on Pascal VOC 2007 and 2012. That's slightly behind Faster R-CNN with VGG16 with 75.9 mAP, trained on MSCOCO+2007+2012. Speed  They reach 91 fps (10ms/image) at image resolution 288x288 and 40 fps (25ms/image) at 544x544. Weakly supervised learning  They test their 9000-class-detection on ImageNet's detection task, which contains bounding boxes for 200 object classes. They achieve 19.7 mAP for all classes and 16.0% mAP for the 156 classes which are not part of MSCOCO. For some classes they get 0 mAP accuracy. The system performs well for all kinds of animals, but struggles with not-living objects, like sunglasses. Example images (notice the class labels):", "pdf_url": "https://arxiv.org/pdf/1612.08242", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/yolo9000.json"}
{"id": "58793266", "bin": "800_900", "summary_sentences": ["This paper suggests a novel explanation for why dropout training is helpful: because it corresponds to an adaptive data augmentation method.", "Indeed, the authors point out that, when sampling a mask of the hidden units in a network (effectively setting the corresponding units to 0), the same effect would have been obtained by feeding as input an example tailored to yield activations of 0 for these units and otherwise the same activation for all other units.", "Since this \"ghost\" example will have to be different from the original example, and since each different mask would correspond to a different \"ghost\" example, then effectively mask sampling is similar to data augmentation.", "While in practice finding a ghost example that replicates exactly the same dropout hidden activations might not be possible, the authors show that finding an \"approximate\" ghost example that minimizes a distance between the target dropout activation and the deterministic activation of the ghost example works well.", "Indeed, they show that training a deep neural net on additional data generated by this procedure yields results that are at least as good as regular dropout on MNIST and CIFAR-10 (actually, the deterministic neural net still uses regular dropout at the input layer, however they do show that the additional ghost examples are necessary to match the neural net trained with dropout at all layers).", "Then the authors use that interpretation to justify a variation of dropout where the dropout rate isn't fixed, but itself is randomly sampled in some range for each example.", "Indeed, if we think of dropout at a fixed rate as a specific class of ghost data being added, varying the dropout rate corresponds to enriching even more the ghost data pool.", "The experiments show that this can help, though not by much.", "Finally, the authors propose an explanation of a property of dropout: that it tends to generate hidden representations that are sparser.", "Again, the authors rely on their interpretation of dropout as data augmentation.", "The explanation goes as follows.", "Training on the ghost data distribution might imply that the classification problem has become significantly harder.", "Specifically, it is quite possible that the addition of new ghost examples generates new isolated class clusters in input space that the model most now learn to  discriminate.", "And they hypothesize that the generation of such additional clusters would encourage sparsity.", "To test this hypothesis, the authors synthetically simulate this scenario, by sampling data on a circle, which is clustered in small arcs each assigned to one of 10 possible classes in cycling order.", "Decreasing the arc length thus increases the number of arcs, i.e. class clusters.", "They show that training deep networks on datasets with increasing number of class clusters does yield representations that are increasingly sparser.", "This thus suggests that dropout might indeed be equivalent to modifying the input distribution by adding such isolated class-specific clusters in input space.", "One assumption behind this analysis is that the sparsity patterns (i.e. the set of non-zero dimensions) play an important role in classification and incorporate most of the discriminative class information.", "This assumption is also confirmed in experiments, where converting the ReLU activation function by a binary activation (that is 1 if the pre-activation is positive and 0 otherwise) after training still yields a network with good performance (though slightly worse).", "#### My two cents  This is a really original and thought provoking paper.", "One interpretation I make of these results is that the inductive bias corresponding to using a deep neural network with ReLU activations is more valuable than one might have thought, and that the usefulness of deep neural networks goes beyond just being black boxes that can learn data-dependent representations.", "Otherwise, it's not clear to me why the ghost data implicitly generated by the architecture would be useful at all.", "This also suggests an experiment where such ghost samples would be fed to  another type of classifier, such as an SVM, to test whether the data augmentation is useful in itself and reflects meaningful structure in the data, as opposed to being somehow useful only for neural nets.", "I note that the results are mostly specific to architectures based on ReLU activations (not that this is a problem, but one should keep this in mind).", "I'd really like to see what the ghost samples look like.", "Do they correspond to interpretable images?", "The authors also mention that exploring how the samples change with training would be interesting to investigate, and I agree.", "Finally, I think there might be a typo in Figure 1.", "While the labels of a) and b) states that the arc length is smaller for a) than b), the plot clearly show otherwise."], "summary_text": "This paper suggests a novel explanation for why dropout training is helpful: because it corresponds to an adaptive data augmentation method. Indeed, the authors point out that, when sampling a mask of the hidden units in a network (effectively setting the corresponding units to 0), the same effect would have been obtained by feeding as input an example tailored to yield activations of 0 for these units and otherwise the same activation for all other units. Since this \"ghost\" example will have to be different from the original example, and since each different mask would correspond to a different \"ghost\" example, then effectively mask sampling is similar to data augmentation. While in practice finding a ghost example that replicates exactly the same dropout hidden activations might not be possible, the authors show that finding an \"approximate\" ghost example that minimizes a distance between the target dropout activation and the deterministic activation of the ghost example works well. Indeed, they show that training a deep neural net on additional data generated by this procedure yields results that are at least as good as regular dropout on MNIST and CIFAR-10 (actually, the deterministic neural net still uses regular dropout at the input layer, however they do show that the additional ghost examples are necessary to match the neural net trained with dropout at all layers). Then the authors use that interpretation to justify a variation of dropout where the dropout rate isn't fixed, but itself is randomly sampled in some range for each example. Indeed, if we think of dropout at a fixed rate as a specific class of ghost data being added, varying the dropout rate corresponds to enriching even more the ghost data pool. The experiments show that this can help, though not by much. Finally, the authors propose an explanation of a property of dropout: that it tends to generate hidden representations that are sparser. Again, the authors rely on their interpretation of dropout as data augmentation. The explanation goes as follows. Training on the ghost data distribution might imply that the classification problem has become significantly harder. Specifically, it is quite possible that the addition of new ghost examples generates new isolated class clusters in input space that the model most now learn to  discriminate. And they hypothesize that the generation of such additional clusters would encourage sparsity. To test this hypothesis, the authors synthetically simulate this scenario, by sampling data on a circle, which is clustered in small arcs each assigned to one of 10 possible classes in cycling order. Decreasing the arc length thus increases the number of arcs, i.e. class clusters. They show that training deep networks on datasets with increasing number of class clusters does yield representations that are increasingly sparser. This thus suggests that dropout might indeed be equivalent to modifying the input distribution by adding such isolated class-specific clusters in input space. One assumption behind this analysis is that the sparsity patterns (i.e. the set of non-zero dimensions) play an important role in classification and incorporate most of the discriminative class information. This assumption is also confirmed in experiments, where converting the ReLU activation function by a binary activation (that is 1 if the pre-activation is positive and 0 otherwise) after training still yields a network with good performance (though slightly worse). #### My two cents  This is a really original and thought provoking paper. One interpretation I make of these results is that the inductive bias corresponding to using a deep neural network with ReLU activations is more valuable than one might have thought, and that the usefulness of deep neural networks goes beyond just being black boxes that can learn data-dependent representations. Otherwise, it's not clear to me why the ghost data implicitly generated by the architecture would be useful at all. This also suggests an experiment where such ghost samples would be fed to  another type of classifier, such as an SVM, to test whether the data augmentation is useful in itself and reflects meaningful structure in the data, as opposed to being somehow useful only for neural nets. I note that the results are mostly specific to architectures based on ReLU activations (not that this is a problem, but one should keep this in mind). I'd really like to see what the ghost samples look like. Do they correspond to interpretable images? The authors also mention that exploring how the samples change with training would be interesting to investigate, and I agree. Finally, I think there might be a typo in Figure 1. While the labels of a) and b) states that the arc length is smaller for a) than b), the plot clearly show otherwise.", "pdf_url": "http://arxiv.org/pdf/1506.08700", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/800_900/kondabmv15.json"}
{"id": "47698609", "bin": "900_1000", "summary_sentences": ["**Summary**  Representation (or feature) learning with unsupervised learning has yet really to yield the type of results that many believe to be achievable.", "For example, we’d like to unleash an unsupervised learning algorithm on all web images and then obtain a representation that captures the various factors of variation we know to be present (e.g. objects and people).", "One popular approach for this is to train a model that assumes a high-level vector representation with independent components.", "However, despite a large body of literature on such models by now, such so-called disentangling of these factors of variation still seems beyond our reach.", "In this short paper, the authors propose an alternative to this approach.", "They propose that disentangling might be achievable by learning a representation whose dimensions are each separately **controllable**, i.e. that each have an associated policy which changes the value of that dimension **while letting other dimensions fixed**.", "Specifically, the authors propose to minimize the following objective:  $\\mathop{\\mathbb{E}}_s\\left[\\frac{1}{2}||s-g(f(s))||^2_2 \\right] - \\lambda \\sum_k \\mathbb{E}_{a,s}\\left[\\sum_a \\pi_k(a|s) \\log sel(s,a,k)\\right]$  where  - $s$ is an agent’s state (e.g. frame image) which encoder $f$ and decoder $g$ learn to autoencode - $k$ iterates over all dimensions of the representation space (output of encoder) - $a$ iterates over actions that the agent can take - $\\pi_k(a|s)$ is the policy that is meant to control the $k^{\\rm th}$ dimension of the representation space $f(s)_k$ - $sel(s,a,k)$ is the selectivity of $f(s)_k$ relative to other dimensions in the representation, at state $s$:  $sel(s,a,k) = \\mathop{\\mathbb{E}}_{s’\\sim {\\cal P}_{ss’}^a}\\left[\\frac{|f_k(s’)-f_k(s)|}{\\sum_{k’} |f_{k’}(s’)-f_{k’}(s)| }\\right]$  ${\\cal P}_{ss’}^a$ is the conditional distribution over the next step state $s’$ given that you are at state $s$ and are taking action $a$ (i.e.", "the environment transition distribution).", "One can see that selectivity is higher when the change $|f_k(s’)-f_k(s)|$ in dimension $k$ is much larger than the change  $|f_{k’}(s’)-f_{k’}(s)|$ in the other dimensions $k’$.", "A directed version of selectivity is also proposed (and I believe was used in the experiments), where the absolute value function is removed and $\\log sel$ is replaced with $\\log(1+sel)$ in the objective.", "The learning objective will thus encourage the discovery of a representation that is informative of the input (in that you can reconstruct it) and for which there exists policies that separately control these dimensions.", "Algorithm 1 in the paper describes a learning procedure for optimizing this objective.", "In brief, for every update, a state $s$ is sampled from which an update for the autoencoder part of the loss can be made.", "Then, iterating over each dimension $k$, REINFORCE is used to get a gradient estimate of the selectivity part of the loss, to update both the policy $\\pi_k$ and the encoder $f$ by using the policy to reach a next state $s’$.", "**My two cents**  I find this concept very appealing and thought provoking.", "Intuitively, I find the idea that valuable features are features which reflect an aspect of our environment that we can control more sensible and possibly less constraining than an assumption of independent features.", "It also has an interesting analogy of an infant learning about the world by interacting with it.", "The caveat is that unfortunately, this concept is currently fairly impractical, since it requires an interactive environment where an agent can perform actions, something we can’t easily have short of deploying a robot with sensors.", "Moreover, the proposed algorithm seems to assume that each state $s$ is sampled independently for each update, whereas a robot would observe a dependent stream of states.", "Accordingly, the experiments in this short paper are mostly “proof of concept”, on simplistic synthetic environments.", "Yet they do a good job at illustrating the idea.", "To me this means that there’s more interesting work worth doing in what seems to be a promising direction!"], "summary_text": "**Summary**  Representation (or feature) learning with unsupervised learning has yet really to yield the type of results that many believe to be achievable. For example, we’d like to unleash an unsupervised learning algorithm on all web images and then obtain a representation that captures the various factors of variation we know to be present (e.g. objects and people). One popular approach for this is to train a model that assumes a high-level vector representation with independent components. However, despite a large body of literature on such models by now, such so-called disentangling of these factors of variation still seems beyond our reach. In this short paper, the authors propose an alternative to this approach. They propose that disentangling might be achievable by learning a representation whose dimensions are each separately **controllable**, i.e. that each have an associated policy which changes the value of that dimension **while letting other dimensions fixed**. Specifically, the authors propose to minimize the following objective:  $\\mathop{\\mathbb{E}}_s\\left[\\frac{1}{2}||s-g(f(s))||^2_2 \\right] - \\lambda \\sum_k \\mathbb{E}_{a,s}\\left[\\sum_a \\pi_k(a|s) \\log sel(s,a,k)\\right]$  where  - $s$ is an agent’s state (e.g. frame image) which encoder $f$ and decoder $g$ learn to autoencode - $k$ iterates over all dimensions of the representation space (output of encoder) - $a$ iterates over actions that the agent can take - $\\pi_k(a|s)$ is the policy that is meant to control the $k^{\\rm th}$ dimension of the representation space $f(s)_k$ - $sel(s,a,k)$ is the selectivity of $f(s)_k$ relative to other dimensions in the representation, at state $s$:  $sel(s,a,k) = \\mathop{\\mathbb{E}}_{s’\\sim {\\cal P}_{ss’}^a}\\left[\\frac{|f_k(s’)-f_k(s)|}{\\sum_{k’} |f_{k’}(s’)-f_{k’}(s)| }\\right]$  ${\\cal P}_{ss’}^a$ is the conditional distribution over the next step state $s’$ given that you are at state $s$ and are taking action $a$ (i.e. the environment transition distribution). One can see that selectivity is higher when the change $|f_k(s’)-f_k(s)|$ in dimension $k$ is much larger than the change  $|f_{k’}(s’)-f_{k’}(s)|$ in the other dimensions $k’$. A directed version of selectivity is also proposed (and I believe was used in the experiments), where the absolute value function is removed and $\\log sel$ is replaced with $\\log(1+sel)$ in the objective. The learning objective will thus encourage the discovery of a representation that is informative of the input (in that you can reconstruct it) and for which there exists policies that separately control these dimensions. Algorithm 1 in the paper describes a learning procedure for optimizing this objective. In brief, for every update, a state $s$ is sampled from which an update for the autoencoder part of the loss can be made. Then, iterating over each dimension $k$, REINFORCE is used to get a gradient estimate of the selectivity part of the loss, to update both the policy $\\pi_k$ and the encoder $f$ by using the policy to reach a next state $s’$. **My two cents**  I find this concept very appealing and thought provoking. Intuitively, I find the idea that valuable features are features which reflect an aspect of our environment that we can control more sensible and possibly less constraining than an assumption of independent features. It also has an interesting analogy of an infant learning about the world by interacting with it. The caveat is that unfortunately, this concept is currently fairly impractical, since it requires an interactive environment where an agent can perform actions, something we can’t easily have short of deploying a robot with sensors. Moreover, the proposed algorithm seems to assume that each state $s$ is sampled independently for each update, whereas a robot would observe a dependent stream of states. Accordingly, the experiments in this short paper are mostly “proof of concept”, on simplistic synthetic environments. Yet they do a good job at illustrating the idea. To me this means that there’s more interesting work worth doing in what seems to be a promising direction!", "pdf_url": "http://arxiv.org/pdf/1703.07718", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/bengiotppb17.json"}
{"id": "97703387", "bin": "900_1000", "summary_sentences": ["Firefly Algorithms for Multimodal Optimization – Xin-She Yang, 2010  This is the third post in a mini-series on nature-inspired optimisation algorithms.", "The flashing light of fireflies is an amazing sight in the summer sky in the tropical and temperate regions.", "There are about two thousand firefly species, and most fireflies produce short and rhythmic flashes.", "The pattern of flashes is often unique for a particular species.", "The flashing light is produced by a process of bioluminescence, and the true functions of such signaling systems are still being debated.", "However, two fundamental functions of such flashes are to attract mating partners (communication), and to attract potential prey.", "In addition, flashing may also serve as a protective warning mechanism.", "The rhythmic flash, the rate of flashing and the amount of time form part of the signal system that brings both sexes together.", "Females respond to a male’s unique pattern of flashing in the same species, while in some species such as photuris, female fireflies can mimic the mating flashing pattern of other species so as to lure and eat the male fireflies who may mistake the flashes as a potential suitable mate.", "The flashing lights of fireflies inspired the Firefly Algorithm.", "Start with an initial population of fireflies spread over the solution space, and let the brightness of an individual firefly be affected or determined by the landscape of the objective function (the one we are trying to optimise).", "Fireflies are attracted to (move towards) brighter fireflies in each iteration.", "At the end of the game (e.g. a specified number of intervals), the brightest firefly is the winner!", "There are three simplifying rules for our artificial fireflies:  All fireflies are unisex, one firefly is attracted to another regardless of sex  Attractiveness is proportional to brightness.", "Both attractiveness and brightness decrease as the distance between two fireflies increases  The brightness is determined by the objective function.", "“For a maximization problem, the brightness can simply be proportional to the value of the objective function.", "Other forms of brightness can be defined in a similar way to the fitness function in genetic algorithms.”  There are two key issues in modelling the firefly algorithm: the variation of light intensity, and the formulation of attractiveness.", "Attractiveness is determined by brightness, but as we also know, ‘beauty is in the eye of the beholder.’ More precisely, attractiveness β varies with the distance rij between fireflies i and j.", "Light intensity decreases with distance from its source too, and light is absorbed in the media, so attractiveness varies with the degree of absorption.", "Light intensity follows an inverse square law (1/r2), and absoption depends on an absorption coefficient γ and the distance r proportional to e-γr.", "We can combine these effects in a Guassian approximation:  I( r ) = I0e-γr2  where I0 is the intensity at the origin.", "Since attractiveness β is proportional to light intensity we can therefore use:  β( r ) = β0e-γr2  And to avoid calculating an exponential function, we can replace this with:  β( r ) = β0/(1 + γr2)  The distance between fireflies is just the Cartesian Distance, which we can extend into multiple dimensions as needed.", "Putting all this together with a randomisation parameter α gives us the equation for moving fireflies on each iteration, firefly i moves towards a brighter firefly j according to:  xi = xi + β0e-γr2ij(xj – xi) + α(rand – 1/2)  It is worth pointing out that the distance r defined above is not limited to the Euclidean distance.", "We can define many other forms of distance r in the n-dimensional hyperspace, depending on the type of problem of our interest.", "For example, for job scheduling problems, r can be defined as the time lag or time interval.", "For complicated networks such as the Internet and social networks, the distance r can be defined as the combination of the degree of local clustering and the average proximity of vertices.", "In fact, any measure that can effectively characterize the quantities of interest in the optimization problem can be used as the ‘distance’ r.  When γ → 0 we have the situation where light intensity does not decrease with distance in an idealized sky, and this corresponds to Particle Swarm Optimisation .", "When γ → ∞ we have ‘very shortsighted fireflies’ !", "This corresponds to the completely random search method.", "As the firefly algorithm is usually in somewhere between these two extremes, it is possible to adjust the parameter γ and α so that it can outperform both the random search and PSO.", "In fact, FA can find the global optima as well as all the local optima simultaneously in a very effective manner….", "Our simulation results for finding the global optima of various test functions suggest that particle swarm often outperforms traditional algorithms such as genetic algorithms, while the new firefly algorithm is superior to both PSO and GA in terms of both efficiency and success rate.", "This implies that FA is potentially more powerful in solving NP-hard problems which will be investigated further in future studies."], "summary_text": "Firefly Algorithms for Multimodal Optimization – Xin-She Yang, 2010  This is the third post in a mini-series on nature-inspired optimisation algorithms. The flashing light of fireflies is an amazing sight in the summer sky in the tropical and temperate regions. There are about two thousand firefly species, and most fireflies produce short and rhythmic flashes. The pattern of flashes is often unique for a particular species. The flashing light is produced by a process of bioluminescence, and the true functions of such signaling systems are still being debated. However, two fundamental functions of such flashes are to attract mating partners (communication), and to attract potential prey. In addition, flashing may also serve as a protective warning mechanism. The rhythmic flash, the rate of flashing and the amount of time form part of the signal system that brings both sexes together. Females respond to a male’s unique pattern of flashing in the same species, while in some species such as photuris, female fireflies can mimic the mating flashing pattern of other species so as to lure and eat the male fireflies who may mistake the flashes as a potential suitable mate. The flashing lights of fireflies inspired the Firefly Algorithm. Start with an initial population of fireflies spread over the solution space, and let the brightness of an individual firefly be affected or determined by the landscape of the objective function (the one we are trying to optimise). Fireflies are attracted to (move towards) brighter fireflies in each iteration. At the end of the game (e.g. a specified number of intervals), the brightest firefly is the winner! There are three simplifying rules for our artificial fireflies:  All fireflies are unisex, one firefly is attracted to another regardless of sex  Attractiveness is proportional to brightness. Both attractiveness and brightness decrease as the distance between two fireflies increases  The brightness is determined by the objective function. “For a maximization problem, the brightness can simply be proportional to the value of the objective function. Other forms of brightness can be defined in a similar way to the fitness function in genetic algorithms.”  There are two key issues in modelling the firefly algorithm: the variation of light intensity, and the formulation of attractiveness. Attractiveness is determined by brightness, but as we also know, ‘beauty is in the eye of the beholder.’ More precisely, attractiveness β varies with the distance rij between fireflies i and j. Light intensity decreases with distance from its source too, and light is absorbed in the media, so attractiveness varies with the degree of absorption. Light intensity follows an inverse square law (1/r2), and absoption depends on an absorption coefficient γ and the distance r proportional to e-γr. We can combine these effects in a Guassian approximation:  I( r ) = I0e-γr2  where I0 is the intensity at the origin. Since attractiveness β is proportional to light intensity we can therefore use:  β( r ) = β0e-γr2  And to avoid calculating an exponential function, we can replace this with:  β( r ) = β0/(1 + γr2)  The distance between fireflies is just the Cartesian Distance, which we can extend into multiple dimensions as needed. Putting all this together with a randomisation parameter α gives us the equation for moving fireflies on each iteration, firefly i moves towards a brighter firefly j according to:  xi = xi + β0e-γr2ij(xj – xi) + α(rand – 1/2)  It is worth pointing out that the distance r defined above is not limited to the Euclidean distance. We can define many other forms of distance r in the n-dimensional hyperspace, depending on the type of problem of our interest. For example, for job scheduling problems, r can be defined as the time lag or time interval. For complicated networks such as the Internet and social networks, the distance r can be defined as the combination of the degree of local clustering and the average proximity of vertices. In fact, any measure that can effectively characterize the quantities of interest in the optimization problem can be used as the ‘distance’ r.  When γ → 0 we have the situation where light intensity does not decrease with distance in an idealized sky, and this corresponds to Particle Swarm Optimisation . When γ → ∞ we have ‘very shortsighted fireflies’ ! This corresponds to the completely random search method. As the firefly algorithm is usually in somewhere between these two extremes, it is possible to adjust the parameter γ and α so that it can outperform both the random search and PSO. In fact, FA can find the global optima as well as all the local optima simultaneously in a very effective manner…. Our simulation results for finding the global optima of various test functions suggest that particle swarm often outperforms traditional algorithms such as genetic algorithms, while the new firefly algorithm is superior to both PSO and GA in terms of both efficiency and success rate. This implies that FA is potentially more powerful in solving NP-hard problems which will be investigated further in future studies.", "pdf_url": "http://arxiv.org/pdf/1003.1466v1.pdf", "source_path": "/Users/sassss/Documents/sasmitha/dsa4213/group project/DSA4213/longsumm dataset/LongSumm-master-github/abstractive_summaries/by_clusters/900_1000/firefly-algorithms-for-multi-model-optimization.json"}
