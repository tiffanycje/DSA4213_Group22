3	71	There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy et al., 2014; Kiros et al., 2015; Socher et al., 2014; Donahue et al., 2015).
5	68	Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017).
11	8	Rajendran et al. (2016) propose a 2839 model to learn common representations between M views and assume there is parallel data available between a pivot view and the remaining M−1 views.
13	2	Related to our work Calixto et al. (2017) proposed a model for creating multilingual multimodal embeddings.
15	40	We also propose a single model for learning representations of images and multiple languages, whereas their model is language-specific.
16	73	In this paper, we learn multimodal representations in multiple languages, i.e., our model yields a joint space for images and text in multiple languages using the image as a pivot between languages.
18	66	We experiment with the Multi30k dataset, a multilingual extension of Flickr30k corpus (Young et al., 2014) consisting of English and German image descriptions (Elliott et al., 2016).
19	250	The Multi30K dataset has 29k, 1k and 1k images in the train, validation and test splits respectively, and contains two types of multilingual annotations: (i) a corpus of one English description per image and its translation into German; and (ii) a corpus of five independently collected English and German descriptions per image.
20	123	We use the independently collected English and German descriptions to train our models.
22	40	Given an image i and its descriptions c1 and c2 in two different languages our aim is to learn a model which maps i, c1 and c2 onto same common space RN (where N is the dimensionality of the embedding space) such that the image and its gold-standard descriptions in both languages are mapped close to each other (as shown in Figure 1).
23	78	Our model consists of the embedding functions fi and fc to encode images and descriptions and a scoring function S to compute the similarity between a description–image pair.
24	137	In the following we describe two models: (i) the PIVOT model that uses the image as pivot between the description in both the languages; (ii) the PARALLEL model that further forces the image descriptions in both languages to be closer to each other in the joint space.
25	14	We build two variants of PIVOT and PARALLEL with different similarity functions S to learn the joint space.
26	15	In both PIVOT and PARALLEL we use a deep convolutional neural network architecture (CNN) to represent the image i denoted by fi(i) = Wi · CNN(i) where Wi is a learned weight matrix and CNN(i) is the image vector representation.
27	7	For each language we define a recurrent neural network encoder fc(ck) = GRU(ck) with gated recurrent units (GRU) activations to encode the description ck.
