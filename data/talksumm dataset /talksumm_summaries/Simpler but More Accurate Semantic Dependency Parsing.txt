2	65	For example, in the sentence Mary wants to buy a book, the word Mary is the subject of both want and buy—either or both relationships could be useful in a downstream task, but a tree-structured representation of this sentence (as in Figure 1a) can only represent one of them.1 The 2014 SemEval shared task on BroadCoverage Semantic Dependency Parsing (Oepen et al., 2014) introduced three new dependency representations that do away with the assumption of strict tree structure in favor of a richer graphstructured representation, allowing them to capture more linguistic information about a sentence.
7	32	Finally, we briefly examine some of the design choices of that architecture, in order to assess which components are necessary for achieving the highest accuracy and which have little impact on final performance.
11	73	The SemEval semantic dependency schemes are also directed acyclic graphs (DAGs) rather than trees, allowing them to annotate function words as being heads without lengthening paths between content words (as in 1b).
25	74	We can formulate the semantic dependency parsing task as labeling each edge in a directed graph, with null being the label given to pairs with no edge between them.
26	59	Using only one module that labels each edge in this way would be an unfactorized approach.
27	41	We can, however, factorize it into two modules: one that predicts whether or not a directed edge (wj , wi) exists between two words, and another that predicts the best label for each potential edge.
29	20	As with many successful recent parsers, we concatenate word and POS tag2 embeddings, and feed them into a multilayer bidirectional LSTM to get contextualized word representations.3 xi = e (word) i ⊕ e (tag) i (1) R = BiLSTM(X) (2) For each of the two modules, we use single-layer feedforward networks (FNN) to split the top recurrent states into two parts—a head representation, as in Eq.
33	48	(3, 4)—which are generalizations of linear classifiers to include multiplicative interactions between two vectors—to predict edges and labels.4 Bilin(x1,x2) = x>1 Ux2 (3) Biaff(x1,x2) = x>1 Ux2 +W (x1 ⊕ x2) + b (4) h (edge-head) i = FNN (edge-head)(ri) (5) h (label-head) i = FNN (label-head)(ri) (6) h (edge-dep) i = FNN (edge-dep)(ri) (7) h (label-dep) i = FNN (label-dep)(ri) (8) s (edge) i,j = Biaff (edge) ( h (edge-dep) i ,h (edge-head) j ) (9) s (label) i,j = Biaff (label) ( h (label-dep) i ,h (label-head) j ) (10) y ′(edge) i,j = {si,j ≥ 0} (11) y ′(label) i,j = argmax si,j (12) The tensor U can optionally be diagonal (such that ui,k,j = 0 wherever i 6= j) to conserve parameters.
34	77	The unlabeled parser scores every edge between pairs of words in the sentence—these scores can be decoded into a graph by keeping only edges that received a positive score.
35	41	The labeler scores every label for each pair of words, so we simply assign each predicted edge its highest-scoring label and discard the rest.
53	24	LSTMs used same-mask recurrent dropout (Gal and Ghahramani, 2016).
54	22	The systems were trained with batch sizes of 3000 tokens for up to 75,000 training steps, terminating early after 10,000 steps pass with no improve- ment in validation accuracy.
56	11	We use biaffine classifiers, with no nonlinearities, and a diagonal tensor in the label classifier but not the edge classifier.
58	28	Du et al. (2015) and Almeida and Martins (2015) are the systems that won the 2015 shared task (closed track).
59	12	PTS17: Basic represents the single-task versions of Peng et al. (2017), which they make multitask across the three datasets in Freda3 by adding frustratingly easy domain adaptation (Daumé III, 2007; Kim et al., 2016) and a third-order decoding mechanism.
60	32	WCGL18 is Wang et al.’s (2018) transition-based system.
63	15	Simply adding in either a character-level word embedding model (similar to Dozat et al.’s (2017)) or a lemma embedding matrix likewise improves performance quite a bit, and including both together generally pushes performance even higher.
65	30	Surprisingly, the PAS dataset seems not to benefit substantially from lemma or character embeddings.
66	20	It has been noted that PAS is the easiest of the three datasets to achieve good performance for; so one possible explanation is that 94% LF1 may simply be near the ceiling of what can be achieved for the dataset.
69	79	Using a hinge loss (like Peng et al. (2017)) instead of a cross-entropy loss might help, since the system would stop focusing on potentially “easy” functional predicates once it learned to predict their argument structures confidently, allowing it to put more resources into modeling more challenging phenomena.
71	18	We train twenty models on the DM treebank for each variation we consider, reducing the number of training steps but keeping all other hyperparameters constant.
72	54	Rank-sum tests (Lehmann et al., 1975) reveal that the basic system outperforms variants with no hidden layers in the edge classifier (W=339; p<.001) or the label classifier (W=307; p<.01).
73	59	Using a diagonal tensor U in the unlabeled parser also significantly hurts performance (W=388; p<.001), likely being too underpowered.
74	89	While the other variations (especially the unfactorized and ReLU systems) appeared to make a difference during hyperparameter tuning, they were not significant here.
76	33	On the other hand, the choice between biaffine and bilinear classifiers comes down largely to aesthetics.
78	32	Unusually, using no nonlinearity in the hidden layers in Eqs.
80	28	Overall, the parser displayed considerable invariance to architecture changes.
82	37	We minimally extended a simple syntactic dependency parser to produce graph-structured dependencies.
84	53	Additionally, we can see that a multitask system relying on a complex decoding algorithm to prune away invalid graph structures isn’t necessary for achieving the level of parsing performance a simple system can achieve (though it could push performance even higher).
85	16	We also find easier or independently motivated ways to improve accuracy—taking advantage of provided lemma or subtoken information provides a boost comparable to one found by drastically increasing system complexity.
