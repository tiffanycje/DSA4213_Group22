16	25	Furthermore, for non-exponential family models, the Gauss-Newton and Fisher approaches are in general different.
17	46	– On three standard benchmarks we demonstrate that (without tuning) second-order methods perform competitively, even against well-tuned state-of-the-art first-order methods.
18	15	As a basis for our approximations to the Gauss-Newton matrix, we first describe how the diagonal Hessian blocks of feedforward networks can be recursively calculated.
22	9	For a training dataset with empirical distribution p(x, y), the total error function is then defined as the expected loss Ē(θ) = E [E]p(x,y).
23	24	For simplicity we denote by E(θ) the loss for a generic single datapoint (x, y).
26	33	The full Hessian, even of a moderately sized neural network, is computationally intractable due to the large number of parameters.
28	40	Each block corresponds to the second derivative with respect to the parameters Wλ of a single layer λ.
34	17	Then we can simply apply the recursion (8) and compute the pre-activation Hessian for each layer using a single backward pass through the network.
39	49	Since the second derivative f ′′ of a piecewise linear function is zero everywhere, the matrices Dλ in (8) will be zero (away from non-differentiable points).
40	53	It follows that if HL is Positive Semi-Definite (PSD), which is the case for the most commonly used loss functions, the pre-activation matrices are PSD for every layer.
41	24	A corollary is that if we fix all of the parameters of the network except for Wλ the objective function is locally convex with respect to Wλ wherever it is twice differentiable.
43	90	Note that this does not imply that the objective is convex everywhere with respect to Wλ as the surface will contain ridges along which it is not differentiable, corresponding to boundary points where the transfer function changes regimes, see Figure 1(c).
44	59	As the trace of the full HessianH is the sum of the traces of the diagonal blocks, it must be non-negative and thus it is not possible for all eigenvalues to be simultaneously negative.
50	26	A Newton update H−1g could therefore lead to an increase in the error.
57	80	Before embarking on this sequence of approximations, we first show that the GN matrix can be expressed as the expectation of a Khatri-Rao product, i.e. blocks of Kronecker products, corresponding to the weights of each layer.
58	21	We will subsequently approximate the expectation of the Kronecker products as the product of the expectations of the factors, making the blocks efficiently invertible.
60	37	Defining Gλ,β as the pre-activation GN matrix between the λ and β layers’ pre-activation vectors: Gλ,β = JhLhλ T HLJhLhβ (15) and using the fact that JhλWλ = a T λ−1 ⊗ I we obtain Ḡλ,β = E [( aλ−1a T β−1 ) ⊗ Gλ,β ] (16) We can therefore write the GN matrix as the expectation of the Khatri-Rao product: Ḡ = E [Q ?
61	46	G] (17) where the blocks of G are the pre-activation GN matrices Gλ,β as defined in (16), and the blocks of Q are: Qλ,β ≡ aλ−1aTβ−1 (18)
64	13	Computing this requires evaluating a block diagonal matrix for each datapoint and accumulating the result.
76	16	In principle, the recursion could be applied for every data point.
87	81	The method will be referred to as Kronecker Factored Recursive Approximation (KFRA).
107	8	We used only the first 50, 000 images for training (since the remaining 10, 000 are usually used for validation).
149	41	Training was run for 40, 000 updates for ADAM with a grid search as in Section 5.1, and for 5, 000 updates for the second-order methods.
151	78	For the CPU, both per iteration and wall clock time the second-order methods were faster than ADAM; on the GPU, however, ADAM was faster per wall clock time.
152	11	The value of the objective function at the final parameter values was higher for second-order methods than for ADAM.
153	14	However, it is important to keep in mind that all methods achieved a nearly perfect cross-entropy loss of around 10−8.
156	61	Interestingly, KFAC performed almost identically to KFLR, despite the fact that KFLR computes the exact preactivation Gauss-Newton matrix.
159	12	As we show in Appendix D the Monte Carlo Gauss-Newton matrix rank is upper bounded by the rank of the last layer Hessian times the size of the minibatch.
164	12	We can draw a parallel between the curvature being zero and standard techniques where the maximum likelihood problem is under-determined for small data sets.
166	26	Our results suggest that, whilst in practice the Gauss-Newton matrix provides curvature only in a limited parameter subspace, this still provides enough information to allow for relatively large parameter updates compared to gradient descent, see Figure 2.
