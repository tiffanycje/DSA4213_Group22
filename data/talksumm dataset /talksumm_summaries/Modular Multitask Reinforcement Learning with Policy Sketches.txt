0	39	This paper describes a framework for learning composable deep subpolicies in a multitask setting, guided only by abstract sketches of high-level behavior.
1	54	General reinforcement learning algorithms allow agents to solve tasks in complex environments.
2	106	But tasks featuring extremely delayed rewards or other long-term structure are often difficult to solve with flat, monolithic policies, and a long line of prior work has studied methods for learning hierarchical policy representations (Sutton et al., 1999; Dietterich, 2000; Konidaris & Barto, 2007; Hauser et al., 2008).
5	28	But is such fine-grained supervision actually necessary to achieve the full benefits of hierarchy?
7	17	Or is it sufficient to simply inform the learner about the abstract structure of policies, without ever specifying how high-level behaviors should make use of primitive percepts or actions?
8	52	To answer these questions, we explore a multitask reinforcement learning setting where the learner is pre- sented with policy sketches.
9	46	Policy sketches are short, ungrounded, symbolic representations of a task that describe its component parts, as illustrated in Figure 1.
10	173	While symbols might be shared across tasks (get wood appears in sketches for both the make planks and make sticks tasks), the learner is told nothing about what these symbols mean, in terms of either observations or intermediate rewards.
11	18	We present an agent architecture that learns from policy sketches by associating each high-level action with a parameterization of a low-level subpolicy, and jointly optimizes over concatenated task-specific policies by tying parameters across shared subpolicies.
12	32	We find that this architecture can use the high-level guidance provided by sketches, without any grounding or concrete definition, to dramatically accelerate learning of complex multi-stage behaviors.
13	275	Our experiments indicate that many of the benefits to learning that come from highly detailed low-level supervision (e.g. from subgoal rewards) can also be obtained from fairly coarse high-level supervision (i.e. from policy sketches).
14	22	Crucially, sketches are much easier to produce: they require no modifications to the environment dynamics or reward function, and can be easily provided by nonexperts.
15	21	This makes it possible to extend the benefits of hierarchical RL to challenging environments where it may not be possible to specify by hand the details of relevant subtasks.
16	32	We show that our approach substantially outperforms purely unsupervised methods that do not provide the learner with any task-specific guidance about how hierarchies should be deployed, and further that the specific use of sketches to parameterize modular subpolicies makes better use of sketches than conditioning on them directly.
20	23	Our contributions are: • A general paradigm for multitask, hierarchical, deep reinforcement learning guided by abstract sketches of task-specific policies.
21	39	• A concrete recipe for learning from these sketches, built on a general family of modular deep policy representations and a multitask actor–critic training objective.
23	246	This makes it possible to evaluate our approach under a variety of different data conditions: (1) learning the full collection of tasks jointly via reinforcement, (2) in a zero-shot setting where a policy sketch is available for a held-out task, and (3) in a adaptation setting, where sketches are hidden and the agent must learn to adapt a pretrained policy to reuse high-level actions in a new task.
24	11	In all cases, our approach substantially outperforms previous approaches based on explicit decomposition of the Q function along subtasks (Parr & Russell, 1998; Vogel & Jurafsky, 2010), unsupervised option discovery (Bacon & Precup, 2015), and several standard policy gradient baselines.
25	62	We consider three families of tasks: a 2-D Minecraftinspired crafting game (Figure 3a), in which the agent must acquire particular resources by finding raw ingredients, combining them together in the proper order, and in some cases building intermediate tools that enable the agent to alter the environment itself; a 2-D maze navigation task that requires the agent to collect keys and open doors, and a 3-D locomotion task (Figure 3b) in which a quadrupedal robot must actuate its joints to traverse a narrow winding cliff.
27	38	For the most challenging tasks, involving sequences of four or five high-level actions, a taskspecific agent initially following a random policy essentially never discovers the reward signal, so these tasks cannot be solved without considering their hierarchical structure.
54	14	We consider a multitask reinforcement learning problem arising from a family of infinite-horizon discounted Markov decision processes in a shared environment.
60	11	For a fixed sequence {(si, ai)} of states and actions obtained from a rollout of a given policy, we will denote the empirical return starting in state si as qi := P1 j=i+1 j i 1R(sj).
68	23	At a high level, this framework is agnostic to the implementation of subpolicies: any function that takes a representation of the current state onto a distribution over A+ will do.
69	135	In this paper, we focus on the case where each ⇡b is represented as a neural network.1 These subpolicies may be viewed as options of the kind described by Sutton et al. (1999), with the key distinction that they have no initiation semantics, but are instead invokable everywhere, and have no explicit representation as a function from an initial state to a distribution over final states (instead implicitly using the STOP action to terminate).
70	55	Given a fixed sketch (b1, b2, .
71	23	), a task-specific policy ⇧⌧ is formed by concatenating its associated subpolicies in sequence.
72	23	In particular, the high-level policy maintains a subpolicy index i (initially 0), and executes actions from ⇡bi until the STOP symbol is emitted, at which point control is passed to ⇡bi+1 .
73	262	We may thus think of ⇧⌧ as inducing a Markov chain over the state space S ⇥ B, with transitions: (s, bi)!
77	9	As described in Section 4.2, it is also possible to share parameters between subpolicies, and introduce discrete subtask structure by way of an embedding of each symbol b. Algorithm 2 TRAIN-LOOP() 1: // initialize subpolicies randomly 2: ⇧ = INIT() 3: `max 1 4: loop 5: rmin 1 6: // initialize ` max -step curriculum uniformly 7: T 0 = {⌧ 2 T : |K⌧ |  `max} 8: curriculum(·) = Unif(T 0) 9: while rmin < rgood do 10: // update parameters (Algorithm 1) 11: TRAIN-STEP(⇧, curriculum) 12: curriculum(⌧) / 1[⌧ 2 T 0](1 ˆEr⌧ ) 8⌧ 2 T 13: rmin min⌧2T 0 ˆEr⌧ 14: `max `max + 1 over all ✓b to maximize expected discounted reward J(⇧) := X ⌧ J(⇧⌧ ) := X ⌧ Esi⇠⇧⌧ ⇥X i iR⌧ (si) ⇤ across all tasks ⌧ 2 T .
79	9	In a standard policy gradient approach, with a single policy ⇡ with parameters ✓, we compute gradient steps of the form (Williams, 1992): r✓J(⇡) = X i r✓ log ⇡(ai|si) qi c(si) , (1) where the baseline or “critic” c can be chosen independently of the future without introducing bias into the gradient.
